{
    "subject-biotechnology": {
        "10.1371/journal.pone.0070686": {
            "author_display": [
                "Yonggang Yang",
                "Meiying Xu",
                "Zhili He",
                "Jun Guo",
                "Guoping Sun",
                "Jizhong Zhou"
            ],
            "title_display": "Microbial Electricity Generation Enhances Decabromodiphenyl Ether (BDE-209) Degradation",
            "abstract": [
                "\nDue to environmental persistence and biotoxicity of polybrominated diphenyl ethers (PBDEs), it is urgent to develop potential technologies to remediate PBDEs. Introducing electrodes for microbial electricity generation to stimulate the anaerobic degradation of organic pollutants is highly promising for bioremediation. However, it is still not clear whether the degradation of PBDEs could be promoted by this strategy. In this study, we hypothesized that the degradation of PBDEs (e.g., BDE-209) would be enhanced under microbial electricity generation condition. The functional compositions and structures of microbial communities in closed-circuit microbial fuel cell (c-MFC) and open-circuit microbial fuel cell (o-MFC) systems for BDE-209 degradation were detected by a comprehensive functional gene array, GeoChip 4.0, and linked with PBDE degradations. The results indicated that distinctly different microbial community structures were formed between c-MFCs and o-MFCs, and that lower concentrations of BDE-209 and the resulting lower brominated PBDE products were detected in c-MFCs after 70-day performance. The diversity and abundance of a variety of functional genes in c-MFCs were significantly higher than those in o-MFCs. Most genes involved in chlorinated solvent reductive dechlorination, hydroxylation, methoxylation and aromatic hydrocarbon degradation were highly enriched in c-MFCs and significantly positively correlated with the removal of PBDEs. Various other microbial functional genes for carbon, nitrogen, phosphorus and sulfur cycling, as well as energy transformation process, were also significantly increased in c-MFCs. Together, these results suggest that PBDE degradation could be enhanced by introducing the electrodes for microbial electricity generation and by specifically stimulating microbial functional genes.\n"
            ],
            "publication_date": "2013-08-05T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 1454,
            "shares": 0,
            "bookmarks": 8,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0070686",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0070686&representation=PDF",
            "fulltext": "IntroductionPolybrominated diphenyl ethers (PBDEs) are a class of flame retardants that have caused considerable concern in recent years due to their elevated levels of detection in the environment, particularly human tissues [1], [2]. Penta-BDE, octa-BDE and deca-BDE are three major commercial PBDE mixtures. The former two products have been included on the Stockholm convention list of priority persistent organic pollutants and banned from the European market since 2003 and effectively eliminated in North America since 2005 [3]. However, deca-BDE, which is mainly composed of deca-BDE (BDE-209) and a small amount of nona-BDEs, accounts for approximately 83% of the worldwide use of PBDEs, and is still legally used in most countries. As a result, BDE-209 was detected as the substantially dominant PBDE congeners in sediment environments [4]. Recent studies demonstrated that BDE-209 could be anaerobically debrominated by microorganisms [5], [6], [7]. However, the degradation of these highly persistent halogenated compounds is very slow and always produces toxic lower brominated PBDE congeners under anaerobic conditions [8].\nIt has been reported that the amendments of co-substrates [7], primer compounds [5] or electron donors [9] could stimulate PBDE biodebromination. However, the main problem for PBDE biodegradation is the low biomass and activity of the functional microbes. Researchers have found that the lack of suitable electron acceptors was the main limitation for organic pollutant degradation under anaerobic condition and the supplementary of electron acceptors could even avoid the production of toxic daughter products by providing necessary electron flow to stimulate the viabilities of indigenous microorganisms [10]. Under aerobic conditions, oxygen is the preferential electron acceptor. However, under anaerobic conditions, electron acceptors are usually limited and the microbial metabolic activities are depressed. In order to stimulate indigenous microbial growth and activity/viabilities, alternative terminal electron acceptors to oxygen, such as nitrate, sulfate and ferric iron were employed in anaerobic bioremediation processes [10], [11]. With the development of microbial fuel cells (MFCs), electrodes were introduced as the solid terminal electron acceptor to stimulate anaerobic biodegradation [12], [13]. In the anode compartment of bioelectrochemical system (BES), microorganisms can respire with anode by converting the energy stored in chemical bonds in organic compounds to generate electricity. Such an electron transfer is highly promising for bioremediation of organic pollutants, such as petroleum hydrocarbons, wastewaters, and contaminated soils and sediments. More and more studies have demonstrated that electrodes can served as electron acceptors for stimulating the anaerobic degradation of organic pollutants in contaminated environments, including petroleum hydrocarbons [13], [14] and chlorinated compounds [15].\nSince the structures between polychlorinated biphenyls (PCBs) and PBDEs are very similar, it is likely that some dechlorinating enzymes may be involved in PBDE transformation [16], [17]. However, little is known about the microbial transformation of PBDEs, especially enzymes necessary for PBDE biodegradation. Our previous studies found that the structure and composition of PBDE-degrading microbial communities could be affected by the environmental parameters, and that most commonly genera involved in PBDE degradation were anodic respiring bacteria, such as Geobacter, Shewenalla, and Pseudomonas [9], [18]. It is necessary to identify key functional genes and their associated populations involved in PBDE degradation so that an effective bioremediation strategy can be developed.\nIn this study, we hypothesized that the introduction of extra electron acceptor through the external circuit would stimulate the diversity and abundance of microbial communities and accelerate PBDE biodegradation. To test those hypotheses, two different operating modes were conducted in dual-chamber MFC systems, one operated under closed-circuit condition (c-MFC) and the other under open-circuit condition (o-MFC) as controls were designed for PBDE degradation. A functional gene array, GeoChip 4.0, which contains 120,054 distinct probes, covering 200,393 coding sequences (CDS) for genes in different processes, was used to examine the impacts of concomitant electricity production on the functional microbial community composition and structure, and the linkages between microbial community structures and PBDE degradation rates. Our results indicated that microbial electricity generation dramatically increased microbial community functional gene diversity and abundance, especially those genes involved in chlorinated solvent remediation and aromatic hydrocarbon degradation, which were correlated with BDE-209 degradation.\nMaterials and Methods\nReactor Setup and Operational Conditions\nTwo differently operating modes (closed-circuit and open-circuit) were conducted in dual-chamber MFC systems which were assembled as previously described [19]. The anodic compartments contained 200 ml of modified M9 medium supplemented with 20 mM lactate (electron donor) and 1.4 µM BDE-209. Prior to the addition of defined medium, BDE-209 resolved in dichloromethane was added to each anodic compartment and evaporated in the dark. The cathode compartments were filled with 200 ml of phosphate buffered saline (PBS) solution containing 50 mM ferricyanide. The BDE-209-degrading enrichment [9] was inoculated to anodic chambers to start the c-MFC and o-MFC operation. All of the MFC systems were incubated at 30°C in the dark without agitation, after the anode compartments were purged with pure nitrogen gas for 5–10 min. Voltage output of the c-MFC under an 800 ohm external resistor was recorded every 4 min using a multimeter (UT71D, Uni-Trend Technology). Lactate was re-supplemented to the systems when the voltage in c-MFC decreased to its lowest value. Anodic cultures in these two kinds of MFC systems were periodically sampled and used for subsequent analyses. The concentrations of unbounded bromine ion in anodic culture were determined using an ion chromatography system (ICS1500, Ionpac AS19/AG19, Dionex) and a gradient KOH eluent (10 mM for 0–10 min, 10–45 mM for 10–25 min, 45 mM for 25–30 min, 10 mM for 30–37 min). After 70-day performance, BDE209-degrading rates and the PBDE congener profiles were analyzed as previously described [9]. In brief, the whole anodic chambers including the culture liquids and anodes were extracted with dichloromethane for 3 times. The resulting extracts were evaporated in a rotary evaporator and re-dissolved with n-hexane. PBDE congeners dissolved in n-hexane were then analyzed by a Shimadzu Model 2010 gas chromatograph (GC) coupled with a Model QP200 mass spectrometer (MS). Standard curves for the quantitative determination of PBDEs were prepared using a gas chromatograph-electron capture detection (GC-ECD).\nSample collections and the subsequent treatments were conducted in triplicate.\n\n\nDNA Preparation and GeoChip 4.0 Analysis\nDNA was extracted using the TIANamp Bacteria DNA Kit (TIANGEN BIOTECH (BEIJING) CO., LTD.) according to the manufacturer’s instructions. DNA amplification and labeling, as well as the purification of labeled DNA, were carried out according to the methods described by Xu et al. [20]. The GeoChip 4.0 synthesized by NimbleGen (Madison, WI, USA) was used to analyze the functional structure of the microbial communities. All hybridizations were carried out at 42°C with 40% formamide for 16 h on a MAUI hybridization station (BioMicro, Salt Lake City, UT, USA). After hybridization, the arrays were scanned with a NimbleGen MS200 microarray scanner (Roche NimbleGen, Madison, 113 WI, USA) at a laser power of 100% and 100% PMT (photomultiplier tube). Signal intensities were measured based on scanned images, and spots with signal-to-noise ratios lower than 2 were removed before statistical analysis as described previously [21].\n\n\nStatistical Analysis\nPre-processed GeoChip data were further analyzed with different statistical methods: (i) cluster analysis was performed using the pairwise average-linkage hierarchical clustering algorithm in the CLUSTER software (http://rana.lbl.gov/EisenSoftware.htm) for microbial community structure and composition, and the results of hierarchical clustering were visualized using TREEVIEW software (http://rana.lbl.gov/EisenSoftware.htm); (ii) principal component analysis (PCA) performed by CANOCO 4.5 for Windows (Biometris – Plant Research International, The Netherlands), and three different non-parametric analyses performed with the Vegan package (v.1.17–12) in R v. 2.13.1 (R Development Core Team, 2011): analysis of similarities (ANOSIM), non-parametric multivariate analysis of variance (Adonis) and Multi-Response Permutation Procedure (MRPP), were conducted to determine the overall functional gene changes in the microbial communities; (iii) microbial diversity index analyzed by Krebs/win version 0.94 (http://www.biology.ualberta.ca/jbrzusto/​krebswin.html), Significant Pearson’s linear correlation (r) analysis and analyses of variance (ANOVA) analyzed by SPSS 16.0 for Windows and response ratio (RR) (Data S1 in File S1) using the formula described by Luo et al. [22] for identifying the genes with significant change; (iv) canonical correspondence analysis (CCA) for revealing the individual or set of environmental variables that significantly explained the variation in functional microbial communities [23], [24]; and (v) partial CCA were calculated using functional gene communities and PBDE congener compositions and hydrochemical parameters for each treatment as covariables. All the analyses were performed by functions in CANOCO 4.5 for Windows (Biometris – Plant Research International, The Netherlands).\n\nResults\nBioreactor Performance\nThe voltage of c-MFC under the external resistor increased to 0.41 V after 12 days performance, and was subsequently decreased with the consumption of electron donor. Supplement of lactate could immediately recover current generation. The maximum voltage increased from 0.41 V to 0.46 V in the first two current generation periods and then showed a slight decrease in the following periods (Figure S1A in File S1). Polarization analysis showed a maximum power density of 3.3 W/m3 (0.28 W/m2) and an internal resistance of 325 ohm for the c-MFCs (Figure S1B in File S1). After 45-day performance, the concentrations of unbounded bromine ions in c-MFCs and o-MFCs were 385.0±30.8 µg/L and 104.4±8.4 µg/L (Figure 1A), respectively. On day 70, significantly (p<0.01) different BDE-209 degradation rate were detected with around 58.9% in c-MFCs and 23.2% in o-MFCs. Obviously different PBDE congener profiles were observed between c-MFCs and o-MFCs. The concentrations of three nona-BDE congeners, BDE-208, -207, -206, were commonly found in both systems with a significant decrease in c-MFCs (Figure 1B), while the lower brominated PBDE congeners, BDE-153 and BDE-183 were only detected in o-MFCs. The concentrations of BDE-209 and the less brominated PBDE products were much lower in c-MFCs, suggesting that the PBDE removal could be markedly improved under microbial current generation condition.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  PBDE congener products of BDE-209 after 70 days performance by c-MFCs and o-MFCs.doi:10.1371/journal.pone.0070686.g001\n\nEffects of Microbial Electricity Generation on the Overall Functional Structure of Anode Microbial Communities\nTo assess the effects of microbial electricity generation on the overall functional structure of anode microbial communities, the anode microbial community functional compositions and structures on day 70 were analyzed using GeoChips 4.0. Among a total of 9111 genes detected, 5646.5±9.2 were from c-MFC systems and only 638.3±56.9 were from o-MFC systems with 87.9% unique to c-MFC samples. For the shared genes, around 12.6% was significantly (p<0.01) increased and only 0.7% was significantly (p<0.01) decreased in c-MFC systems. The overall microbial functional diversity was significantly (p<0.02) higher in c-MFC based on the Shannon-Weiner (H’) index. The functional gene numbers and abundances of all functional gene categories were also significantly (p<0.01) increased in c-MFCs by computing the response ratio (RR). The total number and abundance of genes detected in c-MFCs were 9.7 and 8.5 times higher than those in o-MFCs, respectively (Table S1 in File S1).\nHierarchical clustering analysis showed that the c-MFC samples were clustered together and well separated from the o-MFC samples and six major gene groups could be visualized (Figure 2A). Within these six groups, 82.7% of detected genes belonged to Group 6, following with Group 4 (7.3%), Group 5 (5.0%), Group 2 (2.8%), Group 1 (1.5%) and Group 3 (0.7%) (Figure 2B). In Group 6, more than 92.3% genes detected were contributed by the c-MFC samples and the total gene abundance from c-MFCs was 28.9 times higher than that from o-MFCs with significant (p<0.01) increases in different gene categories as the following trend: energy process (15.7)<metal resistance (26.3) <phosphorus (27.1)<nitrogen cycling (27.3)<stress (28.6) = organic remediation (28.6)<carbon cycling (33.6)<sulphur (59.8). Within the 550 species detected in Group 6, 381 species were unique to the c-MFC samples.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Cluster analysis of functional genes detected using GeoChip 4.0.The figure was generated using CLUSTER and visualized in TREEVIEW. Black indicates signal intensities below the threshold value and red indicates a positive hybridization signal. The color intensity indicates differences in signal intensity. The samples from c-MFCs and o-MFCs on day 70 were clearly separated in two groups. Six different gene patterns were observed and indicated by numbers in the tree (A), and also illustrated in the graphs (B). (c1, c2, c3 represented the three replications of c-MFCs and o-1, o-2, o-3 represented those from o-MFCs.).\ndoi:10.1371/journal.pone.0070686.g002The abundances of most genes involved in carbon (C), nitrogen (N), phosphorus (P) and sulfur (S) cycling showed significant increases in c-MFCs. Among the C cycling genes detected, more than 86.2% involved in carbon degradation and almost all of these genes showed significantly (p<0.05) higher abundances in c-MFCs than in o-MFCs (Figure S2 in File S1). Totally, 745 N cycling genes were detected and belonged to 15 gene families involved in assimilatory N reduction, dissimilatory N reduction to ammonium, N2 fixation, denitrification processes, and ammonification (Figure S3 in File S1). Among those 15 gene families detected, 12 of them had significantly (p<0.05) higher abundances in c-MFCs than in o-MFCs. Higher number and abundance of the genes involved in S cycling were detected in c-MFCs (Figure S4 in File S1). For the detected 113 dsrA/B genes encoding dissimilatory sulfite reductase, 98 were unique to c-MFCs and four were unique to o-MFCs. For the genes involved in P cycling, 86 genes were detected and more than 82.1% of them were unique to c-MFC systems. The total abundance of ppk genes encoding polyphosphate kinase in c-MFCs was significant (p<0.05) higher than that in o-MFCs (Figure S5 in File S1). Details for these genes and their changes in c-MFC and o-MFC systems are described in the Data S2 in File S1.\nAll detected genes were derived from 574 genera with around 51.2% unique to c-MFCs and only 1.4% unique to o-MFCs. For the total signal intensities of the commonly detected genera, 11 were significantly increased at the p<0.01 level, and 72 were significantly increased at the p<0.05 level in c-MFCs. The total signal intensity of most well known versatile anodic respiring genera, Geobacter spp., Shewanella spp., Pseudomonas spp., and Desulfovibrio spp., were significantly enriched in c-MFCs at the p<0.01 or p<0.05 level, while no significant difference in the total signal intensity was observed for known dehalogenating microorganisms, such as Dehalococcoides spp. [25] and Desulfuromonas spp. [26].\n\n\nStimulations of the Functional Genes Involved in Aromatic Hydrocarbon Degradation and Chlorinated Solvent Remediation\nSome previous studies have shown that dioxygenases for catalyzing the oxidation of aromatic compounds were responsible for PBDE transformation [16], [17]. Consistently, GeoChip results showed that more than 75.5% of the organic remediation genes detected were involved in aromatic degradation and the aromatic degradation gene numbers detected in c-MFCs was around 10.5 times of those in o-MFCs. The abundances of relative genes for polycyclic aromatics, aromatic carboxylic acid, chlorinated aromatics, nitroaromatics and other aromatics showed significant (p<0.05) increases in c-MFCs (Figure 3A), and the genes for heterocyclic aromatics degradation were only detected in c-MFCs. Among the 45 aromatic degradation genes with significant changes in RR (Table S2 in File S1), only two of them, pimF from Bordetella petrii (163261940) for aromatic carboxylic acid degradation and nahA from Caulobacter sp. K31 (167645881) for polycyclic aromatics degradation, showed decreases under electricity generation conditions. The significant stimulation of the aromatic degradation genes suggests that these genes may be involved in PBDE transformation, and that the aromatic hydrocarbons transferred from BDE-209 could be further degraded under current generation condition.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  The normalized signal intensity of the detected key gens families involved in aromatic compound degradation (A) and chlorinated solvent remediation (B) under both c-MFCs and o-MFCs.The signal intensities were the sum of detected individual gene sequences for each functional gene, averaged among three samples. All data are presented as mean±SE. *** p<0.001, **p<0.05, *p<0.10.\ndoi:10.1371/journal.pone.0070686.g003Furthermore, GeoChip detected 52 chlorinated solvent remediation genes from a variety of microorganisms with 48 unique to c-MFCs, 2 unique to o-MFCs and one shared by both. The reductive dehologenase gene (rd) from Dehalococcoides ethenogenes 195 (57233751) was shared by both. The two unique genes to o-MFCs were the pyrroloquinoline quinone (PQQ)-dependent ethanol dehydrogenase gene (exaA) from Gluconacetobacter diazotrophicus PAl 5 (162146364) and the reductive dehologenase gene (rd) from Dehalococcoides sp. VS (163811600). The 48 unique chlorinated solvent remediation genes detected in c-MFCs belonged to five gene families, including cmuA for methyltransferase, dehH for hydrolase, dehH109 for haloacid dehalogenase, exaA for pyrroloquinoline quinone (PQQ)-dependent ethanol dehydrogenase and rd for reductive dehalogenase, and the highest gene abundance was observed for exaA (Figure 3B). The stimulation of these gene families indicated that not only reductive dehologenation genes/pathways, but also hydroxylation and methoxylation genes/pathways may be involved in the BDE-209 degradation.\n\n\nShifts of the Functional Genes Involved in Energy Process\nAmong the genes involved in energy process detected, the relative abundances of cytochrome genes and hydrogenase genes showed significant (p<0.05) increase in c-MFCs (Figure 4A). Among 106 genes involved in energy process, 91 genes were unique to the c-MFC samples and only three hydrogenase genes from Desulfovibrio vulgaris RCH1 (241879224, 241880985) and Anaeromyxobacter sp. K (197124673) were unique to o-MFCs. Seven P450 genes detected were all unique to c-MFC samples, and one with the highest signal intensity was from Burkholderia sp. 383 (78062539). Based on response ratio, cytochrome c class I genes from Geobacter sp. M21 (191162597) and Pseudomonas putida GB-1 (167034533) were significantly (p<0.05) increased in c-MFCs.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  The normalized signal intensity of the detected key gens families (A) and the organisms (B) involved in energy transformation process under both c-MFCs and o-MFCs.The signal intensities were the sum of detected individual gene sequences for each functional gene, averaged among three samples. All data are presented as mean±SE. **p<0.05, *p<0.10.\ndoi:10.1371/journal.pone.0070686.g004All detected energy process genes belonged to 13 genera, among which seven genera were shared by both c-MFC and o-MFC samples, and six were unique to c-MFCs. Although no significant difference was observed from the known dehalogenating bacteria, Dehalococcoides spp., the three significant (p<0.05) enriched anodic respiring genera in c-MFCs (Figure 4B), Geobacter spp., Pseudomonas spp. and Shewanella spp., are all capable of aromatic degradation and dechlorination. These results suggest that the introduction of electrode may stimulate the versatile functional microbes for current generation and PBDE degradation.\n\n\nThe Composition of PBDE Congener as a Predominant Factor Shaping Microbial Community Functional Structure\nTo determine the correlations between the microbial community functional structure and the BDE-209 degradation, PCA was performed based on all detected functional genes, different functional gene categories and PBDE congeners, respectively. All of the samples from the c-MFCs and o-MFCs were well separated along PC1 when the functional genes detected were used as the species (Figure 5A). However, the opposite patterns were observed when the PBDE congeners were used as the variables (Figure 5B). These results suggested that the microbial functional gene compositions were significantly influenced by the microbial electricity generation and their abundances were increased with the degradation of PBDE congeners.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Principal-component analyses (PCA) of entire functional gene communities (A) and PBDE congeners (B) detected on day 70.Open circles represent samples collected from o-MFC systems and solid circles represent samples collected from c-MFC systems.\ndoi:10.1371/journal.pone.0070686.g005In order to determine the most significant factors shaping the microbial community structure, CCA combined with forward selection procedure and variance inflation factors was conducted to assess the relationships between microbial community structure, electricity generation, PBDE congener compositions and hydrochemical parameters in the systems. Consistent with PCA results, no matter the entire functional gene community or the gene categories for chlorinated solvent remediation or aromatic degradation were used as the species, the samples from c-MFCs and o-MFCs were well separated by the first axis which could explained more than 60% of the total variances. On the basis of variance in inflation factors, three variables were selected: electric voltage, the concentration of BDE-183 and BDE-206. The specified CCA model was significant (p = 0.018). Furthermore, these three factors could significantly (p = 0.032) explain the functional genes involved in chlorinated solvent remediation and aromatic degradation.\nTo separate the effects of electricity generation, PBDE congener composition and the hydrochemical parameters on microbial community structure after 70 days performance, a CCA-based variation partitioning analysis was conducted. The electric voltage, PBDE congener composition and hydrochemical parameters showed a significant (p = 0.041) correlation with the microbial functional gene structure. Electric voltage explained substantially more variation (49.41%, p = 0.001) than PBDE congener composition (27.02%, p = 0.028), whereas hydrochemical parameters independently explained 4.10% (p = 0.428) of the observed variation (Figure 6). About 19.40% of the functional community variation based on GeoChip data remained unexplained by the above selected variables. These results suggest that after 70 days performance the microbial electricity generation changed the microbial community structure and accelerated the PBDEs transformation, and the PBDE congeners conversely acted as the predominant factor for shaping the functional microbial community.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Variation partitioning based on CCA for all functional gene signal intensities.(A) General outline. (B) All functional genes. A CCA-based VIF was performed to identify the variables important to the microbial community structure. PBDE congener variables included BDE183, BDE-206 and BDE-208. Hydrochemical parameters included pH and the concentration of bromine ion.\ndoi:10.1371/journal.pone.0070686.g006\nThe contamination of sediments and soils with persistent hydrophobic organic pollutants in large areas requires the development of effective bioremediation strategies. As the highly persistent hydrophobic organic pollutants, BDE-209 has been frequently detected in sediment and soil samples. It has been evidenced that BDE-209 could be slowly debrominated under anaerobic conditions by microorganisms. The amendments of trichloroethene as co-substrate [7], 4-bromobenzoic acid, 2,6-dibromobiphenyl, tetrabromobisphenol A or hexabromocyclododecane as primer compound [5] could significantly accelerate PBDE debromination. However, these strategies are unsuitable for bioremediation due to the toxicity of those added compounds. Our previous studies found that the amount of carbon source in sediment was enough to support PBDE debromination and the addition of exogenous electron donors could change the microbial community structure, as well as the PBDE congener compositions, but no significant decrease in total PBDE congeners was obtained [9], [18].More and more studies have demonstrated that the lack of suitable electron acceptors was the most limiting factor for biodegradation of organic pollutants in anaerobic sedimentary environments [27], [28]. The addition of electron acceptors has been applied as the promising strategies to stimulate the abundances and activities of the indigenous microorganisms for in situ anaerobic bioremediation of contaminated aquifers [29]. To date, chemical components, such as Fe(III), nitrate and sulfate, have been widely used to accelerate anaerobic biodegradation rate. However, all of these components are soluble, very easy to diffuse away from the point of application, and inappropriate for bioremediation of persistent organic pollutants, such as PBDEs, which require a long-term treatment. Electrodes, which could be permanently placed at a specific point of application, offer a low-maintenance source electron acceptor for promoting anaerobic biodegradation of organic contaminants [13], [14]. In this study, our results clearly showed that the functional microbial gene abundances and PBDE transformations could be significantly enhanced with simultaneous electricity generation, suggesting that the bioremediation of the persistent hydrophobic organic pollutants in anaerobic sedimentary conditions could be accelerated when microbial electricity was generated. It is interesting that the composition and amount of PBDE congeners generated from BDE-209 degradation were much less than our previous studies [9]. Since the same analyzing methods were used in those studies, it is reasonable to presume that compounds other than PBDE congeners were generated in this study which means new microbial BDE-209 degradation pathways might be involved. Some other enzymatic reactions such as hydroxylation or methoxylation of PBDEs have been observed in animals, birds, fishes and algae [30]. It is possible that similar reactions can be catalyzed by microorganisms, as suggested by the functional gene results in this study. Further researches are needed to confirm this hypothesis.To date, very few PBDE-degrading microbial strains were isolated. Given the structural similarity between PCBs and PBDEs, some known dechlorinators have been used to degrade the PBDEs, such as Rhodococcus jostii RHA1 [17], Sphingomonas sp. PH-07 [16] and Dehalococcoides sp. [7]. Although the involvements of some known dechlorinators in PBDE transformation were evidenced, very little information about the enzymes necessary to achieve PBDE biodegradation has been reported. Until now, only the typical dioxygenases required for PCB degradation have been identified to be involved in transformation of PBDEs by using reverse transcription quantitative PCR and gene recombinant analyses in a pure culture system [16], [17]. However, more and more evidence indicates that it is possible that an entirely different enzyme or multiple enzymes are responsible for PBDE transformation [17].GeoChip, as a comprehensive functional gene array, has been proven to be an ideal tool for analyzing microbial communities and linking their structure with environmental factors as well as ecosystem functioning [20], [21], [31], [32], [33]. In this study, GeoChip 4.0 was used to detect the functional microbial community composition and structure for PBDE transformation in c-MFC and o-MFC systems. Our results clearly showed that most important functional genes, including those involved in chlorinated solvent remediation, aromatic degradation and energy transformation process, were significantly stimulated and BDE-209 degradation was dramatically enhanced with simultaneous electricity generation. The significant positive correlations were generally observed between a range of functional gene categories and the removal rates of PBDEs, indicating that PBDE degradation process is complex and a variety of functional genes could be involved in PBDE transformation. Interestingly, for functional genes involved in chlorinated solvent remediation, not only those genes encoding reductive dehalogenase, but also those encoding methyltransferase and dehydrogenase were significantly increased in c-MFCs. These results suggest that reductive dehologenation, hydroxylation and methoxylation may be all involved in the degradation of BDE-209. The total signal intensities of most important microbes, which are capable of not only dechlorination but also anodic respiration, significantly increased in c-MFCs, suggesting that the introduction of electrode provided the condition for microbial electricity generation and enhance PBDE degradation. It will be necessary to combined some open format metagenome technologies to identify the new members and pathways involved in PBDE degradation during microbial electricity generation, since GeoChip can only detect the genes contained in the array.In summary, our results indicate that PBDE degradation could be stimulated with the significant increases of a variety of microbial functional genes after 70 days performance with electrode as the electron acceptor to produce microbial electricity. Our results also imply that PBDE biodegradation is a complex process in which reductive debromination, hydroxylation, and methoxylation, as well as aromatic degradation, may be all involved simultaneously. These findings strongly support the core hypothesis that the degradation of persistent organic pollutants in the anaerobic environments could be stimulated by the introduction of electrodes for microbial electricity generation. However, to elucidate the detailed PBDE biodegradation pathways under microbial electricity generation condition, it is essential to launch an integrated and comprehensive monitoring program to track the dynamics and adaptive responses of microbial communities to PBDE contamination together with other physical and chemical analysis in terms of microbial electricity generation, PBDE transformation and composition of their products."
        },
        "10.1371/journal.pone.0047205": {
            "author_display": [
                "Shaohua Chen",
                "Chenglan Liu",
                "Chuyan Peng",
                "Hongmei Liu",
                "Meiying Hu",
                "Guohua Zhong"
            ],
            "title_display": "Biodegradation of Chlorpyrifos and Its Hydrolysis Product 3,5,6-Trichloro-2-Pyridinol by a New Fungal Strain <i>Cladosporium cladosporioides</i> Hu-01",
            "abstract": [
                "\n        Intensive use of chlorpyrifos has resulted in its ubiquitous presence as a contaminant in surface streams and soils. It is thus critically essential to develop bioremediation methods to degrade and eliminate this pollutant from environments. We present here that a new fungal strain Hu-01 with high chlorpyrifos-degradation activity was isolated and identified as Cladosporium cladosporioides based on the morphology and 5.8S rDNA gene analysis. Strain Hu-01 utilized 50 mg·L−1 of chlorpyrifos as the sole carbon of source, and tolerated high concentration of chlorpyrifos up to 500 mg·L−1. The optimum degradation conditions were determined to be 26.8°C and pH 6.5 based on the response surface methodology (RSM). Under these conditions, strain Hu-01 completely metabolized the supplemented chlorpyrifos (50 mg·L−1) within 5 d. During the biodegradation process, transient accumulation of 3,5,6-trichloro-2-pyridinol (TCP) was observed. However, this intermediate product did not accumulate in the medium and disappeared quickly. No persistent accumulative metabolite was detected by gas chromatopraphy-mass spectrometry (GC-MS) analysis at the end of experiment. Furthermore, degradation kinetics of chlorpyrifos and TCP followed the first-order model. Compared to the non-inoculated controls, the half-lives (t1/2) of chlorpyrifos and TCP significantly reduced by 688.0 and 986.9 h with the inoculum, respectively. The isolate harbors the metabolic pathway for the complete detoxification of chlorpyrifos and its hydrolysis product TCP, thus suggesting the fungus may be a promising candidate for bioremediation of chlorpyrifos-contaminated water, soil or crop.\n      "
            ],
            "publication_date": "2012-10-08T00:00:00Z",
            "article_type": "Research Article",
            "citations": 9,
            "views": 4521,
            "shares": 0,
            "bookmarks": 5,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0047205",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0047205&representation=PDF",
            "fulltext": "IntroductionSynthetic organophosphates (OPs) are the most frequently and widely used insecticides, accounting for an estimated 34% of world-wide insecticide sales [1], [2]. Most organophosphorus insecticides share a similar structure, containing three phosphoester linkages and are hence often termed phosphotriesters [3]. This class of pesticides have acute neurotoxicity attributing to their ability to suppress acetylcholinesterase (AchE) [4], and various clinical effects can occur due to their poisoning in human beings [5]–[9].\nChlorpyrifos, [O,O-diethyl O-(3,5,6-trichloro-2-pyridyl) phosphorothioate] is one of the most extensively used broad-spectrum OPs, and its phosphorus is linked to a sulfur with a double bond (P = S) (Fig. 1). It is used throughout the world to control a variety of chewing and sucking insect pests and mites on a range of economically important crops, including citrus fruit, bananas, vegetables, potatoes, coffee, cocoa, tea, cotton, wheat, rice, and so on [10]. It is also registered for use on lawns, ornamental plants, animals, domestic dwellings as well as commercial establishments [11].\nA consequence of the persistent usage and broad-spectrum applicability of chlorpyrifos is widespread contamination in natural environment, leading to serious damage to non-target organisms [12]–[14]. Previous reports revealed that a wide range of water and terrestrial ecosystems have been contaminated with chlorpyrifos [15]–[17]. Moreover, much evidence suggests that chlorpyrifos may affect the endocrine system, respiratory system, cardiovascular system, nervous system, immune system, as well as the reproductive system due to its high mammalian toxicity [18]–[27]. Additionally, chlorpyrifos can be converted to 3,5,6-trichloro-2-pyridinol (TCP) in natural environment, a persistent metabolite that is refractory to microbial degradation [28], [29]. It has been suggested that the accumulated TCP in liquid medium or soil, which has antimicrobial property, prevents the proliferation of microorganisms involving in degrading chlorpyrifos [30]. TCP is classified as persistent and mobile by the US Environmental Protection Agency (EPA) with a half life ranging from 65 to 360 d in soil, depending on the soil type, climate, and other conditions [30]. As the major degradation product of chlorpyrifos, TCP has greater water solubility than its parent molecule and causes widespread contamination of soils and aquatic environments [4], [29], [30]. It is therefore essential to eliminate these pollutants from the environments.\nMicrobial detoxification of chlorpyrifos has become the focus of many studies because other methods of removing chlorpyrifos residues are impractical or costly or are themselves environmentally hazardous [5]. To date, several chlorpyrifos-degrading bacterial strains including Enterobacter strain B-14 [31], Stenotrophomonas sp. strain YC-1 [32], Sphingomonas sp. strain Dsp-2 [33], Paracoccus sp. strain TRP [29], Bacillus pumilus strain C2A1 [4], and Bacillus laterosporus strain DSP [34] have been isolated from diverse sources; however, only Paracoccus sp. strain TRP and Bacillus pumilus strain C2A1 were able to degrade both chlorpyrifos and TCP. One recently isolated cyanobacterium, Synechocystis sp. strain PUPCCC 64, was also capable of degrading chlorpyrifos [35]. However, there is limited information concerning the ability of fungus to degrade chlorpyrifos, e.g. only Verticillium sp. strain DSP [36], [37] and Acremonium sp. strain GFRC-1 [38]. Fungi are critical to the biogeochemical cycle and are responsible for the bulk of the degradation of environmental xenobiotics in the biosphere [39]. Moreover, the ability of fungi to form extended mycelial networks, the low specificity of their catabolic enzymes and their independence from utilizing organic chemicals as a growth substrate make fungi well suited for bioremediation processes [40]. However, the potential use of fungus in bioremediation of OPs has not received the attention it deserves. This is the first report to our knowledge involving in the biodegradation of both chlorpyrifos and its hydrolysis product TCP by fungus.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Molecular structure of chlorpyrifos.doi:10.1371/journal.pone.0047205.g001In the current study, a new fungus Cladosporium cladosporioides strain Hu-01 able to degrade not only chlorpyrifos but also TCP was isolated and characterized. Moreover, the optimal culture conditions were investigated and the degradation pathway was elucidated.\nMaterials and Methods\nChemicals and Media\nChlorpyrifos standard (97% purity) was obtained from Dow AgroSciences, USA. TCP standard (99% purity) was purchased from Sigma-Aldrich, USA. Chromatographic-grade methanol was purchased from Burdic & Jackson, USA. All other chemicals and reagents used were of pure analytical-grade and available commercially. Stock solutions (10 g·L−1) were prepared with methanol, sterilized by membrane filtration (0.45 µm), and stored in dark bottles at 20°C before use.\nCzapek-Dox medium (CDM) containing (in gram per litre) sucrose, 30; NaNO3, 2; KCl, 0.5; MgSO4, 0.5; K2HPO4, 1; Fe2(SO4)3, 0.01; and peptone, 0.5; and mineral salt medium (MSM) containing (in gram per litre) (NH4)2SO4, 2; MgSO4·7H2O, 0.2; CaCl2·2H2O, 0.01; FeSO4·7H2O, 0.001, Na2HPO4·12H2O, 1.5; and KH2PO4, 1.5 were used for the isolation of fungal strains.\n\n\nEnrichment and Isolation of the Chlorpyrifos-degrading Fungi\nActivated sludge samples were collected as the inoculum from an aerobic chlorpyrifos-manufacturing wastewater treatment system located in Jiangmen, China, which had produced chlorpyrifos for over 15 years. Enrichment and isolation of degrading strains were carried out in MSM by using an enrichment culture technique as described in detail previously [41]–[43]. Colonies with different morphologies grown on the plates were picked and purified using the streaking method. The isolates were tested for their capacities to degrade chlorpyrifos and TCP. One pure isolate showing the highest degradation was selected for further study and designated Hu-01.\n\n\nIdentification and Characterization of Strain Hu-01\nThe isolate was grown on CDM agar plates for 5 d and its morphology was investigated by light microscope (BH-2 Olympus, Japan) and scanning electron microscope (XL-30 ESEM, Philips Optoelectronics Co., Ltd, Holland). Colony morphology was observed on CDM agar plates incubated at 27°C at 1, 2, 3, 4, and 5 d. Total genomic DNA was prepared according to the method of Sambrook and Russell [44]. The 5.8S rDNA gene was amplified by polymerase chain reaction (PCR) with the universal primers ITS 5 (5′-GGAAGTAAAAGTC GTAACAAGG-3′) and ITS 4 (5′-GCATATCAATAAGCGGAGGA-3′) [45]. Reaction conditions consisted of initial denaturation at 94°C for 3 min, followed by 35 cycles of denaturation at 94°C for 30 s, annealing at 55°C for 1 min, and extension at 72°C for 1 min, with the last cycle followed by a ten-minute extension at 72°C. After purification by agarose gel electrophoresis, PCR fragments were ligated to the linearized vector pMD20-T (TaKaRa Biotechnology Co. Ltd., China), and transformed into competent Escherichia coli DH5α cells. Sequencing of the cloned insert was performed by Shanghai Invitrogen Technology Co. Ltd., China. The resulting sequence was compared with the genes available in the GenBank nucleotide library by a BLAST search through the National Center for Biotechnology Information (NCBI) internet site. Multiple alignments of 5.8S rDNA were performed by ClustalX 1.8.1 with default settings, and phylogenesis was analyzed using MEGA 4.0 software. An unrooted tree was built using the neighbor-joining method [46].\n\n\nInoculum Preparation\nInoculum was prepared by growing the isolate in 50 mL of CDM for 3 d at 27°C with shaking at 150 rpm. Mycelia were harvested by centrifugation at 4600 × g for 5 min, washed with 0.9% sterile saline and resuspended in 50 mL saline. Two percent of this suspension was used as inoculum for chlorpyrifos and TCP biodegradation studies [47].\n\n\nOptimization of the Culture Conditions for Chlorpyrifos Degradation by Strain Hu-01\nThe culture conditions favoring chlorpyrifos degradation by strain Hu-01 was determined by using response surface methodology (RSM) [48]. The significant factors that were selected as independent variables were pH, temperature, and culture time based on the results of preliminary one-factor-at-a-time experiments. A five-level (−1.68, −1, 0, 1, 1.68) central composite rotatable design (CCRD) consisting of 23 experimental runs with three replicates at the center point was generated by statistic analysis system (SAS) software package (Version 9.0, SAS Institute Inc., Cary, NC, USA). Regression analysis was performed on the data obtained from the design experiments. Coding of the variables was carried out according to the following equation (Eq.(1))(1)where xi is the dimensionless value of an independent variable, Xi is the real value of an independent variable, X0 is the real value of an independent variable at the center point, and ΔXi is the step change of real value of the variable i corresponding to a variation of a unit for the dimensionless value of the variable i. The symbols and levels of the three independent variables are presented in Table 1. The dependent variable was the degradation of 50 mg·L−1 chlorpyrifos by strain Hu-01 by hour 12. Randomised block design was conducted in order to minimise the effects of unexplained variability in the observed response because of extraneous factors [49]. The data were analyzed by using the response surface regression procedure of the SAS software to fit the following quadratic polynomial equation (Eq.(2))(2)where Yi is the predicted response, Xi and Xj are variables, b0 is the constant, bi is the linear coefficient, bij is the interaction coefficient, and bii is the quadratic coefficient, respectively.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  The symbols and levels of three independent variables used in central composite rotatable design (CCRD).doi:10.1371/journal.pone.0047205.t001The coefficient of determination (R2) and adjusted R2 (Adj.R2) were used for the verification of the significance of the quadratic polynomial model. The significant variables were screened based on the F-test and the P-value at the 95% significance level [50]. On the basis of the analysis of variance (ANOVA), parameters with a significance level (P) greater than 5% were removed to obtain the final reduced model. The final model can be displayed as three-dimensional (3D) response surface plots by varying two factor levels while keeping the other factor at a constant level [51]. The SAS software package was applied for the regression analysis and the graphical presentation.\n\n\nIdentification of Metabolic Products during Chlorpyrifos Degradation\nThe degradation products of chlorpyrifos in mycelium-free filtrates of the fungal cultures grown in MSM containing 50 mg·L−1 of chlorpyrifos were determined by gas chromatopraphy-mass spectrometry (GC-MS) (Agilent 6890N/5975, USA). The mycelium-free filtrates were collected at 1, 2, 3, 4, 5, and 6 d, respectively. Non-inoculated media served as controls. The metabolic products confirmed on the basis of mass spectrum were matched with authentic standard compounds from the National Institute of Standards and Technology (NIST, USA) library database.\n\n\nDegradation Kinetics of Chlorpyrifos and TCP by Strain Hu-01\nThe abilities of strain Hu-01 to degrade chlorpyrifos and its hydrolysis product TCP were investigated in MSM under the optimal culture conditions. Chlorpyrifos and TCP were added to 250-ml Erlenmeyer flasks to give the final concentration of 50 mg·L−1, respectively. The flasks were incubated at 27°C on a platform shaker at 150 rpm for 7 d. The experiment was conducted in triplicate with non-inoculated samples as control. The samples were collected periodically and the chemical residues were measured by high performance liquid chromatography (HPLC) (Agilent 1100, USA).\n\n\nChemical Analysis\nA modified method from Gao et al. [52] was used to determine the residues of chlorpyrifos and TCP. In brief, samples (30 mL) were extracted using the mixture of acetone and dichloromethane in an equal volume ratio (1:1, v/v). After concentration by using rotary evaporator (Heidolph, Germany), residues dissolved in methanol were determined by HPLC equipped with a Hypersil ODS2 C18 reversed phase column (4.6 nm × 250 mm, 5 µm) with array detection from 190–400 nm (total scan). A mixture of methanol and water (70:30, v/v) was used as the mobile phase after acidification to pH 3 with concentrated phosphoric acid. The injection volume was 10 µL.\nThe degradation products of chlorpyrifos were identified according to the method of Xu et al. [29] with modification. In brief, the mycelium-free cultures were extracted with petroleum ether and the supernatant was dehydrated, dried and re-dissolved in methanol. After filtration with 0.45 µm membrance (Millipore, USA), the samples were detected by GC-MS system equipped with auto-sampler, an on-column, split/splitless capillary injection system, and with HP-5MS capillary column (30.0 m × 250 µm × 0.25 µm) with array detection from 30–500 nm (total scan). The operating conditions were as follows: the column was held at 80°C for 5 min, ramped at 8°C·min−1 to 200°C (first ramp), held at 200°C for 5 min, ramped at 15°C·min−1 to 260°C (second ramp), and then held at at 260°C for 5 min. The temperatures corresponding to transfer line and the ion trap were 280°C and 230°C, respectively, and the ionization energy was 70 eV. The injection volume was 1.0 µL with a split ratio of 1:7 at 260°C. Helium was used as a carrier gas at a flow rate of 1.0 mL·min−1.\n\n\nData Analysis\nResults were assessed by ANOVA and statistical analyses were performed on three replicates of data obtained from each treatment. The significance (P<0.05) of differences was treated statistically by one-, two-, or three-way ANOVA and evaluated by Tukey’s test.\n\n\nEthics Statement\nNo specific permits were required for the described field studies. No specific permissions were required for these locations. We confirm that the location is not privately-owned or protected in any way. We confirm that the field studies did not involve endangered or protected species.\n\nResults\nIsolation and Identification of the Fungal Strain Hu-01\nAfter five rounds of transfer, a total of 8 pure isolates able to grow with chlorpyrifos as the sole carbon source were obtained from the enrichment culture (Table S1). One fungal strain, designated Hu-01, was selected for further study due to its highest degradation activity on both chlorpyrifos and TCP. This fungus degraded and tolerated chlorpyrifos and TCP with concentrations as high as 500 mg·L−1, and completely metabolized the added chlorpyrifos and TCP (both 50 mg·L−1) within 5 and 6 d, respectively.\nStrain Hu-01 was an obligately aerobic fungus with strong spore production. Colonies were dark green, round, and with concentric rings and entire margin when grown on CDM agar plates. Spores were oval, umbilicate, and with dimensions of 4.0 to 6.0 µm in length and 2.0 to 3.0 µm in width (Figure S1). Mycelia were dark olivaceous which differed from most genus of fungi. A single fragment of 574 bp was obtained after the PCR amplification. The DNA G+C content is 50.9 mol %. Phylogenetic analysis of the 5.8S rDNA gene sequences revealed that strain Hu-01 grouped among Cladosporium species and was closely related to C. cladosporioides strain ATT097 (GenBank accession No. H607834) and C. cladosporioides strain CY141 (GenBank accession No. HQ607983) with high identities (>99%) (Fig. 2). Thus, strain Hu-01 was tentatively identified as C. cladosporioides based on the morphology and 5.8S rDNA gene analysis. The partial 5.8S rDNA gene sequences have been deposited at the GenBank under the accession No. EF405864. The C. cladosporioides strain Hu-01 was also deposited in China Center for Type Culture Collection under the collection No. CCTCC M 20711.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Phylogenetic tree based on the 5.8S rRNA sequence of strain Hu-01 and related strains.Numbers in parentheses represent the sequences accession number in GenBank. Numbers at the nodes indicate bootstrap values from the neighborhood-joining analysis of 1,000 resampled data sets. Bar represents sequence divergence.\ndoi:10.1371/journal.pone.0047205.g002\n\nOptimization of the Culture Conditions for Chlorpyrifos Degradation by Strain Hu-01\nResponse surface methodology (RSM) based on the central composite rotatable design (CCRD) was employed to investigate the main and interactive effects of significant variables including pH (X1), temperature (X2), and culture time (X3) on chlorpyrifos degradation by strain Hu-01. The design matrix of the variables together with the experimental responses is given in Table 2. Subsequently, the data from Table 2 were assessed by response surface regression procedure of SAS software package, and the results of the quadratic polynomial model fitting in the term of analysis of variance (ANOVA) were shown in Table 3. By applying the multiple regression analysis on the experimental data, the following quadratic polynomial equation (Eq.(3)) was derived to explain the chlorpyrifos degradation:(3)where Y is the predicted chlorpyrifos degradation (%) by strain Hu-01; X1, X2, and X3 are the coded values for the pH, temperature, and culture time, respectively.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Central composite rotatable design (CCRD) matrix and the response of dependent variable for chlorpyrifos degradation.doi:10.1371/journal.pone.0047205.t002The statistical significance of Eq.(3) was also evaluated by performing F-test (Table 3) and t-test (Table 4). The statistical analysis indicated that the model linear term coefficient of X2 and X3 and the quadric term coefficient of X1, X2 and X3 showed significant effects (P<0.05) on chlorpyrifos degradation. On the contrary, the linear term coefficient of X1, and the cross-product coefficients were not significant (P>0.05), so the quadratic polynomial equation (Eq.(4)) was modified to be:(4)\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 3.  Analysis of variance (ANOVA) for the fitted quadratic polynomial model for chlorpyrifos degradation.doi:10.1371/journal.pone.0047205.t003The adequacy of the model was examined by the determination coefficient (R2 = 0.9650), which explained 96.50% of the response variability, suggesting that the predicted values of the model were well correlated with the experimental values (Table 4). The high value of the adjusted R2 (0.9408) further supported the accuracy of the model. These results suggested that the response equation provided a suitable and reasonable model for the CCRD experiment. Besides, the low coefficient of variation (CV = 5.8%) demonstrated a good precision and reliability of the experiments. The developed model thus could be adequate for prediction within the range of variables employed.\nA three-dimensional (3D) response surface was plotted to directly display the effects of temperature and culture time on the chlorpyrifos degradation by strain Hu-01 while keeping the value of pH (the non-significant variable) at a zero level (Fig. 3). The model predicted a maximum chlorpyrifos degradation of 85.9% at the stationary point. At the stationary point, the optimum levels for the three variables of X1, X2 and X3 were observed to be −0.1460, −0.2627 and 0.6851 in terms of the coded units, that is, pH 6.5, temperature 26.8°C, and culture time 4.7 d, respectively. So the optimum culture conditions for chlorpyrifos degradation by strain Hu-01 were determined to be pH 6.5, 26.8°C, and 4.7 d.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 4.  Effect estimates for the fitted quadratic polynomial model for chlorpyrifos degradation.doi:10.1371/journal.pone.0047205.t004\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Response surface plot showing the effects of temperature and culture time on chlorpyrifos degradation by strain Hu-01 with pH = 6.5.doi:10.1371/journal.pone.0047205.g003\n\nMetabolic Products during Chlorpyrifos Degradation\nThe metabolic products of chlorpyrifos degradation by strain Hu-01 in mycelium-free filtrates were extracted and confirmed by GC-MS. The GC-MS results showed that two peaks at retention times (RT) of 15.135 and 9.140 min representing metabolites A and B, respectively.\nEach of the two peaks was identified on the basis of its mass spectra and the NIST library identification program. The peak at retention time of 15.135 min corresponded to chlorpyrifos standard. This peak disappeared concomitantly with formation of another new peak with a retention time of around 9.140 min. Based on the characteristic fragment ion peaks and molecular ion (m/z), the new peak was identified as TCP (Fig. 4). However, this new peak was transient and it disappeared finally. Eventually, no persistent accumulative metabolite was detectable by GC-MS after 6 d of incubation.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Mass spectra of 3,5,6-trichloro-2-pyridinol (TCP) produced from chlorpyrifos degradation by strain Hu-01.A: sample; B: authentic standard TCP from the National Institute of Standards and Technology (NIST, USA) library database.\ndoi:10.1371/journal.pone.0047205.g004Based on the metabolic products formed, the degradation pathway for chlorpyrifos by strain Hu-01 was proposed (Fig. 5). That is to say, the parent chlorpyrifos (m/z = 351) was first metabolized by hydrolysis to produce TCP (m/z = 197) and diethylthiophosphoric acid (DETP) (m/z = 172). Subsequently, the hydrolysis product TCP was further transformed by ring breakage, resulting in its complete detoxification. These results thus indicated that the added chlorpyrifos (50 mg·L−1) was completely degraded by strain Hu-01 without any accumulative products after 6 d of incubation.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  The proposed pathway for the chlorpyrifos degradation by strain Hu-01.doi:10.1371/journal.pone.0047205.g005\n\nDegradation Kinetics of Chlorpyrifos and TCP during Biodegradation Studies\nThe degradation kinetics of chlorpyrifos and its hydrolysis product TCP by strain Hu-01 were shown in Fig. 6. Introduction with strain Hu-01, rapid degradation was observed at the beginning of incubation, apparently there was no lag period and 89.0% of the 50 mg·L−1 chlorpyrifos initially added to the medium was degraded within 1 d. After 5 d of incubation complete degradation was achieved by the fungus; whereas the non-inoculated control this value decreased only by 14.4%. In the case of TCP, similar accelerated degradation was observed and 93.5% of the added TCP (50 mg·L−1) was eliminated within 1 d. After 6 d of incubation, there is no TCP detectable in the medium. In contrast, more than 88% of the added TCP remained in the medium that was not inoculated after completion of the experiment. During the chlorpyrifos degradation, trace amounts of TCP was also detected in the medium inoculated with strain Hu-01, but no TCP was found at the end of experiment. Therefore, incubation with strain Hu-01 resulted in greater chlorpyrifos and TCP degradation as compared to that in the non-inoculated controls.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Degradation kinetics of chlorpyrifos and 3,5,6-trichloro-2-pyridinol (TCP) during biodegradation studies.□, non-inoculated control (chlorpyrifos); △, non-inoculated control (TCP); ▪, introduction with strain Hu-01 (chlorpyrifos); ▴, introduction with strain Hu-01(TCP). Error bars represent the standard deviation of three replicates.\ndoi:10.1371/journal.pone.0047205.g006To confirm the effects on biodegradation of chlorpyrifos and TCP by strain Hu-01, the first-order kinetic model (Eq.(5)) adapted from Cycoń et al. [3] was used to determine the rate of total chemical reduction(5)where C0 is the initial concentration of chemical at time zero, Ct is the concentration of chemical at time t, k and t are the degradation rate constant (h−1) and degradation period in hours, respectively.\nThe algorithm as expressed in Eq.(6) was used to describe the theoretical half-life (t1/2) values of chlorpyrifos and TCPwhere ln2 is the logarithmic function, k is the rate constant (h−1).\nThe kinetic parameters for all runs calculated from the above equations (Eq.(5) and Eq.(6)) are presented in Table 5. In these studies, the degradation rate and the chemical concentration were in direct proportion, and the degradation process corresponded to the first-order kinetics with a R2 ranging from 0.9165 to 0.9904, revealing that the experimental data fit well with the model. The k for chlorpyrifos and TCP with strain Hu-01 were 0.1398 and 0.2201 h−1, respectively; while the control the k were 0.0010 and 0.0007 h−1, respectively. The t1/2 for chlorpyrifos and TCP with strain Hu-01 were 5.0 and 3.1 h, respectively. In contrast, the t1/2 for chlorpyrifos and TCP in the non-inoculated controls were 693.0 and 990.0 h, respectively. Compared with the controls, the t1/2 for chlorpyrifos and TCP greatly reduced by 688.0 and 986.9 h, respectively. These results thus confirmed that inoculation with strain Hu-01 significantly enhanced degradation of chlorpyrifos and TCP. At the end of the reaction, both pollutants were successfully eliminated by the fungal strain C. cladosporioides Hu-01.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 5.  Kinetic parameters of degradation of chlorpyrifos and 3,5,6-trichloro-2-pyridinol (TCP) by strain Hu-01.doi:10.1371/journal.pone.0047205.t005\nGiven the widespread chlorpyrifos use around the world and prevalent human exposure, effective techniques for disposal of this pollutant are critically needed [6], [9], [53], [54]. Bioremediation provides a cheap and environmentally friendly way to remove toxic pollutants from the environment [2]. In the current study, a novel fungal strain C. cladosporioides strain Hu-01 responsible for chlorpyrifos degradation was isolated from organophosphate-contaminated soils using an enrichment procedure. This particular isolate completely degraded not only chlorpyrifos but also its hydrolysis product TCP. This feature is rarely reported in other chlorpyrifos-degrading microorganisms. In most cases reported to date, the individual isolate tended to transform chlorpyrifos by hydrolysis of ester linkage to yield TCP, which in turn accumulated in the batch cultures or soils and enhanced degradation could not occur owing to its antimicrobial properties [4], [28], [29], [30], [32]. TCP is associated with estrogenic activity and has recently been listed as potential endocrine disrupting chemical by the Environmental Protection Agency (EPA) of the USA [55]. Compared with the importance of TCP degradation issue, studies concerning its fate and degradation in the environment are very limited. Ralstonia sp. strain T6 was reported to metabolize 100 mg·L−1 TCP within 12 h, but it could not transform chlorpyrifos [30]. As far as we know, this is the first report about a fungal strain involving in chlorpyrifos and TCP degradation.There has been some success in the use of degrading bacteria for bioremediation of chlorpyrifos. For example, Dubey and Fulekar [56] demonstrated that Stenotrophomonas maltophilia strain MHF ENV20 isolated from rhizosphere played an important role in reducing chlorpyrifos level in soil. Plasmid-mediated bioaugmentation for the enhanced biodegradation of chlorpyrifos in soil was recently achieved by the indigenous bacteria, including members of the Pseudomonas and Staphylococcus genera [34]. Unfortunately, relative contributions of fungi to degradation processes have seldom been quantified. Fungi possess the biochemical and ecological capacity to detoxify environmental xenobiotics, either by chemical modification or by influencing chemical bioavailability [36]. However, despite dominating the living biomass in soil and being abundant in aqueous systems, fungi have not received the attention they deserve. Though several fungal strains have been reported to degrade chlorpyrifos, yet attempts to isolate fungi in pure culture able to mineralize chlorpyrifos have often failed [36]–[38], [57]. For example, mineralization of chlorpyrifos (50 mg·L−1) was achieved by co-culture of Serratia sp. TCR and Trichosporon sp. TCF, but the pure fungal strain TCF alone could not transform chlorpyrifos [57].In order to obtain efficient degradative fungi, in the last few years our laboratory has screened a wide range of contaminated soils from organophosphate-manufactuer system in China for chlorpyrifos degradation activity. A fungal strain, designated Hu-01, capable of completely degrading chlorpyrifos and its hydrolysis product TCP was successfully isolated from the soil. 5.8S rDNA gene sequencing and the morphological characteristics strongly revealed that strain Hu-01 belongs to the genus Cladosporium. To the best of our knowledge, there is no information regarding the ability to metabolize organophosphorous pesticides by fungal strain belonging to the genus Cladosporium. Nevertheless, several studies have suggested that Cladosporium species have some potential bioactivity, such as antimicrobial activity and the ability to degrade aromatic compounds [47], [58], [59].Notably, C. cladosporioides strain Hu-01 was found to efficiently degrade chlorpyrifos over a broad range of temperature and pH particularly at low pH, as summarized in Table 2. This feature gives pesticide degraders advantages in the variable environments, because they survival and utilize xenobiotic compounds even exposed to adverse conditions. In contrast, previous studies showed that rapid degradation of chlorpyrifos by Enterobactor sp. was observed at high pH while this activity was very slow at acidic pH [60]. Anwar et al. [4] reported that chlorpyrifos was more efficiently degraded at basic as well as neutral pH whereby more than 80% of the added pesticide was degraded by Bacillus pumilus strain C2A1 and no lag phase was observed at relatively higher pH. However, at acidic pH only 50% degradation was observed with longer lag phase. Our results contrasted with previous findings and the chlorpyrifos degradation by strain Hu-01 was higher at the acidic conditions. It is possible that some key enzyme(s) responsible for chlorpyrifos degradation have their optimum enzymatic activity at relatively low pH value. Another important feature which is worth mentioning is that this strain engaged in efficient degradation of chlorpyrifos and its hydrolysis product TCP up to the concentrations, as high as 500 mg·L−1. In contrast to other reports on toxic effects of chlorpyrifos on diverse microorganisms [28], [29], [61], our results indicated that chlorpyrifos metabolism activity of strain Hu-01 was not subject to complete catabolite repression by high chlorpyrifos concentrations. High chlorpyrifos tolerance and degradation capability of C. cladosporioides strain Hu-01, makes this strain suitable for decontamination and remediation of contaminated sites.In this study, the optimum culture conditions for chlorpyrifos degradation by C. cladosporioides strain Hu-01 were determined by using response surface methodology (RSM). RSM is an empirical modeling system that has been successfully applied to improve and optimize complex processes, including fermentation processes and medium components for a variety of microorganisms [50], [51]. Previous studies have shown that the application of statistical experimental design techniques in biodegradation processes can result in improved yields of degradation and allow the rapid and economical determination of the optimum culture conditions with fewer experiments and minimal resources [47], [49], [62]–[65]. In the present study, RSM is first employed to optimize culture conditions favouring chlorpyrifos degradation. The optimization parameters investigated include pH, temperature and culture time. The results of the experiments were statistically analyzed and the significance and effect of each factor on responses was evaluated. The optimum degradation conditions were determined to be pH 6.5, 26.8°C, and culture time 4.7 d. Under these conditions, strain Hu-01 reached a maximum chlorpyrifos degradation of approximately 85% within 12 h. Moreover, a mathematical model (Eq.(4)) was developed herein, and this model could be effectively used to predict and optimize the chlorpyrifos degradation conditions by strain Hu-01 within the limits of chosen factors.It was generally considered that ester hydrolysis was the primary degradation pathway of chlorpyrifos [66], [67]. In most cases, the degrading microorganisms tend to metabolize chlorpyrifos by hydrolysis to form diethylthiophosphoric acid (DETP) and TCP [4], [29], [31], [32]. However, due to its resistant to enhanced degradation, studies on further metabolism and identification of intermediate products of chlorpyrifos have not been extensive [29]. In our studies, degradation of chlorpyrifos was accompanied by a transient accumulation of TCP. Moreover, the only intermediate product disappeared quickly. Obviously, TCP was further transformed without any other persistent metabolites by strain Hu-01. Finally, no persistent accumulative metabolite was detected by GC-MS after 6 d of incubation. Some other metabolites might have formed and been immediately degraded by the fungus. In our previous work, we have purified the chlorpyrifos hydrolase from the fungus [52]. Based on these results, the metabolic pathway of chlorpyrifos was first proposed (Fig. 5).Previous studies suggested that the conditions for environmental microorganism enrichment and screening are of vital importance in the selection of pure isolates with high survivability in the environment and maximum degrading activity towards xenobiotics [29]. However, it has been problematic to isolate a pure isolate capable of utilizing chlorpyrifos and TCP for growth substrates [4]. Several reported microorganisms degraded chlorpyrifos and TCP co-metabolically, which needed extra carbon sources for enhanced degradation [68], [69]. In our studies, the fungal strain isolated from the contaminated soils appeared to be highly efficient in degrading chlorpyrifos and TCP in MSM without any other carbon sources and utilizing these pollutants as the sole carbon sources. It could utilize 89.0% and 93.5% of the added chlorpyrifos and TCP (both 50 mg·L−1) within 1 d, respectively (Fig. 6). The t1/2 for chlorpyrifos and TCP greatly reduced by 688.0 and 986.9 h, respectively as compared to the controls (Table 5). These results demonstrated the isolate may be suitable for the efficient and rapid bioremediation of contaminated environments.In conclusion, the C. cladosporioides strain Hu-01 isolated in the present study completely metabolized not only chlorpyrifos but also its hydrolysis product TCP. This is the first report involving in the biodegradation of both chlorpyrifos and TCP by a fungal strain from the genus Cladosporium. Moreover, metabolization of chlorpyrifos and TCP by the same strain is of vital importance because TCP possesses antimicrobial activities, and it tends to accumulate in the batch cultures or soils and hence prevents the proliferation of microorganisms responsible for the parent compound degradation through catabolite repression. In addition, strain Hu-01 was found highly efficient in degrading chlorpyrifos over a wide range of temperature and pH particularly at low pH. This feature gives pesticide degraders advantages in the variable environments. Another important feature which is worth mentioning is that this strain harbors the metabolic pathway for the detoxification of chlorpyrifos, and it completely degraded chlorpyrifos without any persistent accumulative metabolites. Furthermore, the fungus utilized chlorpyrifos and TCP as the sole carbon sources for growth, thus suggesting adaptation to oligotrophic environments. The ability to survive at high concentration of chlorpyrifos and enhanced degradation make this isolate an ideal candidate for its application in chlorpyrifos bioremediation."
        },
        "10.1371/journal.pone.0081993": {
            "author_display": [
                "Yuhong Zhang",
                "Xiaolu Xu",
                "Xiaojin Zhou",
                "Rumei Chen",
                "Peilong Yang",
                "Qingchang Meng",
                "Kun Meng",
                "Huiying Luo",
                "Jianhua Yuan",
                "Bin Yao",
                "Wei Zhang"
            ],
            "title_display": "Overexpression of an Acidic Endo-β-1,3-1,4-glucanase in Transgenic Maize Seed for Direct Utilization in Animal Feed",
            "abstract": [
                "Background: Incorporation of exogenous glucanase into animal feed is common practice to remove glucan, one of the anti-nutritional factors, for efficient nutrition absorption. The acidic endo-β-1,3-1,4-glucanase (Bgl7A) from Bispora sp. MEY-1 has excellent properties and represents a potential enzyme supplement to animal feed. Methodology/Principal Findings: Here we successfully developed a transgenic maize producing a high level of Bgl7AM (codon modified Bgl7A) by constructing a recombinant vector driven by the embryo-specific promoter ZM-leg1A. Southern and Western blot analysis indicated the stable integration and specific expression of the transgene in maize seeds over four generations. The β-glucanase activity of the transgenic maize seeds reached up to 779,800 U/kg, about 236-fold higher than that of non-transgenic maize. The β-glucanase derived from the transgenic maize seeds had an optimal pH of 4.0 and was stable at pH 1.0–8.0, which is in agreement with the normal environment of digestive tract. Conclusion/Significance: Our study offers a transgenic maize line that could be directly used in animal feed without any glucanase production, purification and supplementation, consequently simplifying the feed enzyme processing procedure. "
            ],
            "publication_date": "2013-12-31T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 949,
            "shares": 1,
            "bookmarks": 4,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0081993",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0081993&representation=PDF",
            "fulltext": "Introductionβ-1,3-1,4-d-Glucans (β-glucans) are the main component of cereal cell walls, particularly in the endosperm cell walls of barley and other grains [1]. It is composed of β-d-glycosyl residues linked through irregular β-1,3 and/or β-1,4 glycosidic bonds. Ruminants can utilize β-glucans through enzyme digestion of rumen microbes. However, monogastric animals such as pig, poultry, and fish do not have such enzymes to decompose the β-glucans. By combining with water, β-glucans increase the viscosity of chyme, block the intestinal surface partially, and prevent the mixing of intestinal endogenous digestive juice with the chyme [2]. Thus β-glucan represents one of the intense anti-nutritional factors in wheat- and barley-based diets [3].\nTo overcome these problems, the most common and effective practice is to add exogenous endoglucanases into animal feed [3]. Majority of endoglucanases are grouped into glycoside hydrolase (GH) families 3, 5, 7, 12 and 16, based on the amino acid sequence and catalytic domain structures (http://www.cazy.org/). According to the degradation mode against glycosidic linkage, endoglucanases have been grouped into four main categories: β-1,3-glucanase (laminarinase, EC 3.2.1.39), β-1,4-glucanases (cellulase, EC 3.2.1.4), β-1,3-1,4-glucanases (lichenase, EC 3.2.1.73), and β-1,3(4)-glucanase (EC 3.2.1.6) [4]. Among them, β-1,3-1,4-glucanase has received significant attention in feed industrial applications because of their hydrolysis ability against grain-based glucan and multiple enzymatic functions. β-1,3-1,4-Glucanase is able to catalyze the hydrolysis of β-glucan into low molecular weight glucose polymers, thus reducing the hydrophilicity and viscosity of chyme and eliminating the anti-nutritional negative effect. Moreover, addition of β-1,3-1,4-glucanase can improve feed intake, enhance animal production, regulate cecal microbiota and increase feed conversion ratio [5]–[8]. Besides, the hydrolysis products from glucans—glucooligosaccharides may serve as fermentable dietary fiber-like substrates and positively affect gastrointestinal tract health [9].\nTo date, commercial feed additive β-1,3-1,4-glucanases are generally from microbial expression systems, commonly Aspergillus japonicus [10], Pichia pastoris [11] and Clostridium thermocellum [8]. This process is flexible and convenient, but has disadvantages like high energy consumption, high equipment cost and serious environmental pollution. Moreover, enzyme addition is a complex process involving enzyme isolation, purification and supplementation, which requires more energy and resources. Thus it's a good way to produce feed enzymes (e.g. β-1,3-1,4-glucanase) in transgenic feed grains directly without any industrial processing.\nTransgenic plants are being developed for both commercial and environmental values. In 2011, the plantation area of transgenic plants reached about 160 million hectares worldwide and was distributed in 29 countries; transgenic maize accounted for nearly one third of the total genetically modified crops [12]. Maize (Zea mays L.) is the main ingredient of animal feed (nearly 50%), and represents an ideal bioreactor of feed enzymes because of its cultivation worldwide. A phytase gene phyA2 from Aspergillus niger has been successfully overexpressed in maize seeds [13].\nIn this study, we developed a genetically stable maize line that had high β-glucanase activity in the seeds. The endo-β-1,3-1,4-glucanase, Bgl7A, from acidophilic Bispora sp. MEY-1 was selected due to its excellent properties as feed additive, such as acidic pH optimum, good thermostability and broad pH stability, highly resistance to proteases, and broad substrate specificity [11]. The gene codon was optimized for better expression in maize.\nMaterials and Methods\nPlant materials\nMaize Hi-II [14] was used for genetic transformation as host variety. The immature embryos, approximately 1.0–2.0 mm long, were preserved on N61-100-25 medium [14] containing 0.2% (w/v) phytagel (Sigma, St. Louis, MO) for callus induction. The commercial maize inbred-line Zheng58 was used as recurrent parent to produce progenies.\n\n\nCodon optimization of the β-1,3-1,4-glucanase gene bgl7A\nTo improve its expression level in transgenic maize, the DNA sequence of native endo-β-1,3-1,4-glucanase gene bgl7A from Bispora sp. MEY-1 (Genbank accession No. FJ695140) [11] was optimized according to the translationally optimal codon usage of maize [15]. Codon adaptation index (CAI), optimal codon usage, GC content and distribution, effective number of codons (Nc), negative CIS elements, negative repeat elements, and mRNA structure were used to evaluate the gene sequence (https://www.genscript.com/cgi-bin/tools/​rare_codon_analysis). Low-usage codons (<15% frequency) were replaced by high-usage ones according to the known codon bias of maize [15]. The modified gene was named bgl7Am that encoded the same amino acid sequence as bgl7A. The optimized gene was synthesized by Genscript (Nanjing, China) and cloned into pUC57 vector to construct the recombinant plasmid pUC57-bgl7Am.\n\n\nPlasmid construction\nThe transformation vector pHP20754 (Fig. 1a) consists of the corn legumin1A (leg1) promoter ZM-leg1A Pro, signal peptide (SP), vacuole targeting sequence (VTS) of corn Proaleurain and the corn leg1 terminator ZM-leg1 Term [16]–[18]. The β-glucanase gene was excised from pUC57-bgl7Am with BamHI and XmaI and subcloned into pHP20754 to produce the expression construct pHP20754-bgl7Am, which was further digested with PvuII to generate the chimeric gene expression cassette (Fig. 1b) for transformation. The plasmid pHP17042BAR carrying the maize histone H2B promoter, the maize Ubiquitin 5′-UTR intron-1, the bar gene and the potato protease II (PINII) terminator [13] was used as the selectable marker for screening of positive transgenic plants. The bar gene expression cassette was excised from pHP17042BAR by digest with HindIII, XhoI and SacI.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Construction of the recombinant vector and the transgenic maize seed.A The recombinant expression vector pHP20754-bgl7Am. B The chimeric gene cassettes for expression in maize. C Ears and seeds of transgenic maize (T1 and BC3) compared with that of wild-type Zheng58.\ndoi:10.1371/journal.pone.0081993.g001\n\nMaize transformation, selection and regeneration\nThe plasmid fragments containing the gene cassette of bgl7Am and bar, respectively, were mixed at the ratio of 1:1 and adjusted the concentration to 200 ng/µL. Maize transformation was carried out with high-velocity tungsten microprojectile (Bio-Rad, Hercules, CA) wrapped by the DNAs of bgl7Am and bar according to the method described before [19]. After recovery, embryonic calli were transferred onto the selective medium supplemented with bialaphos as the selectable marker. The positively transformed calli were cultivated in differentiation medium and rooting medium in succession. Seedlings (T0 plants) were transplanted into greenhouse and pollinated with the inbred-line Zheng58 to produce T1 seeds. Seeds were dried on the plant and harvested 35–45 days after pollination. Zheng58 was used as recurrent parent for backcrossing to produce filial generations (T1, BC1, BC2 and BC3). β-Glucanase activity determination in the kernels and PCR for the bgl7Am gene of seedlings were used in combination to screen the transgenic lines.\n\n\nPCR detection of exogenous gene integration\nThe specific primers bgl7am-875F (5′-ACGGCAAGGTCATCCAGAACGCGAAGG-3′) and 20754-398R (5′-TTCCTGGCAAATCACTCGGTGTATC-3′) were used for PCR detection of the positive plants harboring bgl7Am. The gene actin as control was amplified using primers AC326F (5′-ATGTTTCCTGGGATTGCCGAT-3′) and AC326R (5′-GCATCACAAGCCAGTTTAACC-3′). Genomic DNA of the maize immature leaves was used as PCR templates. The recombinant plasmid pHP20754-bgl7Am and the genomic DNA of Zheng58 were used as the positive and negative controls, respectively.\n\n\nSouthern blot analysis\nFive grams of maize leaves of generations T1 to BC3 were ground to powder in liquid nitrogen, and the genomic DNA was extracted with the CTAB method. Genomic DNA of Zheng58 was used as the negative control. About 50 µg of genomic DNA was digested by EcoRI and HindIII and then separated on a 0.8% (w/v) agarose gel. The agarose gel was transferred onto a hybond-N+ nylon membrane (GE Healthcare, Uppsala, Sweden) with a Trans-Blot SD system followed by UV-crosslinking. A digoxin-labeled probe containing a 800-bp fragment of bgl7Am was used for southern-blot hybridization. Immunologic process was conducted following the instructions of DIG-high prime DNA labeling and detection starter kit II (Roche, Indianapolis, IN).\n\n\nWestern blot analysis\nFive milligrams of lyophilized purified Bgl7A produced in Pichia pastoris GS115 [11] was used for the production of polyclonal antibody in rabbits. Recombinant proteins were extracted from seed meals. Kernels were ground with a high-throughput tissue homogenizer Geno/Grinder 2010 (SEPX CertiPrep, Metuchen, NJ).\nTo extract protein from seed meals, 30 mg of seed powder were placed into a 1.5-mL tube containing 300 µL extraction buffer (50 mM citric acid-Na2HPO4, pH 3.5). The tube was agitated on a shaker at room temperature for 1 h. After centrifugation at 5000× g for 10 min, the supernatant was incubated with 2-fold volume of pre-cooled acetone for 30 min, followed by centrifugation at 14,000× g for 10 min. The protein precipitate was dissolved in 30 µL of deionized water, and the protein sample was divided into two equal parts. One was deglycosylated with endo-β-N-acetylglucosaminidase (Endo H) according to the supplier's instructions (New England Biolabs, Ipswich, MA), the other remained intact. Protein extract of purified Bgl7A from P. pastoris and Zheng58 were used as the positive and negative controls, respectively. Proteins from the stem, root and leaf of a transgenic plant of generation BC1 were extracted in the same way and used for tissue specificity analysis.\nProteins were separated on SDS–PAGE (12% acrylamide, 0.4% acryl-bisacrylamide). and transferred onto PVDF membrane (Pall, Port Washington, NY). The polyclonal antibody raised in rabbits was added into the membrane confining liquid for prehybridization. The goat anti-rabbit IgG labeled with alkaline phosphatase was used as the secondary antibody. BCIP/NBT kit (Zomanbio, Beijing, China) was used for color development. To identify the proteins, bands were excised from the gel and analyzed using matrix assisted laser desorption/ionization time of flight mass spectrometry (MALDI-TOF-MS) at Tianjin Biochip Corporation (Tianjin, China).\n\n\nβ-Glucanase activity assay and enzyme characterization\nCrude proteins of five randomly selected seeds were extracted with extraction buffer as described above, and the supernatant was subject to β-glucanase activity assay. β-Glucanase activity was determined by measuring the amount of reducing sugar released from lichenan with the method of 3,5-dinitrosalicylic acid (DNS) [11], [20]. One unit of enzyme activity was defined as the amount of enzyme required to release 1 µmol of reducing sugar per minute from 1.0% lichenan in citric acid-Na2HPO4 (50 mM, pH 3.5) at 60°C for 10 min. β-Glucanase activities of generations T1, BC1, BC2 and BC3 of transgenic maize and Zheng58 were all evaluated. Each reaction and its control were run in triplicate. The enzyme properties of Bgl7AM derived from maize was determined using crude proteins from BC1 seeds as in Luo et al. (2010). The pH optimum of the protein was determined at 60°C and pH 1.0–6.0. The pH stability was determined by measuring the residual activity under standard conditions (pH 5.0, 60°C and 10 min) after pre-incubation at 37°C and pH 1.0–9.0 for 1 h. The optimal temperature was determined at 25–80°C at pH 5.0. Thermal stability of the enzyme was determined by assessing the residual enzyme activity under standard conditions after incubation of the enzyme at 70°C for various durations.\n\n\nEvaluation of anti-inactivation stability over feed pelleting process\nFeed pelleting was carried out with a twin-screw extruder (DSE-25 Extruder Lab-Station Brabender OHG, Duisburg, Germany). Part of the maize seeds were mixed and extruded at 70°C or 80°C, respectively. β-Glucanase activities and dry matter content (DM) values were determined before and after pelleting. Zheng58 seeds were treated as the non-transgenic control. Stability comparison was conducted with the β-glucanases derived from transgenic maize seeds and P. pastoris. Crude Bgl7A derived from P. pastoris with equal enzyme activity to transgenic maize was added into Zheng58 seeds, followed by pelleting treatment as described above. And the β-glucanase activity was detected after pelleting. One-way analysis of variance (ANOVA) was performed using the Duncan's multiple-range test to compare treatment means. Significance was defined at P<0.05.\n\nResults\nConstruction and transformation of transgenic vector pHP20754-bgl7Am\nThe CAI value and GC content of bgl7A were 0.715 and 49.6%, respectively. After codon optimization and gene modification, the CAI value and GC content of bgl7Am was increased to 0.937 and 67.0%, respectively (Figure S1 and S2). These higher values are better for exogenous gene expression in maize. Furthermore, effective Nc, negative CIS elements, negative repeat elements, and mRNA structure of the target gene were also considered in gene modification (Figure S1, S2 and S3). As a result, native bgl7A and synthetic bgl7Am shared 82.2% nucleotide sequence identity but encoded identical amino acid sequences.\nThe 1221-bp bgl7Am was inserted into the expression vector pHP20754 between the embryo-specific ZM-leg1A promoter and ZM-leg1 terminator (Fig. 1b), which is a transcriptionally active spacer region that allows highly efficient transgene expression. The positive calli of maize Hi-II were regenerated on bialaphos medium and identified by PCR.\n\n\nPlant regeneration and phenotypic evaluation\nThe regenerated young plants described above showed good growth in the greenhouse. A total of 27 independent transgenic lines were obtained. Based on β-glucanase activities of the seeds, 330 seeds of three independent transgenic events (40, 46 and 51-1) were selected to cultivate in fields and backcross with Zheng58 for progeny production. As shown in Fig. 1c, the ears and seeds of generation T1 showed significant phenotypic difference from Zheng58. This difference was generally subsided in the later generations because of the successive backcrossing with Zheng58. Up to transgenic generation BC3, the traits of transgenic maize were almost the same as that of non-transgenic Zheng58 through visual observation. The result suggests that the inserted exogenous gene has no negative impact on the maize seed.\n\n\nDetermination of exogenous gene integration\nPCR assay with primers specific for bgl7Am was used to evaluate the inheritance of transgenic maize. Gene fragments of about 500 bp were detected in the transformation events 40, 46 and 51-1 (Fig. 2a). PCR results of actin gene (~300 bp) indicated the high quality of genomic DNA (Fig. 2b). To confirm the gene integration and the copy number of bgl7Am in transgenic plants, the genomic DNAs of three positive transgenic plants of event 40 were analyzed by southern blot after restriction digest with EcoRI and HindIII. The bgl7Am probe was prepared with a 800-bp fragment of the bgl7Am gene. There is only one EcoRI restriction site located between the promoter and the bgl7Am gene in the expression cassette of pHP20754-bgl7Am. A total of three bands of ~3.5, 5.0 and 6.0 kb, respectively, were detected in the positive lane via EcoRI digest, but not in non-transgenic Zheng58 (Fig. 2c). There is no HindIII site in the bgl7Am. While HindIII cut the gene expression cassette twice and released an internal fragment of 2.5 kb (Fig. 2c). These results indicate that there are three copies of bgl7Am in event 40.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  PCR analysis and southern blot analysis of the transgenic maize.A PCR detection of the gene bgl7Am in the genomic DNA of transgenic plant leaves of generation BC1. Lane 1, the plasmid PHP20754-bgl7Am as positive control; lane 2–9, the transgenic plants; lane 10, the non-transgenic Zheng58. B PCR detection of the gene actin. Lane 1, the plasmid PHP20754-bgl7Am; lane 2–9, the transgenic plants; lane 10, the non-transgenic Zheng58. C Southern blot analysis of bgl7Am in transgenic plants. The EcoRI and HindIII-digested genomic DNA was hybridized with the bgl7Am probe. Lane 1, the DIG-labeled molecular weight markers; lane 2–4, transgenic plants digested by EcoRI (arrowhead indicate the positive bands); lane 5, non-transgenic Zheng58 digested by EcoRI; lane 6, non-transgenic Zheng58 digested by HindIII; lane 7–9, transgenic plants digested by HindIII (arrowhead indicate the positive band); lane 10, PCR fragment of the bgl7Am as a positive control (arrowhead).\ndoi:10.1371/journal.pone.0081993.g002\n\nEvaluation of site-specific expression\nTo determine the expression efficiency of exogenous Bgl7AM, proteins were extracted from two BC1 plants of event 40 that had high β-glucanase activities. In western blot analysis, no band was detected in the negative control of Zheng58 (Fig. 3a). The positive control, Bgl7A expressed by P. pastoris, showed a band of about 60 kDa, the same as that reported before [11]. One main band of ~60 kDa was identified on the PVDF membrane after hybridization with the antibody (Fig. 3a). This molecular weight (60 kDa) was much higher than the predicted molecular weight (45.3 kDa). After Endo H treatment, the band had no significant reduction in molecular weight (Fig. 3a). It suggested that other post-translation modifications rather than N-glycosylation, such as O-linked glycosylation, phosphorylation, acetylation or methylation, may occur in the transgenic maize. The band was verified to be Bgl7AM through MALDI-TOF-MS analysis (Figure S4). Except for the seeds, proteins extracted from the root, stem and leaf of the positive lines had no objective band (Fig. 3b), indicating the tissue specificity of Bgl7AM. Moreover, Bgl7AM present in seeds are more convenient for storage, transportation and direct utilization.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Western blot analysis of recombinant Bgl7AM from the transgenic maize.A Western blot analysis of the Bgl7AM from transgenic maize seeds. Lane M, the protein molecular markers; Lane 1, 3, 5, the proteins isolated from transgenic maize seeds; lane 2, 4, 6, the proteins isolated from transgenic maize seeds and treated with Endo H; lane 7, the non-transgenic Zheng58 as a negative control; lane 8, the purified P. pastoris Bgl7A as a positive control. B Western blot analysis of the Bgl7AM from different tissues of the transgenic maize (leaf, stem, root and seeds). Lane M, the protein molecular markers; purified P. pastoris Bgl7A as a positive control; Z58 refers to non-transgenic Zheng58 (negative control).\ndoi:10.1371/journal.pone.0081993.g003\n\nEvaluation of seed-derived β-glucanase activity\nPositive transgenic plants of transgenic event 40 were selected for β-glucanase activity assay. Approximately 200–400 seeds of each generation were assessed using the DNS method (Table 1). Compared with the non-transgenic Zheng58 that had β-glucanase activity of 3300 U/kg of seeds, T1 seeds (207,800 U/kg) showed approximately 63-fold activities of Zheng58. Both the maximal and average activities of BC1, BC2 and BC3 seeds were increased slightly. The maximal β-glucanase activity of BC3 seeds was up to 779,800 U/kg, which was 236 folds of that of Zheng58. About 47% of the seeds showed over 200,000 U/kg of β-glucanase activity. The result further confirmed that bgl7Am is genetically stable over generations in maize.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  β-Glucanase activities of the transgenic maize seeds within four generations.doi:10.1371/journal.pone.0081993.t001\n\nCharacterization of maize seed-derived Bgl7AM\nThe crude proteins of transgenic BC1 seeds were characterized (Fig. 4), and compared with Bgl7A of P. pastoris reported before [11]. Bgl7AM had a pH optimum at 4.0, while Bgl7A exhibited high activity at pH 1.5, 3.5 and 5.0 (maxima). Both enzymes remained active at pH 1.0–8.0. The temperature optimum of Bgl7AM was 70°C, which was 10°C higher than that of Bgl7A. Moreover, thermostability of Bgl7AM was improved. After incubation at 70°C for 15 min, Bgl7AM retained 50% of the initial activity while Bgl7A remained less than 30%.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Property comparison of recombinant endo-β-1,3-1,4-glucanase expressed in maize (BGL7AM) and P. pastoris (BGL7A).A Effect of pH on β-glucanase activity of BGL7AM and BGL7A at 60°C. B pH stability of BGL7AM and BGL7A. After incubation at 37°C for 1 h in buffers ranging from pH 1.0 to 9.0, the β-glucanase activity was assayed in 100 mM citric acid-Na2HPO4 (pH 5.0) at 60°C. C Temperature-dependent activity profiles of BGL7AM and BGL7A in 100 mM citric acid-Na2HPO4 (pH 5.0). D Thermostability of BGL7AM and BGL7A pre-incubated at 70°C at pH 5.0. The aliquots were removed at different time points then measure residual β-glucanase activity at 60°C and pH 5.0. Error bars represent the standard deviation of triplicate measurements.\ndoi:10.1371/journal.pone.0081993.g004\n\nEvaluation of anti-inactivation stability in feed pelleting\nThe β-glucanase activities of Bgl7A (from P. pastoris) and Bgl7AM (from transgenic maize seeds) were determined after feed pelleting at 70°C and 80°C, respectively (Table 2). The initial β-glucanase activities in transgenic line or in Zheng 58 by supplementation of Bgl7A were set to 77,860 U/kg and 85,350 U/kg, respectively. After pelleting at each of the tested temperatures, Bgl7A lost more activities than Bgl7AM. In combination with the data of enzyme characterization, Bgl7AM was more excellent than Bgl7A even though they had complete identical amino acid sequences.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Stability of of Bgl7A and Bgl7AM during feed pelleting*.doi:10.1371/journal.pone.0081993.t002\nSo far several β-glucanase genes have been introduced into plants for different purposes. Endo-β-1,3-glucanase (laminarinase) can defend plants against fungal pathogens, introduction of its coding genes into crops is a plausible strategy to develop durable resistance against fungal pathogens [21]. Over the last two decades, transgenic plants harboring endo-β-1,4-glucanase (cellulase) genes have taken more attention for conversion of cellulosic biomass into fermentable sugars [22], [23]. Production of recombinant endo-β-1,4-glucanases E1 in transgenic plants have been reported in Arabidopsis [24], leaf and root tissues of maize [25], [26] and rice seeds [27]. Endo-β-1,3-1,4-glucanase (lichenase) is an important enzyme additive in monogastric animal feed to decompose β-glucan in cereals [3], [6], [28]. Up to now, β-1,3-1,4-glucanases have been expressed in transgenic barley [29], [30] and potato [31] for feed purpose. However, maize seed has never been used for production of endo-β-1,3-1,4-glucanase. Here we developed a transgenic maize line that overexpressed an endo-β-1,3-1,4-glucanase from Bispora sp. MEY-1 in seeds. Compared with enzyme production by microbial fermentation and other transgenic crops, transgenic maize seed has several advantages, such as low cost production, cultivation worldwide and direct utilization in animal feed. On the other hand, the genetic manipulation of maize is more easily. For feed industry's interest, maize seed as the major feed ingredient represents an ideal bioreactor to produce feed enzymes.About 65% of the maize seed produced in China is used as feed. If maize seeds express sufficient endo-β-1,3-1,4-glucanase, no supplementation of microbial glucanase will be required. To achieve high-level expression of bgl7Am in maize seed, several strategies have been utilized in combination, including a synthetic gene with preferred maize codons [15], a strong tissue-specific promoter, and an excellent transformation receptor with high competence and regeneration capacity that improves the transformation efficiency [13], [14]. As a result, the average and maximum glucanase activities in maize seeds without purification and enrichment were up to 239,300 and 779,800 U/kg seeds, respectively (Table 1). Previous feeding trials have shown that the effectiveness of glucanase as a feed additive was maximized at approximately 30,000 units per kg of diets [5]. Typically maize grains constitute 50% of the animal diet, thus the transgenic maize seeds having an average glucanase activity of 239,300 U/kg is high enough to substitute the glucanase supplement. When the transgenic maize line developed in this study is propagated in field, it will enhance the nutritive values of glucan-abundant grains such as wheat and barley. The development of transgenic maize will not only reduce the loss of resources and simplify the production process, but also provide an environmental friendly approach to produce enzymes.Moreover, Bgl7AM has good thermostability and excellent acidic stability, which are important factors for supplementation to animal feed. Thermostability is a key index of feed enzyme because of the high temperature during feed processing. Since most β-1,3-1,4-glucanases are not stable during coating of feed pellets (70–90°C), selection of a thermostable β-1,3-1,4-glucanase with high activity is of great interest to the animal feedstuff industries [8], [32], [33]. Bgl7AM retains most activities after pelleting at 80°C. This thermostability allows it to survive the heat generated from maize pressing into feed pellets and pasteurization. Similar results that plant-derived enzymes showed better stability have been reported. [34], [35]. This phenomenon might be ascribed to the different folding patterns and disulphide bond formations in microbes and plants [35]. Protection from maize seed starch might be the other cause.Furthermore, feed enzyme should be stable within the acid environment of monogastric animals' digestive tract, in which the pH value is lower than pH 3.0 in stomach (pH 1.3–3.5 for pigs and pH 2.8–4.8 for chickens) [36]. An acidic-tolerable β-glucanase has been isolated from Trichoderma koningii ZJU-T, with optimal activity at pH 2.0 [33]. Molecular modification approaches have been employed to enhance the activity of a β-1,3-1,4-glucanase at acidic pH [37]. Compared with counterparts, Bgl7AM is highly active and stable within pH 1.0–4.0, and retains above 90% of its activity at pH 3.0, the average pH in the animal digestive tract.This study provides an environment-friendly and low-cost approach to produce transgenic maize with social and ecological significance. It's the first report that produces a biologically active endo-β-1,3-1,4-glucanase in transgenic maize seeds. Approaches to increase the seed glucanase activities are preceding, including selection of more transgenic events and application of stronger promoters. In the future studies, we'll evaluate its direct application effectiveness in animal feed by comparison with traditional feed supplemented with glucanases."
        },
        "10.1371/journal.pone.0080591": {
            "author_display": [
                "Vincenzo Lionetti",
                "Simone Lorenzo Romano",
                "Giacomo Bianchi",
                "Fabio Bernini",
                "Anar Dushpanova",
                "Giuseppe Mascia",
                "Martina Nesti",
                "Franco Di Gregorio",
                "Alberto Barbetta",
                "Luigi Padeletti"
            ],
            "title_display": "Impact of Acute Changes of Left Ventricular Contractility on the Transvalvular Impedance: Validation Study by Pressure-Volume Loop Analysis in Healthy Pigs",
            "abstract": [
                "Background: The real-time and continuous assessment of left ventricular (LV) myocardial contractility through an implanted device is a clinically relevant goal. Transvalvular impedance (TVI) is an impedentiometric signal detected in the right cardiac chambers that changes during stroke volume fluctuations in patients. However, the relationship between TVI signals and LV contractility has not been proven. We investigated whether TVI signals predict changes of LV inotropic state during clinically relevant loading and inotropic conditions in swine normal heart. Methods: The assessment of RVTVI signals was performed in anesthetized adult healthy anesthetized pigs (n = 6) instrumented for measurement of aortic and LV pressure, dP/dtmax and LV volumes. Myocardial contractility was assessed with the slope (Ees) of the LV end systolic pressure-volume relationship. Effective arterial elastance (Ea) and stroke work (SW) were determined from the LV pressure-volume loops. Pigs were studied at rest (baseline), after transient mechanical preload reduction and afterload increase, after 10-min of low dose dobutamine infusion (LDDS, 10 ug/kg/min, i.v), and esmolol administration (ESMO, bolus of 500 µg and continuous infusion of 100 µg·kg−1·min−1). Results: We detected a significant relationship between ESTVI and dP/dtmax during LDDS and ESMO administration. In addition, the fluctuations of ESTVI were significantly related to changes of the Ees during afterload increase, LDDS and ESMO infusion. Conclusions: ESTVI signal detected in right cardiac chamber is significantly affected by acute changes in cardiac mechanical activity and is able to predict acute changes of LV inotropic state in normal heart. "
            ],
            "publication_date": "2013-11-19T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 606,
            "shares": 0,
            "bookmarks": 1,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0080591",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0080591&representation=PDF",
            "fulltext": "IntroductionThe ideal index of ventricular performance, which describes the pumping properties of the left ventricle, should be sensitive to changes of left ventricular (LV) inotropic state, independent of loading conditions, easily reproducible and safe [1]. Left ventricular pressure-volume (PV) loops are considered to be the gold standard for complete hemodynamic assessment during each cardiac cycle [2]–[3] and are widely used in research to evaluate the LV inotropic state and to quantify mechanical ventricular dyssynchrony in large animal model [4]–[5] and humans [6]–[7]. However, the PV loops assessment by conductance catheter is an invasive and complex approach, and its clinical application does not enable a permanent monitoring of ventricular contractility in patients with pacemaker implants.\nIn fact, the continuous real-time assessment of the LV myocardial contractility by an implanted device could provide useful information to calibrate the electrical stimulation in proportion to the real inotropic state of the heart and to monitor maladaptive hemodynamic response to electrical therapy of the heart [8] and the overall patient care [9]. Permanent pacemakers and defibrillators could be equipped with haemodynamic sensors suitable for diagnostic applications as well as for the autoregulation of the device itself [10]. At present, the haemodynamic guidance has been proposed in the adaptation of the pacing rate to the metabolic demand [11]–[15] and in atrio-ventricular (AV) and interventricular (VV) delay setting [16]–[18].\nThe signal of intracardiac impedance is a well known parameter for assessing in real time the acute changes of LV performance of failing heart at rest [19] and during adrenergic stress [17], [20]. Pacing leads bearing just standard electrodes can be used to assess cardiac impedance [21]. Different electrode arrangements are proposed for impedance assessment [17], [21]–[24], namely the unipolar configuration (current is applied between the ventricular tip electrode and the device case and the corresponding voltage drop is measured at the same spots), the bipolar configuration (current is applied between two cardiac electrodes and the corresponding voltage drop is measured at the same spots), or different multipolar configurations (current is applied between two cardiac electrodes and the voltage drop is measured on another electrode pair). The transvalvular impedance (TVI) is a particular example of bipolar configuration, where the atrial pole is represented by the ring electrode placed in right atrium and the ventricular pole by either the tip or ring electrode of the right ventricular lead. The timing of rise and decrease during the cardiac cycle suggests that TVI fluctuation reflects opposite changes in ventricular volume, as TVI increases in the ejection phase and decreases during ventricular filling [15], [25]–[27]. Since it is known the relationship between changes in right ventricular TVI and LV stroke volume (SV) during electrical and pharmacological stimulation in patients [25], it is unknown whether the RV TVI depends upon changes of LV inotropic state. In our study, we have analysed different TVI signals and performed a validation study of the end-systolic TVI (ESTVI) as index of LV contractility. For this purpose, we analysed ESTVI and simultaneous PV loops obtained by conductance catheter during different acute loading and inotropic conditions in healthy pigs.\nMaterials and Methods\nAnimal handling and instrumentation\nSix healthy sexually mature male farm pigs (35±2 kg, body weight), fasted overnight, were sedated with a cocktail of tiletamine hydrochloride and zolazepam hydrochloride (8 mg/kg i.m.) and\npremedicated with atropine sulfate (0.1 mg/kg). General anesthesia was subsequently induced with propofol (2–4 mg/kg) and maintained with 1% isoflurane in 60% air and 40% oxygen. Mechanical ventilation was adjusted based on arterial blood gas values [28]. Body temperature was maintained at 36.5°–39°C. Arterial pressure was measured via a fluid filled catheter inserted trough the right carotid artery and attached to a P23ID strain-gauge transducer; although, LV pressure, the maximum and minimum of the first derivative of LV pressure (dP/dtmax and dP/dtmin) and LV volume were measured using a pressure-volume conductance catheter (Millar Instruments Inc, Houston TX, USA) percutaneously inserted through the femoral artery and carefully advanced into the LV cavity under fluoroscopic guidance [29]. A 8F Fogarty large balloon occlusion catheter (Edwards Lifesciences, USA) was advanced into the inferior vena cava (IVC) through a right femoral venotomy; although, an additional large balloon catheter was advanced into the descending thoracic aorta trough the left carotid artery [29]. TVI was measured by standard leads for permanent pacing (Medico 366 and 400, Medico Spa, Padova, Italy) inserted trough the right external and internal jugular vein and advanced in right atrium and ventricular apex under fluoroscopic guidance. While the J-shaped atrial lead was positioned in the right appendage, the RV lead was positioned in the mid-low septum, as previously described in patients [25]–[27].\n\n\nHemodynamic measurements\nThe hemodynamic parameters were determined during one respiratory cycle and comprised the heart rate (HR), the mean arterial pressure (MAP), LV end-diastolic (EDV) and end-systolic volume (ESV), LV end-diatsolic (EDP) and end-systolic pressure (ESP), the maximum derivative of change in pressure rise over time (dP/dtmax), the maximum derivative of change in pressure fall over time (dP/dtmin). The stroke volume (SV) was calculated as the difference between EDV and ESV. LV DP/dtmax is an index of the isovolumetric phase of the contraction, which is sensitive of preload, but not of afterload [30]. Conversely, the slope of the end-systolic pressure-volume relationship (ESPVR) (Ees) of the left ventricle was calculated using the first 7–10 beats during brief IVC occlusion [5]. Ees is an ejection phase measure of LV contractility, which is minimally affected by preload and afterload [31]. In addition, we measured the LV contractile state by calculation of LVESP/LVESV at each beat [1]. Effective arterial elastance (Ea) was calculated as an index of LV afterload [32]. The measurement of the area of the LV pressure-volume loop during a cardiac cycle was used as an index of stroke work (SW) [33].\nThe load-dependent and independent LV performance was evaluated simultaneously to the TVI measurement during each experimental condition. All hemodynamic signals were recorded on an eight-channel Gould polygraph recorder (model 5900; Gould Inc., Cleveland, OH, USA). The analogic signals were recorded through an analogic-digital interface (National Instruments), at a sampling rate of 250 Hz [4]. Digitized data were analysed off-line by custom-made software. Pressure–volume data were analysed off-line by a single observer. All conductance volumes were corrected for parallel conductance and the gain constant Î±.\n\n\nTVI measurements\nThe atrial and the ventricular pacing leads were connected in parallel with an external dual-chamber stimulator (PSA 490, Medico Spa, Padova, Italy) and with the custom-made TVI recorder. TVI was derived between the atrial ring and ventricular tip electrodes, applying subthreshold current pulses of 40 µA at 4 KHz. The waveform was sampled and stored at 1 KHz, together with the atrial and ventricular electrograms (AEGM, VEGM), one surface ECG lead, and one accessory signal (either LVP or LVV) derived as analog output of the PV recording equipment. All tracings were displayed in real-time and then analyzed off-line using commercial software (AcqKnowledge, Microsoft, USA). The VEGM signal represented the time marker of electrical ventricular activation and the trigger of TVI measurements. TVI increased in systole and decreased in diastole. The minimum and maximum values recorded in 500 ms after VEGM detection were considered, respectively, as the end-diastolic (ED) and ES TVI. The peak-to-peak TVI amplitude (pkpkTVI) was calculated as the difference between ESTVI and EDTVI. The ratio of the maximum TVI increase in 100 ms to the pkpkTVI in each cardiac cycle (TVI fractional change in 100 ms: TVIfc) represented an index of TVI rate of rise.\n\n\nExperimental Protocol\nThe experiments were conducted in anesthetized and hemodynamically stable animals. We first simultaneously measured hemodynamic and TVI parameters at rest. We accurately evaluated the feasibility of conductance catheter-derived P-V loops and TVI waveform during transient reduction of preload, acute increase of LV afterload, during low dose dobutamine stress (LDDS) and following esmolol infusion. Acute reduction of LV preload was induced by transient occlusion of the IVC via inflation of large balloon [4] in order to produce a 20% drop in systolic blood pressure [34]. Conversely, the brief inflation of the intra-aortic balloon afterwards induced a transient increase of LV afterload [34]. The LV function and TVI signals were afterwards assessed during transient inotropic stimulus by low dose dobutamine stress (LDDS, 10μg/kg/min i.v. for 10 minutes), as previously described [35]. LDDS is a well-established test used to provide a quantitative assessment of the LV contractile reserve in both large animal models [28], [35] and patients [36]–[37].\nAfter a 10-minute washout period, the TVI and hemodynamic parameters were evaluated during transient beta-adrenergic blockade induced by esmolol (bolus of 500μg and continuous infusion of 100 µg·kg−1·min−1 ), a cardioselective beta1 receptor blocker [38]. Basal measurements were repeated before any test in order to update all reference values. Once the experimental protocol was completed, the anesthetized pigs were euthanized with an intravenous injection of saturated KCl solution. Animal instrumentation and experimental protocol were approved by the Animal Care Committee of the Italian Ministry of Health and was in accordance with the Italian law (DL-116, Jan. 27, 1992), which is in compliance with the National Institutes of Health publication Guide for the Care and Use of Laboratory Animals.\n\n\nStatistical Analysis\nAll data are mean values ± standard error of the mean. SPSS 11 for Windows (SPSS Inc, Chicago, IL, USA) was utilized for statistical analysis. Intragroup comparisons were performed using the one way analysis of variance followed by the Bonferroni post-hoc pairwise multiple comparisons.\nThe changes of ESTVI, LVSV, dP/dtmax and LVESP/LVESV were calculated as the ratio between each parameter at baseline and during hemodynamic test. Correlations between groups of values were evaluated calculating the best fit, based on least-squares regression analysis. A good correlation was accepted at R value of ≥0.6. For all the statistical analyses, significance was accepted at P value of <0.05.\n\nResults\nHemodynamic and P-V loop analysis\nLeft ventricular function during changes in loading conditions. As shown in Table 1, MAP, LVESP, LVEDP, LVdP/dtmax, LVdP/dtmin, LVESV, LVEDV were significantly decreased during preload reduction in the presence of unchanged heart rate and reduced LVSV by 37.5±1.86% (p<0.05) compared to baseline (16±1.6 vs 24.5±1.5 ml). The LVESP/LVESV and LVSW were respectively reduced by 10.8±7.1 (p<0.05) and 55±15% (p<0.01) compared to normal loading conditions. Conversely, MAP, LVESP, LVdP/dtmax, LVdP/dtmin and LVESP/LVESV were significantly increased during afterload increase in the presence of reduced LVSV by 54.16±2.8% (p<0.01) compared to baseline (10.5±2 vs 24.5±1.5 ml) (Table 1). The LV Ees and Ea were respectively increased during afterload increase by 389.8±29.5 (p<0.00001) and 384.2±18.6 (p<0.001) compared to condition of reduced LV preload (Table 1).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Absolute values and percentage changes compared with respective baseline values during changes in loading conditions.doi:10.1371/journal.pone.0080591.t001Left ventricular function during changes in inotropic state. As shown in Table 2, HR, MAP, LVESP, LVdP/dtmax, LVdP/dtmin, LVESP/LVESV and LVSW were significantly increased during LDDS in the presence of increased LVSV by 41.6±1.76% (p<0.05) compared to baseline (35±1.8 vs 24±1.5 ml). Similarly, the LVEes was increased by 157±4.5 (p<0.001) compared to baseline. Conversely, the administration of esmolol significantly reduced HR, LVESP, LVdP/dtmax, LVdP/dtmin, LVESP/LVESV and LVSW in the presence of reduced LVSV by 40±2.86% compared to baseline (14±2.4 vs 24±1.5 ml) (p<0.05) (Table 2). The LVEes was significantly reduced by 38±1% compared to resting condition (baseline).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Absolute values and percentage changes in contractile indices compared with respective baseline values during inotropic modulation.doi:10.1371/journal.pone.0080591.t002\n\nTVI signals during changes in loading and inotropic conditions\nAs shown in Table 1 and Figure 1, the acute reduction of LV preload at rest induced a significant increase of ES and EDTVI signals compared to baseline, and a significant reduction of the pk-pk TVI. The above TVI signals reached baseline values following complete IVC balloon deflation (data not shown). Conversely, the transient increase of the LV afterload caused a significant increase of EDTVI in the presence of unchanged ESTVI compared to baseline, yet the pk-pk TVI was reduced by 29±18.75% (Figure 1). As shown in Table 2 and Figure 1, the ESTVI was significantly reduced by 2.23±0.9% during LDDS in the presence of small reduction of EDTVI, and also the pk-pk TVI was reduced.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  ESTVI (A), EDTVI (B) and pk-pk TVI (C) changes during different loading and inotropic conditions. * p<0.05 vs baseline; # p<0.05 vs LDDS.doi:10.1371/journal.pone.0080591.g001The abovementioned TVI values during were reduced compared to baseline following esmolol administration, yet pk-pk TVI was unchanged (Table 2 and Figure 1).\n\n\nRelationship between RVTVI signals and LV contractility\nAs shown in Figure 2A, ESTVI was significantly and inversely related to LVSV during preload reduction. The direct relationship between ESTVI and LVdP/dtmax during preload reduction was significant and weak (Figure 2B). Conversely, ESTVI was directly and significantly related to dP/dtmax (Figure 2D) during LV afterload increase. As shown in Figure 3, we found a direct and significant correlation between changes in ESTVI and LVSV (Figure 3A) or dP/dtmax (Figure 3B) in response to LDDS. In addition, there was a direct a significant correlation between changes in ESTVI and dP/dtmax following esmolol infusion (Figure 3D). As shown in Figure 4B, C and D, we found a significant and direct correlation between ESTVI and LVEes during increasing of LV afterload, LDDS and esmolol infusion. No significant correlations were found between the abovementioned hemodynamic parameters and EDTVI or pk-pk TVI (data not shown).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  RV ESTVI-LV contractility relationship during changes in different loading conditions.Correlations between changes in RV end systolic TVI (ESTVI) and in left ventricular (LV) stroke volume (SV) or maximum of the first derivative of LV pressure (dP/dtmax) during preload reduction (A,B) and increase of LV afterload (C, D). Changes normalized to baseline values.\ndoi:10.1371/journal.pone.0080591.g002\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  RV ESTVI-LV contractility relationship during changes in different inotropic conditions.Correlations between changes in RV end systolic TVI (ESTVI) and in left ventricular (LV) stroke volume (SV) or in maximum of the first derivative of LV pressure (dP/dtmax) during low dose dobutamine stress (LDDS) (A,B) and following esmolol infusion (C, D). Changes normalized to baseline values.\ndoi:10.1371/journal.pone.0080591.g003\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  RV ESTVI- LV Ees relationship during changes in different loading and inotropic conditions.Correlation between changes in RV end-systolic TVI (ESTVI) and left ventricular Ees during preload reduction (A), increase of afterload (B), low dose dobutamine stress (C) and following esmolol infusion (D). Changes normalized to baseline values.\ndoi:10.1371/journal.pone.0080591.g004\nThe continuous and minimally invasive real-time assessment of the myocardial contractility is a clinically relevant issue, mainly in patients receiving a pacemaker-induced electrical stimulation of the heart. The implantable haemodynamic sensors, currently proposed for clinical use, are generally aimed at recording intracardiac pressure or other indirect markers of the cardiac contraction strength that have been correlated with the LV dP/dt max [39], [40]. However, dP/dt max is an index of the isovolumetric phase of the LV contractile function, which is more sensitive of preload and does not reflect the myocardial inotropism of the left ventricle [30]. Conversely, the slope of the end-systolic pressure-volume relationship (ESPVR) should be determined to measure the mechanical performance of the myocardium in a load-independent fashion [31–1] and to assess the properties of innovative inotropic sensors, such as TVI, under different hemodynamic conditions.In our experimental study, preload changes were induced by IVC occlusion that reduced venous return, RV filling and pulmonary output. A secondary decrease of LVSV, LVdP/dtmax, LVESP/LVESV and LVSW was clearly evident within 2 cycles after beginning balloon inflation. In this experimental condition, both EDTVI and ESTVI were increased, and pk-pkTVI was reduced (Figure 1). In the presence of preload reduction, we found that ESTVI significantly predicts the changes of LVSV (Figure 2A). Our experimental findings confirmed the inverse relationship between RVTVI and RV volume [15], [25]–[27]. In fact, TVI increased whenever the RV volume was supposed to decrease, providing correct information on relative changes in RV preload and stroke volume. However, ESTVI was not so sensitive to predict changes of LVdP/dtmax (Figure 2B) and Ees (Figure 4A) at that magnitude of preload reduction.After recovering the baseline hemodynamic values, an acute increase of LV afterload was obtained by partial occlusion of the thoracic aorta, which entailed a prompt rise in LV pressures and Ees. The LVSV was decreased and LVESV was increased compared to baseline. The acute reduction of the LV output caused a transient reduction of RV filling, with associated reduction in the pulmonary SV, which at the steady state must be equal to LVSV. In agreement with this model, the EDTVI and ESTVI were increased and pk-pkTVI was decreased (Figure 1). However, the ESTVI did not predict transient reduction of LVSV (Figure 2C) and LVdP/dtmax (Figure 2D) in the presence of increased isovolumetric contractility. Conversely, we found that ESTVI accurately predicts the increase of LV Ees during acute increase of LV afterload (Figure 4B).Changes in RV preload were generally coupled with corresponding LV SV modifications. However, the observed increase of ESTVI during rise of LV afterload probably was mostly related to the reduction of RV preload. Knowledge of TVI changes under different hemodynamic conditions might allow a comprehensive interpretation of simultaneous modifications in the contraction strength. It could be essential in the regulation of the AV delay in a dual-chamber stimulator, which usually aims at the optimal ventricular filling (i.e.: the highest possible preload).The direct relationship found between ESTVI and LVEes during inflation of the aortic balloon suggested ESTVI as potential index of LV inotropic state. For this purpose, we performed the abovementioned measurements during selective inotropic conditions.Low dose dobutamine stimulation induced a marked positive inotropic response characterized by the expected significant increase of LV pressures, stroke work and LVEes. In spite of the clear-cut effects on pressure-related parameters, the LVSV was not significantly increased compared to baseline, probably due to the high cardiac rate and consequent shortening of the total filling time. We observed, in fact, a small reduction of the ESTVI and EDTVI in the presence of unchanged pk-pk TVI compared to baseline condition. However, we observed that ESTVI accurately predicts changes of LV contractility (Figures 3B) during positive inotropic stimulus even in the absence of significant preload changes. Our experimental data confirmed previous study demonstrating that dobutamine markedly increases RV contractility and intracardiac impedance [39]. In order to better validate the inotropy-sensing of the ESTVI, we repeated the TVI and PV loops assessment following the administration of esmolol, a selective blocker of cardiac beta-adrenergic receptors type 1. Esmolol induced a large reduction of LV dP/dt max, SW ad Ees in the presence of marked reduction of LVSV. Surprisingly, we observed a significant reduction of ESTVI and EDTVI in the presence of unchanged pk-pk TVI. In addition, we found a direct and significant correlation between changes of ESTVI and LV dP/dtmax (Figure 3D) or LV Ees (Figure 4D). On the basis of our results, we suppose that the SV-ESTVI inverse relationship was unpredictable in the presence of negative inotropic stimulus (LVEes < 2 mmHg/ml) and reduced preload. Since changes of myocardial electrical activity does not affect myocardial electrical impedance measurements [41], it is conceivable that the changes of ES TVI signals in the presence of blocks of cardiac beta-1 adrenoreceptors mainly reflect changes of myocardial impedance rather than right ventricular blood pool impedance. In our experimental model, the infusion of esmolol could have compromised contractile function to such an extent as to cause the formation of moderate myocardial edema [42], which is expected to reduce the myocardial electrical impedance [43]. This finding supports ESTVI as sensor of myocardial contractile function related to changes of myocardial impedance.Our findings are encouraging and suggest that ESTVI has good potential for use in the permanent beat-by-beat surveillance of the LV inotropic state in patients with pacemakers. However, there is still much investigation required before ESTVI is considered a reliable sensor of LV inotropic state in the presence of disarrangement of myofilaments, myocardial dyssynchrony or heart failure.The study was performed in the presence of intrinsic AV conduction. It remains to be established whether the TVI properties reported in this study might be affected by changes in right ventricular pacing."
        },
        "10.1371/journal.pone.0067101": {
            "author_display": [
                "Ecaterina Vasluian",
                "Ingrid G. M. de Jong",
                "Wim G. M. Janssen",
                "Margriet J. Poelma",
                "Iris van Wijk",
                "Heleen A. Reinders-Messelink",
                "Corry K. van der Sluis"
            ],
            "title_display": "Opinions of Youngsters with Congenital Below-Elbow Deficiency, and Those of Their Parents and Professionals Concerning Prosthetic Use and Rehabilitation Treatment",
            "abstract": [
                "Background: Youngsters with unilateral congenital below-elbow deficiency (UCBED) seem to function well with or without a prosthesis. Reasons for rejecting prostheses have been reported earlier, but unfortunately not those of the children themselves. Furthermore, reasons for acceptance are underexplored in the literature. Objectives: To investigate opinions of children and early and late adolescents with UCBED, and those of their parents and healthcare professionals, concerning (1) reasons to wear or not to wear prostheses and (2) about rehabilitation care. Methods: During one week of online focus group interviews, 42 children of 8–12 y/o, early and late adolescents of 13–16 and 17–20 y/o, 17 parents, and 19 healthcare professionals provided their opinions on various topics. This study addresses prosthetic use or non-use of prosthetics and rehabilitation care. Data were analyzed using the framework approach. Results: Cosmesis was considered to be the prime factor for choosing and wearing a prosthesis, since this was deemed especially useful in avoiding stares from others. Although participants functioned well without prostheses, they agreed that it was an adjuvant in daily-life activities and sports. Weight and limited functionality constituted rejection reasons for a prosthesis. Children and adolescents who had accepted that they were different no longer needed the prosthesis to avoid being stared at. The majority of participants highly valued the peer-to-peer contact provided by the healthcare professionals. Conclusions: For children and adolescents with UCBED, prostheses appeared particularly important for social integration, but much less so for functionality. Peer-to-peer contact seemed to provide support during the process of achieving social integration and should be embedded in the healthcare process. "
            ],
            "publication_date": "2013-06-24T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 993,
            "shares": 1,
            "bookmarks": 4,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0067101",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0067101&representation=PDF",
            "fulltext": "IntroductionCongenital upper limb defects affect between 19.5 and 21.5 births per 10,000 [1], [2]. A considerable group of congenital upper limb anomalies result in reduction deficiencies (5.56 births per 10,000) [3]. Children with such impairments often receive prosthetic treatment in order to improve their functionality and to avoid developmental problems [4]. It is doubtful that prostheses fulfill these aims, since the rejection rate is high 35–45% [5], while no difference in functionality is seen between prostheses wearers and non-wearers [6], [7]. Furthermore, prosthesis use seems to reduce manipulation, exploration, variation, and adaptation in the daily-life activities of young children with unilateral congenital below-elbow deficiency (UCBED) [8]. By developing compensatory strategies and auxiliary movements using other body parts (e.g., head, legs, and trunk) to perform a task [9], children also tend to be more independent without prostheses [4]. Thus it is still unclear why some continue wearing prostheses.\nProstheses are typically accepted when people with upper limb impairment face a great deal of difficulty in daily-life activities, have a higher level of amputation (above the elbow), when the abilities of the prostheses are considered to be “fair,” and when wearers are satisfied, in general, with their healthcare [10]–[12]. Advantages of early fitting with a prosthesis in children with UCBED are inconclusive in the literature [13]–[15] and are not associated with satisfaction with the prosthesis, functional use of the prosthesis, or motor skills [16].\nProstheses are often rejected when people do not experience many challenges in daily-life activities, have lower levels of amputation, are unsatisfied with certain features of the prostheses (sweating, cosmesis, or interface discomfort), or are unsatisfied with all healthcare areas (i.e., fitting, follow-up, repair, training, and information provision) [10]–[12]. Abnormal truncal movements that usually accompany the performance of activities in prosthetic users may also determine the rejection of prostheses [17]. Parents also play a role in the rejection of prostheses mostly because of disappointment with the limited benefits of prostheses, insufficient involvement in the treatment, and disappointment regarding socio-emotional guidance [13].\nThe literature is generally concerned with the reasons for rejection of prostheses in adults and provides abundant information as to quantitative outcomes. Information on self-reported reasons that elucidate why children and early and late adolescents choose or continue to wear a prosthesis is scarce. Knowing how psychosocial factors, vis-à-vis the more technical aspects, contribute to the rejection or acceptance of the prosthesis would be of great interest. Children’s and adolescents’ ideas about what aspects could be improved in a prosthesis have yet to be investigated. The rationale or role of the parents in choosing a prosthesis or in the decision to wear one is also unclear. The approach healthcare professionals take toward improving children’s quality of life, including prosthetic prescription, has been previously described [18]–[20]. Nevertheless, there is not much information about patients’ feedback about rehabilitation care, especially the feedback from children. Therefore, the direction of the current study is aimed at elucidating these aspects of how youngsters with UCBED function; the means chosen is a qualitative study design.\nThe aims of this study are (1) to investigate the opinions of children and early and late adolescents with UCBED, and that of their parents and professionals as to the reasons to wear or not to wear prostheses, and their opinions about (2) rehabilitation care, and to compare the differences in opinions and perspectives among children, early and late adolescents, parents, and healthcare professionals.\nMethodsThe current study is a part of a larger study which focused on the aspects of functioning of children and adolescents with UCBED: activities, participation, prosthetic use or non-use, psychosocial functioning, and rehabilitation care. The results concerning activities and participation, and those concerning psychosocial functioning have been published by De Jong and colleagues [21], [22]. The aim of this published first study was to assess whether youngsters with UCBED encounter activity or participation limitations and, if so, what are their coping strategies for those limitations. The published second study investigated the psychosocial functioning of youngsters with UCBED, with a focus on their feelings about their deficiency and what their coping strategies are in terms of those feelings. The larger study as a whole was designed as a qualitative research study, using online focus group interviews for the data collection.\n\n1 Study Design\nQualitative studies offer the possibility of gaining insight into underexplored research topics. Online focus group interviews are useful for exploring opinions, for obtaining a range of views from different age categories, and for observing interactions among a wide range of participants. Compared to classic face-to-face focus groups, the online version offers anonymous participation which minimizes the influence of social pressure and favors a more open interaction; it provides a comfortable environment, and by avoiding the transcription process is inexpensive and time-efficient [23]–[25]. Online focus group interviews were considered appropriate for this study, because they are specifically suitable when rare diseases are the subject of interest and participants live in a widespread area. A group of 8 to 15 participants is believed to work successfully in asynchronous focus groups [26]–[28], and even 19 participants have been used in online settings [29].\n\n\n2 Ethics Statement\nEthical approval was obtained from the Medical Ethical Committee of the University Medical Center Groningen, the Netherlands (number M09.079327). Each participant or child’s parent/guardian provided an informed consent, and completed a demographic questionnaire prior to the beginning of the study. For the participation of the youngsters in the study, informed consent was obtained from the parents/guardians of those participants aged 8–11 y/o, from the participants aged 12–17 y/o and also from their parents/guardians, and it was obtained from the participants only, when aged 18–20 y/o. Regardless of whether their child actually participated, parents/guardians signed a separate informed consent allowing their own participation in the focus group interviews.\nThe participants were informed that they could contact an independent physician for any distress they experienced and that they could withdraw from the study at any time without any consequences. The confidentiality of the participant was also ensured by assigning a codename (for example, children were given names of types of fruit) to every participant. These codenames were used by the participants during the study and for the purpose of analysis. The credentials of the participants were accessible only to the researchers and to no one else.\n\n\n3 Population\nFive categories of participants were considered: children, early and late adolescents, parents, and healthcare professionals who had worked with the UCBED population.\n3.1 Inclusion criteria for children and early and late adolescents.Purposive sampling was used [30], meaning that both prosthetic wearers and non-wearers with particular characteristics were selected: (1) aged between 8 and 20 years old, and (2) UCBED at a transradial level with a non-syndromic cause. Three categories were defined in concordance with school age: children aged 8–12 years old (primary school), early adolescents aged 13–16 years old (secondary school), and late adolescents aged 17–20 years old (secondary or higher education). By grouping participants in age categories, we aimed to detect specific age-related opinions on the research topics.\n\n3.2 Inclusion criteria for parents and healthcare professionals.Eligible parents were those whose children met the criteria of inclusion for children and early and late adolescents. Eligible healthcare professionals were those with work experience with the UCBED pediatric group.\n\n3.3 Exclusion criteria.Individuals with insufficient proficiency in the Dutch language and limited mental capacity were excluded.\n\n3.4 Recruitment.Participants (except for healthcare professionals) were recruited through national rehabilitation centers and patient organizations. Patient organizations advertized the study on their websites and in newsletters. Twenty-five random people per group were approached, taking into account age, gender, prosthetic wearing/non-wearing, and referral center. Participants received a package with detailed information, a form for informed consent, and a letter approved by the attending rehabilitation physician stating that the physician supports the study and inviting the child or the parent to participate. Professionals were approached through rehabilitation centers and orthopedic workshops in the Netherlands.\n\n\n\n4 Procedure\nAn expert provided methodological recommendations for designing and conducting the online focus group interviews. A website with five forums, one forum per group, was designed to facilitate the online focus group interviews. Participants were able to log in anonymously and post messages at any time of the day they preferred and from the location they preferred, within the timeframe of one week. Participants were instructed to omit names of people or rehabilitation centers.\nA question about a specific topic was posted every morning during the first five days. The last two days were assigned to open discussions between group participants. The participants who did not access the website on a particular day would receive a reminder the following day asking them to answer not only the current day’s question but also the question from the previous day. The participants were required to post at least one message as an answer to each of the five questions.\nThis study addressed aspects of the prosthetic use or non-use (day 3) and rehabilitation care (day 5), formulating queries as follows: “Tell us why you wear or do not wear a prosthesis,” “Tell us how you evaluate the rehabilitation team and technicians,” and “Do you have suggestions for improvement for them?” The rest of the topics were covered on other days: activities (day 1), participation (day 2), and psychosocial functioning (day 4).To ensure the correct understanding of the questions, the authors formulated them according to the participant’s age. The study questions and the website with its five forums were pilot-tested on a group of non-impaired children and independent adults. Minor difficulties with understanding the questions and with using the forums were encountered during the pilot test. The website and the questions were improved based on participants’ suggestions. To enable the comparison of perspectives between groups, parents and professionals were asked to express their feedback from the child’s perspective. Multiple perspectives are important for gaining a richer and broader understanding of the studied population [27], and to help clinicians find suitable solutions for the barriers experienced by the parties dealing with UCBED, that is, children, early and late adolescents, parents, and healthcare professionals.\nIn order to cover a broad area of interest, the questions were based on the World Health Organization’s International Classification of Functioning, Disability and Health for Children and Youth (ICF-CY). ICF-CY addresses issues on two levels: functioning and disability (body functions, body structures, activities, and participation), and contextual factors (environmental and personal factors) [31].\nIn order to address a possible bias induced by the lack of nonverbal communication, emoticons were made available. This enabled participants to express their feelings. Two moderators were online every day of the study from 8 a.m. to 11 p.m. to ensure that the online focus group interviews were conducted properly. They (IdJ and HRM) followed the moderator’s principles [32] to allay some of the moderator’s influences. The two moderators facilitated an interactive discussion between participants, but avoided influencing or dominating the discussions. Moderators refrained from rephrasing and evaluating statements; instead, they repeated comments using the participant’s words, and provided positive reinforcement by using neutral comments and probing. Both moderators were experienced in the field of child and hand rehabilitation, in addition to a background in human movement sciences, and were not involved in the treatment of the participants. HRM had experience with qualitative data collection methods in pediatric populations. Moderators were in contact, during the study period, with a very experienced rehabilitation physician working with this type of patient. Whenever clarifications of an answer were needed or new information/issues appeared, moderators posted additional questions to individual or all participants until no other new information appeared. This is similar to reaching data saturation [30]. All the data is available in the Dutch language or, if requested, a translation in English can be provided as well.\n\n\n5 Data Analysis\nThe most common methods in healthcare research used to analyze qualitative data are thematic analysis, grounded theory, and the framework approach [33]. The framework approach enables, as does thematic analysis, the corroboration of predefined research questions with the themes that emerge in the study. The advantage, however, is that it starts deductively from the clearly predefined objectives of the study, and is systematic and transparent, allowing easy access to the analytical process for the researcher as well as for other people [34]. The framework approach was used to analyze the data from this study. The approach contains five steps in which data is screened, condensed, and mapped into a thematic framework:\n5.1 Familiarization.The data generated on the days allocated to prosthetic use and rehabilitation care were read by three authors (EV, HRM, and CvdS). The rest of the data was also read to extract remarks about prosthetic use and rehabilitation care. Key ideas and themes were identified in a meeting with the three authors. The themes were derived from subjects frequently mentioned by the participants.\n\n5.2 Identifying a thematic framework.A coding framework was developed by EV to structure the collected information around key issues and themes (Table S1). Based on the aims of the study, the themes were grouped into main categories such as “reasons to wear a prosthesis,” “reasons not to wear a prosthesis,” or “tips for making a prosthesis better, adaptive devices, and other creative solutions.” In addition, for each main category a “general” theme category was created for data not matching the other themes. The data in the “general” theme category (e.g., frequency, time and place for wearing the prosthesis) when considered appropriate were made available in the Results section to provide detailed information for the themes.\n\n5.3 Indexing.EV and HRM tested the coding framework on ten percent of the data. After discussing minor differences in the manner of coding, agreement was reached upon the final version of the coding framework. EV correlated text pieces from the entire dataset with the appropriate code.\n\n5.4 Charting.EV displayed the pieces of text corresponding to the matched code and affiliation group in the form of a matrix. The columns contained the framework themes, while the lines contained each participant’s quotes on the theme. The quotes of wearers or non-wearers were thus easily identifiable from the matrix. The data accessibility of the matrix facilitated the analysis of the perspectives of the different groups, and of wearers and non-wearers.\n\n5.5 Mapping and interpretation.The resulting matrix was verified for the correct code by HRM and CvdS. In order to draw conclusions, EV, HRM, and CvdS analyzed the matrix separately. All three discussed the similarities and differences that occurred. Consensus was found on interpretations and conclusions.\n\n\nResultsFrom the total of 125 eligible participants, 77 (62%) participated in the study. Forty-two were either children, early adolescents, or late adolescents; 16 were parents; and 19 were healthcare professionals. No differences in age, gender, and provenance center were found between participants and non-participants. Non-wearers were represented by participants who had experience with prostheses (children 47%, early adolescents 54%, late adolescents 58%, and children of parents 63%), and participants without previous prosthetic experience (Table 1). Myoelectric prostheses were the most popular among wearers (Table 1). The healthcare professionals group consisted of five physiatrists, six occupational and physical therapists, six certified prosthetists, and two psychologists.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Characteristics of participants (n = 77).doi:10.1371/journal.pone.0067101.t001The participants were active in interacting with each other and with moderators. Each participant posted at least one message as an answer to each study question. Parents and healthcare professionals provided the most extensive answers.\n\n1 Reasons to Choose and Wear Prostheses\n1.1 Cosmetic, social, emotional, and identity reasons.Prostheses were chosen and worn primarily to provide cosmesis. Cosmesis helped participants of all age categories to manage relationships with the people in their environment. A frequently mentioned reason was to prevent adverse reactions like teasing and staring. For children, the prosthesis also offered a normal body appearance, while for early and late adolescents wearing a prosthesis allowed them to establish a good first impression and gave them a feeling of self-confidence.\n\n“For walking on the street I found it [the prosthesis] enjoyable; everyone finds you normal then, because you then have two hands.” (10 y/o girl, non-wearer with prosthesis experience)\n\nThe prostheses were worn every day, yet limited to being worn in public. In a safe home environment, the prosthesis had nothing to add and was therefore removed. The cosmesis also became more important during transitional periods such as puberty.\n\n“At puberty, I noticed that they ask for it [the prosthesis] from a cosmetic point of view… They especially want a prosthesis, for example, when they go to secondary school.” (Healthcare professional)\n\nProfessionals noticed that rejection of prosthesis use occurred in some children as soon as they became accustomed to a new environment.\n\n1.2 Functionality, manipulation, dexterity reasons.Along with cosmesis, functionality was important for children and adolescents in the process of choosing and wearing prostheses. Being able to experience activities of daily life in a normal way, to grip with the impaired upper limb, and curiosity about whether the prosthesis offered more dexterity also led participants to opt for prostheses.\n\n“A cosmetic prosthesis often has to be practical too; that’s why children/adolescents often want a myo [myoelectric prosthesis] then.” (Healthcare professional)\n“I wanted to know if it would be handy or not to wear a prosthesis. I wanted to try and become handier so that everything might be a bit easier. ” (13 y/o girl, non-wearer with prosthetic experience)\n\nWearers and non-wearers regarded the prosthesis as a “useful help accessory” for activities like managing school tasks, cutting, grasping, holding, and lifting.\nActivity-specific use was noticed in early and late adolescents for activities such as cycling and driving more safely, or for leisure purposes such as playing sports like volleyball, hockey, and football.\nAt other times, participants managed to function perfectly well without prostheses. However, activities such as lifting heavy objects, playing sports like volleyball or hockey, or doing some jobs such as delivering newspapers were not performed without prostheses by several early adolescents.\n\n1.3 Physical reasons.Some prosthetic wearers in every group considered wearing a prosthesis as something beneficial for muscle development, locomotion, posture, and balance.\n\n“When I play soccer, I have my prosthesis on… I have the feeling that I have better balance with it [the prosthesis] on and that I can manage better if I fall.” (16 y/o girl, wearer)\n\n\n1.4 Parents and prosthesis choice.Wearers in children’s and late adolescent groups specified that they had been too young to make the choice on their own when the choice was initially made. Parents had therefore played an important role in the process of acquisition and wearing of prostheses.\nSome parents had based their choice on the information and instructions about the benefits of early fitting that they had received from healthcare professionals. Other parents had followed their personal beliefs. They wanted to give the child the opportunity to experience a prosthesis so as to provide him/her with the knowledge to be able to make an informed choice later in life. Another reason for parents to choose a prosthesis for their child was that they had wanted to overcome the emotional stress of having a child with an upper-limb impairment.\n\n“When she was little, we allowed our daughter to use a prosthesis in the morning and go without the prosthesis in the afternoon. This way she could discover herself what was most suitable for her.” (Parent of a 13 y/o girl, non-wearer with prosthetic experience)\n“There are parents that want a prosthesis per se, because that way they see their child as more complete, and they find it less difficult for themselves and the family.” (Healthcare professional)\n\n\n\n\n2 Reasons not to Choose and Wear Prostheses\n2.1 Cosmetic, social, emotional, and identity reasons.Child non-wearers confronted the staring issue head on. They wanted acceptance and respect from the environment without having to wear a prosthesis. Early adolescents experienced self-confidence and self-identity without a prosthesis. Professionals explained this self-confidence on the part of adolescents as a result of realizing that they were able to perform everything just as well without the prosthesis.\nLate adolescents, non-wearers, had negative feelings regarding the prosthesis. For them, the prosthesis was a statement about being disabled by highlighting the upper limb defect.\n\n“I felt myself disabled with that thing [the prosthesis] on… When I was wearing it, I had the feeling that it even made me stand out more [than without the prosthesis].” (20 y/o girl, non-wearer with prosthetic experience)\n\nNon-wearers with or without prosthetic experience reached the stage of accepting their situation. The prosthesis could not substitute for a real hand; it was “a dead thing” or “a doll’s hand,” and it did not belong to the child. In that sense, the cosmesis of a prosthesis lost its value.\n\n“I did not want it [the prosthesis] anymore and I thought, ‘I am how I am,’ and that worked just as well.” (9 y/o girl, non-wearer with prosthetic experience)\n“I never wanted it [the prosthesis] before, because I considered it a fake hand… I’m also not ashamed about it [the affected hand] [smiley face].” (11 y/o boy, non-wearer without prosthetic experience)\n\n\n2.2 Functionality, manipulation, dexterity reasons.Children and adolescents felt more functional, more dexterous, or faster without prostheses. The majority of non-wearers were able to perform “everything and more” without the prosthesis. Parents and professionals noticed that children and adolescents saw little or no functional value in wearing prostheses.\n\n“Meanwhile he [parent’s child] is at an age now (8 y/o), at which he has become very dexterous with his arm … He doesn’t see his [affected] arm as a limitation and I think for him walking around with a prosthesis the whole day has no added value.” (Parent of an 8y/o boy, non-wearer with prosthetic experience)\n\nWearers, on the other hand, specified that they did not use their prostheses for activities like eating, playing, tying shoelaces, manual work at school, or working with a computer, because they were more dexterous or had better grip without them.\n\n“I’ve been able to tie my shoelaces with and without a prosthesis since I was 3! I find it easier without the prosthesis, because then I have more grip on the lace.” (15 y/o girl, wearer)\n\n\n2.3 Technical and interface reasons.The most often mentioned complaint and reason for not wearing the prosthesis was a prosthesis’s weight. The myoelectric prosthesis often required extra support with the sound hand to counterbalance the weight. Discomfort caused by the interface contact with the stump or the technical limitations of the prosthesis itself were also discussed. The interface caused stump irritations, sweating, bad odor, and difficulties fixing the stump in the socket.\n\n“I found it annoying that the prosthesis was just stuck on my arm, and it [the arm] was sweating, and that’s why it [the prosthesis] was difficult at first to put on and off.” (10 y/o girl, non-wearer with prosthetic experience)\n\nThe prosthesis had a limited number of movements and grip functions. Other complaints of non-wearers include the presence of liners, frequent technical failure, and damaged or dirty gloves. Putting on and taking off were perceived as a difficult and laborious process. Manufacturing times were considered long, and learning to use a prosthesis was energy- and time-consuming.\nTechnical issues were not considered by the wearers to be reason enough not to wear prostheses, but rather as aspects that needed improvement.\n\n2.4 Physical reasons.Non-wearers were very disturbed by the lack of sensorial feedback from the stump, along with arm and shoulder fatigue, and pain from using prostheses.\n\n“My arm was really tired after a day wearing a prosthesis and without [the prosthesis] not at all. With the prosthesis on, my shoulder used to start hurting easily. Were these reasons, a tired arm and pain in the shoulder, the most important reasons to stop wearing the prosthesis? Yes, actually they were.” (16 y/o girl, non-wearer with prosthetic experience)\n\n\n2.5 Parents and the prosthesis choice.Parents who did not opt for a prosthesis for their child made this choice because they “first wanted to see his [child’s] functionality without a prosthesis.” Other parents considered a prosthesis to be useless, based on users’ stories about daily-life experiences with prostheses.\n\n\n\n3 Tips for Improving Prostheses\nLate adolescents, parents, and professionals suggested lowering the costs of prostheses. Furthermore, they desired prostheses that were lighter, more attractive, easier to manipulate, and that had more hand positions and separate finger movements, sensorial feedback, and better glove quality. The harnesses on body-powered prostheses seemed to be very annoying, especially for boys:\n\n“Harnesses can indeed be a problem, particularly among boys that want to get rid of the ‘bra’ […].” (Healthcare professional)\n\nAlternatives for prosthetic wearing.The participants were creative in developing alternatives to wearing prostheses. The children or their relatives developed special techniques using body parts such as stump, head, trunk, mouth, or knees, and creative strategies such as bandages or tape to tie an object around the stump or to tie a magnet to it for holding objects.\nAdaptive devices for the arm or prosthesis received a lot of attention among participants, especially for non-wearers with or without prosthetic experience, and were described as helpful tools for performing specific activities such as cycling, eating, playing sports, and playing a musical instrument. Professionals and parents suggested developing more adaptive devices, although it appeared to be difficult to get the costs of adaptive devices reimbursed.\n\n\n\n4 Rehabilitation Care\n4.1 General opinions.The participants generally experienced good rehabilitation care. Many late adolescents were neutral, perceived the care as appropriate, or could not recall how they had felt about it. The participants had received proper guidance in choosing a prosthesis and had been adequately informed about functioning with a short arm and with a prosthesis.\n\n4.2 Peer contact.A recurrent theme in all groups was peer-to-peer contact. Parents with young children were eager to know what the possibilities and limitations were for their child in terms of normal functioning and development. Parents received answers to these questions during meetings with peer parents. Emotional support from experienced parents diminished the anxiety of less-experienced parents.\n\n“We saw children in the peer-group meetings who were older [than their child] and they told us how they had found a solution for all the little problems. We benefited a lot from this and we still really enjoy going to these meetings… I think it can be very comforting for ‘new’ parents to have contact right away with ‘experienced’ parents so that a lot of the anxiety is taken away.” (Parent of a 13 y/o boy, wearer)\n\nChildren and early adolescents also benefitted from peer-to-peer contact. Children referred to those meetings as “fun-time.” Emotional support was offered even during the course of the online focus group to one child who was going through a difficult time.\n\n“Right now I don’t want to be around other children.” (9 y/o boy, non-wearer with prosthetic experience)\nReaction from a participant: “I think it’s sad that ‘codename’ [referring to the previous participant] is so sad; you’ve got to remember that you’re perfect the way you are. [sad face]” (11 y/o boy, non-wearer without prosthetic experience)\n\nEarly adolescents added that the meetings were informative and emotionally helpful for them. They found out more about novel prostheses and solutions for performing difficult activities.\n\n“I go about once a year to the meetings. I am the oldest one there, and so many people ask me things. I like this and also learn things, because they [other participants] help you with new things and improvements.” (14 y/o girl, wearer)\n\nThe online focus group was seen by the children and early adolescents as an opportunity to share information about ways to perform certain activities like playing a musical instrument, playing sports, or tying shoelaces.\n\n4.3 Psychosocial assistance.Some children regarded the psychologist as vague and found the psychological tests unpleasant, or they simply did not want to talk and answer the question, “How are you doing?” However, early adolescents and parents mentioned that emotional and psychosocial help from the rehabilitation team was useful when they encountered difficult moments.\n\n“I always enjoyed an hour with the social worker the most, always nice talks, and she helped me at the same time with things that were difficult for me at that time, such as bullying and other things.” (16 y/o girl, non-wearer with prosthetic experience)\n\nProfessionals all agreed that psychosocial disciplines are an important and valuable part of the rehabilitation treatment.\n\n4.4 Themes discussed by professionals.Professionals recognized that the clients’ expectations were often too high. Children or their parents believed that a prosthesis could solve their problems with the short arm, but the outcome was not always the one they had aimed for.\nAlthough professionals admitted that the current tendency of healthcare providers was to prescribe prostheses, and that more practice was needed until the child performed automatically with the prosthesis, some professionals had different ideas.\n\n“I think that if you consider providing a prosthesis, then you should at least ensure that the child is not clumsier with a prosthesis than without it; so practicing is needed until his prosthesis can be pretty automatically manipulated.” (Healthcare professional)\n\nThey stated that the team should not strive for bilateral handling of UCBED children, but that the child should grow up with a positive self-image and should be able to fulfill his wishes with or without a prosthesis. These professionals realized that they should listen carefully to the client’s needs and to the strategies they had already found on their own, and should avoid imposing their own knowledge excessively.\n\n\nThe children and adolescents with UCBED interviewed in our study seemed to choose and wear prostheses mostly for cosmetic reasons in order to avoid people staring at them. In adults with upper limb amputation, similar [35]–[37] and opposite outcomes were found (i.e., cosmesis was less important) [38]. On the other hand, our findings acknowledged that poor prosthetic cosmesis influenced the non-choice and rejection of the prosthesis [10], [13]. The authors of a systematic review noticed a trend in qualitative studies in terms of reporting about the importance of cosmesis [39]. This being the case, the cosmetic aspects of prostheses in youngsters with UCBED deserve the full attention of manufacturers and of those recommending or prescribing them.In terms of the World Health Organization’s ICF classification, children and adolescents with UCBED have a body structure impairment [31]. Therefore, one might expect their functionality to be affected as well. However, the results of our study suggest that the functionality of children and adolescents is good, since many were able to perform activities with or without prostheses; this idea is supported in the literature as well [6], [7]. The use of creative strategies (using sweatbands and/or other body parts for grasping and holding objects in place, choosing easier activities) to facilitate activities and participation in daily living may be an alternative to the use of prostheses [22].In contrast to people with acquired arm amputations, children and adolescents with UCBED have no “sense of loss” regarding the short arm [40]. If children and adolescents with UCBED argue that they do not experience activity limitations and participation restrictions and have no “sense of loss,” then there is no reason for them to believe they have an impairment and to feel disabled. However, there are mechanisms that make these youngsters aware of the impairment. Along with body structures and functions, activities and participation, the ICF considers the environmental and the personal factors [31]. Environmental and the personal factors (gender, educational level, ability to adjust) may influence participation of people with amputations [41] and our findings support this.When the children and adolescents with UCBED in our study did start to use prostheses, people from their close environment (parents, healthcare professionals) or from their external environment (strangers) exerted a great influence in this regard. Providing the child with a prosthesis in order to improve functionality or to disguise the impairment may be considered as strategies on the part of the parents to cope with their child being disabled. These strategies have been previously described [19], [42]. Later on, when children and early and late adolescents become aware of the impact exerted by the short arm on their life, they find solutions to the problems they encounter. In addition to dealing with staring and hostile reactions from people, people with impairment of the upper limb have to deal with their own identity and values concerning body image, sexuality, and career [40]. This is the moment when cosmesis becomes more important and influences the choice of a prosthesis.In the context of prosthetic use for cosmetic purposes, the concept of normality becomes a matter for discussion. One way to achieve normality for people with disabilities is to adjust and to fit into society [43]. In the research we conducted, participants of all ages experienced a need for normality, especially during transitional periods (a new school or applying for a job), which has been reported in previous studies as stressful events [40], [44], [45]. Therefore, more psychological attention and information about cosmetic options is needed from healthcare providers, especially in critical transitional phases like puberty.For many children and adolescents in the study, the way to adjust to the environment and to ensure normality was to wear prostheses so as to appear bodily complete. Being able to perform daily, leisure, and school activities in the same way as their non-disabled peers may also be considered a form of normality. In these circumstances, the prosthesis seems to represent a source of empowerment that facilitates integration into society [43]. For a balanced relationship between youngsters with UCBED and their environment, it would also be appropriate for those people in their environment to adjust their way of thinking, perceiving, and approaching youngsters with UCBED.Another way of achieving normality is to accept and acknowledge the impairment [43]. This was the case with the non-wearers in our current study. The non-wearers’ wish for inclusion in society was based on being valued and accepted as they were. This might well mean that the psychosocial contribution of the prosthesis in combating others’ staring at them is unnecessary after all. By not wearing an unnatural-looking prosthesis, children and adolescents believed they were not altering their appearance. This helped them reinforce their self-esteem and improve their self-identity. In addition, if prostheses are seen as having no functional gain [8], [13], [46], as being technically unsatisfactory and physically uncomfortable [47]–[49], and sometimes actually hampering effective performance [8] – issues we also found in the present research – the added value of the prosthesis disappears and rejection of it occurs. Interestingly, some of the participants succeeded in embracing acceptance and in using the prosthesis for some daily-life activities and in playing sports, a phenomenon also described in the literature [7], [12]. These observations question prosthetic functionality and necessity: “Are prostheses the best solution for children’s and adolescent’s needs?” Our study also highlighted the perceived value children and adolescents expressed regarding the use of adaptive devices. These devices are light-weight, designed for specific activities, easy to manipulate and to put on [50]. Therefore, considering adaptive devices as an option for rehabilitating children and adolescents with UCBED may be of great value.Study participants, whether wearers or non-wearers, seemed to have the same expectations from a prosthesis when they decided to choose for one (i.e., nicer appearance and better functionality). After wearing and testing it, these expectations were not met for non-wearers, and only partially met for wearers. This discrepancy between a person’s wishes and the outcomes of prosthetic use, detected by healthcare professionals in the present study, has also been reported in the literature by parents of these children [13] and by adults [51]. Providing information and clarifying the real possibilities and limitations of prosthetic use for consumers would serve to balance expectations versus real-life possibilities. More opportunities for trying and using prostheses before purchasing them would allow children and early and late adolescents to make a more informed choice. Providing these opportunities could be organized in the form of banks with prosthetic simulators that could be rented. A prosthetic simulator is a prosthesis which is adapted with fastening systems and can be attached on any type of arm (amputated, normal) [52].The current research results were in line with the findings of other studies that stated that peer-to-peer contact provided emotional assistance for parents and children, as well as understanding, interaction, and identification with people in the same situation [9], [40], [53]. Incorporating regular peer-to-peer meetings into healthcare would address important aspects of the harmonious development of children and early and late adolescents with UCBED.Patient-centered care was supported by healthcare professionals in our study. Patient-centered care considers three assumptions that would improve rehabilitation care: the patient (1) is the customer, (2) is the “owner of his body, mind, and soul,” and (3) has requested a service in a health matter, so the service provided should focus on the patient’s desires [54].A subject of novelty in the literature and a strength of this study is the fact that children, early adolescents and late adolescents themselves were interviewed, and not only people in the immediate environment (e.g., parents), as in the majority of studies. Along with reasons for rejection – preferentially treated in the literature – the current study also explored the determinants for wearing prostheses in children and early and late adolescents with UCBED. Their opinions about prosthetic use and rehabilitation care allowed for a better understanding of the needs that a young person with UCBED experiences at a certain stage of life. The use of online focus group interviews proved to be an efficient method for collecting a large amount of data in a short period of time. For youngsters with UCBED, the online interaction was easy-going and convenient, since it offered anonymity and flexible participation hours [24], [25], [29].This study also has some limitations. Opinions about prosthetic wear in the children and parents groups may have been underexplored due to the low number of wearers in these two groups. However, in all groups, the majority of the current non-wearers had previously worn prostheses. As such, opinions of non-wearers were also valuable for determining reasons for wearing prostheses. There were more females than males in the early adolescent, late adolescent, and parent groups. They might have influenced the results by highlighting the importance of cosmesis, but studies with a majority of males found the cosmetic aspect very important as well [35], [51], [55]. One may argue that the age of fitting the first prosthesis varies between the groups and might have had an influence on reporting reasons for prosthetic use. No clear proof exists in the literature regarding possible relationships between age of fitting and prosthetic use in later life [15], [16].The findings of this study should be interpreted in the context of qualitative studies and focus groups. Future studies in larger populations, designed as interviews or questionnaires, might explore in detail the reasons why children and early and late adolescents with UCBED either wear prostheses or do not do so.Children and early and late adolescents with UCBED seem to choose and wear prostheses mainly for cosmetic reasons, in order to achieve social integration and not because of limited functionality. Peer-to-peer contact, organized by the rehabilitation teams in conjunction with other institutions, appeared to be an important informational and emotional support for children, early adolescents, and parents. When working with UCBED youngsters there should also be a focus on the importance of the cosmetic possibilities offered by a prosthesis. Extending the treatment options beyond prostheses to other solutions – such as, for example, the use of adaptive devices – would ease some daily-life activities for these children and adolescents. Further research should also focus on the psychosocial events and experiences in this young group."
        },
        "10.1371/journal.pone.0023195": {
            "author_display": [
                "Sabine Neuss",
                "Bernd Denecke",
                "Lin Gan",
                "Qiong Lin",
                "Manfred Bovi",
                "Christian Apel",
                "Michael Wöltje",
                "Anandhan Dhanasingh",
                "Jochen Salber",
                "Ruth Knüchel",
                "Martin Zenke"
            ],
            "title_display": "Transcriptome Analysis of MSC and MSC-Derived Osteoblasts on Resomer® LT706 and PCL: Impact of Biomaterial Substrate on Osteogenic Differentiation",
            "abstract": [
                "Background: Mesenchymal stem cells (MSC) represent a particularly attractive cell type for bone tissue engineering because of their ex vivo expansion potential and multipotent differentiation capacity. MSC are readily differentiated towards mature osteoblasts with well-established protocols. However, tissue engineering frequently involves three-dimensional scaffolds which (i) allow for cell adhesion in a spatial environment and (ii) meet application-specific criteria, such as stiffness, degradability and biocompatibility. Methodology/Principal Findings: In the present study, we analysed two synthetic, long-term degradable polymers for their impact on MSC-based bone tissue engineering: PLLA-co-TMC (Resomer® LT706) and poly(ε-caprolactone) (PCL). Both polymers enhance the osteogenic differentiation compared to tissue culture polystyrene (TCPS) as determined by Alizarin red stainings, scanning electron microscopy, PCR and whole genome expression analysis. Resomer® LT706 and PCL differ in their influence on gene expression, with Resomer® LT706 being more potent in supporting osteogenic differentiation of MSC. The major trigger on the osteogenic fate, however, is from osteogenic induction medium. Conclusion: This study demonstrates an enhanced osteogenic differentiation of MSC on Resomer® LT706 and PCL compared to TCPS. MSC cultured on Resomer® LT706 showed higher numbers of genes involved in skeletal development and bone formation. This identifies Resomer® LT706 as particularly attractive scaffold material for bone tissue engineering. "
            ],
            "publication_date": "2011-09-14T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 4,
            "views": 3356,
            "shares": 0,
            "bookmarks": 15,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0023195",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0023195&representation=PDF",
            "fulltext": "IntroductionHuman multipotent mesenchymal stem cells (MSC) are multipotent stem cells and represent a particularly attractive source for tissue engineering, because they are readily isolated and expanded and can differentiate into several mature cell types, such as adipocytes, chondrocytes, osteoblasts and myocytes [1], [2 and references therein]. Due to their mesenchymal origin and their osteogenic differentiation capacity, MSC are particularly promising cells for bone replacement. Using a specific culture medium, they can differentiate towards osteoblasts in vitro within three weeks [3]. However, a biomaterial scaffold is required for bone tissue engineering (BTE) to allow for immobilisation of cells in a spatial structure. A variety of potential biomaterials exist, which are cytocompatible for MSC, e. g. collagen, fibrin and non-oxide ceramics [4]–[8]. For scaffolds in general, cyto- and biocompatibility are the criteria of upmost importance and they need to meet further specific application-dependant criteria. For BTE, a scaffold should be osteoinductive or at least osteoconductive and has to be long-term degradable to allow for an autologous replacement of the transplanted area. Besides such specific requirements, a fundamental bottleneck must be overcome to use MSC for BTE – the adequate supply of the cells. This problem will become more critical when aiming at engineering of bulk tissue to fill bone defects after injury or elimination of tumors, particularly when autologous bone production is desired. Such goals necessitate the maintainance of large numbers of undifferentiated cells embedded in biocompatible matrices to provide sufficient starting material.\nA number of commercially available cell culture materials is accessible, such as standard tissue culture poly(styrene) (TCPS), Primaria™, poly(ethylene terephthalate) (PET) and TC plastics biocoated with Matrigel™ or single extracellular matrix proteins, but given properties of the polymers make them ineligible for tissue engineering. Further, frequently the animal origin of biocoating products disqualify them from use in human clinical applications. Currently, the published literature contains a large number of studies in which stem cells are seeded on one or more readily available materials but frequently such studies lack systematic approaches to determine cell responses.\nIn our recent work we established a biomaterial test platform to assess the compatibility of stem cells and biomaterials for tissue engineering in a highly standardised and systematic manner. The assessment of stem cell/biomaterial interactions is multifactorial and requires a stringent analysis of parameters, such as material surface and bulk properties, cytotoxicity, cell adhesion, cell morphology, viability, proliferation, necrosis and apoptosis [5]. In our present study, we analysed in more detail the interactions of MSC with two commercially available synthetic and resorbable polymers of our panel of biomaterials, Poly(ε-caprolactone) (PCL) and poly[(L,L-lactide-co-trimethylene carbonate)7/3] ( = PLLA-co-TMC = Resomer® LT706).\nPCL is a linear, semicrystalline, synthetic aliphatic homopolyester, which has a degradation time of two to three years [9]–[11]. PCL is the most studied degradable polymer of the polyester family [12] and is already approved by the Food and Drug Administration for diverse applications in the human body. Furthermore, PCL is currently under consideration for use in bone tissue engineering [13]–[16].\nIn contrast, there is only poor knowledge on Resomer® LT706. This linear and semi-crystalline poly-lactide-based polymer is similar in chemistry to Suprathel, which is now used in clinical trials for skin replacement after burns [17], [18]. Further, poly(1,3-trimethylene carbonate) copolymerised with D,L-lactide was shown to be long-term biodegradable and biocompatible for soft tissue engineering [19], but so far, polymers related to Resomer® LT706 were not analysed with respect to their usefulness for bone replacement. The random copolymer PLLA-co-TMC (Resomer® LT706) is characterised by an intermediate-term degradation, slower than poly(glycolic acid) (PGA), poly(D,L-lactide acid) (PDLLA), and poly(glycolic acid-co-trimethylene carbonate) (PGA-co-TMC), but faster than the long-term degradable PCL [20]–[22]. Although, Resomer® LT706 has some advantages regarding its mechanical properties and processing behavior, it is used only in a few studies as potential scaffold for hard and soft tissue engineering [23], [24].\nIn the present study, we analysed the two polymers in comparison to TCPS for their osteoinductive capacity and thus for their potential use in BTE. Therefore, we seeded MSC on PCL and Resomer® LT706 samples with defined, ultraflat topography to exclude topography-dependant changes in cell adhesion, morphology and proliferation. MSC adhered on both polymers and showed only minor differences in morphology and viability. When subjected to osteogenic induction medium (OIM), MSC differentiated towards the osteoblastic fate. Alizarin red stainings, realtime PCR, scanning electron microscopy (SEM) and energy-dispersive X-ray spectroscopy (EDS) analysis revealed a similar frequency of MSC-derived osteoblasts on the two polymers. However, detailed studies on the molecular level including whole genome expression analysis unravelled differences in the biomaterial-based propensity towards the osteogenic fate of MSC supported by these two polymers.\nMethods\nPolymer synthesis\nThe polymers were produced as previously described [5] (Neuss et al., 2008a).\n\n\nPoly(ε-caprolactone) (PCL)\nPCL with a molecular weight of 80.000 g/mol was purchased from Sigma-Aldrich GmbH (Germany). For PCL foils, 3 g of granules were used. These granules were placed on Teflon®-covered metal plates, the temperature was raised to 85°C and maintained for 5 min. A load of 1000 kg was applied for 1 min at 85°C. After cooling down to room temperature, foils were then washed several times with isopropanol (Fluka, Germany), aqueous 0.02 mM Tween 80 (Roth, Germany) and 8 M urea (Roth, Germany) and then rinsed vigorously with demineralised water. PCL samples were then dried in a vacuum oven for 24 h at 40°C. Samples were placed in TCPS wells, stored at 4°C, and protected from light. All steps were carried out under sterile conditions.\n\n\nResomer® LT706\nResomer type poly(L-lactic acid-co-trimethylene carbonate) ( = P(LLA-co-TMC; lactic acid-trimethylene carbonate ratio 70:30, LT706, 1.2–1.6 dL/g) was purchased from Boehringer Ingelheim Pharma GmbH & Co. KG (Germany). Foils were prepared by melt-pressing technique. Therefore, granules were ground to powder in a cryo-mill and 1.2 g of the powder were placed between Teflon®-covered metal plates. This was followed by a 5 min incubation at 180°C. A load of 1000 kg was applied for 9 min. The P(LLA-co-TMC) foil was allowed to cool down to room temperature, removed from the metal plates and further processed as for as for PCL.\n\n\nCoating of biomaterials with radiolabeled fibronectin and vitronectin\n100 µl of either fibronectin or vitronectin (100 µg diluted in 100 µl PBS; BD, Heidelberg, Germany) were mixed in a silanised counter vial (3.5 ml volume) with 1 µl Na125I solution (0.1 mCi; 3.7 MBq; 45 pmol; Amersham Europe, Freiburg, Germany) and 5 µl Chloramine-T solution (10 µg Chloramine-T trihydrate, solved in 100 µl PBS) using a magnetic stir bar. After 5 min, the incorporation of 125I into fibronectin and vitronectin was measured by TCA precipitation. Therefore, a melted glass capillary was immerged first into the iodinising mixture and then into 200 µl of BSA solution (10% (m/m) BSA, 1% (m/m) NaI, 0.01% (m/m) NaN3). After vigorously mixing of the solution, an aliquot of 10 µl was admixed with another 200 µl BSA solution. Afterwards, 2 ml ice-cold 10% trichloroacetic acid was added. The precipitated material consisting of BSA and target protein was spinned down, separated from the supernatant and measured using a gamma counter COBRA II Auto-Gamma (Packard, Dreieich, Germany). To separate labelled protein from non-incorporated iodine, the mixture was purified by gel filtration using a Sephadex® G-25 NAP™5 Column (Pharmacia Biotech AB, Uppsala, Sweden). The column was equilibrated with PBS before use. The identified radiolabelled proteins were stored at −20°C. Radiolabeled fibronectin and vitronectin were dissolved in PBS and DMEM with 10% FCS in a concentration of 10 µg/ml.\nRadiolabeled protein adsorption study was carried out by incubation of TCPS, Resomer® LT706 and PCL disc surfaces with 2 ml of the different solutions, each with 125I-labeled fibronectin or vitronectin (10 µg/ml), respectively. The activity was adjusted to 100.000 cpm/500 µl (1.67 kBq/500 µl; 45.0 pCi/500 µl) using a tracer. To test the resulting activity, samples of 500 µl volume were measured using the gamma counter COBRA II Auto-Gamma during a period of 3 min. After treatment of biomaterial substrates with radiolabelled protein (incubation time 1 h) and subsequent washing, activity was measured using the gamma counter during a period of 5 min.\nSignificant differences between samples were analysed using student's t-test for 10 independent measurements of each coating.\n\n\nIsolation of human mesenchymal stem cells\nHuman mesenchymal stem cells (MSC) were isolated from femoral head spongiosa of patients with total hip joint endoprosthesis (TEP) after written consent was obtained from the patients. The study was approved by the Ethics Committee of the University Clinics, Aachen, Germany. MSC were characterised as previously described [5]–[8], [25]; Fig. S2. In brief, after rinsing the spongiosa with stem cell medium, spongiosa was removed and the remaining cell suspension was centrifuged for 10 min at 500×g. Thereafter, the cell pellet was resuspended in stem cell medium and cells were seeded in a T75 culture flask and cultured at 37°C in a 21% O2 and 5% CO2 humidified atmosphere. After 24 hours, non adherent (hematopoietic) cells were removed by medium change. Mesenchymal stem cells were expanded in growth medium (PAN Biotech, Aidenbach, Germany) consisting of 60% DMEM low glucose, 40% MCDB-201, 2% FCS, 1×ITS-plus (insulin-transferrin-selenic acid+BSA-linoleic acid), 1 nM dexamethasone, 100 µM ascorbic-acid-2-phosphate, and 10 ng/ml EGF. Medium was replenished every 3–4 days. At 80–90% confluency, stem cells were trypsinised with stem cell trypsin (CellSystems, St. Katharinen, Germany) and reseeded in a density of 5,000 cells/cm2 for optimal proliferation.\n\n\nDifferentiation of MSC towards osteoblasts\nFor osteogenic differentiation, MSC were seeded in a density of 3.1×104 cells/cm2 on TCPS and on foils of the two polymers PCL and Resomer® LT706. Twenty-four hours after seeding, the growth medium was replaced with osteogenic induction medium (OIM) consisting of DMEM low glucose (PAA, Coelbe, Germany), 100 nM dexamethasone, 10 mM sodium β-glycerophosphate, 0.05 mM L-ascorbic-acid-2-phosphate (all Sigma, Steinheim, Germany) and 10% FCS (PAN Biotech, Aidenbach, Germany). Medium was changed every 2–3 days. After 21 days of osteogenic differentiation, cells were fixed with 70% ethanol for 1 hour, washed three times with demineralised water and then stained with an Alizarin red solution (40 mM, pH 4.1, Sigma) for 10 minutes. Finally, cells were washed three times with PBS (phosphate-buffered saline).\nFor quantification, the Alizarin red precipitates were solubilised. Briefly, stained samples were incubated with 800 µl acetic acid (10%) for 30 min. Then, supernatant was transferred into a 1.5 ml tube and boiled for 10 min at 85°C, followed by a 5 min incubation on ice. After centrifugation (15 min, 15,000×g), 500 µl of the supernatant were transferred into another 1.5 ml tube and mixed with 200 µl of 10% ammonium hydroxide. Samples of 150 µl were transferred into a 96 well microtiter plate and optical density was measured at 405 nm using a standard ELISA reader.\nP-value to detect statistically relevant differences for the different biomaterials was calculated with student's t-test (n = 3 with two replicates each).\n\n\nScanning electron microscopy (SEM)\nMSC/polymer biohybrids were fixed in 3% glutaraldehyde for at least 24 hours, rinsed with sodium phosphate buffer (0.2 M, pH 7.39, MERCK, Darmstadt, Germany) and dehydrated by incubating consecutively in 30%, 50%, 70% and 90% acetone and then three times in 100% acetone for 10 minutes. The biohybrids were critical-point-dried in liquid CO2, and then sputter-coated with a 30 nm gold layer. Samples were analysed using an environmental scanning electron microscope (ESEM XL 30 FEG, FEI, PHILIPS, Eindhoven, The Netherlands) in a high vacuum environment.\n\n\nEnergy-Dispersive X-ray Spectroscopy\nEnergy dispersive X-ray spectroscopy (EDS) is an analytical technique used for element analysis samples. EDS was performed on the XL 30 FEG scanning electron microscope (FEI, Eindhoven, The Netherlands) using an EDAX Falcon Genesis Spectrum 5.21 energy-dispersive X-ray spectroscopy system with an ultrathin window liquid nitrogen cooled Si(Li) X-ray detector (EDAX Inc. Mahwah, NJ, USA). For the EDS an accelerating voltage of 20 kV was used.\n\n\nReverse Transcriptase Polymerase Chain Reaction (RT-PCR)\nTotal RNA was isolated using the RNeasy mini Kit according to the manufacturers' instructions (Qiagen, Hilden, Germany). Reverse transcription was done with 1 µg of total RNA using the High Capacity cDNA Reverse Transcription Kit (Applied Biosystems, Darmstadt, Germany). PCR was as follows: denaturation at 95°C for 1 min, annealing at 58°C (osteocalcin), 60°C (alkaline phosphatase) for 1 min, extension at 72°C for 1 min (30 cycles), and a final extension at 72°C for 10 min. PCR products were analysed by electrophoresis using a 2% agarose gel and visualised with ethidium bromide. Primer sequences were used as listed in Table 1.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Primer sequences for PCR and RT-qPCR.doi:10.1371/journal.pone.0023195.t001\n\nRealTime PCR (RT-qPCR)\nRealTime PCR was performed using SybrGreen and Taqman technology. Briefly, 10 µl SybrGreen Master Mix (Applied Biosystems, Darmstadt, Germany) was mixed with 1 µl (10 pg) Primer forward, 1 µl (10 pg) Primer reverse, 6.8 µl water and 1.2 µl (60 ng) template. Then the samples were subjected to the following program: initial denaturation at 95°C for 10 min, followed by 40 cycles of denaturation at 95°C for 15 sec and annealing/extension at 60°C for 1 min. Primer sequences and sizes of amplicons are listed in table 1.\n\n\nWhole Genome Expression Analysis\nFor whole genome expression analysis, MSC were seeded in a density of 3.1×104 cells/cm2 on TCPS, Resomer® LT706 and PCL and cultured for 21 days in growth medium (GM) or in osteogenic induction medium (OIM). Cells at day 0 served as control. Total RNA was isolated using the RNeasy micro Kit according to the manufacturers' instructions (Qiagen, Hilden, Germany). The RNA quality was assessed using RNA 6000 NanoChips with the Agilent 2100 Bioanalyzer (Agilent; Santa Clara, CA, USA). Probes for the GeneChip® Human Gene 1.0 ST Arrays (Affymetrix, Santa Clara, CA, USA) were prepared and hybridised to the arrays according to the Affymetrix GeneChip® Whole Transcript (WT) Sense Target labeling Assay Manual. Briefly, for each sample, 300 ng of total RNA was reverse trancribed into cDNA using a random hexamer oligonucleotide tagged with a T7 promoter sequence (5′-GAATTGTAATACGACTCACTATAGGGNNNNNN-3′). After second strand synthesis, double-stranded cDNA was used as template for amplification with T7 RNA polymerase to obtain antisense cRNA. Random hexamers were then used to reverse transcribe the cRNA into single stranded sense strand cDNA. The cDNA was then fragmented by UDG (uracil DNA glycosylase) and APE1 (apurinic/apyrimidic endonuclease 1). Fragment size was checked using the Agilent 2100 Bioanalyzer (fragment size between 50–200 bp). Fragmented sense cDNA was biotin-labelled with TdT (terminal deoxynucleotidyl transferase) and probes were hybridised to the GeneChip® Human Gene 1.0 ST Arrays at 45°C for 16 hours. Hybridised arrays were then washed and stained on Fluidics Station 450 and scanned on a GeneChip® Scanner 3000 7G (both Affymetrix).\nThe image data were analysed with GCOS (Affymetrix). For statistical analysis data were processed by R software (R Development Core Team, 2005). Gene expression levels were normalised with RMA algorithm [26]. Principal component analysis (PCA) and hierarchical clustering were done on whole genome transcripts by the R Stats Package [27]. Hierarchical clustering was performed using Pearson correlation coefficient and the Average linkage method. RankProd, a non-parametric statistic, was employed for identification of differentially expressed genes [28]. Transcripts with a fold change more than 2 and P-value less than 0.01 between two conditions were considered as being differentially expressed. Gene ontology (GO) and pathway over-representation analysis was done using DAVID bioinformatics resources [29].\n\nResults\nResomer® LT706 and PCL enhance MSC differentiation towards osteoblasts\nIn a previous study, we analysed MSC on a grid-based platform in contact to different polymers to determine cytocompatibility of polymers for future MSC-based tissue engineering applications [5]. Now, MSC were cultured for 21 days either in growth medium (GM) or in osteogenic induction medium (OIM) on two of the cytocompatible, synthetic, biodegradable polymers, Resomer® LT706 and PCL, which are potentially osteoinductive. To analyse the osteogenic differentiation, calcium accumulations were visualised by Alizarin red staining (Fig. 1A), followed by dissolving the dye and subsequent quantification of the staining (Fig. 1B). Alizarin red stain residues were not retained by any of the biomaterials (Fig. S3). As shown in Figure 1, polymers did not induce osteogenic differentiation by themselve in GM and OIM was always required to guide MSC into osteogenic fate. However, although OIM was required to initiate osteogenic differentiation of MSC on the two polymers, biomaterial properties influenced calcium accumulations.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Osteogenic differentiation of MSC on polymers.A) Alizarin red staining of calcium accumulations on MSC, cultured on two polymers and on tissue culture polystyrene (TCPS) for 21 days either in growth medium (GM) or in osteogenic induction medium (OIM). PCL = Poly(ε-caprolactone); LT706 = Resomer® LT706; B) Quantification of Alizarin red staining via dissolving the dye and subsequent absorption measurement (λ = 405 nm); n = 3 with each 2 replicates; p>0.05; C) Electron microscopic view (SEM) of MSC, cultured for 21 days on Resomer® LT706 and PCL either in GM or in OIM. From left to right: higher magnifications of the previous picture (120× up to 20.000×). White boxes in 20.000× magnification images represent the area, which was subjected to EDS analysis (D); D) EDS Analysis of surfaces of Resomer® LT706 and PCL after a 21 day cultivation period with MSC either in GM or in OIM. Prominent peaks of calcium and phosphate were detected in samples cultured in OIM, but not in samples cultured in GM (compare y-axis scale).\ndoi:10.1371/journal.pone.0023195.g001As shown in Figure 1, both polymers resulted in a stronger alizarin red staining than TCPS. Further, the amount of calcium accumulations on Resomer® LT706 was significantly higher, than on TCPS and on PCL. Since the Alizarin red staining is an indirect measure for osteogenic differentiation, we analysed the differentiation in more detail. The ultrastructural analysis depicted in Figure 1C illustrates the morphology of MSC on the two polymers after a three-week culture period in GM or in OIM (Fig. 1C). Our results show that although MSC morphology on the two polymers differed during initial adhesion [5], MSC morphology is identical on both polymers after 21 days of culture and independent of the culture medium (GM vs. OIM). Higher magnifications showed analogous particles densely covering the biomaterial surfaces under differentiation conditions (OIM) on both polymers, which were qualitatively identified as consisting of calcium and phospate by EDS analysis (Fig. 1D). Such calciumphosphate particles are indicative for an advanced osteogenic differentiation of MSC on both polymers, Resomer® LT706 and PCL. These results were confirmed by the expression of osteogenic markers, such as alkaline phosphatase (which is already expressed in unstimulated MSC), osteocalcin, bone sialoprotein and the transcription factor Cbfa-1 after 21 days of cultivation in OIM on the polymers (Fig. 2). By demonstrating (i) positive Alizarin red staining and (ii) expression of standard osteogenic markers on the RNA level, well-accepted standard assays were performed to show osteogenic differentiation of MSC on Resomer® LT706 and PCL. However, we reasoned that conventional standard assays are not sufficient to comprehensively investigate the osteogenic fate of MSC and thus we postulated, that there might be differences in the quality of the MSC-derived osteoblasts on the molecular level. Therefore, we performed whole genome expression analysis to determine biomaterial-related differences in osteogenic differentiation.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Expression of osteogenic markers on RNA-level.A) Semiquantitative polymerase chain reaction (PCR) of the osteogenic markers osteocalcin (246 bp) and alkaline phosphatase (238 bp) expressed in MSC on TCPS (d0) or cultured for 21 days in osteogenic induction medium (OIM) on TCPS, Resomer® LT706 and PCL. ß-Actin (400 bp), loading control: DNA marker (100 bp ladder) = 600 bp. Results of one representative experiment out of three are shown. B) Microarray-data of three independent experiments (n = 3) for osteocalcin, bone sialoprotein, Cbfa-1 and alkaline phosphatase expressed in MSC on TCPS (d0) or cultured for 21 days in OIM on TCPS, Resomer® LT706 and PCL.\ndoi:10.1371/journal.pone.0023195.g002\n\nWhole genome expression analysis identifies specific responses of Resomer® LT706\nTo determine the molecular events ongoing in MSC differentiation on Resomer® LT706 and PCL, samples were analysed by whole genome expression profiling. First, data were subjected to principal component analysis (PCA, Fig. 3A) to discriminate related samples (similar gene expression) from distant samples (different gene expression). Figure 3A reveals the following information: the closer the samples are, the more related is the gene expression pattern. Thus, PCA demonstrated (i) general changes in gene expression during the 21 day culture period (compare day 0 samples D1–3 cultured in GM, with all other samples), (ii) donor variations, since donor 3 is separated from donors 1 and 2 after 21 days of culture, but (iii) the same shift of all three donors occurred from culture in GM and to culture in OIM (arrow in Fig. 3A). Hierarchical clustering (Fig. 3B) of the same data set also revealed general changes in gene expression over time (samples D1–3, grey dottet line), donor variations as well as the shift between growth and differentiation conditions (blue dottet lines vs. red dottet lines, respectively). Additionally, this analysis also identified clustering of Resomer® LT706 with TCPS, independent of donor and independent of growth (GM) and differentiation (OIM) conditions (Fig. 3B). Thus for each donor, Resomer® LT706 and TCPS consistently clustered together when cultured in GM or in OIM (e.g. L3 OIM/T3 OIM or L2 GM/T2 GM). In most cases, these neighboring clusters were adjacent to the corresponding donor in the respective medium on PCL (e.g. P3 GM is next to the L3 GM/T3 GM pair). Although the Alizarin red staining was much stronger on Resomer® LT706 than on TCPS (Fig. 1A, B), the impact of Resomer® LT706 on MSC gene expression seemed to be comparable to TCPS, both after culture in GM and OIM. In contrast, PCL resulted in a different gene expression profile compared to Resomer® LT706 and TCPS, but in most cases, PCL samples were in direct proximity to TCPS and Resomer® LT706 in the respective culture conditions (blue and red dottet lines; Fig. 3B).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Whole genome expression analysis of MSC, cultured for 21 days on TCPS, Resomer LT706 and PCL in growth medium (GM) or in osteogenic induction medium (OIM).A) Principal component analysis (PCA) demonstrates donor variations, but all donors show the same shift in position upon differentiation. Orange circles = MSC cultured for 21 days in GM; green circles = MSC cultured for 21 days in OIM; blue circles = MSC expanded on TCPS in GM before differentiation experiment; T = TCPS; L = Resomer® LT706; P = PCL; D = Donor at day 0; 1,2,3 = different donors (n = 3); PC1, 2, 3 = Principal Component 1, 2 and 3; B) The dendrogram shows clustering of TCPS and Resomer® LT706, independent of donor and independent of growth (GM) and differentiation (OIM) conditions. T = TCPS; L = Resomer® LT706; P = PCL; D = Donor at day 0; 1,2,3 = different donors (n = 3); red dotted lines = OIM; blue dotted lines = GM.\ndoi:10.1371/journal.pone.0023195.g003The results shown in Figure 3 indicate donor variations, as expected for primary cells, but a similar influence on gene expression by TCPS and Resomer® LT706, which was independent of culture medium. Hence, we show that culture medium has a stronger influence on MSC differentiation than the biomaterial substrates. Alizarin red staining was stronger for PCL and Resomer® LT706 compared to TCPS (Fig. 1A, B), yet PCL affected more genes within the same culture conditions than Resomer® LT706 when compared to cells on TCPS. This analysis describes the overall impact on the gene expression profile and the identity of differentially expressed genes is discussed below.\n\n\nBiomaterial substrate and culture medium impact on gene expression of MSC and MSC-derived osteoblasts\nResomer® LT706, PCL and TCPS are semi-crystalline polymers with a characteristic, textured structure and ultraflat topography. Thus, the biomaterial surface physicochemistry could be responsible for the specific changes in gene expression rather than the biomaterial topography. Depending on the surface physicochemistry, proteins adsorb to the biomaterial, yet the underlaying mechanisms are still not fully understood. In this context we emphasize that cells do not adhere directly to biomaterial surfaces but rather adhere to a protein layer, which promptly adsorbes to biomaterials, when exposed to cell culture medium. Within the two cell culture media (GM and OIM) the composition of proteins is quite different, which presumably results in different protein layers on the biomaterial surfaces when cultured either in GM or in OIM (2% FCS vs. 10% FCS, respectively). Further, the amount of adsorbed proteins and the orientation of active groups of the adsorbed proteins could differ depending on the biomaterial physicochemistry. Accordingly, this leads to different cell behaviours on the very same biomaterial, when cultured in different media. However, within the same culture medium, different biomaterials can influence cells in different ways.\nThe heatmap of expression data shown in Figure 4A demonstates the impact of biomaterial substrate and culture medium on gene expression of MSC and MSC-derived osteoblasts. We detect two main gene expression pattern: one is representative for cells cultured in GM (Fig. 4A, right) and one for cells cultured in OIM (Fig. 4A, left), regardless of the biomaterial substrate meaning that the impact of the growth medium is higher than the impact of the biomaterials. However, within the two groups, MSC cultured on Resomer® LT706 and TCPS cluster together, while PCL form a separate branch, which is in line with the results depicted in Figure 3. Again, these results demonstrate the influence of culture medium on MSC, which is more prominent than the influence of biomaterial substrates.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Impact of the biomaterials and culture media on MSC at growth and differentiation state.A) Heatmap representation of medium-dependant effects on MSC, cultured for 21 days on TCPS, Resomer® LT706 and PCL, either in growth medium (GM) or in differentiation medium (OIM). T = TCPS; L = Resomer® LT706; P = PCL; D = donor at day 0; 1,2,3 = different donors (n = 3); B) Heatmap representation of biomaterial impact on differentiation state (MSC cultured in OIM after 21 days). Genes boxed by a discontinuous line are similarly expressed in TCPS and Resomer® LT706 (upper box) or in Resomer® LT706 and PCL (lower box). Genes highlighted in orange were analysed by RT-qPCR. T = TCPS; L = Resomer® LT706; P = PCL; 1,2,3 = different donors; C) RT-qPCR for SFRP4, PRELP, COMP, COL11A1, ELN and CCL2 to confirm gene array results of Figure 4C. Expression of genes was normalised to the reference gene ß-actin. TCPS was used as calibrator. n = 3.\ndoi:10.1371/journal.pone.0023195.g004As shown by hierarchical clustering (Fig. 3 and Fig. 4A), MSC on Resomer® LT706 and TCPS show a similar gene expression pattern and this translates into a small number of differentially expressed genes. As described above, MSC do not attach directly to the biomaterial surface, but to the protein layer, which covers the biomaterials after exposure to the cell culture medium. As shown in Figure S1, fibronectin (FN) and vitronectin (VN), two main components of serum involved in cell adhesion, are adsorbed to TCPS, Resomer® LT706 and PCL. While the amount of adsorbed FN is very similar for TCPS and PCL (approx. 120 ng/cm2), Resomer® LT706 is covered with approx. 150 ng FN/cm2. However, the relation between absorbed FN and VN is the same for TCPS and Resomer® LT706 (fibronectin to vitronectin ratio = 1.2), while the relation between FN and VN on PCL is 1.04. These results were not significantly different either when we used a buffer system including FN and VN or serum-containing medium including FN and VN (not shown). Thus, the ratio of adsorbed FN to adsorbed VN might be key to MSC gene expression.\nFinally, differentially regulated genes of MSC cultured on Resomer® LT706, PCl and TCPS under differentiation conditions (OIM) were subjected to hierarchical cluster analysis and data depicted in heat map format (Fig. 4B). This analysis shows that Resomer® LT706 and TCPS clustered together and PCL formed a distinct cluster. Some donor variations were also seen in this heat map representation, which is in line with the PCA analysis (Fig. 3A). In addition, this analysis identified a large cluster of genes, with a similar expression pattern in Resomer® LT706 and TCPS (Fig. 4B, upper boxed area). Furthermore, we also identified genes that showed a similar expression in MSC on Resomer® LT706 and PCL, but are differentially expressed in MSC on TCPS, e.g. CCL2 (Fig. 4B, bottom boxed area).\nWell-known osteogenic marker genes were expressed in MSC cultured on all three biomaterials (TCPS, Resomer® LT706 and PCL; compare Fig. 2). Thus, the expression and interaction of other genes involved in osteogenesis and chondrogenesis (Fig. 4C) might be important to regulate MSC differentiation towards osteoblasts either via osteogenesis or ossification on these biomaterials. To further support the data of whole genome gene expression analysis, we perfomed quantitative PCR. Therefore, we choosed a panel of genes involved in skeletal development and osteogenic differentiation, highlighted in orange in Figure 4B. As shown in Figure 4C, gene expression of MSC-derived osteoblasts was influenced by biomaterial substrates or – by keeping in mind the results on FN and VN described above - by serum proteins which are adsorbed to the biomaterials before cell attachment.\nSecreted frizzled related protein 4 (SFRP4), Proline/arginine-rich end leucine-rich repeat protein (PRELP), cartilage oligomeric matrix protein (COMP), α1 XI collagen (COL11A1) and elastin (ELN) expression is higher in MSC cultured in OIM on Resomer® LT706 and TCPS in comparison to PCL (Fig. 4C). Chemokine (C-C motif) ligand 2 (CCL2) expression is increased under differentiation conditions on Resomer® LT706 and PCL (compare Fig. 4C with Fig. 4B).\n\n\nGene ontology (GO) overrepresentation analysis detects biological categories related to skeletal development and bone formation\nGene functions are described in a controlled vocabulary format referred to as gene ontology (GO; www.geneontology.org) and thus differentially regulated genes between the three biomaterials cultured in GM and OIM were subjected to GO overrepresentation analysis, respectively. The main categories of the two culture conditions are shown for MSC cultured on Resomer® LT706, PCL and TCPS in GM (Table 2) and for MSC cultured on Resomer® LT706, PCL and TCPS in OIM (Table S2). Some categories exist in both lists, like developmental process, system development and extracellular region. However, bone specific categories, such as bone remodelling, calcium ion binding, bone mineralisation, ossification and biomineral formation are – as expected - not found in the GO list for the growth conditions (GM), but confined to the GO list for the differentiation conditions (OIM).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  GO list with genes differentially expressed in MSC cultured on TCPS, Resomer® LT706 and PCL during growth conditions. (GM)doi:10.1371/journal.pone.0023195.t002\nMSC are the natural precursor cells of mesenchymal tissue, such as fat, bone and cartilage. They reside in a tissue-specific niche, awaiting signals for tissue regeneration, if necessary [30]. In the last decade, MSC were extensively investigated for their usefulness in bone tissue engineering and diverse scaffold materials were suggested to provide an appropriate three-dimensional environment [31]–[34]. Promising tissue-engineered constructs are already on the way from bench to bedside [35]. However, the impact of scaffolds on osteogenic differentiation on the molecular level was rarely analysed and restricted to some well-known osteogenic marker genes, such as alkaline phosphatase, runt related transcription factor 2, type 1 collagen, bone sialoprotein and osteocalcin [34], [36].In the present study, we initially investigated a panel of different polymers for their influence on the osteogenic differentiation of MSC. We then focused on the two most prominent synthetic polymers Resomer® LT706 and PCL. Both polymers are linear, semi-crystalline polyesters, which are hydrolytically or enzymatically degradable. Resomer® LT706 is degradable within 8–12 month, while PCL is long-term degradable (>24 month) [20].PCL is an FDA proved and widely used engineering polymer with excellent physical, chemical and mechanical properties [37], [38]. It is already analysed in the context of bone tissue engineering, either as pure PCL [39], as composite scaffold e.g. together with hydroxyapatite or calcium phosphate [40], [41] or modified with a peptide layer such as RGD [16]. All of these studies suggest PCL as suitable scaffold material for bone tissue engineering.In contrast, Resomer® LT706 is a relatively novel and less-investigated synthetic polymer, described for the first time as Poly(L-lactide-co-TMC) in 2005 by Pospiech and coworkers [42]. Materials with related chemistry are shown to be biodegradable and biocompatible for soft tissue engineering [19] and used in clinical trials for skin replacements [17], [18]. However, our recent work showed that a prediction of cell behaviour on a chemically related material is not possible [5]. Resomer® LT706 was not analysed in the context of bone tissue engineering or osteogenic differentiation of MSC so far.We compared the osteogenic differentiation of MSC on the two elastomeric and long-term degradable synthetic polymers Resomer® LT706 and PCL. TCPS served as control. Our XPS data revealed that the two polyesters Resomer® LT706 and PCL do not differ qualitatively in their elemental composition, but quantitatively, indicated by the C/O ratio. The C/O-values for Resomer® LT706 and PCL are 1.7 and 2.6, respectively (Table S1). Further, XPS analysis of our biomaterial surfaces shows that PCL contains less oxygen atoms than Resomer® LT706, indicating PCL as more hydrophobic than Resomer® LT706. However, contact angle measurements detected the opposite, with Resomer® LT706 being more hydrophobic than PCL with the corresponding contact angles of 75° and 69°, respectively. The higher hydrophobic characteristic of Resomer® LT706 is a result of its molecular structure. Resomer® LT706 consists of 68–72% L-lactide units and thus includes sterically demanding and hydrophobic methyl groups in high frequencies at the surface. In contrast, PCL only includes methylene groups in its backbone. TCPS – in contrast to polystyrol – does not consist of phenyl rings, but also includes surface modifications (established by physical plasma) of hydroxyl-, carboxyl- and amino-groups on the surface and a contact angle of only 54° (Table S1).To allow for an adequate adhesion of MSC on our substrates (TCPS, Resomer® LT706, PCL and TCPS), an initial adsorption of cell adhesion mediators from culture medium, such as fibronectin (FN) and vitronectin (VN), is crucial, since in general, the less adsorbed proteins the less cell adhesion is mediated. As shown in our adsorption studies with radiolabelled FN and VN (Figure S1), Resomer® LT706 is covered with the highest amount of FN proteins, compared to TCPS and PCL. Further, the ratio of FN to VN on Resomer® LT706 is comparable to that on TCPS, albeit absolut values are higher for Resomer® LT706. It is known that extracellular matrix (ECM) proteins are involved in cell adhesion not only by their own presence, but also by their relation to other ECM proteins [43], [44]. This ratio of different ECM proteins then can result in differences in cell adhesion, morphology, proliferation, gene expression as well as ECM secretion. The similar ratio of FN/VN on Resomer® LT706 and TCPS compared to PCL (1.2 and 1.04, respectively) might be the reason for a comparable initial adhesion and morphology of MSC, which differs from that on PCL, where the cells are more roundish after 24 h of incubation [5]. However, MSC proliferation within 7 days after cell seeding was comparable for TCPS, Resomer® LT706 and PCL [5] and cell morphology was identical after 21 days of culture in GM or OIM (Fig. 1C). Although we detected a comparable proliferation of MSC on all three substrates, osteogenic differentiation capacity was different.In the present study, we found that both Resomer® LT706 and PCL cyused stronger alizarin red stainings compared to TCPS, which however was most significant for Resomer® LT706. The fact that the alizarin red staining is stronger for Resomer® LT706 than for PCL, though MSC proliferation is comparable on both polymers [5], suggests that Resomer® LT706 is more potent in supporting osteogenic differentiation than PCL. A positive Alizarin red staining is indicative for osteogenic MSC fate, but stains only calcium accumulations and thus provides only indirect evidence. We were thus interested in the quality and the molecular mechanisms underlying osteogenic differentiation of MSC on Resomer® LT706 and PCL. As initial step in this direction we performed whole genome expression analysis using Affymetrix gene arrays. Bioinformatic tools then unravelled (i) donor variations as expected for primary cells, (ii) clusters of samples representative for MSC cultured in growth medium (GM) or in osteogenic induction medium (OIM), indicating a stronger influence of culture medium than of biomaterial substrates, and (iii) clusters of TCPS and Resomer® LT706 under growth (GM) and differentiation conditions (OIM), demonstrating a similar influence on MSC gene expression. Pairs of TCPS and Resomer® LT706 were detected in all bioinformatic analysis. Although both materials are quite different e.g. in surface chemistry (polarity, hydrophobicity, surface charge) and bulk properties (e.g. stiffness), the impact on gene expression related to osteogenesis of MSC is comparable. Yet, TCPS is not useful for bone tissue engineering because of its fabrication characteristics based on its chemical and physical properties. Again this supports our recent study, showing that related biomaterials do not inevitably result in the same cellular response, while unrelated materials might do [5]. The heat map in Figure 4B shows that most genes were expressed on a similar level in MSC cultured on TCPS and Resomer® LT706, while only a few genes were differentially expressed on these two materials.We extended our gene array results by RT-qPCR of the six genes secreted frizzled related protein 4 (SFRP4), Proline/arginine-rich end leucine-rich repeat protein (PRELP), cartilage oligomeric matrix protein (COMP), α1 XI collagen (COL11A1), elastin (ELN) and chemokine (C-C motif) ligand 2 (CCL2) (Fig. 4C).For all of these genes published data suggest a role in skeletal development or osteogenic differentiation. SFRP4 is expressed in periosteum and bone tissue but overexpression of SFRP4 suppresses osteoblast proliferation [45]. PRELP is a connective tissue matrix protein, which is expressed in cartilage and in osteoblasts [46]. COMP is involved in skeletal development and osteoblast differentiation [47] and detectable in MG-63 cells, an osteoblast cell line. Mutations of COMP are related to specific diseases, such as pseudoachondroplasia and multiple epiphyseal dysplasia [48]. COL11A1 is essential for normal skeletal development, but has to be suppressed for terminal osteoblast differentiation [49]. Elastin degradation products promote osteogenic differentiation and elastin-related calcification is suggested to be involved in tissue repair processes [50]. Finally, CCL2 - which is known as chemokine for the recruitment of cells of the immune system, such as monocytes - is secreted by MSC and this secretion increases the differentiation into mature osteoblasts [51].Compared to MSC cultured on TCPS and Resomer® LT706, all genes showed lower expression in MSC cultured on PCL, except CCL2. Since all these genes are involved in skeletal development and bone formation, our results suggest Resomer® LT706 as more suitable for bone tissue engineering than PCL.In summary, we analysed osteogenic differentiation of MSC on Resomer® LT706 and PCL. Alizarin red stainings and expression of conventional osteogenic transcripts show that both biomaterials support osteogenic differentiation fate. However, whole genome expression analysis revealed differences in gene expression and genes involved in skeletal development and bone formation are more expressed at higher levels in MSC cultured on Resomer® LT706. Thus, this novel, long-term degradable and osteoconductive synthetic polymer is suggested as particularly attractive scaffold material for bone tissue engineering with superior properties compared to the currently being used material PCL. The in vitro transformation of MSC on Resomer® LT706 to more osteogenic genotypes might also translate to phenotypes. This hypothesis has to be verified in future in vivo models."
        },
        "10.1371/journal.pone.0022122": {
            "author_display": [
                "Jiangang Wang",
                "Yan Li",
                "Jiahai Shi",
                "Jie Han",
                "Chunlei Xu",
                "Changsheng Ma",
                "Xu Meng"
            ],
            "title_display": "Minimally Invasive Surgical Versus Catheter Ablation for the Long-Lasting Persistent Atrial Fibrillation",
            "abstract": [
                "Objective: To assess the efficacy of video-assisted minimally invasive surgical vs. catheter ablation for the long-standing persistent AF. Methods: We performed a retrospective comparative analysis in a series of 166 long-standing persistent AF patients treated between 2006 and 2009 with either video-assisted minimally invasive ablation (83 patients), or catheter ablations (83 patients). The catheter group was screened using a ‘pair-matched case-control’ methodology in order to select appropriate statistical comparison candidates out of 169 long-standing persistent AF patients which were potentially suitable for surgery, but have been treated with catheter approaches in order to balance major prognostic factors between the two groups. Follow-up for all patients ranged from 1 to 3.6 years. Results: No patient died postoperatively. One patient suffered from stroke in the surgical group but recovered before discharge. Freedom from AF was obtained in 59.0% and 74.7% during follow-up in the catheter group and the surgical group respectively (P = 0.047). Patients in the catheter group had a significantly higher rate of recurrent arrhythmia (P = 0.011, hazard ratio: 0.555, 95% CI: 0.354 to 0.872) compared with the surgically treated group. The freedom from antiarrhythmic drugs was 44.6% in the catheter group and 61.4% in the surgical group (P = 0.043). Conclusions: The video-assisted minimally invasive ablation was safe and effective, and had an optimistic success rate for patients with long-standing persistent AF in our retrospective comparative study. Thus, further randomized studies addressing this issue seem to be justified. "
            ],
            "publication_date": "2011-07-13T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 3,
            "views": 1980,
            "shares": 0,
            "bookmarks": 3,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0022122",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0022122&representation=PDF",
            "fulltext": "IntroductionAtrial fibrillation (AF) affects proximately 10 million patients in China, undoubtedly the largest demographic group in the world [1]. Catheter ablation targeting pulmonary vein isolation (PVI) has been evolved over the past decade and has become the common treatment for AF [2], but it often requires repeated procedures, and is associated with serious, although infrequent, complications [3], [4]. It has limited success in persistent and permanent AF, although different ablation strategies beyond PVI have been described to improve the outcome in this patient group [5], [6]. Wolf and colleagues reported encouraging early experiences with a stand-alone, minimally invasive, video-assisted thoracoscopic surgical technique for the epicardial ablation of AF on a beating heart with the use of dry radiofrequency (RF) bipolar energy [7].\nTo assess the clinical outcome of surgical ablation we performed a retrospective analysis comparing two pair-matched groups of patients with long-standing persistent AF, one treated with video-assisted minimally invasive ablation, and the other one treated with conventional catheter approach.\nMethods\n2.1 Objective\nIn this restrospective study we compared the surgical minimally invasive surgery method versus the internal medicine catheter ablation method in order to evaluate an optimal proceeding for the treatment of long-standing atrial fibrillation and we hypothesised that minimally invasive surgery is the superior approach.\n\n\n2.2 Patients\nBetween 2006 and 2009, 91 consecutive patients with long-standing persistent AF were treated with video-assisted thoracoscopic surgical ablation at the Atrial Fibrillation Centre, Beijing Anzhen Hospital. Long-standing persistent AF was defined as continuous AF lasting longer than 1 year, resistant to either electrical or pharmacological cardioversion. Indications for surgical AF ablation were: Drug-refractory; inability to tolerate antiarrhythmic drugs or anticoagulation therapy as well as left ventricular ejection fraction of 30% or greater. The patients should also have been able to provide a written informed consent, their life expectancy should have been at least 2 years and they must have been able to attend scheduled follow-up visits. The exclusion criteria in the surgical group were: left ventricular ejection fraction lower than 30%, sick sinus syndrome, severe pleural adhesions as well as prior attempts with catheter ablation for a treatment of atrial fibrillation, what means that none of the patients in the surgical group, which we have chosen for the comparison, underwent a previous catheter ablation. In order to assess the role of surgery in the treatment of long-standing persistent AF, treatment outcome in this group of patients was compared with that of patients managed with conventional catheter approaches. The inclusion criteria for a treatment with catheter AF ablation were: Symptomatic AF refractory or intolerance to at least one class 1 or 3 antiarrhythmic medication and patients with heart failure and/or reduced ejection fraction. In addition in rare clinical situations, it may be appropriate to perform catheter AF ablation as the first line therapy. The exclusion criteria in the catheter group were: Left ventricular ejection fraction <30 percent, presence of left atrial thrombus on transesophageal echocardiography and prior attempts with catheter or surgical ablation for an atrial fibrillation treatment.\nTo secure a relatively reliable comparative analysis between patients treated with and without surgery, we performed a matched case-control procedure. From the group of patients treated with catheter approaches we selected cases with similar favorable prognostic factors matching those in the surgical group and from a total of 169 patients treated with catheter approaches, we selected 89 cases which also would have been potentially suitable for surgery. Each patient from the surgical group has then been pair-matched to a patient from the screened catheter group, taking into account the most important prognostic factors, which were duration of AF, left atrial dimension and sex. Patients from both groups for whom no comparable control case could be found were then excluded from further analysis. As a result of this process, 166 patients were selected for a comparative analysis; 83 treated with minimal invasive surgery versus 83 managed with catheter approach (Table 1).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Patient characteristics.doi:10.1371/journal.pone.0022122.t001\n\n2.3 Catheter ablation\nAblation was guided by an electroanatomical mapping system (CARTO, Biosense Webster) and performed with a temperature-controlled, quadripolar, deflectable catheter with a 3.5-mm irrigated-tip catheter (Navistar, Biosense Webster) to encircle the left and right pulmonary vein (PV) 1 to 2 cm from their ostia, with additional lines in the posterior left atrium or roof and along the mitral isthmus. Two decapolar circular mapping catheters (Lasso, Biosense-Webster Inc) were placed inside the ipsilateral superior and inferior PVs or within the superior and inferior branches of a common PV to verify PVI during radiofrequency current ablation. RF current was delivered at each site until the local electrogram amplitude was reduced by at least 80%. Contiguous lesions were delivered at about 20 mm from the PV ostia. Additional ablation within the circles was performed in most of the patients, outside the PVs, where the local electrogram amplitude exceeded 0.2 mV. Additional lines were created in the posterior wall between the 2 encircling PV lesions and from the lower aspect of the left inferior PV to the mitral annulus. If AF was still present at the end of PVI, transthoracic cardioversion was used to restore sinus rhythm (SR). The end point was defined as the absence of any PV spike recorded on the 2 Lasso catheters placed within the ipsilateral PVs after at least 30 minutes following the PV isolation during SR.\n\n\n2.4 Surgical Technique\nThe video-assisted thoracoscopic surgical procedure was developed to perform an electrically PVI bilaterally. In brief a bipolar RF clamp and RF generator system (AtriCure, Inc, Cincinnati, Ohio) is used to achieve linear, transmural ablation lesions. The procedure is conducted after a general anesthesia administered with a double-lumen endotracheal tube and a transesophageal echocardiography used in the operating room is included in order to verify the left atrial appendage (LAA) excisions at the end of the procedures. As shown in Fig. 1 (A–B), each set of PV was encircled and ablated on the left atrium by using an epicardial-placed bipolar RF clamp and no other epicardial lesions were placed. The ligament of Marshall was taken down in all patients and a detailed ganglionic plexus (GP) mapping and ablation was performed. Thoracoscopic LAA exclusion was then performed using a Thoracic Endoscopic Linear Cutter EZ45G (Ethicon Endo-Surgery, Inc, Cincinnati, OH), with non buttressed 4.8-mm staples (Fig.1C). Appropriate stapler angle and adequate appendage coverage was enhanced by direct visualization through the working port.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Video-assisted thoracoscopic surgical ablation.(A) Right thoracoscopic view of the bipolar clamp in place on the left atrial antrum, medial to the right PV. (B) Left thoracoscopic view of the bipolar clamp in place on the left atrial antrum, medial to the left PV. (C) The left atrial appendage is excised by stapling it with a stapler.\ndoi:10.1371/journal.pone.0022122.g001\n\n2.5 Intraoperative Electrophysiologic Testing\nIntraoperative electrophysiologic testing was performed, which includes bilateral PV antrum, baseline and post isolation sensing, pacing and GP detection. A baseline positive sensing result (rapid and disorderly atrial potentials) in the PV antrum area could be detected before PVI, and a negative sensing result (no atrial potentials) could be detected in the same area after ablation, which is called entrance block. A positive baseline pacing result is defined as the atrial and ventricular capture, what is the contraction of the atrium and ventricle in response to the electrical stimulus being sent from the temporary pacemaker (Oscor Pace 203H DDD External Dual-Chamber Pacemaker; Oscor Inc, Palm Harbor, Fla). A negative postablation pacing result means that no capture is obtained in the same area after ablation. A combined positive baseline pacing and negative postablation pacing result is called exit block. Achieving both entrance and exit block is regarded as a transmural lesion blocking of the conduction in the PV antrum area. Meanwhile, GP activity detection is included in the electrophysiologic testing procedure, as described by Mehall et al, which consist of a basically mapping by using a high-frequency stimulus (10 V, 800 times per second) for 5 seconds or more [8]. A positive response was defined as sinus bradycardia (<40 beats/min) or asystole, atrioventricular block, or hypotension occurring ventricular asystole of the onset of high-frequency stimulus.\n\n\n2.6 Postablative Medical Management\nPostablative anticoagulation treatment was done according to the American College of Cardiology/American Heart Association/European Society of Cardiology guidelines [9]. The decision whether to apply warfarin or aspirin has been done on the base of the patients Cardiac Failure, Hypertension, Age, Diabetes, Stroke (CHADS) score. Patients received 200 mg of amiodarone orally per day for three months. In the 4th month anticoagulant drug administration was discontinued when SR was present, what was examined by a 24–48 hour Holter monitoring. An AF or atrial flutter (AFL) episode was defined after documentation by means of ECG or Holter monitoring as at least 30 seconds lasting. During the follow-up period, if ECG analysis showed recurrence of AF or AFL which sustained for more than 8 hours and the anticoagulation value was adequately (International normalized ratio >2.0), a direct-current cardioversion was recommended, and a circumferential PV catheter ablation was performed.\n\n\n2.7 Follow-up procedure\nFollow-up controls were obtained from office visits at an outpatient building, mailed medical records received from local hospitals, and questionnaires. As a standard method for monitoring patient' heart rhythm we used a 24 to 48 hour Holter monitoring and additional twelve-lead ECG analysis for AF or AFL recurrence. Transthoracic UCG were evaluated at discharge and 1, 3, 6 and 12 months postoperatively. After one year, patients were seen every 6 months by their referring cardiologist. Free ECG examinations as well as free 24 to 48 hour Holter monitoring (Del Mar Reynolds Medical, Inc, Irvine, Calif) were offered at the Atrial Fibrillation Centre, Beijing Anzhen Hospital.\n\n\n2.8 Ethical approval and consent\nThe protocol was approved by ( Institutional Review Board or Ethics Committee of Beijing Anzhen Hospital, Capital Medical University). All participants in this study gave a written informed consent.\n\n\n2.9 Statistical analysis\nContinuous data were presented as the mean ± standard deviation and categorical variables were expressed as the number of cases and percentage. The significance of the differences between the groups was assessed by the Student t test or Mann-Whitney U test for continuous variables and chi-square test for categorical variables. Cumulative event rates were calculated according to the Kaplan-Meier method. All tests were 2-tailed, and p<0.05 was considered significant. The data were analyzed with the SPSS for Windows version 12.0 (SPSS Inc., Chicago, IL, USA).\n\nResults\n3.1 Immediate results during hospital observation\nAll patients successfully underwent the ablation procedure and there was no PV injury, in-hospital mortality, or necessity of reoperation because of bleeding. Circumferential PV catheter ablations were performed within a mean of 39±14 minutes of RF energy. The catheter group had a statistically longer procedure time for the entire ablation (231±27 minutes) than the surgical group for the entire operation (143.4±26.2 minutes) (P = 0.016). One patient with AF rhythm suffered from stroke in the surgical group, due to poor anticoagulation status (international normalized ratio<2.0 ), but recovered without any sequelae before discharge.\nThe changes in the cardiac rhythms during stay in hospital are described in Table 2. During ablation, AF termination (conversion in SR) was seen in 34.9% of the catheter group and in 45.7% of the surgical group of patients, respectively (P = 0.15). AF organization into AFL occurred more often in the catheter group, 24 patients (28.9%), 13 right sided and 11 left sided, than in the surgical group, 5 patients (6.0%), 2 right sided and 3 left sided (P<0.001). AF was documented to be 39.8% in total of the two groups. Temporary pacemakers were used in 4 patients from the surgical group because of bradycardia.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Cardiac Rhythm in Hospital.doi:10.1371/journal.pone.0022122.t002During hospitalization, AF and AFL recurred in 16 (19.3%) and 22 (26.5%) of the patients in the catheter group after a mean time of 2.0±1.5 days, and 12 (14.5%) and 2 (2.4%) patients in the surgical group after a mean time of 6.0±1.4 days, respectively (P = 0.70 and P<0.001). At discharge from the hospital, SR and AF was maintained in 86.8% and 4.8% of the patients in the catheter group and 63.9% and 28.9% in the surgical group respectively (P<0.001).\nThe average length of stay in hospital was 6.1±2.6 and 4.8±2.5 days in the surgical group and catheter group, respectively (P = 0.56).\n\n\n3.2 Follow-up Results\nAverage followed-up was 2.2years (range 1.0 to 3.6 years) after the procedures. During the follow-up period, two patients (2.4%) died in the catheter group, one for unknown reason and the other one with encephalorraghia. One patient (1.2%) died in the surgical group for unknown reason.\nTaking a blanking period of 3 months after surgery into account, 49 (59.0%) and 62 patients (74.7%) did not have a ‘single’ registration of AF during follow-up in the catheter group and surgical group respectively (P = 0.047). Additionally, patients in the catheter group had a significantly higher rate of recurrent arrhythmia (P = 0.011, hazard ratio: 0.555, 95% CI: 0.354 to 0.872). (Fig.2)\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Freedom from recurrence after AF ablation.CI = confidence interval.\ndoi:10.1371/journal.pone.0022122.g002In the catheter group, a second ablation procedure was performed in 23 patients with arrhythmia recurrence. Following the 2nd redo procedure, a third redo ablation was performed in 6 patients. Two patients who had failed the third redo ablation were treated then with the surgical procedure and were free from AF after follow-up.\nFive patients with recurrent AF, within the surgical group, who agreed to a second treatment, underwent a catheter ablation successfully. Two of these patients were proven to have a bidirectional block based on tests performed during the surgery suffered recurrent AF. Subsequent CARTO electrophysiologic mapping showed complex fractionated atrial electrograms (CFAEs) in both atriums. The CFAEs ablation converted AF into atrial tachycardia, with the average AF cycle length of the 2 patients increasing from 140±12 ms to 214±23 ms after CFAEs ablation. Three-dimensional activation time-sequence mapping of the left atrium, combined with entrainment mapping, was then performed to clarify the underlying focal or macroreentry mechanism. The resulted “Entrainment” refers to higher tachycardia that occurs consistently during high frequency pacing; when the pacing frequency stops or is reduced below the original tachycardia, the tachycardia will drop back to a baseline level. The two patients were found to have several focal gaps originating from the roof and bottom of PVI lines. One of them a gap originating from the roof of the left superior PV down to the left inferior PV. In the third patient with atrial tachycardia a microreentrant cycle around the base of occlusive left appendage was found. An entrainment was performed to verify the reentrant mechanism. Interruption of the circuit as well as ablation of CFAEs mainly located at the posterior of the left atrial terminated the arrhythmia in this patient. The forth patient suffering AFL was identified anticlockwise (anterior of left atrial bottom area to posterior left atrial roof), with the average AF cycle length 302 ms. Ablation at the atrial roof between the two superior PVs, and ablation of CFAEs terminated the AFL. The fifth patients had mitral valve annulus-related atrial flutter. The creation of lesions from the left inferior PV to the mitral valve annulus was required to terminate the AFL in this patient. At the latest follow-up, all five patients were free of arrhythmias and independent of antiarrhythmic drug (AAD).\nFrequently, the decision to terminate AAD was made after 3-month monitoring. For patient who had a successful procedure, the freedom from AAD was 44.6% and 61.4% in catheter group and surgical group respectively (P = 0.043).\nDuring the follow-up, cerebrovascular accidents occurred in two patients (2.4%) of the surgical group, and in one patient (1.2%) of the catheter group. The 1- and 3- year actuarial survival free from stroke rates were both 99.0%±1.0% in the catheter group, and both 96.0%±1.5% in the surgical group, respectively (P = 0.27).\nEleven (6.6%) patients had to undergo preoperative permanent pacemaker implantation because of sick SR. During the follow-up, in three patients (3.6%) permanent pacemakers were implanted in the catheter group and in one patient (1.2%) of the surgical group. The 1- and 3- year actuarial survival free from implanting permanent pacemaker were both 97.0%±2.0% in the catheter group, and 100% and 97.0±3.0% in surgical group, respectively (P = 0.20).\n\nThis is the first case-matched retrospective study comparing catheter and video-assisted minimally invasive ablation techniques in patients with long-standing persistent AF. Although at discharge from the hospital, the maintenance of SR was higher in the catheter group, we found that minimally invasive ablation resulted in long-term maintenance of SR in 74.7 percent of the patients (vs. 59.0% in the catheter group, P = 0.047). Additionally, a Kaplan-Meier analysis showed that patients in the catheter group had a significantly higher rate of recurrent arrhythmia.Catheter-based PVI is relatively effective in maintaining SR for paroxysmal AF (65% to 70% success rate); for non paroxysmal AF, however, the PVI success rate is dramatically lower (20% to 30%) [10]–[12]. Therefore, different strategies adjunctive to PVI or using other concepts have been developed. These strategies include: creation of various ablation lines (mitral isthmus, roof line, posterior lines to isolate the posterior wall); ablation based on the CFAEs as well as cardiac autonomic denervation [13]–[16]. Oral et al evaluated catheter ablation with chronic AF. After a follow-up period of 7–14 months, only a mean of 33% of patients were in SR [17]. Tilz et al reported on 205 consecutive patients with long-standing persistent AF who underwent PVI, left linear and CFAEs catheter ablation. After a mean of 1.7±0.8 procedures, 135 of 199 patients (67.8%) remained in SR. Eighty-six patients (43.2%) remained in SR following PVI performed as the sole ablative strategy [16] and our results are in accordance with these previous reports who also adopted this technique. The PVI procedure appears to have a low success rate; however, the comparative advantages of a minimally invasive procedure have not been systematically evaluated yet.The success rate achieved in the most challenging surgical groups of patients, those with long-standing persistent AF, was not clear. Earlier in our series, the long-standing persistent AF group shows 44.4% and 71.4% SR restoration rates at 3 and 6 months postoperatively, respectively [18]. Similar research also reports 67% and 72% success rates, respectively [19], [20].Meanwhile our results show an optimistic therapeutic effect for long-standing persistent AF. This could be either due to a successful addition of bipolar ablation or due to the additional GP-ablation. During the minimally invasive operation, the energy is delivered between the jaws together with a transmurality feedback algorithm, and the transmurality of the isolation lesion is theoretically ensured. Besides the transmurality algorithm and facilitated multiple isolation lesions, the epicardial electrophysiologic testing further helps to evaluate the quality of the atrial lesion. What needs to be specially mentioned is that there are possible (24 patients in the surgical group (28.9%)) persistent regular and small potentials sensed even if ablations were done on a certain side. Often, these diminutive potentials recorded after ablation can represent “far-field” left atrial potentials. The effect of this “positive” postablation sensing and GP detection results on short-term and midterm therapeutic outcomes is under observation in a concurrent study. In this study pacing results show that all patients who were in SR after bilateral ablations did not show “atrial and ventricular capture” and accordingly achieved “exit block”.Investigators have showed that conversion of a premature depolarisation to AF depends on GP stimulation [21]. They could even initiate ectopic foci from the PV by GP stimulation alone. It is clear that the autonomic nervous system plays a key role in the initiation and maintenance of AF. Pappone first showed that patients with a vagal response during ablation had a higher success rate in comparison to patients without a vagal response [22]. Scherlag were the first to compare PVI with and without GP-ablation in a prospective study [23]. The follow-up data suggest better outcome with additional GP-ablation. The reported success rates vary between 29% and 84% [24]. Earlier in our series, ectopic foci outside the PV played an important role in persistent AF [19]. GP-ablation is, therefore, expected to contribute to the success of procedures for long-standing persistent AF.This study supports the feasibility of amputation of the LAA through the minimally invasive technique. In addition after amputation of the LAA, postoperative thromboembolic events were substantially reduced. During the follow-up, only two patients had cerebrovascular events.The main limitation of this study is the retrospectivity. A larger, randomized and prospective trial with catheter based and minimally invasive ablation for lone AF is underway and will better define the efficacy of these two procedures in accordance with published guidelines.A second limitation is that PVI with GP-ablation is not adequate to treat long-standing persistent AF. Patients with long-standing persistent AF pose a challenge because various pathophysiologic changes that occur in the atria of such patients may adversely affect the success of various treatment strategies. We reported electrophysiologic findings in patients presenting with recurrent atrial arrhythmia after minimally invasive procedures. Gaps at the roof and bottom of the PVI ring may contribute to the underlying mechanism of recurrent arrhythmias [18]. The connecting lesions between the left superior PV and left appendage and right inferior PV to the mitral valve annulus, as well as the roof line of the LA are all important; however, it is difficult to ablate these sites when the heart is beating using the present device during minimally invasive procedures. With the recently available Coolrail (AtriCure, Inc, West Chester, Ohio, USA), totally left-sided maze III lesion sets can be achieved relatively easy. In long-standing persistent AF, the efficacy of this procedure could benefit from its more extensive lesion set.Our results suggest that in patients with long-standing persistent AF, the minimally invasive PVI with GP-ablation had the better ability to maintain SR. A further randomized study assessing the role of minimally invasive surgery in AF patients seems to be justified."
        },
        "10.1371/journal.pone.0071090": {
            "author_display": [
                "Kátia Regina da Silva",
                "Roberto Costa",
                "Elizabeth Sartori Crevelari",
                "Marianna Sobral Lacerda",
                "Caio Marcos de Moraes Albertini",
                "Martino Martinelli Filho",
                "José Eduardo Santana",
                "João Ricardo Nickenig Vissoci",
                "Ricardo Pietrobon",
                "Jacson V. Barros"
            ],
            "title_display": "Glocal Clinical Registries: Pacemaker Registry Design and Implementation for Global and Local Integration – Methodology and Case Study",
            "abstract": [
                "Background: The ability to apply standard and interoperable solutions for implementing and managing medical registries as well as aggregate, reproduce, and access data sets from legacy formats and platforms to advanced standard formats and operating systems are crucial for both clinical healthcare and biomedical research settings. Purpose: Our study describes a reproducible, highly scalable, standard framework for a device registry implementation addressing both local data quality components and global linking problems. Methods and Results: We developed a device registry framework involving the following steps: (1) Data standards definition and representation of the research workflow, (2) Development of electronic case report forms using REDCap (Research Electronic Data Capture), (3) Data collection according to the clinical research workflow and, (4) Data augmentation by enriching the registry database with local electronic health records, governmental database and linked open data collections, (5) Data quality control and (6) Data dissemination through the registry Web site. Our registry adopted all applicable standardized data elements proposed by American College Cardiology / American Heart Association Clinical Data Standards, as well as variables derived from cardiac devices randomized trials and Clinical Data Interchange Standards Consortium. Local interoperability was performed between REDCap and data derived from Electronic Health Record system. The original data set was also augmented by incorporating the reimbursed values paid by the Brazilian government during a hospitalization for pacemaker implantation. By linking our registry to the open data collection repository Linked Clinical Trials (LinkedCT) we found 130 clinical trials which are potentially correlated with our pacemaker registry. Conclusion: This study demonstrates how standard and reproducible solutions can be applied in the implementation of medical registries to constitute a re-usable framework. Such approach has the potential to facilitate data integration between healthcare and research settings, also being a useful framework to be used in other biomedical registries. "
            ],
            "publication_date": "2013-07-25T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 1965,
            "shares": 0,
            "bookmarks": 10,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0071090",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0071090&representation=PDF",
            "fulltext": "IntroductionOver the past few years, the worldwide volume of healthcare and clinical research data generated has been significantly expanded [1]–[3]. Data sources now encompass multiple registries and clinical trials as well as the progressive implementation of hospital administration and electronic health record (EHR) systems [1]–[6]. As a special case of data collection systems, medical device registries have been essential to guide improvements in technology and to facilitate the refinement of patient selection in order to maximize outcomes with current and new device options [4], [5], [7], [8]. Studies derived from well-designed and well-conducted medical devices registries can provide a real-world view of clinical practice, patient outcomes, safety, comparative effectiveness and cost effectiveness and may strengthen a number of evidence development and decision making process [4], [5], [7]–[14].\nDespite its huge potential for both biomedical research as well as the potential to positively affect clinical practice and healthcare policies, medical registries are frequently surrounded by process problems that substantially decrease their value [4], [15], [16]. These include missing data and poor data quality, which is related to how the research component of the registry is connected to clinical workflow and how personnel involved in the data collection are trained [4]. Compatibility problems with other health registries or publicly available data sets, which are associated with how data elements are structured and defined to accomplish the registry's intended purposes are other weakness presented in large quantity of electronic medical registries [4], [17]–[20].\nAlthough web-based electronic data capture (EDC) systems have become more prevalent across the globe, the data collection for research purposes is still a challenging process [4], [20]–[22]. Lack of harmonization between the clinical and research workflows is time consuming for both clinical staff and patients [23], [24]. In addition, many hospitals and healthcare facilities that participate in studies present different data capture systems for both healthcare and research settings resulting in effort duplication, ultimately leading to data inconsistency [4], [18]–[20].\nAdopting standardized data elements and a common terminology is arguably the key to facilitate the exchange of data across studies and to promote interoperability between different EHRs systems [4], [17]–[20]. The objective of this study is therefore to describe a reproducible, highly scalable, standard framework for a device registry implementation addressing both local data quality components as well as global linking problems. In the first section of our article we set the theoretical background, while in the second section we provide a clinical use case involving a pacemaker registry implementation designed to systematically collect interpretable long-term safety and outcomes data.\nMethods\nRegistry description\nThe Pacemaker Registry Open Data Collection is derived from the SAFE-LV PACE randomized trial (“Safety and the Effects of Isolated Left Ventricular Pacing in Patients With Bradyarrhythmias,” ClinicalTrials.gov study ID NCT01717469). This randomized controlled study is being conducted to compare the effects of conventional right ventricular (RV) pacing versus left ventricular (LV) pacing in patients with atrioventricular block. Our main hypothesis is that isolated LV pacing through the coronary sinus can be used safely and provide greater hemodynamic benefits to patients with atrioventricular block and normal ventricular function who require only the correction of heart rate. Specifically, our aims are to evaluate the safety, efficacy and the effects of LV pacing using active-fixation coronary sinus lead – Attain StarFix® Model 4195 OTW Lead, compared to RV pacing in patients with implantation criteria for conventional pacemaker stimulation.\nIn this registry we are creating a large and interoperable database to report pacemaker long-term outcomes. All clinical data stored will maintain full patient confidentiality according to Good Clinical Practices (GCP) and the Health Insurance Portability and Accountability Act (HIPAA) [25] and will be freely available to allow collaboration between researchers around the world. Main advantages of this open data collection include the incentive for interdisciplinary and multi-institutional collaborations, along with the creation of clinical and policy measures in a more timely manner.\n\n\nGlocal registry methodology\nThe Institutional Review Board of the Clinics Hospital of the University of São Paulo Medical School (São Paulo, Brazil) approved this study. All participating subjects provided written informed consent. All elements in this article comply with a reproducible research protocol [26].\nThe device registry implementation comprised a group of generic processes successfully applied to project management, including the initiation, planning, execution, monitoring and controlling, and closing. The sequence included: (1) data planning used to define the common data standards and terminology as well as the representation of the research workflow, (2) development of electronic case report forms using REDCap (Research Electronic Data Capture), (3) the process of data collection according to the clinical research workflow, (4) the aggregation between the registry data and other systems, (5) data quality control and data analysis using statistical methods and, finally (6) the data dissemination through the registry Web site (Figure 1).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Registry processes representation.Legend: ACC/AHA =  American College of Cardiology/ American Heart Association; CDISC =  Clinical Data Interchange Standards Consortium; CRF =  Case Report Form; HL7 =  Health Level Seven; NCDR =  National Cardiovascular Data Registry; REDCap =  Research Electronic Data Capture.\ndoi:10.1371/journal.pone.0071090.g001\n\nDefining Data Elements\nOver the last few years, the American College of Cardiology (ACC) and the American Heart Association (AHA) have started an initiative to develop and publish clinical data standards that can be used in a variety of data collection efforts for a range of cardiovascular conditions [27], [28]. The ACC/AHA Writing Committee to Develop Clinical Data Standards for Electrophysiology was charged with providing standard definitions to relevant terms in the care of patients with a diagnosis of arrhythmia and implanted cardiac electronic devices [29].\nOur registry adopted all applicable data elements and definitions in accordance with ACC/AHA available published data standards, including those developed for Electrophysiology, Atrial Fibrillation, Acute Coronary Syndromes, Heart Failure, and Cardiac Imaging [29]–[33]. Other data sources included data elements from large device clinical trials and registries, such as CTOPP (Canadian Trial of Physiologic Pacing) [34], MOST (Mode Selection Trial in Sinus Node Dysfunction) [35], COMPANION (Comparison of Medical Therapy, Pacing, and Defibrillation in Heart Failure) [36], REVERSE (REsynchronization reVErses Remodeling in Systolic Left vEntricular Dysfunction) [37]. We also reviewed case report forms, data elements, and definitions from international data collection efforts. Examples of these data sources include the ACC National Cardiovascular Data Registry (NCDR) [38], [39], Health Level Seven International (HL7) [40], Clinical Data Interchange Standards Consortium (CDISC) [41] and Cancer Data Standards Registry and Repository (caDSR) [42], [43]. Finally, we also included standardized definitions for clinical endpoints and adverse events in cardiovascular trials from the US Food and Drug Administration (FDA) [44].\n\n\nDefining the Registry Workflow – Clinical Activity Model\nBased on discussions with practicing clinicians and participatory observation of the clinic by two of the authors (KRS and RC), UML (Unified Modeling Language) activity diagram models were prepared to represent the clinical registry as well as the data collection workflow. A comparison of these clinical and data collection workflow models was then conducted to ensure the detection of potential areas where the activities related to data collection might not be in perfect alignment with the activities executed in the daily clinical workflow, ultimately leading to data quality issues, rework, and other processes inefficiencies. These diagrams were modeled according to UML version 2.0 [45]. All activity diagrams were created using ArgoUML (version 0.34) [46].\n\n\nElectronic Data Collection\nOnce the absence of potential workflow dissonance was ensured through the modeling, electronic case report forms (CRFs) were developed using the REDCap [21] EDC tool hosted at a local server within the firewall of the University of São Paulo Health System. REDCap is a secure web-based software and workflow methodology for electronic collection and management of research data (Figure 2). Among other characteristics it provides (1) an intuitive interface for validated data entry, with automated data type and range checks; (2) audit trails for tracking data manipulation and export procedures, (3) automated data export procedures to common statistical packages, and (4) procedures for importing data from external sources [21]. Research coordinators performed data capture by using a tablet computer through a secure Wi-Fi, ultimately allowing for portable data collection at the point of care.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  REDCap Data Entry.Footnote: Case report forms are accessible to users who have sufficient access rights and it contains field-specific validation code sufficient to ensure data integrity.\ndoi:10.1371/journal.pone.0071090.g002\n\nPersonnel Training for Data Collection\nWe performed a semi-structured training with the clinical research coordinators. Our goal was to provide a general overview of the registry database, while concurrently identifying specific factors which could compromise the integrity of the data collection. To ensure a standardized and consistent data collection we developed a standard operating procedure (SOP) specifically related to the primary data collectors tasks. This SOP provides a description of all data elements collected as well as the sources used to obtain the data.\nAfter the training process, the data entry activities of clinical research coordinators were closely monitored for three months by the principal investigators (RC and KRS) to assess whether data collection was conducted according to the study protocol. We used the REDCap report tool for monitoring and querying patient records. Corrective actions were taken to address problems related to data inconsistency and missing information, involving retraining and immediate feedback on issues such as missing, out-of-range values and logical inconsistencies.\n\n\nData Augmentation\nThe purpose of data augmentation was to augment variables to the research component of our pacemaker registry data sets from clinical and administrative sources, ultimately enhancing our ability to evaluate important research questions. The original dataset was augmented by incorporating data derived from three different instances: (1) EHR from the Clinics Hospital of the University of São Paulo Medical School (HCFMUSP); (2) Brazilian governmental database and (3) Linked Open Data (LOD) Collection. In the following section we describe the methodology used to perform the data integration across these sources.\nLinking Registry Data with Local Electronic Health Records.Whereas the study is being conducted at Heart Institute (InCor) – Clinics Hospital of the University of São Paulo Medical School, all demographics characteristics as well as healthcare information are available in several databases from legacy systems. Given the heterogeneity of these multidatabase systems, each patient has a unique identifier (ID) making it possible to associate the right health information with the right individual.\nIn order to avoid duplicate data entry, the EHR from the Clinics Hospital of the University of São Paulo Medical School (HCFMUSP) was integrated to the EDC through the REDCap API (Application Program Interface). The REDCap API is an interface that allows external applications to connect to REDCap remotely, and it is used for programmatically retrieving or modifying data or settings within REDCap. As the API is a built-in feature of REDCap, no installation is required and this tool implements the use of tokens as a means of authenticating and validating all API requests that are received. In addition, the API also implements data validation when the API is used for data import purposes in order to ensure that only valid data will be stored. By using the REDCap API, it was possible to retrieve useful demographic information directly from the sources of hospital systems.\n\nLinking Registry Data and Governmental Database.The original data set was augmented by incorporating publicly available data from the Brazilian governmental database known as DATASUS (Information Technology Department of the Brazilian Unified Health System, or SUS) [47]. This database produces a significant volume of information and provides the reimbursed values by the government for public healthcare organizations in both inpatient and outpatient care systems. For inpatients the common unit describing hospital charges is the hospital admission authorization, which is in accordance with the Hospital Information System. In addition, this database provides other information such as: reasons for hospitalization, length of hospital stay, socio-demographic characteristics, diagnoses, medical procedures, healthcare service providers and also the values paid for each procedure performed by public healthcare organizations.\nWe created a repository to store all anonymized data derived from DATASUS under the Amazon Elastic Compute Cloud – Amazon EC2 [48]. This repository hosts a MySQL server where the database is available in a normalized format. In this repository, we have stored a set of databases that comprises the basis for hospital accountability of the Brazilian Unified Health System (SUS), in which all diagnoses and procedures are coded according to ICD-10. Through this repository was possible to retrieve useful information such as reimbursed values by the government for pacemaker implantation as well as length of hospital stay.\nThis database is available in CSV (comma separated values) format files and all data are updated monthly on the Web site of DATASUS.\n\nLinking Registry Data with Linked Open Data Collections.In addition, we also enriched our registry by adding open semantic web data source for clinical trials named Linked Clinical Trials (LinkedCT) [49]. Each clinical trial in this database is associated with a brief description of the trial, related conditions, interventions, eligibility criteria, sponsors, locations and other additional information. This mapping was implemented by means of a SPARQL query interconnecting our dataset with the Linked Life Data (LLD) endpoint [50]. This approach enables the identification of correlated clinical trials and investigators in order to generate new opportunities for scientific collaboration.\n\nData de-identification.All data – including images, lab tests and any associated information – were de-identified before insertion into the repository as required by HIPAA (Health Insurance Portability and Accountability Act) regulations to ensure that protected health information (PHI) was not inappropriately used or disclosed [25]. The de-identification was performed by indicating a variable as PHI element during the project development process in REDCap and also by selecting those variables prior to exporting the data.\n\nData modeling resources.Our data repository also contains an instance of the R statistical language (version 2.15.1) [51], along with the RStudio Server version 0.96 IDE (integrated development environment). Through this infrastructure users can easily manipulate statistical scripts, generate reports, and directly upload them to the server on the same environment.\n\n\n\nData quality control, association and prediction reports\nWe established a system to generate automated data quality control and prediction reports based on the R statistical language. This system involves a set of packages enabling literate programming and reproducible research standards to automatically transform the statistical results into a real-time reports deployed in HTML (HyperText Markup Language) and PDF (Portable Document Format), both available from our central Web site [52]. Reports are created using the knitr package [53] and the Markdown language [54] in combination with R [51]. Specifically, we use R Markdown files with subsequent transformations to HTML and PDF performed through pandoc [55]. Documents are then presented on our Web server through the rApache package [56], ultimately ensuring that data quality reports are maintained up to date. Scripts for all of our procedures are available at our Github repository [57].\nAssociation reports are also provided as a mechanism for exploratory graphical analysis. Among them, we included the MINE (Maximal Information-based Nonparametric Exploration) algorithm [58], a sophisticated, robust algorithm used for exploratory analyses. Extensive use of exploratory graphical methods is facilitated by the use of the R package ggplot2 [59], along with other methods for data manipulation. Additional integrated services included the use of BigQuery [60] for manipulating large data sets as well as Google prediction services [61].\n\n\nOpen Design\nIn order to provide incentives for other researchers to join the collaboration and start creating analyses using the dataset, we have created a special section on our Web site [52] and Github repository [57] with a data dictionary and de-identified data sets in an Open Data format.\n\nResults\nPacemaker Registry Detailed Use Case\nThe use case model describes the process of information exchange involved in our pacemaker registry, detailing the infrastructure developed to enable interoperability between the EHR and REDCap. For this use case, the workgroup has prioritized the electronic data capture of standardized data elements in order to leverage a core set of widely useful clinical data from EHR systems to increase the effectiveness and efficiency of clinical research activities. The following diagram (Figure 3) illustrates the stakeholders involved in the processes described in this use case.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Pacemaker Registry Use Case Stakeholders.doi:10.1371/journal.pone.0071090.g003Indication of pacemaker implantation in a patient presenting bradyarrhythmia is the condition determining the start of this use case. By assessing patients, healthcare team entered demographic and clinical data into the EHR. Research coordinators identify subjects for the study based upon whether they meet the protocol eligibility criteria. Once study subjects were enrolled in the study, a core set of data may be exchanged from the clinical EHR system to REDCap as previously described in the “Linking Registry Data with Local Electronic Health Records” section. Research coordinators were responsible for completing the data retrieve from the EHR into the REDCap as well as for electronic data capture of additional study-specific data during the course of the study. All collected data is transmitted to the Data Work Group for validation and later to the Research Investigators Team. The Data Work Group is responsible for data maintenance, information exchange, and data aggregation with other databases. (Table 1, Figure 4).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Pacemaker Registry Activity Diagram to Support Data Exchange between EHR system and REDCap.Footnotes: (1) The study design is communicated to Clinical Trial Team, specifically to the Research Coordinator. (2) Case Report Form (CRF) is developed using the REDCap EDC tool hosted at a local server within the firewall of the University of São Paulo. Once built the CRF, the Registry Administrator will assign users rights for system access. (3) Patient is admitted to facility and the healthcare team entered demographic and clinical data into the EHR. (4) Research coordinators identify eligible study subjects by consulting the EHR patients records. (5) After patient enrollment, a REDCap API request is send to the Data Work Group for retrieving and importing socio-demographic information directly from the sources of hospital systems. (6), (7) Information is exchanged between the EHR and REDCap. (8) Registry administrator oversees all data collected by research coordinators. (9) CRF is transmitted from the research coordinators to the Data Work Group for data validation, data quality control e data analysis. (10) Data Work Group transmits CRF and aggregated data to the Research Team and Registry Administrator.\ndoi:10.1371/journal.pone.0071090.g004\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Pacemaker Registry Use Case Description.doi:10.1371/journal.pone.0071090.t001\n\nPacemaker Registry Workflow\nThe registry UML-AD represents the activity workflow associated with data capture for subjects meeting study criteria for inclusion in our registry. This workflow illustrates the process of patient care throughout diagnosis, assessment, treatment, and long-term monitoring of patients undergoing pacemaker implantation. In addition, this registry workflow was aligned with the clinical workflow to enhance quality of the data captured and also facilitate understanding of the clinical care and research processes as a common reference by both clinicians and technologists (Figure 5).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Pacemaker Registry Activity Diagram.Footnote: This figure represents the alignment between clinical (white flowchart) and research (blue flowchart) workflows.\ndoi:10.1371/journal.pone.0071090.g005\n\nClinical Data Standards\nMost variables contained in the CRFs were based on standardized data elements proposed by ACC/AHA Clinical Data Standards [28]–[33]. We also used variables derived from cardiac devices randomized trials [34]–[37], as well as NCI Thesaurus and CDISC data standards [42], [43]. (Table 2) The authors added specific pacemaker data elements which are not yet available in the standardization sources used in this study. Data standards for each variable class are detailed in Supporting Information (Table S1).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Pacemaker Registry Clinical Data Standards Elements.doi:10.1371/journal.pone.0071090.t002\n\nData quality control and prediction reports\nAnalysis of the data quality was performed in three instances: (1) Exploratory analysis of missing data to map the frequency, location and effect of missing data in a given dataset or variable class; (2) Descriptive statistics (mean, standard deviation and frequency) of subsets in different moments of data collection to establish a confidence limit; (3) Benford's Law or first-digit law in order to monitor for possible data fabrication. Data association and prediction plots were generated based on boxplots for reports of numeric data and association plots for categorical data. We also used the application of the MINE algorithm [58] to explore the association between two pairs of numeric variables, both linear and nonlinear. Corresponding code for the generation of automated reports in HTML and PDF is available in our Github repository [57] and graphs for each data analysis performed are available under Supporting Information (Report S1).\n\n\nData Augmentation\nScripts for the data augmentation are available under our Github repository [57]. A full report in HTML and PDF formats converted from our script is available on our central web site. As an example of an augmented variable, a summary of reimbursed values paid by the government during a hospitalization for pacemaker implantation and the length of hospital stay are presented in Table 3. The data in this table indicate the variation in costs and length of hospital stay according to the geographic region. Additional details about each Brazilian state are provided under Supporting Information (Table S2).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 3.  Reimbursed values paid by Brazilian government for pacemaker implantation according to geographic region.doi:10.1371/journal.pone.0071090.t003Table 4 shows a total of 130 clinical trials available at LinkedCT which are potentially associated with this pacemaker registry. The SPARQL endpoint is provided into our Github repository [57], as well as a full report with detailed conditions, interventions, eligibility criteria, sponsors, locations and other additional information. Additional details about each clinical trial are provided under Supporting Information (Table S3).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 4.  Cardiac Pacemaker Clinical Trials available at LinkedCT.doi:10.1371/journal.pone.0071090.t004\n\nOpen Design and Data Dissemination\nThe Open Data collection includes de-identified raw data sufficiently enough to describe the demographic and clinical profile of patients submitted to pacemaker implantation as well as surgical and clinical outcomes associated with both study interventions (Table 5). The following illustration (Figure 6) is derived from our Web site, in which all data will be updated every six months.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Pacemaker Registry Website.Figure 6A – Pacemaker Registry Website – General Information. Figure 6B – Pacemaker Registry Website – Open Data Collection.\ndoi:10.1371/journal.pone.0071090.g006\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 5.  Pacemaker Registry Open Data Collection.doi:10.1371/journal.pone.0071090.t005\nThe foundational work to create this pacemaker registry is part of a broader program to address the lack of data interoperability between the clinical and research settings. In this manuscript, we describe the infrastructure behind our Pacemaker Registry involving a diversity of steps such as: a comprehensive database planning, the alignment between research and clinical workflows, the adoption of clinical data standards, the development of electronic case report forms using REDCap, the aggregation between registry data and other systems and, finally the open data collection dissemination by the registry Web site.This methodological study is also an effort to implement glocal (global and local) data integration through a reproducible research protocol, which can be applied to other medical registries. In the scope of our study, “global” integration involves the adoption of global data standards and data interchange to facilitate information sharing within and across institutions. “Local” integration implies in integrating workflow between research and healthcare settings, and also in the interoperability between EHR and EDC systems.Successful registries depend on a sustainable workflow model that should be aligned to the daily clinical practice with minimal disruption [1]–[8]. Previous studies suggested [23], [24], [44] that workflow efficiency is a valuable factor for enhancing data quality and integrity since inefficient process may result in errors related to data collection and transcription, as well as unnecessary redundancy in the data collection [4], [5], [18]–[20]. In our study, we have made an effort to align the EDC system with the clinical workflow and we are currently working on the integration between EHR and EDC systems. In particular, the REDCap functionalities allowed us to develop an efficient interface between healthcare and research data collection, enabling the reuse of EHR data.For the development of interoperability and internationalization of our registry we focused firstly on data standards by using all existing standards terminologies whenever possible. It included all standard terminologies published by ACC/AHA [28]–[33], as well as data elements derived from large device clinical trials and other sources as NCDR [38], [39], HL7 [40], CDISC [41] and caDSR [42], [43]. The use of established data standards is crucial for semantic interoperability between information systems, which will be increasingly important as the use of electronic health information system is becoming widely available around the globe. It is also important to consider that the adoption of data standard terminologies not only improves the efficiency in establishing registries but also promote more effective sharing, combining, or linking of data sets from different sources and institutions. In addition, the use of well-defined standards for data elements ensures that the meaning of data captured in different systems is the same.Several different methods can be applied for the assurance of data quality and quality control in medical registries [4]–[6]. These methods may include site visits, ongoing training programs, use of standardized definitions and regular audits of the data for completeness and consistency [4]–[6]. The importance, registries should probably monitor not only data quality but also associations and clinical predictions. In order to monitor data quality, we established a system to generate automated data quality control and prediction reports based on the statistical language R [51]. As our registry is an ongoing study, the results provided here are empirical examples from a limited number of patients. However, automated data quality control and prediction reports will be frequently updated and will be available under our data repository [57].The demand for timely real-world data to support decision-making has driven the development of an increasing numbers of open data collections [17]–[20]. Adoption of open data policy is being encouraged not only by the U.S. government but globally by the editors of peer-reviewed journals [67]. Of the importance, open global databases are inherently necessary to accelerate the speed of evidence-based medicine and for an efficient, cost-effective healthcare system to improve the quality of patient care [1]–[8], [17]–[20]. Within our Open Data Collection protocol, socio-demographic, comorbidities and clinical characterization of patients undergoing pacemaker implantation will be publicly available in real time on a clouded-based repository following the concept of open data collection and under privacy, security and confidentiality policies (HIPAA) [25]. In addition to the data made available within clinicaltrials.gov, these variables will assist in the characterization of the study population for proper interpretation of published study results. The most important aspect of this approach is to foster a continuum between clinical care and clinical research leveraging the evidence development which may be successfully translated into better patient outcomes.Using data derived from a randomized clinical trial is both a limitation and strength of our study. While randomized clinical trials are often conducted under high scientific methodological standards, their generalizability could be limited by including selected populations. On the other hand, the randomization of patients included in our registry will allow the comparison of long-term outcomes between different treatment alternatives, which is a key strength of this open registry collection. Implementation of other technology solutions such as integration with a platform for adverse events monitoring, protocols for data augmentation through natural language processing (NLP), open literature repositories connected to R Markdown files and protocols for enhance patients follow-up are future perspectives that will guide our next efforts. Finally, this registry can not only be used for the comparison of data within pacemaker patients but also as a source for comparison and benchmarking between different conditions within and between institutions. We believe that the framework proposed in this article can be a useful tool for creating high quality and interoperable medical registries."
        },
        "10.1371/journal.pone.0063181": {
            "author_display": [
                "Stephen T. Chambers",
                "David Murdoch",
                "Arthur Morris",
                "David Holland",
                "Paul Pappas",
                "Manel Almela",
                "Nuria Fernández-Hidalgo",
                "Benito Almirante",
                "Emilio Bouza",
                "Davide Forno",
                "Ana del Rio",
                "Margaret M. Hannan",
                "John Harkness",
                "Zeina A. Kanafani",
                "Tahaniyat Lalani",
                "Selwyn Lang",
                "Nigel Raymond",
                "Kerry Read",
                "Tatiana Vinogradova",
                "Christopher W. Woods",
                "Dannah Wray",
                "G. Ralph Corey",
                "Vivian H. Chu",
                "International Collaboration on Endocarditis Prospective Cohort Study (ICE-PCS) Investigators "
            ],
            "title_display": "HACEK Infective Endocarditis: Characteristics and Outcomes from a Large, Multi-National Cohort",
            "abstract": [
                "\nThe HACEK organisms (Haemophilus species, Aggregatibacter species, Cardiobacterium hominis, Eikenella corrodens, and Kingella species) are rare causes of infective endocarditis (IE). The objective of this study is to describe the clinical characteristics and outcomes of patients with HACEK endocarditis (HE) in a large multi-national cohort. Patients hospitalized with definite or possible infective endocarditis by the International Collaboration on Endocarditis Prospective Cohort Study in 64 hospitals from 28 countries were included and characteristics of HE patients compared with IE due to other pathogens. Of 5591 patients enrolled, 77 (1.4%) had HE. HE was associated with a younger age (47 vs. 61 years; p<0.001), a higher prevalence of immunologic/vascular manifestations (32% vs. 20%; p<0.008) and stroke (25% vs. 17% p = 0.05) but a lower prevalence of congestive heart failure (15% vs. 30%; p = 0.004), death in-hospital (4% vs. 18%; p = 0.001) or after 1 year follow-up (6% vs. 20%; p = 0.01) than IE due to other pathogens (n = 5514). On multivariable analysis, stroke was associated with mitral valve vegetations (OR 3.60; CI 1.34–9.65; p<0.01) and younger age (OR 0.62; CI 0.49–0.90; p<0.01). The overall outcome of HE was excellent with the in-hospital mortality (4%) significantly better than for non-HE (18%; p<0.001). Prosthetic valve endocarditis was more common in HE (35%) than non-HE (24%). The outcome of prosthetic valve and native valve HE was excellent whether treated medically or with surgery. Current treatment is very successful for the management of both native valve prosthetic valve HE but further studies are needed to determine why HE has a predilection for younger people and to cause stroke. The small number of patients and observational design limit inferences on treatment strategies. Self selection of study sites limits epidemiological inferences.\n"
            ],
            "publication_date": "2013-05-17T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 3,
            "views": 4598,
            "shares": 0,
            "bookmarks": 11,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0063181",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0063181&representation=PDF",
            "fulltext": "IntroductionThe HACEK group of bacteria (Haemophilus species, Aggregatibacter species, Cardiobacterium hominis, Eikenella corrodens, and Kingella species) are a small, heterogeneous group of fastidious, gram-negative bacteria that frequently colonize the oropharynx and have long been recognised as a cause of infective endocarditis (IE). These organisms have been historically reported as causing infection in <5% of patients of IE [1], [2], and 0.8–6% of patients in recent population-based studies [3]–[5].\nDue to the relative rarity of HACEK endocarditis (HE), the clinical description and outcome has, of necessity, been derived from compilation of data from small case series and case reports [6]–[10]. These reports are limited by non-standardized data collection and selective reporting of patients. Consequently, the features of HE identified cannot be compared rigorously with other forms of IE.\nThe International Collaboration on Endocarditis Prospective Cohort Study (ICE-PCS) was designed to provide a large multi-national resource of prospectively collected, well defined patients of IE using a standardised data set. The objective is to improve understanding of the clinical characteristics and outcome of IE in a multi-national cohort of patients. In this report we describe the characteristics of patients with HE, and compare the risk factors, clinical characteristics, and outcomes of HE with IE caused by other pathogens (non-HE).\nPatients and Methods\nDatabase\nICE-PCS has been described previously [11], [12]. Briefly, participating members from 64 sites in 28 countries reported patients prospectively on a standard case report form from June 2000 through to September 2006. The case report form included 275 variables and was developed by ICE collaborators according to standard definitions [11]. All patients were included from sites that met performance criteria for participation. These criteria include the following: minimum enrolment of 12 patients per year in a centre with access to cardiac surgery; the presence of patient identification measures to ensure consecutive enrolment and to minimise ascertainment bias [11]; high quality data with query resolution.\n\n\nEthical Statement\nInitial institutional review board approval for ICE-PCS came from the Duke International Centre. All participating sites had institutional review board or ethical committee approval or a waiver and informed consent (verbal or written) or a waiver of consent from all patients based on local standards as required by the Duke Coordinating Centre.\n\n\nStudy Sample\nPatients in the ICE-PCS database were included in this study if they had definite or possible IE according to the modified Duke's criteria. HACEK isolates were identified and antibiotic susceptibility testing performed in the participating centres. Antibiotic therapy was decided by the treating physician at the individual study site.\n\n\nDefinitions\nInfective endocarditis was defined according to the modified Duke Criteria [13]. Infective endocarditis was considered to be left sided if no right sided (tricuspid or pulmonary valve) vegetations were present on echocardiographic examination, surgery, or autopsy. Community-acquired IE was defined as signs or symptoms of IE developing before hospitalization in a patient without extensive out-of-hospital contact with health care interventions or systems. Hospital acquired IE was defined as symptom onset and diagnosis occurring in a patient hospitalized for more than 48 hours.Health care–associated infection was defined as cases in which signs or symptoms consistent with infective endocarditis developed before hospitalization in patients with extensive out-of-hospital contact with health care interventions. Extensive out of hospital intervention included one or more of the following,(1) receipt of intravenous therapy, wound care, or specialized nursing care at home within the 30 days prior to the onset of IE; (2) visiting a hospital or hemodialysis clinic or receiving intravenous chemotherapy within the 30 days before the onset of IE; (3) hospitalization in an acute care hospital for 2 or more days in the 90 days before the onset of IE; or (4) residing in a nursing home or long-term care facility[14], [15].\nCancer was defined as any malignant neoplasm except basal or squamous cell carcinomas of the skin. The category “other chronic diseases” included connective tissue or rheumatologic disease, chronic liver or kidney disease, chronic neurological conditions, and other chronic infectious and inflammatory conditions. A diagnosis of heart failure was accepted on the basis of clinical evaluation performed by the care team and defined according to the New York Heart Association classification system [16]. Stroke was defined as an acute neurological deficit of vascular aetiology lasting more than 24 hours [17]. Systemic embolisation included embolisation to any organ including the skin. Valve surgery included all surgery performed on heart valves at any time during hospitalisation regardless of urgency. Rates of surgery and mortality include events that occurred during the index hospitalisation and one year follow-up. Repeat IE was defined as a further episode of IE fulfilling the modified Duke criteria. Confirmed relapse was defined as a repeat episode caused by the same microorganism on molecular analysis, as the preceding episode; confirmed new infection as a repeat episode caused by a different species or the same species but a different strain by molecular analysis; and possible relapse as repeat episode caused by a microorganism of the same species within 6 months of the initial episode without molecular analysis [18].\n\n\nGeographic regions\nGeographic regions participating in ICE included the following: United States (10 sites), South America (9 sites from Argentina, Brazil, and Chile), Europe (27 sites from Austria, Croatia, France, Germany, Greece, Ireland, Italy, the Netherlands, Romania, Russia, Slovenia, Spain, Sweden, and the United Kingdom), Australia and New Zealand (9 sites), Asia and Middle East (8 sites in India, Israel, Lebanon, Malaysia, Saudi Arabia, Singapore, and Thailand,) and South Africa (1 site).\n\n\nMicrobiological methods\nBlood cultures and sensitivity testing was performed by accredited laboratories using standard methods. Sensitivity testing was most commonly those of the Clinical and Laboratory Standards Institute (CLSI)\n\n\nStatistical analysis\nContinuous variables were represented as medians with 25th and 75th percentiles. Categorical variables were represented as frequencies and percentages of the specified group. Simple comparisons were made with the Wilcoxon rank-sum test or the chi square test as appropriate. For all tests, a p value of 0.05 or less was considered statistically significant. Missing data for each variable were excluded from the denominator. Variables found to have a simple association with the outcome of interest (p<0.10) were considered for the final multiple variable model in a stepwise fashion. The variables included in the final multiple variable adjusted regression model were selected based on a combination of statistical significance (p<0.05) and clinical judgment. The generalized estimating equation method was used to produce consistent parameter estimates that measure association between the incidence of outcome and clinical covariates while accounting for the correlation in treatment and outcomes of patients from the same hospital. Final parameter estimates were converted to ORs with corresponding 95% Wald CIs. All statistical analyses were performed using SAS version 9.2 (SAS Institute, Cary, NC, USA).\n\nResultsSeventy seven (1.4%) of 5591 patients diagnosed with IE in ICE-PCS had HACEK endocarditis (66 definite and 11 possible) and PVE was present in 27 (35%). The prevalence of HE differed significantly between the study sites (p = 0.009), with a low prevalence in North America and a high prevalence in Australia/New Zealand. The HE cases by region were: North America (5/992, 0.5%), South America (8/518, 1.5%), Australia/New Zealand (23/979, 2.3%), Europe (35/2806, 1.2%), Asia/Middle East (5/277, 1.8%), and Africa (1/19).\n\nFeatures of HE by species\nThe HACEK isolates were speciated in 76 (99%) cases with Haemophilus species the most common (40%) (Table 1). Of all HACEK species, only Kingella spp. was not associated with an episode of prosthetic valve endocarditis (PVE). PVE was more common in A. actinomycetemcomitans than H. parainfluenzae IE (10, 67% v 5, 18%; respectively p<0.01). Clinical manifestations of IE of more than 1 months duration were recorded more often in A. actinomycetemcomitans (8, 53%) and Cardiobacterium IE (6, 55%) than H. parainfluenzae (3, 11%; both p<0.01). Aortic valve vegetations were identified on echocardiography more commonly in Cardiobacterium IE (8, 89%) than in H. parainfluenzae (6, 32%; p<0.05) and A. actinomycetemcomitans (2, 29%; p<0.05) IE. Mitral valve endocarditis was common in H. parainfluenzae (10, 53%) and A. actinomycetemcomitans (6, 86%). Of the five cases with Oslers' nodes four occurred with A. actinomycetemcomitans IE.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  HACEK organisms isolated from definite and probable cases of HACEK endocarditis.doi:10.1371/journal.pone.0063181.t001\n\nClinical features of HACEK and non-HACEK endocarditis\nBaseline characteristics and predisposing factors of HE are shown in Table 2. The median age of patients with HE (47.4 years; IQR 35.6–57.1) was significantly lower than non-HE (60.5 years; IQR 45.3–72.7) and males predominated (56, 73%). Factors more commonly associated with HE than Non-HE endocarditis were Osler's nodes (7% vs 3%, p = 0.02) and vascular immunological phenomena (32% vs 20%, p = 0.008) and the presence of mechanical valves (30% vs 18%, p = 0.02). Factors less commonly associated with HE than non-HE endocarditis were health care provision (1% vs 24%, p<0.001), and diabetes mellitus (8% vs 18%, p = 0.02). There was no difference in the proportion with fever or splenomegaly between HE and non-HE, nor with native valve predisposition for IE or congenital heart disease (Table 2).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Important features of HACEK endocarditis compared with all other causes of infective endocarditis in database*.doi:10.1371/journal.pone.0063181.t002\n\nTransfers from another facility\nThere was no difference in the number of cases transferred from another facility between HE (30, 39%) and non-HE (2288, 41%; p = 0.6). In HE there were more cases transferred with native valve endocarditis (24, 80% vs 23, 49%; p = 0.01 ), new or worsening murmurs (20, 67% vs 19, 40% p = 0.008), regurgitation on echocardiography (23, 77% vs 23, 49%; p = 0.02), and need for valvular surgery (aortic valve 11, mitral valve 12) ( 23, 77% vs 8, 17% p<0.001 ) compared with those directly admitted. There was a borderline significant increase in stroke among transferees (11, 38% vs 8, 17%; p = 0.06) and CHF (7, 32% vs 4 9%; p = 0.10) and no difference in the numbers of cases with symptoms longer than 1 month (5, 17% vs 13, 28% p = 0.41) or length of hospital stay (median 23 IQR 15–42 vs median 27 IQR 14–42, p = 0.56).\n\n\nDiagnosis\nBlood cultures were drawn in all 77 patients with HE. Three of four patients with negative blood cultures had received antibiotics in the previous seven days. Additional culture positive sites were heart valves (20), joint fluid (2), pacemaker wires (1), urine (1), and other (5). One patient was diagnosed by PCR of infected tissue. Echocardiography was performed in 97% of HE (transthoracic only 15, transesophageal only 9, both transthoracic and transesophageal 51). Vegetations were identified in a lower proportion in HE than non-HE (71% vs 83%, p = 0.01) (Table 2). There was no difference in the proportions of mitral and aortic valve vegetations identified between HE and non-HE. Only 1 case of tricuspid valve endocarditis was recorded in HE. New regurgitation and paravalvular complications were not significantly different from non-HE (Table 2).\n\n\nAntimicrobial susceptibility testing\nThe causative organisms are shown in table 1. Of the isolates tested 24/25 (96%) were penicillin susceptible (1 resistant strain of A. aphrophilus), 48/49 (98%) were ampicillin susceptible (1 resistant strain, of A. aphrophilus), 50/50 (100%) were ceftriaxone susceptible, and 30/32 (94%) were gentamicin susceptible (2 resistant strains of H. parainfluenzae).\n\n\nTreatment\nAntimicrobial therapy was reported in 50 (65%) patients. Of these 37 (74%) were treated with ceftriaxone (in combination with an aminoglycoside in 17 and ampicillin in 6) , 6 with a penicillin derivative (ampicillin in 3, penicillin G in 2, and penicillinase-resistant penicillin in one,in combination with an aminoglycoside), and 3 with cefazolin/cefalothin (in combination with an aminoglycoside), and 4 unspecified. All cases of HE were treated with antimicrobial agents that would be active as predicted by susceptibility testing. Cardiac surgery was performed on 31 (40%) patients a median of four days (IQR 1–19) after admission. The aortic valve was replaced in 17 patients, mitral valve in 13, tricuspid valve in one and an intracardiac device was removed in one patient.\n\n\nOutcomes\nThe in-hospital mortality of HE was less than one quarter that of the non-HE (3, 4% vs 998,18%; p<0.001). Of the three HE deaths with one had been treated surgically. Heart failure was significantly less frequent in HE than non-HE (15% vs. 30%, p = 0.004). Stroke complicated a higher proportion of cases with HE than non-HE (25% vs. 17%, p = 0.05) and there was a relative excess of haemorrhagic stroke over embolic stroke in HE (44% vs 17%, p = 0.006). The presence of a stroke increased the length of stay by 20 days despite occurring in a significantly younger age group (Table 3). On multivariable regression analysis the independent factors associated with stroke were increasing age in 10 year intervals (OR 0.62; CI 0.49–0.90; p<0.01) and mitral valve vegetations (OR 3.60; CI 1.34–9.65; p<0.01). Eleven of 25 (44%) cases of HE with mitral valve vegetations suffered a stroke compared with 484/2009 (24%) in non-HE (p = 0.03). The frequency of systemic embolization, excluding central nervous system, intracardiac abscess and mycotic aneurysm were not significantly different in HE than non-HE (Table 2).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 3.  Univariate and multivariate analysis for the risk of stroke in HACEK endocarditis*.doi:10.1371/journal.pone.0063181.t003At one year follow-up, three additional cases of HE had died (heart failure 1, unrelated causes 1, unknown 1); however the cumulative death rate was significantly lower than non-HE (6,11% vs 1627, 39%; p<0.001) (Table 2). Four cases had undergone valvular surgery; three had been treated medically and one surgically.\nThere was one possible relapse 4 months after completing therapy with an unspecified HACEK organism. This organism was not available for further speciation. In addition one patient with HE had another episode of endocarditis with a methicillin susceptible S. aureus.\nHACEK native and prosthetic valve endocarditis.Comparison of the clinical features of native valve and prosthetic valve HE demonstrated that native valve HE occurred at an older age (median 56.3 (range 41–67) vs median 43.8 (range 32–54) years, p = 0.003), and that a higher proportion had Osler's nodes (5, 20% vs 0, 0%; p = 0.002) and systemic embolization (10, 37% vs 5, 12% v; p = 0.01) than prosthetic valve HE (Table 4). There was no significant difference in the proportion with stroke (7, 26% vs 11, 24%; p = 0.85), or congestive heart failure (5, 19% vs 6, 14%; p = 0.74) or surgical treatment (8, 30% vs 22, 45%; p = 0.23) or length of median hospital stay between these groups.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 4.  Comparison between the features of native valve HACEK endocarditis and prosthetic valve HACEK endocarditis*.doi:10.1371/journal.pone.0063181.t004Of those with PVE 8 (30%) required surgical treatment and 19 (70%) were treated with medical therapy alone. There were no in-hospital deaths in either treatment group. Of the 24 PVE patients with 1 year follow-up data, there was one death (cause unknown) and three who required cardiac surgery in the medically treated group, but no deaths, relapses or further surgery requirement in the surgically treated group. By comparison, among those with native valve HE, there were two in-hospital deaths and 1 death with-in the 1 year follow-up period.\n\n\nThis report describes the findings of a large series of HE and non-HE cases of bacterial endocarditis reported in a standardised manner in which geographic distribution, frequency of clinical features, risk factors and outcomes have been compared. Both groups were subject to referral bias from transfers to the study centres [19], [20], but this is unlikely to confound these comparisons as the proportion transferred was very similar in the two groups, and the pattern of features of the transferees with HE was similar to that reported in the ICE cohort. However the frequency of some clinical features are influenced by transfers between hospitals, and the results need to be interpreted in the light of this limitation.The marked geographic difference in the prevalence of HE (10-fold) between the highest (New Zealand) and the lowest countries (United States of America) confirms the findings of an earlier, smaller sample of cases in the ICE cohort [11]. The range is similar to the range of 0.8–6.1% reported in recent single and multi-centre studies. [3]–[5]. The high prevalence of HE in New Zealand is unlikely to be due to referral patterns given the low proportion transferred from another facility. Other possible reasons for the variation include the prevalence of risk factors such as frequency of prosthetic devices [12], oral health [21], transmission pathways of HACEK organisms within populations [22], regional health care access, and diagnostic bias.Some clinical features varied with the causative species. H. parainfluenzae was the commonest cause of HE, as has been reported in population based studies [23]. H. parainfluenzae endocarditis was less likely to have an insidious onset than both A. actinomycetemcomitans and C. hominis confirming previous reports [7]. C. hominis was strongly associated with aortic valve infection and A. actinomycetemcomitans endocarditis was a frequent cause of PVE, and vascular immunological manifestations [6], [7], [9], [10]. Despite this, we found that HE has sufficient important clinical features in common that distinguish it from non-HE to retain clinical usefulness. These features include younger age of presentation, community acquisition, a higher proportion with vascular/immunological manifestations, a lower proportion with co-morbidities and an excellent outcome.Prosthetic valve endocarditis was common (35%), although the prevalence of HACEK PVE was not significantly higher than non-HE PVE in this study (p = 0.07). However, the comparator includes a heterogeneous group of organisms with variable propensity to cause PVE. For example, previous studies of the ICE cohort have found the proportion of PVE in Staphylococcus aureus IE to be 16%, viridans streptococcal IE to be 15%, and coagulase-negative staphylococcal IE to be 32% [11]. Thus it appears that HACEK organisms have a predilection for prosthetic valves. This finding is more marked in late PVE (>1 year after surgery) as HE causes late PVE in a large majority of cases [12], but only about half of cases of S. aureus and coagulase-negative staphylococci PVE, suggesting mechanical valves are a particular risk for HE [17]. In addition this study may underestimate the true proportion with PVE because of the high number of native valve HE cases transferred from other centres. Pre-existing native valve and congenital cardiac abnormalities were common in HE but there was no significant increase in these conditions compared with non-HE. Previous studies have suggested these risk factors may occur more frequently in HE, but this will be subject to changes in the epidemiology of native valve lesions such as rheumatic fever and the widespread availability of cardiac surgery [22].Most cases of HE were treated with a third generation cephalosporin and a minority with ampicillin with or without an aminoglycoside; however there were insufficient cases to correlate outcomes with these recommended regimens [24], [25]. The incidence of penicillin resistant strains was limited to an isolate of A. aphrophilus species. β-lactamase producing strains of C. hominis have been reported but not in A. actinomycetemcomitans to our knowledge [26]. The proportion of all HE cases requiring cardiac surgery (40%) was similar to non-HE and to that reported in the literature [12], [27]. However in PVE the requirement for surgery was lower (30%) which compares favourably with published rates for PVE overall (49%) [12], [27]. The favourable outcome of both medically and surgically treated HACEK PVE demonstrates that HE is readily controlled and treated with antimicrobial agents despite the presence of a prosthetic valve.The outcome of HE was excellent overall with an in-hospital mortality of 3% which is less than one quarter of the mortality for non-HE and one sixth that of S. aureus endocarditis [28]. Heart failure was diagnosed in 15% of cases compared with 30% in non-HE, and 37% reported for S. aureus endocarditis [27], [28]. The younger age group and lack of co-morbidities, in addition to pathogen-specific characteristics may favour a good in-hospital and 1 year outcomes. These results would not be affected by survivor bias given the very high survival rate of HE. With respect to PVE, the numbers were too small to make meaningful comparison for other major complications including stroke, congestive heart failure and abscess formation.The major complication of HE was stroke (25%), and this complication almost doubled the length of hospitalisation. This figure over-represents the true incidence of stroke in HE as there was an increased frequency of stroke in those transferred from other facilities. Nevertheless stroke is conspicuously common in HE compared with non-HE, and the reported frequency in S. aureus endocarditis (20%) and viridans streptococcal IE (8%) in the ICE cohort [29]. Mitral valve IE was an important risk factor for stroke as reported previously, but organism specific effects on the nature of the vegetations in HE may also make a significant contribution to the prevalence of stroke, as 44% of patients with mitral valve HE suffering a stroke compared with 24% of non-HE. This may be related to the long antecedent history with organisms such as A. actinomycetemcomitans [7].While embolism was the predominant cause of stroke in HE there was relative excess of haemorrhagic stroke. The reasons for this are not clear but it is possible micro-vascular/immunological manifestations of IE which were significantly more frequent in HE than non-HE and might contribute to the development of cerebral microbleeds which are a strong predictor of subsequent intracranial haemorrhage [30]. Anticoagulant therapy is unlikely to contribute to the occurrence of stroke but may increase the conversion of embolic to haemorrhagic events. [31].There are several additional limitations of this study. Because of small numbers both possible and definite cases were included to increase statistical power. Despite this there were a limited number of cases of HACEK endocarditis and the observational design and long-term follow-up limited to one year limits our ability to draw any firm conclusions regarding optimal antimicrobial therapy or surgical treatment strategies. Furthermore the self selection of centres to participate in the ICE study, and the heavy weighting toward Europe, North America and Australasia with few sites in Asia, and Africa has meant that the population sample may not be representative of any specific region. Thus important geographical differences may have been missed and any epidemiological inferences from this study are limited.Our findings suggest that there is sufficient similarity in presentation and outcome to justify considering the HACEK organisms as a group at present. Despite the high prevalence of stroke, HE has a remarkably low mortality rate, suggesting that current antibiotic therapy with surgery when needed, is very effective. The reasons why HE shows apparent disparities in geographical distribution, occurs in a younger age group, has a propensity to infect prosthetic valves, and is associated with a high incidence of stroke are worthy of further investigation."
        },
        "10.1371/journal.pone.0052726": {
            "author_display": [
                "Qiang Chen",
                "Hua Cao",
                "Gui-Can Zhang",
                "Liang-Wan Chen",
                "Dao-Zhong Chen",
                "Qian-Zhen Li",
                "Zhi-Huang Qiu"
            ],
            "title_display": "Atrioventricular Block Subsequent to Intraoperative Device Closure Atrial Septal Defect with Transthoracic Minimal Invasion; A Rare and Serious Complication",
            "abstract": [
                "Objectives: Atrioventricular block (AVB) is a infrequent and serious complication after percutaneous ASD closure. In this study, we report on the incidence of AVB associated with intraoperative device closure of the ASD with transthoracic minimal invasion, and the outcomes of this complication in our center. Methods: Between May 2006 and January 2011, a total of 213 secundum-type ASD patients were accepted in our hospital for intraoperative and transthoracic device closure with a domestic occluder. All patients were assessed by real-time transthoracic echocardiography (TTE) and electrocardiograph (ECG). Results: All patients were occluded successfully under this approach. Immediate postprocedure third-degree AVB was observed in two patients. Since heart rates were in the range of about 50 to 55 beats per minute, no intervention was needed except for close observation for one patient. Another patient who recovered sinus rhythm intermittently during the operation was fitted with a temporary pacemaker. Approximately one week following glucocorticoid treatment, the AVB resolved spontaneously in these two patients. Mobitz type II AVB occurred in three patients during the procedure. Two patients developed post-operative cardiac arrest and were rescued successfully with cardiopulmonary resuscitation. One other patient changed to Mobitz type I AVB after three days. During the follow-up period, which ranged from six months to five years, no further occurrence of AVB was found. Conclusions: Intraoperative and transthoracic device closure of secundum ASDs with domestic occluder resulted in excellent closure rate. AVB is an infrequent but serious complication during and after device closure of a secundum ASD. AVB is a complication that warrants greater attention and long-term follow-up. "
            ],
            "publication_date": "2012-12-28T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 846,
            "shares": 0,
            "bookmarks": 0,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0052726",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0052726&representation=PDF",
            "fulltext": "IntroductionAtrial septal defect(ASD) is one of the most common congenital cardiac defects, accounting for approximately 6% to 10% of all congenital cardiac defects. [1], [2] Transcatheter ASD closure has gradually matured as an alternative to surgical closure and has been employed increasingly in recent years, especially in cases of secundum ASD. Cases of secundum ASD have become more amenable to closure since the introduction of the Amplatzer occluder. The major advantage of transcatheter closure is that requires neither a median sternotomy nor cardiopulmonary bypass. In addition, the complication rate for transcatheter ASD closure, is lower than with surgery. [3]–[5] Short- and long-term follow-ups have revealed many of the shortcomings of device closure. [6]–[8] The two-percent incidence of arrhythmia after the device implantation is considerably lower than the thirty-percent incidence reported after surgical closure. [9] The most serious complication experienced by patients has been the occurrence of complete atrioventricular block (AVB). To our knowledge, there are few reports of studies involving large numbers of patients undergoing device closure of ASD. In this article, we reported the incidence of AVB during and after intraoperative and transthoracic device closure ASD using a domestic occluder in our center, as well as the outcome of the complications.\nMaterials and Methods\nDevice\nThe ASD occluder (Dong Guan Ke Wei Medical Apparatus Co. Ltd, China) was consisted of an occluder made from an alloy of nickel and titanium, a metal sheath, a pushing rod, and a hook. (Fig. 1) The double disc occluder had a loop on the right disc with a 100-cm thread through the loop, facilitating its withdrawal into the sheath, which was 40 cm in length and 4 to 10 mm in diameter sheath. The occluder for each patient was selected in accordance with the transthoracic echocardiography (TTE) result, with a diameter 2 to 6 mm in excess of the maximum defect diameter. The occluder was loaded into the sheath. [10]–[12].\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  The occlusion devices.doi:10.1371/journal.pone.0052726.g001\n\nProtocol\nThe detection of ASD was made by pre-operative TTE, performed initially using standard transthoracic color imaging and Doppler interrogation from subxyphoid, apical, and parasternal views before the closure procedure. During the operation, TTE with a GE vivid 7 multiplane probe (General Electric Company) was used for the accurate determination of ASD location, morphologic features, size (the largest diameter), and visualization of rims. ASD diameter was measured by TTE using two-dimensional imaging and colour flow Doppler on long and short axis views. (Fig. 2) The atrial septal occluder was chosen according to the largest diameter of the ASD, allowed for a margin of 2 to 6 mm in excess of the diameter.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  The 38 mm ASD measured by TTE.doi:10.1371/journal.pone.0052726.g002Under general anesthesia, each patient was placed in a supine position and draped for exposure of the entire chest, with the right hemithorax elevated approximately 30 degrees. A right anterior submammary minithoracotomy (3–5 cm in length) was made through the fourth intercostal space, and a small rib spreader was used in the manipulation incision to facilitate instrumentation. The pericardium was opened and suspended to expose right atrium. Two parallel sutures of approximately 8 to 16 mm in diameter were placed on the anterolateral right atrium, and heparin (1 mg/kg, IV) was administered to maintain an activated clotting time >250 seconds. After the occluder was drawn into the delivery sheath, a 1-cm incision was opened in the right atrium, and the delivery sheath was inserted. The sheath was advanced under continuous TTE guidance, through the ASD into the left atrium. (Fig. 3) The left disc was deployed first by pushing the rod. The sheath was withdrawn as the left disc was adjusted parallel to the atrial septum, and the right disc was deployed on the other side to occlude the ASD. (Fig. 4) The sheath was moved in a to-and-fro motion to ensure a secure position across the defect. The disc position was assessed using TTE to ensure there was no significantly residual shunt, no distortion of the atrioventricular valve, and no obstruction of the venae cavae or coronary sinus. The thread was cut, and the sheath was withdrawn with the suture tied snugly. The chest was closed using routine procedures, and the patient was instructed to take oral dipyridamole or aspirin for three months as an anticoagulant.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  The sheath positioned from the right atrial free wall into the left atrial cavity across the ASD.doi:10.1371/journal.pone.0052726.g003\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Final image shown after the 44 mm occluder were deployed and the sheath was withdrawn.doi:10.1371/journal.pone.0052726.g004\n\nParticipants\nIn the period between May 2006 to January 2011, 203 patients (98 males and 105 females) with secundum-type ASD were evaluated for intraoperative device closure with our domestic ASD occluder. Those with other coexisting cardiac anomalies were excluded from the study. All patients in the study group received intraoperative device closure. The patients ranged in age from 3 months to 62 years (mean ± standard deviation: 20.0±16.8 years) and weighed from 3.5 to 72 kg (mean: 37.6±20.1 kg). Inclusion criteria for intraoperative device closure were the same as those for surgical or transcatheter closure: hemodynamically significant L-R shunts, and/or significant chamber enlargement, and/or mild-to-moderate pulmonary hypertension despite medical therapy or history of infective endocarditis.\nRoutine examinations included a standard electrocardiogram, chest X-ray, and blood tests. Arrhythmias were detected in 17 patients before the operation: complete right bundle branch block (1 patient), incomplete right bundle branch block (1 patien), left anterior hemiblock (1 patient), left bundle branch block (2 patients), Mobitz type I AVB (2 patients), sinus bradycardia (5 patients), premature atrial beats (3 patients), and atrial fibrillation (2 patients)).\n\n\n        Ethics\n      The present study was approved by the ethics committee of our university and was conducted according to the tenets of the Declaration of Helsinki. Written informed consent was obtained from the patients or their parents.\n\nStatistical Analysis\nContinuous data were presented as mean± standard deviation and range.\n\nDelivery of the occluder was successful in all patients. The size of the ASD as measured by TTE ranged from 5 to 44 mm (mean 20.8±10.8 mm). The size of the implanted occluder ranged from 6 to 48 mm (mean 23.8±11.2 mm). Immediate post-procedure third-degree AVB was observed in two patients. Since their heart rates ranged from about 50–55 beats per minute, no intervention was needed except for close observation of one patient. Another patient recovered sinus rhythm intermittently during the operation and had a temporary pacemaker inserted. The AVB resolved spontaneously in these two patients after about one week of glucocorticoid treatment. Mobitz type II AVB occurred in three patients during the procedure. Two patients developed post-operative cardiac arrest and were successfully resuscitated. One other patient changed to Mobitz type I AVB after three days. New minor transient arrhythmia complications-temporary sinus bradycardia, premature atrial beats, and sinus tachycardia-developed in 32 patients in the course of the device deployment. These conditions were treated easily with drugs or resolved spontaneously. Immediate post-procedural Mobitz type I AVB was observed in one patient, requiring only close observation.In those patients who had a successful attempt, the total follow-up period ranged from six months to five years. Out-patient follow-up was by functional, echocardiographic, and ECG assessment. There were no instances of progressive arrhythmia. To date, none of the patients in the study has developed complete AVB."
        }
    },
    "subject-chemistry": {
        "10.1371/journal.pone.0079218": {
            "author_display": [
                "Karson S. Putt",
                "Randall B. Pugh"
            ],
            "title_display": "A High-Throughput Microtiter Plate Based Method for the Determination of Peracetic Acid and Hydrogen Peroxide",
            "abstract": [
                "\nPeracetic acid is gaining usage in numerous industries who have found a myriad of uses for its antimicrobial activity. However, rapid high throughput quantitation methods for peracetic acid and hydrogen peroxide are lacking. Herein, we describe the development of a high-throughput microtiter plate based assay based upon the well known and trusted titration chemical reactions. The adaptation of these titration chemistries to rapid plate based absorbance methods for the sequential determination of hydrogen peroxide specifically and the total amount of peroxides present in solution are described. The results of these methods were compared to those of a standard titration and found to be in good agreement. Additionally, the utility of the developed method is demonstrated through the generation of degradation curves of both peracetic acid and hydrogen peroxide in a mixed solution.\n"
            ],
            "publication_date": "2013-11-18T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 2,
            "views": 799,
            "shares": 0,
            "bookmarks": 0,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0079218",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0079218&representation=PDF",
            "fulltext": "IntroductionThe advantages of using the antimicrobial properties of peracetic acid increasingly have been recognized and are gaining usage in a number of industries including the treatment of wastewater [1], usage in hospitals, medical clinics and pharmacies [2], [3], dental practices [4], aquaculture [5], the dairy industry [6] and food preparation [7], such as seeds and sprouts [8], fruits and vegetables [9] and poultry and other fowl [10]. Additionally, other properties of peracetic acid have found use in other industries, such as in the treatment of bioenergy crops [11] where it is used to remove difficult to digest components and in the bleaching of paper products [12]. As the usage of peracetic acid gains greater acceptance, the need for accurate and higher throughput methods to quantitate the concentration of peracetic acid are required.\nPeracetic acid is commercially available in many different concentrations and mixtures, however nearly all peracetic acid preparations contain hydrogen peroxide. Hydrogen peroxide is often a reactant used in the synthesis of peracetic acid and can also be a byproduct of peracetic acid degradation. As these two different peroxides have different chemical properties and biological effects, the concentration of both the peracetic acid and the hydrogen peroxide often must be determined. Classically, peracetic acid and hydrogen peroxide have been determined using reduction/oxidation titrations followed by the formation of iodine from iodide [13]. In the 100 years since this initial work was performed, many other methods have been developed to specifically quantitate peracetic acid.\nMethods for the separation, derivatization and detection of these peroxides, such as gas chromatography [14], [15], [16] and HPLC [17], [18], [19] have been developed. While these chromatographic methods are able to quantitate both peracetic acid and hydrogen peroxide, like nearly all chromatographic methods, large numbers of solutions cannot be rapidly processed. In addition to these methods, potentiometric and amperometric methods have also been developed [20], [21], [22], [23]. However, most of the electrical signal based methods can only determine peracetic acid and not hydrogen peroxide; have limits in sensitivity, dynamic range or specificity; may require frequent calibrations and require specialized or custom made materials. Optical cuvette based methods employing enzymes such as catalase also have been developed [24]. However, these methods rely on enzyme kinetics and involve continuously measuring the sample until a short lived absorbance minimum is reached.\nAdditionally, one of the main challenges for an accurate measurement using these techniques is the creation of a standard curve from known materials. While pure hydrogen peroxide can be readily obtained, pure distilled peracetic acid is highly reactive and can be difficult to safely produce. The breakdown of both these peroxides, especially the peracetic acid can also cause difficulties in maintaining accurate calibration curves. Even if these issues could be overcome, none of the methods listed above can rapidly quantitate both peracetic acid and hydrogen peroxide concentrations in many different samples in a short period of time.\nCurrently, the greatest throughput microtiter plate based method that has been developed utilizes the reaction between peracetic acid and 2,2A-Azino-bis (3-ethylbenzothiazoline)-6-sulfonate [25]. While this method does allow for the rapid determination of peracetic acid, it does not quantitate the levels of hydrogen peroxide. Even with all of these methods and their improvements, titrations are still the most widely regarded and commonly utilized industrial method to quantitate peracetic acid and hydrogen peroxide concentrations [25]. Clearly there is a need for a rapid and higher throughput method to accurately determine the concentration of peracetic acid and hydrogen peroxide in solution. In an effort to develop such a rapid and higher throughput method, the chemistry relied upon in the peracetic acid and hydrogen peroxide titrations were explored and modified to create a microtiter plate based absorbance method.\nMaterials and Methods\nMaterials\nCerium (III) sulfate, cerium (IV) sulfate, 0.5 M iodine solution, potassium iodide, sodium hydroxide, 0.1 M sodium thiosulfate solution, 98% sulfuric acid, hydrogen peroxide, peracetic acid/hydrogen peroxide mixture and starch solution were purchased from Sigma-Aldrich (St. Louis, MO). Ferroin indicator was purchased from Taylor Technolgies (Sparks, MD). Dulbecco's phosphate buffered saline solution (PBS) was purchased from Mediatech (Manassas, VA). SpectraMax 384Plus absorbance plate reader was purchased from Molecular Devices (Sunnyvale, CA). Flat bottom 96-well polystyrene plates (Corning #3598), 50 mL polypropylene tubes (BD #352098), 15 mL polypropylene tubes (BD #352099) and all other materials were purchased from VWR (Atlanta, GA). TableCurve2D was purchased from Systat Software (San Jose, CA).\n\n\nAbsorbance spectra\n200 µL of various concentrations of the compounds were added in quadruplicate to the wells of a Corning 96-well flat-bottom plate. The absorbance was read between 300 and 900 nm every 2 nm on a Molecular Devices SpectraMax 384Plus. The average absorbance from a DI water blank was subtracted to yield the final absorbance curve.\n\n\nCerium (IV) standard curve\n100 µL of a 10 to 100,000 µM solution of cerium (IV) sulfate in 10% (v/v) sulfuric acid/water was added in quadruplicate to the wells of a 96-well plate. 100 µL of deionized water was added to each well containing the cerium (IV) sulfate solution. 200 µL of deionized water was added to wells to serve as a blank. The absorbance of the plate was read at 300 to 500 nm every 25 nm. The values of the blank wells were subtracted and the linear range then was determined for each wavelength.\n\n\nIodine standard curve\n100 µL of a 10 to 100,000 µM solution of iodine was added in quadruplicate to the wells of a 96-well plate. 50 µL of deionized water followed by 50 µL of a 3% (v/v) sulfuric acid/water solution were added to each well containing the iodine solution. 200 µL of deionized water was added to wells to serve as a blank. The absorbance of the plate was read at 300 to 600 nm every 25 nm. The values of the blank wells were subtracted and the linear range then was determined for each wavelength.\n\n\nPeracetic acid/hydrogen peroxide determination via titration\nA volume of peracetic acid/hydrogen peroxide was weighed in a 250 mL beaker. 50 mL of a 1 M ice-cooled sulfuric acid/water solution was added to the beaker and the beaker was placed on ice. 3 drops of ferroin indicator were added and the solution was titrated with various concentrations of cerium (IV) sulfate until the disappearance of the salmon color and the appearance of a light blue color. 10 mL of a 2.5 M potassium iodide solution was added to form a dark brownish-red color. The solution was then titrated with various concentrations of sodium thiosulfate. When the solution lightened to a brown-yellow, 1 mL of starch solution was added to produce a dark purple color. Additional sodium thiosulfate was added until the solution turned a salmon color. The volumes and molarity of the cerium (IV) sulfate and sodium thiosulfate were used to calculate the concentration of hydrogen peroxide and peracetic acid respectively.\n\n\nHydrogen peroxide determination via microtiter plate based method\n100 µL of a sample solution was added to the wells of a 96-well plate. 200 µL of deionized water was added to wells to serve as a blank. 100 µL of a 10 or 100 mM solution of cerium (IV) sulfate in 10% (v/v) sulfuric acid/water was added to each sample well and the absorbance of the plate immediately was read at 300, 400 and 450 nm. The DI water blank's absorbance was subtracted from the sample's absorbance and concentration of remaining cerium (IV) sulfate was then determined using the standard curve generated as described above. The amount of cerium (IV) sulfate consumed in the reaction was calculated, which is equivalent to twice the amount of hydrogen peroxide present in the sample.\n\n\nTotal peroxide determination via microtiter plate based method\n100 µL of a sample solution was added to the wells of a 96-well plate. 200 µL of deionized water was added to wells to serve as a blank. 50 µL of a 3% (v/v) sulfuric acid/water solution were added to each well containing the iodine solution. 50 µL of a 250 mM potassium iodide solution was added to each sample well. The absorbance was read immediately at 350, 450 and 550 nm. The DI water blank's absorbance was subtracted from the sample's absorbance and the concentration of iodine formed was determined using the standard curves generated as described above. The amount of iodine formed is equivalent to the amount of total peroxide present in the sample. The concentration of peracetic acid can be calculated by subtracting the concentration of hydrogen peroxide from the amount of total peroxide present in the sample.\n\n\nEffect of sulfuric acid concentration on total peroxide determination\n100 µL of a 15 mM total peroxide solution or 100 µL DI water was added to the wells of a 96-well plate. 200 µL of deionized water was added to wells to serve as a blank. 50 µL of various concentrations of sulfuric acid were added to each well, except the blank wells. 50 µL of a 250 mM potassium iodide solution was added to each well, except the blank wells. The absorbance was read at 550 nm every minute for 60 minutes. The DI water blank's absorbance was subtracted from all other absorbance values. The concentration of iodine formed was determined using the standard curve generated as described above.\n\n\nPeracetic acid/hydrogen peroxide degradation\nA commercially available preparation of peracetic acid/hydrogen peroxide in acetic acid was diluted in PBS to yield a final concentration of ~150 mM peracetic acid and ~45 mM hydrogen peroxide. Sodium hydroxide was added until the pH reached 7.5. Various volumes of solution were sampled at various times and the concentrations of peracetic acid and hydrogen peroxide were determined via titration as described above. Additionally, 100 µL of sample was added in quadruplicate to the wells of two 96-well plates and the amount of hydrogen peroxide or total peroxides were determined as described above.\n\nResults and Discussion\nIdentification of quantifiable chemical reaction species\nTo create an assay amenable for the rapid determination of both peracetic acid and hydrogen peroxide in numerous mixed solutions, a microtiter plate based spectral method was desired. To create such a method, the reactions utilized in various published titration methods were explored. In the modified titration protocol of Greenspan and MacKeller [26], cerium (IV) sulfate is reacted specifically with hydrogen peroxide followed by the addition of potassium iodide which is converted into iodine by the remaining peracetic acid.\nIn order to determine if the reactants and/or products could be directly observed without the use of indicator solutions, such as the ferroin and starch solutions utilized in the titration, the absorbance spectra of cerium (III) sulfate, cerium (IV) sulfate, potassium iodide, iodine, sulfuric acid and peracetic acid/hydrogen peroxide solution were measured (see Figures S1 through S6). Most of these chemicals are clear or white and unsurprisingly did not have any significant absorption in the visible spectrum. However, the cerium (IV) sulfate and iodine did exhibit strong absorption in the blue region, giving rise to their yellow color. Unfortunately, since both of these chemical species absorb similar wavelengths of light, they could not be quantitated together in a single reaction mixture. Therefore, a two reaction strategy was employed to determine the concentration of both peroxides.\n\n\nSelective Hydrogen Peroxide Quantitation\nAs shown in Figure 1A and B, the reaction of the colored cerium (IV) sulfate to the colorless cerium (III) sulfate proceeds in the presence of hydrogen peroxide, but not in the presence of peracetic acid. The selectivity of this reaction can therefore be used to specifically quantitate the concentration of hydrogen peroxide in solution. To accomplish this, a standard curve of cerium (IV) sulfate was established. First, 100 µL of various concentrations of cerium (IV) sulfate and 100 µL of DI water were added to the wells of a 96-well plate. The absorbance of cerium (IV) sulfate can be measured at multiple wavelengths depending upon the concentration of interest. Multiple standard curves were generated at various wavelengths (see Figures S7 through S15) and three wavelengths were chosen that spanned a low, medium and high concentration range (Figure 1C). At 300, 400 and 450 nm, a 50 to 2500 µM, 2500 to 10,000 µM and 5000 to 100,000 µM cerium (IV) sulfate solution could be quantitated respectively.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Overview of selective hydrogen peroxide quantitation.A.) Chemical reaction between hydrogen peroxide and cerium (IV) sulfate. B.) Chemical reaction between peracetic acid and cerium (IV) sulfate. C.) Multiple wavelength cerium (IV) sulfate standard curve (n = 4). Error bars represent standard deviation and are typically smaller than the plotted marker.\ndoi:10.1371/journal.pone.0079218.g001Since cerium (IV) sulfate and hydrogen peroxide are the reactants, the amount of cerium (IV) sulfate consumed is directly proportional, by a factor of two, to the amount of hydrogen peroxide initially present. Utilizing this relationship, a simple microtiter plate method was developed based upon the absorbance of cerium (IV) sulfate. First, 100 µL of sample or DI water is added to the wells of a 96-well plate, followed by the addition of a solution of cerium (IV) sulfate. The concentration of the cerium (IV) sulfate solution added can be adjusted based upon the expected values of hydrogen peroxide in the sample. As an additional note, the cerium (IV) sulfate solution should be contain at least a sulfuric acid concentration of 10% (v/v) as a lower concentration may cause the cerium (IV) sulfate to crash out of solution upon addition to the aqueous sample, obfuscating the absorbance reading.\nThe reaction between cerium (IV) sulfate and hydrogen peroxide is immediate, often with the visible evolution of oxygen at high concentration of hydrogen peroxide. The absorbance of the plate is read immediately and the starting concentration of the cerium (IV) sulfate solution, from the DI water wells and the ending concentration, from each sample well is calculated using the standard curve generated above. The final cerium (IV) sulfate concentration is subtracted from the initial to determine the amount consumed in the reaction. Due to the stoichiometry of the reaction, the amount of cerium (IV) sulfate consumed is equal to twice the concentration of hydrogen peroxide.\nWhile this reaction is highly selective for hydrogen peroxide, as the hydrogen peroxide is consumed the equilibrium reaction between it and peracetic acid is shifted towards the formation of hydrogen peroxide from the breakdown peracetic acid into acetic acid. This additional hydrogen peroxide then can react with the cerium (IV) sulfate causing a false increase in concentration as time passes. The possibility also exists of a very slow reaction between the peracetic acid and the cerium (IV) sulfate as well. Because of these confounding factors, the plate absorbance should be read immediately as the false increase in hydrogen peroxide concentration, as measured by the decrease in absorbance was determined to be ~2% per minute (see Figures S16).\nTo compare the results obtained from the plate based hydrogen peroxide detection to those obtained from a titration, several dilutions of a peracetic acid/hydrogen peroxide solution were made and the concentration of hydrogen peroxide was determined by both methods. As can be seen in Figure 2, the two methods show good agreement generally being within the error of one another.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Comparison of hydrogen peroxide quantitation.The amount of hydrogen peroxide in various dilutions from a stock peracetic acid/hydrogen peroxide solution were determined via titration or the microtiter plate based method (n = 3, error bars represent standard deviation).\ndoi:10.1371/journal.pone.0079218.g002\n\nTotal Peroxide Quantitation\nAs shown in Figure 3A and B, the reaction of the colorless potassium iodide to the highly colored iodine proceeds in the presence of both hydrogen peroxide and peracetic acid, unlike the specific reaction of cerium (IV) sulfate. Utilizing the potassium iodide reaction, the total amount of peroxides present can be determined. To accomplish this, a standard curve of iodine was established. First, 100 µL of DI water, 50 µL of 3% sulfuric acid and 50 µL of various concentrations of iodine were added to the wells of a 96-well plate. Similar to the method above, the absorbance of iodine can be measured at multiple wavelengths depending upon the concentration of interest. Multiple standard curves were generated at various wavelengths (see Figures S17 through S27) and again three wavelengths were chosen that spanned a low, medium and high concentration range (Figure 3C). At 350, 450 and 550 nm, a 100 to 1000 µM, 250 to 10,000 µM and 250 to 100,000 µM iodine could be quantitated respectively.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Overview of general peroxide quantitation.A.) Chemical reaction between hydrogen peroxide and potassium iodide. B.) Chemical reaction between peracetic acid and potassium iodide. C.) Multiple wavelength iodine standard curve (n = 4). Error bars represent standard deviation and are typically smaller than the plotted marker.\ndoi:10.1371/journal.pone.0079218.g003The addition of sulfuric acid to the reaction between potassium iodide and peroxide greatly increases the rate. As shown in Figure 4A, low concentrations of sulfuric acid do not result in a rapid reaction. A fairly wide range of concentrations centered at ~3% sulfuric acid provide a window for a stable reaction. However, at very high concentrations of sulfuric acid an increase in iodine production is observed. At these high acid concentrations, the potassium iodide itself can react to generate iodine (Figure 4B). This auto-reaction of potassium iodide is only significant if very low levels of total peroxides are being quantitated and this bias can be mitigated by reading the plate quickly after the addition of potassium iodide. From these results and the stability of the generated iodine in various concentrations of sulfuric acid (see Figure S28), a concentration of 3% sulfuric was determined to be most optimal.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Effect of sulfuric acid concentration on peroxide – potassium iodide reaction.A.) Various concentrations of sulfuric acid were used to react (A) 15 mM total peroxides with 250 mM potassium iodide or (B) only 250 mM potassium iodide (n = 3). Error bars represent standard deviation.\ndoi:10.1371/journal.pone.0079218.g004To compare the results obtained from the plate based total peroxide detection to those obtained from a titration, several dilutions of a peracetic acid/hydrogen peroxide solution were made and the concentration of total peroxide was determined by both methods. As can be seen in Figure 5, the two methods were in good agreement generally being within the error of one another. However, the values reported are total peroxides. To yield the concentration of peracetic acid alone, the values obtained from the specific hydrogen peroxide method can be subtracted from the total peroxide concentration.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Comparison of total peroxide quantitation.The amount of total peroxide in various dilutions from a stock peracetic acid/H2O2 solution were determined via titration or the microtiter plate based method (n = 3, error bars represent standard deviation).\ndoi:10.1371/journal.pone.0079218.g005\n\nPeracetic acid – Hydrogen Peroxide Degradation\nThe formation and degradation of both hydrogen peroxide and peracetic acid can proceed down multiple pathways. The hydrogen peroxide can react with acetic acid to form peracetic acid. The hydrogen peroxide can also breakdown into oxygen and water while the peracetic acid can break down into oxygen and acetic acid or reverse its formation reaction and return to hydrogen peroxide and acetic acid. Due to these multiple pathways, which can occur in a pH dependent manner, the degradation time of these two peroxides together in solution can be difficult to calculate.\nTo show the utility of the microtiter plate based method and to determine the degradation of both peracetic acid and hydrogen peroxide in a PBS solution buffered to physiological pH, the degradation of a peracetic acid/hydrogen peroxide solution was determined by both the microtiter plate and titration methods. As shown in Figure 6, both peracetic acid and hydrogen peroxide rapidly degraded with less than 2% and 0.2% of the starting concentration remaining after 96 hours respectively. Interestingly, the hydrogen peroxide degraded faster than if it were in solution alone. This is mostly likely due to it reacting with the acetic acid in solution to form more peracetic acid which then in turn degrades to acetic acid and oxygen. It should also be noted that the degradation curves from the two different methods yielded very similar results for both peracetic acid and hydrogen peroxide.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Degradation of peracetic acid and hydrogen peroxide.Three independent peracetic acid/H2O2 dilutions were created in PBS and initially adjusted to a pH of 7.5. The concentration of peracetic acid and H2O2 were determined via titration (A.) and the microtiter plate based method. Data represents average of all three dilutions and error bars represent standard deviation.\ndoi:10.1371/journal.pone.0079218.g006\nThe quantitation of peracetic acid and hydrogen peroxide is of great importance in many industries. However, the availability of rapid quantitation methods that can determine both peroxides in numerous samples was found to be lacking. We were able to adapt the most widely used titration chemistries, the conversion of cerium (IV) sulfate to cerium (III) sulfate and potassium iodide to iodine for use in a two step absorbance microtiter plate method. This new plate based method could be used to rapidly determine the concentration of both hydrogen peroxide and peracetic acid in numerous samples and was found to closely agree with the values obtained during titrations for total peroxides, peracetic acid and hydrogen peroxide. The utility of this method was demonstrated by querying the degradation profiles of hydrogen peroxide and peracetic acid in physiological buffer conditions. In summary, a new, convenient, rapid, absorbance based high-throughput method for the determination of hydrogen peroxide and peracetic acid based upon titration chemistries has been developed."
        },
        "10.1371/journal.pone.0089416": {
            "author_display": [
                "Katarina Bengtsson",
                "Sara Nilsson",
                "Nathaniel D. Robinson"
            ],
            "title_display": "Conducting Polymer Electrodes for Gel Electrophoresis",
            "abstract": [
                "\nIn nearly all cases, electrophoresis in gels is driven via the electrolysis of water at the electrodes, where the process consumes water and produces electrochemical by-products. We have previously demonstrated that π-conjugated polymers such as poly(3,4-ethylenedioxythiophene) (PEDOT) can be placed between traditional metal electrodes and an electrolyte to mitigate electrolysis in liquid (capillary electroosmosis/electrophoresis) systems. In this report, we extend our previous result to gel electrophoresis, and show that electrodes containing PEDOT can be used with a commercial polyacrylamide gel electrophoresis system with minimal impact to the resulting gel image or the ionic transport measured during a separation.\n"
            ],
            "publication_date": "2014-02-19T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 1097,
            "shares": 0,
            "bookmarks": 3,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0089416",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0089416&representation=PDF",
            "fulltext": "IntroductionThe quest for increased capacity and cost reduction drives miniaturization (and automation) of chemical analysis methods in life science and chemistry. However, many standard techniques, such as gel electrophoresis (GE) of proteins, DNA fragments, and other large molecules, do not lend themselves to straightforward miniaturization to “lab-on-a-chip” systems. In the specific case of GE, the uniformity of the gel, the resolution of the detection method, and the presence of undesired chemical reactions at the electrodes each impart limitations on how small effective separation devices can be. Advances in gel polymerization technology and the use of difference gel electrophoresis [1] (DIGE) have reduced the magnitude and consequences of spatial variations in gel properties dramatically, and improved imaging and detection methods have increased the resolution of the resulting gel images. In this paper, we begin to address the third (and less-frequently discussed) challenge - the electrochemical reactions at the electrodes - by performing a preliminary study exploring the potential for using electrochemically-active conjugated polymer electrodes instead of metal electrodes. Such polymer electrodes can eliminate undesired electrochemical products, reduce manufacturing costs, inhibit cross-contamination between gels, and prevent the consumption of the aqueous electrolyte, which is particularly important when reducing the volume of gel used in systems which are not immersed in electrolyte.\nSimply put, electrophoresis requires the maintenance of a direct current (DC) or steady electric field in an electrolyte. This is traditionally accomplished through Faradaic reactions occurring at the interface where each electrode (anode and cathode) contacts the electrolyte. In most cases, the overall electrochemical reaction is the electrolysis of water, forming O2 gas, H+, and H2O2 at the anode, and H2 gas, and OH− at the cathode. These products are all undesirable, as the gases produced effectively reduce the active electrode area, and the acid and base can negatively impact the molecules (proteins) being separated, particularly when electrophoresis of proteins in their native structure is intended. Furthermore, miniaturized devices often contain relatively small quantities of water, limiting that available for electrolysis before the device literally dries out.\nThis challenge can be addressed by oxidizing and reducing material that remains attached to or within the electrodes. The first class of materials most would consider includes metals such as Ag/AgCl, which are often either too reactive or otherwise incompatible with proteins and DNA fragments. However, the collection of pi-conjugated polymers developed for the printed electronics industry includes many materials that are both conductive and electrochemically active, and are an attractive alternative to metals due to their low cost, flexibility, biocompatibility, and ability to be deposited using a variety of processes.\nThe oxidation of most electrochemically-active pi-conjugated polymers can be represented as follows:where P0 represents a segment of an undoped polymer chain, X− represents an anion (such as Cl−), and e− represents an electron. The oxidized polymer segment can be reduced via the reverse process.\nWe chose to test the suitability of a widely-used blend of poly(3,4-ethylenedioxythiophene) (PEDOT) and sodium poly(styrenesulfonate) (PSS) in the “bath-free” PHAST (TM) SDS-PAGE system from GE (here, General Electric) Healthcare Life Sciences (formerly Pharmacia). The polymer blend, denoted PEDOT:PSS, forms an aqueous emulsion that can easily be printed, coated, or cast into various forms. The negatively-charged PSS acts as the counter ion when PEDOT is oxidized (positively charged):where M+ represents a metal cation (such as Na+). This type of electrochemistry is the basis for a wide variety of electronic devices, such as electrochemical transistors [2], [3], electrochromic displays [4], and bioelectronic [5] and microfluidic [6], [7] devices. PEDOT:PSS, when manufactured, contains a mixture of doped (oxidized, positively charged) and undoped (reduced, neutral) PEDOT segments, and can therefore be used for both the anode and cathode material. Note that previous (capillary) electrophoresis systems employing conjugated polymers appear to have been designed to simply use the polymer electrodes to drive electrolysis, rather than to mitigate electrolysis [8].\nHowever, there are limitations when using π-conjugated polymer electrodes to mitigate the electrolysis of water in applications like gel electrophoresis. Most polythiophenes, including PEDOT, are susceptible to irreversible overoxidation when large potentials (>1 V) are applied in the presence of water and/or oxygen, particularly under basic conditions [9], [10]. Overoxidation breaks the conjugation in the polymer, rendering the polythiophene non-conductive and therefore unusable for electronic or electrochemical applications. Another limitation of oxidizing and/or reducing a pi-conjugated polymer electrode instead of electrolyzing water is the limited electrochemical capacity available. Unlike the water, which in some GE configurations is seemingly limitless, the capacity available for oxidation or reduction is a function of the size of the polymer electrode and the relative fraction of the doped and undoped material contained within. We have previously measured this capacity to be 10 C per g of “dry” PEDOT [6]. Both of these limitations can be overcome with proper device design, as demonstrated by the results presented in this work.\nIn addition to eliminating undesirable side-reactions at the electrodes, replacing or supplementing the fixed, expensive Pt electrodes in GE systems with electrochemically-active conducting polymer electrodes allows the electrodes to be incorporated into the gel during production via relatively inexpensive printing or coating processes. Including the electrodes in a prefabricated gel package, in turn, reduces the possibility of contamination between gels run successively in the same equipment, and facilitates reliable electronic contact between the gel and the driving electronics, enabling further miniaturization of GE systems, particularly when techniques such as DIGE help eliminate the effect of imperfections and variations within the gels themselves.\nAs such, in the work presented here, we have verified the compatibility of PEDOT:PSS with buffers containing sodium dodecylsulfate (SDS) and tris(hydroxymethyl)aminomethane (TRIS) and widely used in polyacrylamide gel electrophoresis (SDS-PAGE), both ex-situ in a simple electrochemical cell, and in-situ, in a real SDS-PAGE protein separation.\nMaterials and MethodsWe chose the PhastSystem from GE Healthcare Life Sciences, for testing our materials and techniques because it is commercially available, offers relatively easy access to the gels and electrodes (e.g., neither gel nor electrodes are submerged in a liquid buffer) and is currently used in laboratories around the world. However, we performed electronic measurements both in a PhastSystem Separation Unit and with an external measurement system based on a Keithley source-measure unit (SMU). The latter system offered considerably higher measurement resolution. The polyacrylamide (PA) gel (PhastGel 8–25, GE Healthcare) and accompanying agarose PhastGel SDS Buffer Strips (GE Healthcare), which together supply the liquid SDS buffer/electrolyte, were used in both measurement setups. Before any of these measurements, we quickly verified the ability of the PEDOT:PSS electrodes to undergo electrochemical oxidation and reduction in the SDS/TRIS electrolyte used in the PhastSystem.\n\nVerification of Electrochemical Compatibility between SDS/TRIS Electrolyte and PEDOT:PSS Electrodes\nTo quickly verify the compatibility of the SDS/TRIS electrolyte with PEDOT:PSS electrodes, we coated a standard glass microscope slide with a thin layer of PEDOT:PSS by spreading a drop of the polymer blend with the edge of a second microscope slide. After allowing the polymer film to dry, a line was scored across the width of the slide with a scalpel, creating two electronically-isolated PEDOT:PSS film electrodes on the glass slide. A thin slice of a PHAST buffer strip (agarose gel containing SDS and TRIS) was placed on top of the electrodes, bridging the gap between them and covering about 50% of each. Applying a potential of 1 V between the electrodes completed the electrochemical circuit.\n\n\nElectronic Measurements and Separation in the PhastSystem\nPEDOT:PSS electrodes were fabricated by molding about 4 g of pre-baked (24 hours at 60°C) Clevios S v3 (Heraeus Precious Metals GmbH), itself a relative viscous paste, in the plastic packaging in which the SDS Buffer Strips arrive from GE Healthcare. The polymer blend was then further dried in a ventilated oven at 55°C for at least 12 hours.\nBefore performing experiments in the PhastSystem, the PEDOT:PSS electrodes were placed on top of a common SDS Buffer Strip, and driven at 1 V via Pt wires inserted into each electrode for 1 hour to oxidize one electrode and reduce the other. The electrodes were then placed on top of new SDS Buffer Strips in the PhastSystem, between the buffer strips, cut to 1/2 their height to accommodate the polymer electrodes, and the Pt-coated electrodes that normally contact the SDS Buffer Strips directly. The electrode that had been reduced in the first step was used as the anode, and the electrode that had been oxidized was used as the cathode. This strategy effectively doubles the capacity of the PEDOT:PSS electrodes compared to their initial capacity, when they consisted of a more-equal blend of oxidized and reduced (neutral) PEDOT. To reduce the size of the polymer electrodes required to meet the capacity requirement for an electrophoretic separation, we cut the PA gels into 11-mm-wide strips. The SDS Buffer Strips were cut to match. The current through the gel was subsequently reduced by a factor of 4 to maintain the same current density that would normally be used if the entire width of the PA gel had been used. All PhastSystem experiments were conducted at 15°C. For performing and visualizing protein separation, we used GE Healthcare’s Full Range Rainbow Recombinant Protein Molecular Weight Marker. The sample to be separated was placed on a 2 µl sample holder (GE Healthcare) in the PhastSystem. The separation was performed using the program outlined in table 1.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  PhastSystem program used.doi:10.1371/journal.pone.0089416.t001\n\nElectronic Measurements with the Keithley SMU\nThe performance of the PEDOT:PSS electrodes was electronically measured and compared to that of Pt wire electrodes in an electrochemical cell shown schematically in figure 1. PEDOT:PSS electrodes were drop-cast onto two glass slides. A Pt wire was embedded into each electrode. An 11-mm-wide strip of the PA gel (PhastGel 8–25) was placed face-up on a lab bench. A buffer strip was placed on either end of the gel, similar to their placement during PhastSystem operation. The glass slides were placed on top of the buffer strips with the polymer electrode facing down. Connecting the Pt wire from each electrode to a Keithley 2636 source-measure unit completed the electrochemical cell, and allowed a constant potential to be applied while the current through the cell was measured.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Schematic of the electrochemical cell tested with PEDOT:PSS electrodes.The Pt wires were inserted into the SDS Buffer Strips for the reference experiments.\ndoi:10.1371/journal.pone.0089416.g001\nResults and DiscussionAs described in the methods section, we built a simple electrochemical cell with two thin PEDOT:PSS electrodes on glass microscope slides connected by a SDS Buffer Strip. The PEDOT:PSS electrodes were reversibly and repeatedly oxidized and reduced by switching the polarity of an applied potential of 1 V. This was observed by a color change (electrochromism) between dark (reduced PEDOT) and light (oxidized PEDOT) blue within the electrodes, demonstrating the transport of ions between and into the electrodes, as shown in the video in the supplementary information (Video S1). Note that only the region of PEDOT:PSS contacting (under) the SDS Buffer Strip (the region directly above the silver pads used for contacting the device with the probes from the power source) is available for electrochemistry. The observed color change confirmed the compatibility of the SDS and TRIS buffer with the PEDOT:PSS, particularly that the ions are able to migrate into the partially-hydrated polymer, allowing the PEDOT through the entire thickness of the electrode to switch.\nNext, we compared the electronic performance of PEDOT:PSS electrodes with that of standard Pt electrodes (employing water electrolysis) in both the PHAST GE system and in a separate setup (shown in figure 1) which offered more accurate current measurement. The experiment in the PhastSystem, the results of which are shown in figure 2, demonstrates the ion-transport equivalence between gels run with Pt electrodes and PEDOT:PSS electrodes using the program described in the methods section, but at a relatively low resolution since the PhastSystem only reports current measurements to the nearest 0.2 mA. These data represent the voltage that the PhastSystem applies in order to maintain the specified current through the device (1 mA). The data from the high-resolution measurements, performed at a constant applied potential of 1 V and then 100 V using a Keithley source-meter, are shown in figures 3 and 4, respectively. For the 1 V experiment, shown in figure 3, the difference between the current through the electrochemical cell with PEDOT:PSS electrodes shows more than 6 times the current through an equivalent cell with Pt electrodes. This demonstrates that the partially-oxidized PEDOT:PSS can be further oxidized or reduced as soon as any potential is applied, where water electrolysis does not begin in earnest until the potential approaches 1.2 V. This assures that, even at larger applied potentials, the Faradaic reactions at the electrodes are performed on the polymer and not on water, preventing electrolytic changes in pH or gas bubble generation. See ref. [6] for more details regarding the electrochemical performance of PEDOT:PSS electrodes.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Comparison between Pt and PEDOT:PSS electrodes in the PhastSystem - Voltage.The potential applied by the PhastSystem while it maintains a constant current (1 mA, see table 1) through equivalent 11-mm-wide PA gels with Pt electrodes (solid black squares) and PEDOT:PSS electrodes (blue circles) as a function of time.\ndoi:10.1371/journal.pone.0089416.g002\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Comparison between Pt and PEDOT:PSS electrodes – current measured at 1 V.Current versus time measured through an electrochemical cell with Pt electrodes (solid black curve) and PEDOT:PSS electrodes (dashed blue curve) for an applied potential of 1 V, where water electrolysis occurs only extremely slowly in a measurement system including a dedicated source-measure unit.\ndoi:10.1371/journal.pone.0089416.g003\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Comparison between Pt and PEDOT:PSS electrodes – current measured at 100 V.Current driven through equivalent 11-mm-wide PA gels as a function of time when 100 V was applied via Pt electrodes (solid black line) and PEDOT:PSS electrodes (dashed blue line).\ndoi:10.1371/journal.pone.0089416.g004For an applied potential of 100 V, which is much closer to the conditions used in most PAGE separations, the current density through the electrochemical cells is nearly the same, as shown in figure 4. Note that the measurement using polymer electrodes shows a slightly higher current density initially, and then decreases almost linearly at a faster rate than the measurement using Pt electrodes. After about 350 seconds, the electrochemical capacity of the PEDOT:PSS electrodes is nearly fully consumed, and the current drops dramatically. This illustrates the previously-described need to design electrodes of sufficient size in any device employing the electrochemical switching of pi-conjugated polymers. Note that the electrodes used for the electrochemical measurements shown in figures 3 and 4 were taken directly from the oven, and not pre-charged using the procedure described in the Materials and Methods section.\nWe performed electrophoresis on GE Healthcare’s Full Range Rainbow Recombinant Protein Molecular Weight Marker to demonstrate the operational equivalence of the PEDOT:PSS electrodes with the standard Pt electrodes in an actual separation. Images of the resulting gels can be found in figure 5. Note that these separations were obtained during the same experiments from which the voltage vs. time data in figure 2 were collected (PhastSystem conditions shown in table 1). This experiment has been repeated at least 6 times and yielded consistent results. We verified that the PEDOT:PSS electrodes were not overoxidized during the separation in the PhastSystem by exchanging the cathode and anode and performing another separation, resulting in a similar gel image and applied voltage history.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Comparison between Pt and PEDOT:PSS electrodes – gel images.Images of the Rainbow Marker separated in an 11-mm-wide PHAST PAGE gel showing the equivalence of the separation of GE Healthcare’s Full Range Rainbow Recombinant Protein Molecular Weight Marker driven with PEDOT:PSS electrodes (left) and with Pt electrodes (right). Protein migration has been driven from the cathode (top) to toward the anode (bottom) using the PhastSystem conditions shown in table 1.\ndoi:10.1371/journal.pone.0089416.g005Although the experiments above employed PEDOT:PSS electrodes in conjunction with the SDS/TRIS buffer strips sold by GE Healthcare Life Sciences, the buffer should not be necessary for maintaining the pH within the gel when electrolysis is avoided. This could require, however, the addition of SDS and electrolyte (salt) to the sample being studied.\nWe have demonstrated that PEDOT:PSS electrodes are chemically and electrochemically compatible with SDS-PAGE separations via electrophoresis of a standard benchmark protein mixture. Aside from introducing the polymer electrodes, no change is required in the protocol for performing SDS-PAGE. This result, in conjunction with our previous demonstration of the reduction of water electrolysis when using PEDOT:PSS electrodes [6], has the potential to pave the way for the development of low-cost, disposable, miniaturized GE systems for accelerated analysis in areas such as proteomics and medical diagnosis (e.g., analysis of proteins associated with tumors in bodily fluids). We hope that this will lead to more advanced and less expensive diagnoses in modern healthcare facilities and in areas of the world where advanced laboratory analyses are not yet readily available."
        },
        "10.1371/journal.pone.0086530": {
            "author_display": [
                "Pankaj Attri",
                "Ku Youn. Baik",
                "Pannuru Venkatesu",
                "In Tae Kim",
                "Eun Ha Choi"
            ],
            "title_display": "Influence of Hydroxyl Group Position and Temperature on Thermophysical Properties of Tetraalkylammonium Hydroxide Ionic Liquids with Alcohols",
            "abstract": [
                "\nIn this work, we have explored the thermophysical properties of tetraalkylammonium hydroxide ionic liquids (ILs) such as tetrapropylammonium hydroxide (TPAH) and tetrabutylammonium hydroxide (TBAH) with isomers of butanol (1-butanol, 2-butanol and 2-methyl-2-propanol) within the temperature range 293.15–313.15 K, with interval of 5 K and over the varied concentration range of ILs. The molecular interactions between ILs and butanol isomers are essential for understanding the function of ILs in related measures and excess functions are sensitive probe for the molecular interactions. Therefore, we calculated the excess molar volume (VE) and the deviation in isentropic compressibility (Δκs) using the experimental values such as densities (ρ) and ultrasonic sound velocities (u) that are measured over the whole compositions range at five different temperatures (293.15, 298.15, 303.15, 308.15 and 313.15 K) and atmospheric pressure. These excess functions were adequately correlated by using the Redlich–Kister polynomial equation. It was observed that for all studied systems, the VE and Δκs values are negative for the whole composition range at 293.15 K. And, the excess function follows the sequence: 2-butanol>1-butanol>2-methyl-2-propanol, which reveals that (primary or secondary or tertiary) position of hydroxyl group influence the magnitude of interactions with ILs. The negative values of excess functions are contributions from the ion-dipole interaction, hydrogen bonding and packing efficiency between the ILs and butanol isomers. Hence, the position of hydroxyl group plays an important role in the interactions with ILs. The hydrogen bonding features between ILs and alcohols were analysed using molecular modelling program by using HyperChem 7.\n"
            ],
            "publication_date": "2014-01-29T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 3,
            "views": 700,
            "shares": 0,
            "bookmarks": 3,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0086530",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0086530&representation=PDF",
            "fulltext": "IntroductionTill date numerous research groups have focused their work on the study of fascinating physical properties of ionic liquids (ILs), due to their wide variety of applications in industries and applied chemistry [1]–[16]. For the applications in chemical and industrial processes, the knowledge of the thermophysical properties of IL is essential, as they represent the basis for the chemical and biological process [17]–[28]. Many of these studies have led to new possible applications for ILs [18]–[21]. Knowledge of structure and properties of ILs is essential for the understanding of their molecular interactions in the binary mixtures [4]–[6], [8]–[16]. Nevertheless, in order to transfer the ILs from laboratory to industry, designing future processes and equipment involving these ionic compounds, an accurate knowledge about their physical properties, either for pure ILs or mixed with other solvents, is crucial. Therefore, a deep knowledge of thermophysical properties of ILs and their liquid mixtures are essentially required for scientific community. Apparently, the physicochemical properties of ILs are quite sensitive toward the structure and nature of cations and anions [7]–[11]. The variations in thermophysical properties of ILs, such as density (ρ) and speed of sound (u) are observed to be very sensitive to the change in ion, mainly due to the microscopic level interactions between solvent molecules [12]–[16].\nBinary mixtures of ILs with other solvents can also improve the thermodynamic and transport properties of working fluids as well as the efficiency of the chemical equipments such as batteries, photoelectrical cells, and other electrochemical apparatus. The use of the binary mixtures of ILs with polar compounds such as alcohols allows the change and control of the properties of the mixtures to suit a given situation [29].Thermodynamic properties of mixtures containing ILs and alcohols are important for both the design of many technological processes and an understanding of the solute–solvent interactions in the mixtures. These properties are required in the development of models for process design, energy efficiency, and in the evaluation of possible environmental impacts [30]. Regarding the study of physical properties for binary mixtures of alcohol+ILs, a large number of works have been published in recent years [31]–[41], showing the interest of the scientific community for this field. While, still there is no experimental or theoretical results are available for the thermophysical properties between the tetraalkylammonium hydroxide and butanol isomers. Additionally, there is no study to show the interactions between the hydroxide anion of the IL and hydroxyl group of alcohols.\nIn this research to study these interaction, we explore and compare the measurements of two thermophysical properties such as ρ and u of binary mixtures involving 1-butanol, 2-butanol and 2-methyl-2-propanol with tetrapropylammonium hydroxide [(C3H7)4N][OH] (TPAH) and tetrabutylammonium hydroxide [(C4H9)4N][OH] (TBAH) ILs over a complete mole fraction range at various temperatures from 293.15 to 313.15 K, with interval of 5 K. Further, the excess molar volume (VE), and deviation in isentropic compressibilities (Δκs) were calculated using experimental data. The resulting VE and Δκs values were found to be strongly dependent on the place of hydroxyl group attached in the chain and also on the interactions between the hydroxide anion of ILs and hydroxyl group of the alcohols. These deviations in physical parameters have been explained in terms of intermolecular interactions between alcohols and ILs. Additionally, the temperature also plays an important role in interaction studies. Moreover, the hydrogen bonding features between ILs and alcohols were carried out to get a deep insight into intermolecular interactions for the studied compounds. These studies were performed according to the semi-empirical calculations by using HyperChem 7.\nMaterials and Methods\nMaterials\n1-butanol, 2-butanol and 2-methyl-2-propanol were obtained from Merck >99% of purity and stored over freshly activated 3 Å molecular sieves and were purified by the standard method described by Riddick et al [42]. A comparison is made for the pure alcohols in Table 1 between the experimental ρ and u values determined in the present study and those reported in the literature [31], [38], [41]–[50]. ILs were synthesized in laboratory and analysed using 1H-NMR, the preparation is given below.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Specifications of pure components and comparison of experimental densities (ρ) and ultrasonic sound velocities (u) with the literature values for alcohols.doi:10.1371/journal.pone.0086530.t001\n\nSynthesis of ILs\nSynthesis of Tetrapropylammonium Hydroxide (TPAH).The synthesis of this IL was carried out in a 250 mL round bottomed flask, which was immersed in a water-bath, fitted with a reflux condenser. Solid potassium hydroxide (40 mmol) was added to a solution of tetrapropylammonium bromide [(C3H7)4N][Br] (40 mmol) in dry methylene chloride (20 mL), and the mixture was stirred vigorously at room temperature for 10 h. The precipitated KBr was filtered off, and the filtrate was then evaporated to leave the crude [(C3H7)4N][OH] as a viscous liquid that was washed with ether (2×20 mL) and dried at 343.15 K for 5 h to obtain the pure IL. The sample was analyzed by Karl Fisher titration and revealed very low levels of water (below 70 ppm). The yield of TPAH was 82%. 1H NMR (DMSOd6): δ (ppm) 0.8 (t, 12H), 1.46 (m, 8H), 2.92 (t, 8H), 4.56 (s, OH). HRMS calculated for C12H29NO (M+ - OH) 203.36, found 203.25.\n\nSynthesis of Tetrabutylammonium Hydroxide (TBAH).A procedure similar to that above for [(C3H7)4N][OH] was followed with the exception of the use of [(C4H9)4N][Br] ([cation]) instead of [(C3H7)4N][Br]. The yield of TBAH was 82%. 1H NMR (DMSOd6): δ (ppm) 0.94 (t, 12H), 1.37 (m, 8H), 1.96 (m, 8H), 3.43 (t, 8H), 4.78 (s, OH). HRMS calculated for C16H37NO (M+ - OH) 259.47 found out to be 259.34.\n\n\n\nExperimental Procedure\nDensity (ρ) and speed of sound (u) measurements.The density (ρ) and speed of sound (u) measurements were performed with an Anton-Paar DSA 5000 with an accuracy of temperature of ±0.01 K. The uncertainties in the density and speed of sound measurements were ±0.00005 g cm−3 and 0.01 m s−1 respectively. Prior to measurements, the instrument was calibrated with deionized water and dry air as standards at 293.15 K.\nThe binary mixtures of butanol isomers and IL were prepared by mass using a high-precision analytical balance with an uncertainty of ±1×10−4 g. All of the samples were prepared immediately before the measurements to avoid variations in composition due to evaporation of the solution. Clear and air bubble free solutions were used to perform the ρ and u experiments at different temperatures. The detailed measurement procedures used were described in detail in our previous research papers [12]–[15].\n\nHydrogen Bonding through Simulation Program.The structures of ILs and alcohols were optimized based on molecular mechanics and semi-empirical calculations using the HyperChem 7 molecular visualization and simulation program [51]–[54]. Initial molecular geometry of butanol isomers and ILs were optimized with the PM3 semi-empirical calculations and single point calculations were carried out to determine the total energies. Now the optimized molecules, alcohols and IL were chosen and then placed on top of each other symmetrically (parallel) with a starting interplanar distance of 2.3 Å and the angle made by covalent bonds to the donor and acceptor atoms less than 1200 was fulfilled. Further, the geometries were optimized using geometry optimizations based on molecular mechanics (using the MM+force field) and PM3 semi-empirical calculations, the Polak-Ribiere routine with rms gradient of 0.01 as the termination condition was used. PM3 uses a set of parameters derived from a variety of experimental versus calculated molecular properties, as compared to other semiempirical methods, including the AM1 procedure [53]. Typically, nonbonded interactions are less repulsive in the PM3 procedure [54]. Hydrogen bonds were displayed using HyperChem “show hydrogen bonds” and “recompute hydrogen bond” options.\n\n\nResults and DiscussionIn order to have the better understanding of the molecular interactions between tetraalkylammonium hydroxide ILs with polar solvents such as 1-butanol, 2-butanol and 2-methyl-2-propanol, we have measured ρ and u properties over the whole composition range at various temperatures such as 293.15, 298.15, 303.15, 308.15 and 313.15 K under atmospheric pressure. The experimental ρ and u values of ILs with alcohols are presented as a function of IL concentration in Table S1 in File S1. Further, Figures 1–4 show the measured ρ and u values for the binary mixtures of different butanol isomers with both ILs (TPAH and TBAH) at all the studied temperatures. Figures 1 and 2 reveal that the variation of ρ values of TPAH or TBAH with 1-butanol, 2-butanol and 2-methyl-2-propanol, shows similar trends. It has been found that the ρ of the mixtures increased with the increasing concentrations of the ILs in alcohols. The effect of the ILs on the ρ in the alcohols has been examined at various temperatures. It has been observed that the ρ values decreased as temperature increased in the all systems. The results in Figure 1a clearly reveal that the ρ values of the TPAH+1-butanol mixture increase sharply up to x1≈0.8000 and later become almost constant at all the temperatures. While for TPAH+2-butanol, ρ values increase up to very rich IL concentration x1≈0.9900, as shown in Figure 1b. On the other hand, ρ values for TPAH+2-methyl-2-propanol increase sharply up to x1≈0.6400 and, no prominent changes have been observed afterwards. The increase in ρ values for TPAH+alcohols mixtures is possibly due to increase in the ion pair interactions between TPAH and alcohols. This shows that the density values of the TPAH+alcohol mixtures are not affected much due to change in the position of hydroxyl group in different isomers.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Densities for the mixtures of TPAH with alcohols vs mole fraction of IL x1 for (a) TPAH+1-butanol; (b) TPAH+2-butanol and (c) TPAH+2-methyl-2-propanol, 293.15 K (□),298.15 K (○), 303.15 K (▵), 308.15 K (▪),313.15 K (•) at various compositions and at atmospheric pressure.The solid line represents the smoothness of these data.\ndoi:10.1371/journal.pone.0086530.g001\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Densities for the mixtures of TBAH with alcohols vs mole fraction of IL x1 for (a) TBAH+1-butanol; (b) TBAH+2-butanol and (c) TBAH+2-methyl-2-propanol, 293.15 K (□), 298.15 K (○), 303.15 K (▵), 308.15 K (▪),313.15 K (•) at various compositions and at atmospheric pressure.The solid line represents the smoothness of these data.\ndoi:10.1371/journal.pone.0086530.g002\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Ultrasonic sound velocity for the mixtures of TPAH with alcohols vs mole fraction of IL x1 for (a) TPAH+1-butanol; (b) TPAH+2-butanol and (c) TPAH+2-methyl-2-propanol, 293.15 K (□), 298.15 K (○), 303.15 K (▵), 308.15 K (▪),313.15 K (•) at various compositions and at atmospheric pressure.The solid line represents the smoothness of these data.\ndoi:10.1371/journal.pone.0086530.g003\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Ultrasonic sound velocity for the mixtures of TBAH with alcohols vs mole fraction of IL x1 for (a) TBAH+1-butanol; (b) TBAH+2-butanol and (c) TBAH+2-methyl-2-propanol, 293.15 K (□), 298.15 K (○), 303.15 K (▵), 308.15 K (▪),313.15 K (•) at various compositions and at atmospheric pressure.The solid line represents the smoothness of these data.\ndoi:10.1371/journal.pone.0086530.g004Whereas, in Figure 2a the ρ values for the TBAH+1-butanol mixtures increases sharply up to x1≈0.6200, later the increase is marginally very less at high concentration of IL region. Whereas, TBAH+2-butanol mixtures have similar trends as shown earlier by TPAH+2-butanol, ρ values increase up to very rich IL concentration x1≈0.9900, as depicted by Figure 2b. Further, TBAH+2-methyl-2-propanol mixture shows the increase in ρ values, while ρ doesn't increase sharply at mole fraction 0.5000–0.9900, which may be due to decrease in ion-pair interactions between TBAH and 2-methyl-2-propanol, as shown in Figure 2c. From Table S1 in File S1, we observed that the densities of investigated systems increase with increasing the length of alkyl chain in IL. It was found that ρ values to be higher in the TBAH+butanol isomers as compared to TPAH+butanol isomers at equimolar mixture. Whereas, according to early documented research articles the density decreases with increase in alkyl chain in a cation or anion [55], [56]. These discrepancies vary from IL to IL and solvent to solvent and also depend on the nature as well as structural arrangement of IL and solvent. Moreover, from close look on the Table S1 in File S1, we observed that with increase in temperature, ρ values of TBAH+butanol isomers decreases more as compared to TPAH+butanol isomers. This might be due to the assumption that the ion-pair interaction decreases more for high alkyl chain+butanol isomers as compared to lower alkyl chain+butanol isomers with the increase in temperature.\nUltrasonic sound velocities (u) prove to be an informative source regarding the properties of different solvents and their mixture. The values of u were found to decrease with an increase in temperature while u values increased with increasing in mole fraction of IL. As noted from Figures 3 and 4, there is a sharp increase of u in all ILs, except in the mixture of TPAH with 1-butanol at 293.15 K, in the mole fraction range from 0.8000 to 0.9900 of IL. Over this range, the u values decrease slightly for the mixtures of TPAH with 1-butanol at 293.15 K. Whereas, no change is observed in rest of the IL+butanol isomers at all investigated temperatures. This u value is significantly increased in IL-solvent interactions when the mole fraction of IL was increased. If we compare the TBAH+1-butanol to TPAH+1-butanol, it has been observed that the u values slightly decrease when the alkyl substituents size of cation increases. Whereas, the same trend was observed on comparing the u values of TBAH+2-methyl-2-propanol to TPAH+2-methyl-2-propanol. It has been found that the u values slightly decrease when the size of cation increases. While the u value for TPAH+2-butanol are lower than TBAH+2-butanol, which again reveal that u values slightly increase as the size of cation increases. Hence, our results lead to conclusion that interactions of ILs with alcohols, depends upon the position of the hydroxyl group.\nThermophysical properties of mixed solvents of ILs with butanol isomers can be tunable. The extent of deviation of liquid mixtures from ideal behavior is best expressed by excess functions. Excess molar volumes (VE) as well as ultrasonic studies are known to provide useful insights into solution structural effects and intermolecular interactions between component molecules. The extent of deviation of liquid mixtures from ideal behavior is best expressed by excess functions. Volumetric properties of binary mixtures of ILs with polar compounds are contributed to the clarification of the various intermolecular interactions existing between the different species found in solution. The excess volumes are determined from the density of pure compounds (ρ1 and ρ2) and mixture (ρm) using a standard equation [15]. The ultrasonic studies have been adequately employed in understanding the nature of molecular interaction in solvent mixed systems. In the chemical industry, knowledge of the ultrasonic and its related properties of solutions are essential in the design involving chemical separation, heat transfer, mass transfer, and fluid flow. Isentropic compressibilities (κs) of the binary mixtures were calculated using the relation from ρ and u. The composition dependence of the VE and Δκs properties represents the deviation from ideal behavior of the mixtures and provides an indication of the interactions between IL and alcohols. These properties were mathematically fitted by variable degree functions using the Redlich-Kister expression:(1)(2)Where Y refers to VE or Δκs. ai are adjustable parameters and can be obtained by least-squares analysis. Values of the fitted parameters are listed in Table S2 in File S1, along with the standard deviations of the fit. The values of VE and Δκs for the binary mixtures at various temperatures as function of ILs concentrations are included in Table S1 in File S1. Figures 5 to 10 display the experimental data for the binary mixtures, and the fitted curves, along with the excess properties of VE and Δκs for the butanol isomers with ILs as function of IL concentrations at different temperatures.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Excess molar volumes (VE) against the mole fraction of TPAH x1 for (a) TPAH+1-butanol; (b) TPAH+2-butanol and (c) TPAH+2-methyl-2-propanol, 293.15 K (□), 298.15 K (○), 303.15 K (▵), 308.15 K (▪),313.15 K (•) at various compositions and at atmospheric pressure.Solid lines correlated by the Redlich-Kister equation.\ndoi:10.1371/journal.pone.0086530.g005\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Excess molar volumes (VE) against the mole fraction of TBAH x1 for (a) TBAH+1-butanol; (b) TBAH+2-butanol and (c) TBAH+2-methyl-2-propanol, 293.15 K (□), 298.15 K (○), 303.15 K (▵), 308.15 K (▪),313.15 K (•) at various compositions and at atmospheric pressure.Solid lines correlated by the Redlich-Kister equation.\ndoi:10.1371/journal.pone.0086530.g006\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 7.  Excess molar volumes (VE) of ILs+alcohols at293.15 K for (a) TPAH+1-butanol(□), TPAH+2-butanol (○) and TPAH+2-methyl-2-propanol (▵); (b) TBAH+1-butanol(□), TBAH+2-butanol (○) and TBAH+2-methyl-2-propanol (▵) at atmospheric pressure.Solid lines correlated by the Redlich-Kister equation.\ndoi:10.1371/journal.pone.0086530.g007\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 8.  Deviation in isentropic compressibilities (Δκs) against the mole fraction of TPAH x1 for (a) TPAH+1-butanol; (b) TPAH+2-butanol and (c) TPAH+2-methyl-2-propanol, 293.15 K (□), 298.15 K (○), 303.15 K (▵), 308.15 K (▪),313.15 K (•) at various compositions and at atmospheric pressure.Solid lines correlated by the Redlich-Kister equation.\ndoi:10.1371/journal.pone.0086530.g008\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 9.  Deviation in isentropic compressibilities (Δκs)against the mole fraction of TBAH x1 for (a) (a) TBAH+1-butanol; (b) TBAH+2-butanol and (c) TBAH+2-methyl-2-propanol, 293.15 K (□), 298.15 K (○), 303.15 K (▵), 308.15 K (▪),313.15 K (•) at various compositions and at atmospheric pressure.Solid lines correlated by the Redlich-Kister equation.\ndoi:10.1371/journal.pone.0086530.g009\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 10.  Deviation in isentropic compressibilities (Δκs) of ILs+alcoholsx1 at293.15 K for (a) TPAH+1-butanol(□), TPAH+2-butanol (○)and TPAH+2-methyl-2-propanol(▵);(b) TBAH+1-butanol(□), TBAH+2-butanol (○) and TBAH+2-methyl-2-propanol(▵)at atmospheric pressure.Solid lines correlated by the Redlich-Kister equation.\ndoi:10.1371/journal.pone.0086530.g010From Figure 5 one can note that the values of VE are negative for all TPAH+butanol isomers systems at all measured temperatures over whole composition range. We have observed that the excess molar volumes present a minimum at x1≈0.5634 for TPAH+1-butanol at all investigated temperatures, whereas we obtained that the VE values present a minimum at x1≈0.4143 for the TPAH+2-butanol system. Further, minimum VE values lie at x1≈0.4244 for TPAH+2-methyl-2-propanol system at all investigated temperatures. The minimum VE values could be due to hydrogen bonds between alcohols and TPAH IL. The decrease in the magnitude of the negative VE values with an increase in the IL composition can be attributed to the decrease of hydrogen bonding. In other words, due to increase in the concentration of the IL results in decrease of packing efficiency. Further, with increase in temperature the magnitude of the negative VE values decreases in all the TPAH+butanol isomers systems. This can again be due to decrease in magnitude of the hydrogen bonding with increase in the temperature.\nIt is interesting to note that the VE values in TPAH+2-methyl-2-propanol mixture show more negative values of VE at the alcohol-rich composition than the TPAH+1-butanol and TPAH+2-butanol mixtures at 293.15 K (Figure 7a), implying that in the TPAH+2-methyl-2-propanol, there are ion-dipole interactions and packing effects with 2-methyl-2-propanol which are stronger than those in the 2-butanol and 1-butanol solution at x1≈0.4200. A comparison between the negative deviation of VE of TPAH+2-methyl-2-propanol, TPAH+1-butanol and TPAH+2-butanol suggests that there is a difference of the hydroxyl position in the alkyl chain, leading to variation in the interactions between the alcohols and TPAH. Molecular interaction between TPAH and alcohols follows the following order at 293.15 K, 2-methyl-2-propanol>1-butanol>2-butanol.\nFurther, the negative VE values are observed for TBAH+butanol isomers at all measured temperatures over whole composition range, except 2-butanol at higher temperatures 293.15 to 313.15 K. The VE values for 1-butanol with TBAH are as represented in Figure 6a. And, we found that negative VE values are observed over the entire mole fraction range at all investigated temperatures. These negative VE values reveal that a more efficient packing or attractive interaction occurred between the TBAH and 1-butanol. 1-Butanol forms a hydrogen bond with the alkyl chain cation, while the interactions decrease at higher temperatures. The interactions between the 1-butanol molecules and the alkyl chain of TBAH are due to ion-dipole or hydrogen bonding interactions. This will reduce the interactions between the tetrabutylammonium cation and hydroxide anion in the IL, which contributes to the negative VE values. Furthermore, the observed positive VE values for TBAH+2-butanol at higher temperatures show that there exist no specific interactions between unlike molecules, as displayed in Figure 6b. The magnitude and sign of VE values are a reflection of the type of interaction staking place in the mixture, which are the result of different effects containing the loss of the dipole interaction from each other and the breakdown of the IL ion pair (positive VE). The interaction between the ion pair of ILs increases as compared to IL+2-butanol interactions, which leads to positive contribution at higher temperature over whole composition range. While, Figure 6c shows the negative VE values for TBAH+2-methyl-2-propanol at all measured temperatures over whole composition range. This might be due to the large difference between the molar volumes of the 2-methyl-2-propanol and TPAH implying that it is possibly due to the fact that the relatively small organic molecules fit into the interstices upon mixing. Therefore, the filling effect of organic molecular liquids in the interstices of ILs, and the ion-dipole interactions between organic molecular liquid and alkyl cation of ammonium ILs, all contribute to the negative values of VE.\nClearly, the observed negative VE values increases further with increasing the temperature in the entire mole fraction range for all IL systems. It is interesting to note that the VE values in the ILs+2-methyl-2-propanol mixture shows more negative values of VE than the IL+2-butanol and IL+1-butanol mixtures at 298.15 K over the alcohol rich concentration range (Figure 7), implying that in the 2-methyl-2-propanol there is strong ion-dipole interactions and packing effects with ILs as compared to 2-butanol and 1-butanol. The magnitude and sign of VE values are a reflection of the type of interactions taking place in the mixture, which reveals that VE values are more negative for TBAH (VE = −8.149 cm3.mol−1 at x1 = 0.2831 for TBAH+2-methyl-2-propanoland VE = −5.787 cm3.mol−1 at x1 = 0.5428 for TBAH+2-butanol) than TPAH (VE = −7.547 cm3.mol−1 at x1 = 0.4143 for TPAH+2-methyl-2-propanol and VE = −5.702 cm3.mol−1 at x1 = 0.5634 for TPAH+2-butanol) in all systems except in ILs+1-butanol systems. For TBAH+1-butanols (VE = −2.448 cm3.mol−1 at x1 = 0.5993) and VE = −3.175 cm3.mol−1 at x1 = 0.4139 for TPAH+1-butanol, hence the VE is more negative for the TPAH+1-butanol than TBAH+1-butanol due to steric hindrance created by the long chain cation of TBAH, that reduces the interaction magnitude between TBAH and 1-butanol. Interestingly, the hydrogen bonding between ILs and butanol isomers has predicted using semiempirical calculations with the help of Hyperchem 7, and those interactions are explicitly elucidated in Figures S1, S2, S3, S4, S5, S6.\nUsing semiempirical calculations for the hydrogen bonding between ILs and butanol isomers, displayed in Figures S1, S2, S3, S4, S5, S6, we calculated heat of formation of the complexes and compared the values with those of the ILs and butanol isomers (as displayed in Table 2). In all the cases, ΔHf of the complex resulting from hydrogen bonding was higher than the sum of ΔHf's of butanol isomers and ILs. It is reasonable to assume that these differences (ΔΔHf), calculated according to equation 3, represent the energies of the hydrogen bond. The energies of the hydrogen bonding can also obtained by using the total binding energies of butanol isomers [54] and ILs, presented in Table 2, instead of ΔHf's for these calculations. The results in Table 2 indicate that the energy required for the formation of a weak hydrogen bond is less than required for the formation of a stronger hydrogen bond:(3)Where ΔHf (1) is the heat of formation of the butanol isomers, ΔHf (2) the heat of formation of the ILs and ΔHf (3) the heat of formation of the complex (butanol isomers and ILs).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Calculated binding energies (E), heats of formations (ΔHf), and estimated hydrogen bond energies (ΔΔHf) (kcal/mol).doi:10.1371/journal.pone.0086530.t002A glance at the Figure S1, illustrated the hydrogen bonding between nitrogen group of TPAH IL with the “-OH” group of 1-buatnol. The binding energy of the TPAH is found to be −3784.96 kcal/mol and that of 1-butanol is found to be −1330.63 kcal/mol, but after the hydrogen bonding occur between the TPAH and 1-butanol, the binding energy of complete system comes out to be −5117.47 kcal/mol (Table 2). Hence, the estimated hydrogen bond energy of the above system is ≈1.87 kcal/mol, which could probably due to the interaction of TPAH with 1-butanol that leads to decrease in energy (less than sum of individual energy of TPAH and 1-butanol), and increase in the strength of hydrogen bonding. Similarly, the Figures S2 and S3, depict the possibility of hydrogen bonding between the nitrogen group of TPAH IL with hydroxyl group of 2-butanol and 2-methyl-2-propanol and the estimated hydrogen bond energies are ≈4.14 and ≈0.64 kcal/mol respectively (Table 2). This shows that the strength of hydrogen bonding is more for TPAH+2-butanol as compared to other butanol isomers. Further, the Figures S4 to S6, clearly again show the hydrogen bonding between the nitrogen group of TBAH with hydroxyl group of 1-butanol, 2-butanol and 2-methyl-2-propanol and now the hydrogen bond energies are ≈0.39, ≈2.29 and ≈0.08 respectively. Hence, we may conclude that the hydrogen bond in case of TBAH+2-butanol is stronger as compared to other butanol isomers.\nOur interpretation of hydrogen bonding between of IL and butanol isomers (based VE data) is quite corroborated with our theoretical calculation of hydrogen bonding of IL+butanol isomers. It is noteworthy that the hydroxyl groups of alcohols are interacting with the nitrogen group of ILs (TPAH and TBAH) (Figures S1, S2, S3, S4, S5, S6). According to literature, the negative VE values are a result of contributions from both the accommodation of organic molecules in the interstice of the IL networks and the ion–dipole interactions between the organic molecules and cation of the ionic liquid [38], [57]. Our experimental results reveal that the negative VE values for entire composition and theoretical calculation suggested that hydroxyl groups of alcohols are interacting with the cation of ILs, as illustrated in Figures S1, S2, S3, S4, S5, S6. Hence, our results are very well correlated with literature results. Therefore, the absolute value of VE is an indicative to the difference in the packing efficiency and the interaction intensity. As can be seen from Figure 7, the VE values for the studied systems follow the sequence: 2-methyl-2-propanol>1-butanol>2-butanol. If only ion–dipole interactions are taken into consideration, the order 1-butanol>2-butanol>2-methyl-2-propanol is understandable. The decreased dielectric constant from 1-butanol (17.8), 2-butanol (16.6) and 2-methyl-2-propanol (10.9) leads to the weaker ion–dipole interaction and in turn resulting in the smaller VE values. Whereas, if we consider the energies of the hydrogen bond (Table 2), the order for ILs+butanol isomers follows: 2-butanol>1-butanol>2-methyl-2-propanol. Our experimental results suggest the order 2-methyl-2-propanol>1-butanol>2-butanol, which reveal that the interactions are not only due to individual contribution of ion-dipole interaction or H-bonding, but it is the combined effect of both the factors. Whereas, another plausible reason is that the butanol isomers makes it easy to accommodate in the interstice of the IL network, and the higher packing efficiency also leads to the larger VE values. While, with increase in temperature there is decrease in VE values in all the systems because at higher temperature the packing efficiency decreases of ILs. On the other hand, the ion-dipole interactions also decrease with the increase in temperature that leads to decrease in VE values.\nFurther, for better understanding of the interactions between the tetraalkylammonium hydroxide ILs, we have calculated the Δκs. As seen in Figure 8, Δκs values of tetraalkylammonium hydroxide ILs+butanol isomers are negative over the full composition range at 293.15 K as a function of ILs concentration.The behavior of Δκs, implies that these mixtures are less compressible than the ideal mixture. This is due to closer approach of unlike molecules and a stronger interaction between components of mixtures that leads to a decrease in the compressibility. From Figure 8a, it can be seen that the minimum Δκs values are observed at mole fraction of IL ≈0.4141 for the TPAH+1-butanol system. The negative Δκs values of TPAH+1-butanol are attributed to the strong attractive interactions due to the solvation of the ions in these solvents, over the complete composition range and at all studied temperatures. Similarly, the curves in Figure 8 (c and d), show that the Δκs values for the 2-butanol or 2-methyl-2-propanol+TPAH systems are negative over the complete composition range and at all studied temperatures, except at the higher temperatures (308.15 and 313.15 K) for ≈0.8100 to 0.9999 composition range. The minimum is approached at mole fraction of IL ≈0.4171, ≈0.3272 and ≈0.3138 for the TPAH+1-butanol, TPAH+2-butanol and TPAH+2-methyl-2-propanol systems at all temperatures, respectively. The negative Δκs values attributed to the strong attractive interactions between the molecules of the components. The negative values of Δκs of the TPAH with butanol isomers imply that solvent molecules around solute are less compressible than the solvent molecules in the bulk solutions. Whereas on further addition of IL, there is decrease in the compressibility graph at all studied temperature ranges. This might be due to the decreased attraction between IL and butanol isomers in IL rich concentration region. Additionally, for the 2-butanol and 2-methyl-2-propanol, there are positive Δκs values at higher temperatures, this is might be again due to decrease in attraction of TPAH and alcohol molecules in the IL-rich concentration region, since the interaction between the ILs increases and whereas decreases in case of IL and alcohols.\nFigure 9 depicts the negative Δκs values of all TBAH+butanol isomers over the full composition range at 293.15 K. The curves in Figure 9 show that the Δκs values for the 1-butanol or 2-butanol or 2-methyl-2-propanol systems are negative over the complete composition range at low temperature. The minimum is approached at mole fractions of IL ≈0.4787, ≈0.4604 and ≈0.1985 for the TBAH+1-butanol, TBAH+2-butanol and TBAH+2-methyl-2-propanol systems, respectively. Our results show that for all the system, TBAH+butanol isomers shows the positive Δκs values in the IL-rich region at the higher temperatures. These results are very similar with the TPAH+alcohols at higher temperature, which might be due to the decrease in the attraction of TBAH and alcohol molecules in the IL-rich concentration region due to the increased interaction between the ILs and the decreased interaction between IL and alcohols. Obviously, the Δκs values in the ILs+2-methyl-2-propanol mixture shows more negative values of Δκs than the ILs+2-butanol and ILs+1-butanol mixtures at 293.15 K over the entire concentration range (Figure 10), implying that in the 2-methyl-2-propanol there is strong ion-dipole interactions and packing effects with ILs as compared to 2-butanol and 1-butanol. The magnitude and sign of Δκs values are a reflection of the type of interactions taking place in the mixture, which reveals that Δκs values are more negative for TBAH+2-methyl-2-propanol (Δκs = −211.351TPa−1 at x1 = 0.1985), than TBAH+1-butanol (Δκs = −168.519TPa−1 at x1 = 0.4787), and least is for TBAH+2-butanol (Δκs = −144.998TPa−1 at x1 = 0.4604). For TPAH+2-methyl-2-propanol (Δκs = −264.189TPa−1 at x1 = 0.3115), TPAH+1-butanol (Δκs = −222.322TPa−1 at x1 = 0.3218), and TPAH+2-butanol (Δκs = −153.949TPa−1 at x1 = 0.0.3244). TPAH+butanol isomers have more negative Δκs values then TBAH+butanol isomers this is might be due to the steric hindrance created by the long chain cation of TBAH, which reduces the interaction magnitude between TBAH and alcohols.\nHowever, after close look about the physical properties of alcohols with ILs, we observed that hydroxyl position of alcohols are playing important role in addition to the cation chain length of the ILs. ILs (TPAH or TBAH) interact strongly with the 2-methyl-2-propanol as compared to the 2-butanol and 1-butanol, this is might be due to more+I-effect of 2-methyl-2-propanol, which increases its tendency to interact with ILs more strongly as compared to 2-butanol and 1-butanol. Also, 1-butanol interacts more strongly as compared to 2-butanol with ILs, which might be due to the steric hindrance the interaction of 2-butanol decreases as compared to 1-butanol. However, there is decrease in VE and Δκs values in all the system due to increase in temperature; this might be due to strong self-association between the alcohol molecules that prevents the alcohol-IL strong interactions. Our experimental results of VE values are very well supported with literature [38], [46], [47]. Wen-Lu Weng [47], showed the interactions of anisole with 2-butanol and 2-methyl-2-propanol, author observed that VE values of the 2-methyl-2-propanol is more negative than 2-butanol. Additionally, the VE values increases (less negative) with increase in temperature. Further, Qian et al. [38], showed that 1-methylimidazolium acetate IL interacts with methanol, ethanol, 1-propanol and 1-butanol, it was observed that in all the systems VE values increase with increase in the temperature. Moreover, during the interaction of formamide with 1-butanol negative VE values have been observed, whereas interaction of formamide with 2-butanol results in positive VE values [46]. These all results by various authors support our above results explanation that interactions between ILs+alcohols depend upon the position of hydroxyl group. Therefore, the physicochemical properties of ILs are quite sensitive toward the structure and nature of interacting molecules.\n\nConclusion\nWe have performed and compared thermophysical properties of binary mixtures of tetraalkylammonium hydroxide based ILs with butanol isomers over the whole composition range at various temperatures (293.15 to 313.15 K, in steps of 5 K). To obtain a more detailed picture of the molecular interactions, we measured temperature dependence properties of ρ and u for ILs with butanol isomers over the whole composition range at various temperatures. The ρ and u values increase with the increasing the cation alkyl chain length of ILs. Our results reveal that the position of hydroxyl group in alcohols leads to alteration of the thermophysical properties of ILs. To measure the non-ideality of the mixtures, we determined VE and Δκs at each temperature as a function of IL concentration. The predicted properties were correlated by the Redlich-Kister type equation. Our studies demonstrate that there is decrease in VE and Δκs values in all the systems due to increase in temperature; this might be due to strong self-association between the alcohol molecules that prevents the alcohol-IL strong interactions. Additionally, according to the theoretical calculations obtained by HyperChem 7, the energy of hydrogen bond is more for low alkyl chain ILs (TPAH) as compared to higher alkyl chain ILs (TBAH) with alcohols. Molecular interactions such as ion-dipole and hydrogen bonding between the butanol isomers and alkyl chain of ILs are suggested to be mainly responsible for variation in the thermophysical parameters. Our findings provide better molecular interactions for the mixing of the solvents and better analysis of the solvation process.\n\nFigure S1. Schematic depiction of the hydrogen bonding interaction between TPAH and 1-butanol molecules, which is predicted by a semiempirical calculation with the help of HyperChem 7.doi:10.1371/journal.pone.0086530.s001(TIF)Figure S2. Schematic depiction of the hydrogen bonding interaction between TPAH and 2-butanol molecules, which is predicted by a semiempirical calculation with the help of HyperChem 7.doi:10.1371/journal.pone.0086530.s002(TIF)Figure S3. Schematic depiction of the hydrogen bonding interaction between TPAH and 2-methyl-2-propanol molecules, which is predicted by a semiempirical calculation with the help of HyperChem 7.doi:10.1371/journal.pone.0086530.s003(TIF)Figure S4. Schematic depiction of the hydrogen bonding interaction between TBAH and 1-butanol molecules, which is predicted by a semiempirical calculation with the help of HyperChem 7.doi:10.1371/journal.pone.0086530.s004(TIF)Figure S5. Schematic depiction of the hydrogen bonding interaction between TBAH and 2-butanol molecules, which is predicted by a semiempirical calculation with the help of HyperChem 7.doi:10.1371/journal.pone.0086530.s005(TIF)Figure S6. Schematic depiction of the hydrogen bonding interaction between TBAH and 2-methyl-2-propanolmolecules, which is predicted by a semiempirical calculation with the help of HyperChem 7.doi:10.1371/journal.pone.0086530.s006(TIF)File S1. Supporting Tables. Table S1. Mole fraction (x1) of IL, density (ρ), ultrasonic sound velocity (u), Excess molar volumes (VE), isentropic compressibility (κs), and deviation in isentropic compressibility (Δκs) for the systems of tetraalkylammonium hydroxide IL with butanol isomers at T = 293.15, 298.15, 303.15, 308.15 and 313.15 K and at atmospheric pressure. Table S2. Estimated Parameters of eq 1 and Standard Deviation σ, for the Systems of ILs with butanol isomers as Function of Temperature.doi:10.1371/journal.pone.0086530.s007(DOCX)"
        },
        "10.1371/journal.pone.0004473": {
            "author_display": [
                "Ke Chen",
                "Lukasz Kurgan"
            ],
            "title_display": "Investigation of Atomic Level Patterns in Protein—Small Ligand Interactions",
            "abstract": [
                "Background: Shape complementarity and non-covalent interactions are believed to drive protein-ligand interaction. To date protein-protein, protein-DNA, and protein-RNA interactions were systematically investigated, which is in contrast to interactions with small ligands. We investigate the role of covalent and non-covalent bonds in protein-small ligand interactions using a comprehensive dataset of 2,320 complexes. Methodology and Principal Findings: We show that protein-ligand interactions are governed by different forces for different ligand types, i.e., protein-organic compound interactions are governed by hydrogen bonds, van der Waals contacts, and covalent bonds; protein-metal ion interactions are dominated by electrostatic force and coordination bonds; protein-anion interactions are established with electrostatic force, hydrogen bonds, and van der Waals contacts; and protein-inorganic cluster interactions are driven by coordination bonds. We extracted several frequently occurring atomic-level patterns concerning these interactions. For instance, 73% of investigated covalent bonds were summarized with just three patterns in which bonds are formed between thiol of Cys and carbon or sulfur atoms of ligands, and nitrogen of Lys and carbon of ligands. Similar patterns were found for the coordination bonds. Hydrogen bonds occur in 67% of protein-organic compound complexes and 66% of them are formed between NH- group of protein residues and oxygen atom of ligands. We quantify relative abundance of specific interaction types and discuss their characteristic features. The extracted protein-organic compound patterns are shown to complement and improve a geometric approach for prediction of binding sites. Conclusions and Significance: We show that for a given type (group) of ligands and type of the interaction force, majority of protein-ligand interactions are repetitive and could be summarized with several simple atomic-level patterns. We summarize and analyze 10 frequently occurring interaction patterns that cover 56% of all considered complexes and we show a practical application for the patterns that concerns interactions with organic compounds. "
            ],
            "publication_date": "2009-02-16T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 15,
            "views": 4259,
            "shares": 0,
            "bookmarks": 26,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0004473",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0004473&representation=PDF",
            "fulltext": "IntroductionProtein-protein and protein-ligand docking are among the central topics in structural biology. The former provides useful input for constructing protein-protein interaction networks and for understanding the protein's function, while the latter provides a basis for selection of drug candidates by virtual screening [1], [2]. To date, interactions between proteins and macromolecules, i.e., protein-protein [3], [4], protein-DNA [5], and protein-RNA [6], have been systematically investigated. Thornton's study compared the size, shape, residue interface propensities and hydrophobicity of the protein-protein interface for four different types of protein-protein complexes [3]. Luscombe and colleagues studies the role of hydrogen bonds, van der Waals contacts and water mediated bonds in protein-DNA interaction. They concluded that the majority of the amino acid-base interactions observed follow general principles that apply across all protein-DNA complexes [5]. Rajamani and colleagues show that the anchor residues in protein-protein interactions maintain similar conformations before and after the binding, which allows for a relatively smooth binding process [7]. Ma's report shows that several structurally conserved residues could be used to distinguish between binding sites and general exposed surface; for instance, conservation of Trp, Phe, and Met residues on the protein surface was shown to be associated with a higher likelihood of formation of a binding site [8]. The principles that govern protein-metal ion interaction were recently reviewed by Dudev and Lim. They summarized several rules with respective to the coordination mode, coordination number, metal selectivity and coordination stereochemistry [9]. In another review by Dudev and Lim, various factors governing metal binding affinity and selectivity were systematically analyzed [10]. The structure and properties of the metal-binding sites were also discussed for specific metal ions like Ca2+ and Zn2+ [11], [12]. On the other hand, although characterization and prediction of protein-ligand interaction sites has attracted attention [13], [14], the protein-ligand interactions were never systematically studied and the rules that govern these interactions were not yet fully disclosed. The protein-ligand recognition is usually performed using an approach in which the protein and the ligand are considered as complementary surfaces [15], or by executing the actual docking process and calculating the protein-ligand interaction energies [16]. A recent study by Thornton's group reveals that pockets binding the same ligand show greater variation in their shapes than can be accounted for by the conformational variability of the ligand, which suggests that the geometrical complementarity is not sufficient to drive molecular recognition process [17]. This prompts our investigation into the interactions between proteins and ligands, in which we analyze both covalent bonds (normal covalent bonds and coordination bonds) and non-covalent bonds (electrostatic force, hydrogen bonds and van der Waals force). We focus on studying small ligands that exclude proteins, peptides, and nucleotides which were already investigated by other groups. Our aim is to find frequent regularities (patterns) that could be used to summarize interactions between the protein and the considered ligands. We analyze each of the major types of bonds for the ligands that are grouped into four categories including organic compounds, metal ions, inorganic anions, and inorganic clusters. In spite of inclusion of proteins characterized by low sequence identity and the diversity of the considered ligands, we found interesting and frequently occurring atomic-level patterns for several types of the considered ligands. We note that “atomic-level” term refers to the fact that patterns concern interactions between individual atoms of the residue and the ligand and it has no relation with the resolution of the considered crystal structures. Although the extracted patterns have been described in the literature, a comprehensive, in terms of the wide range of interaction and ligand types, overview of such interactions was not attempted. We systematically and conveniently summarize several major different interactions, we discuss specific details of these interactions across different residue types and ligands, e.g., the number of residues and the residues types that are involved in the coordination bonds with specific metal ions, and we quantify their relative abundance, which can be used to asses their importance in protein-ligand interactions. We also show, using a case study that concerns recent blind (without the knowledge of the ligand) geometric method for prediction of the binding sites, that usage of several patters in tandem improves the binding site predictions and that the sites predicted using patterns are complementary to the results based on the geometric analysis of the protein surface. Discovery of such interaction patterns would not only provide a comprehensive overview of protein-ligand interactions, but it would also facilitate design of binding site prediction methods and high-throughput molecular docking procedures.\nMaterials and Methods\nNormal covalent and coordination bonds\nThe interaction between a non-hydrogen atom A1 of a residue and a non-hydrogen atom A2 of a ligand is defined as the covalent bond if the residue and the ligand do not have the opposite charge that would result in electrostatic force and the distance d of these two atoms satisfies(1)where radius(Ai) represents the radius of Ai. As discussed by Davis and colleagues [18], in a typical 3Å resolution structure, the uncertainty of the position of the individual atoms can easily be 0.5Å or more. The marginal 0.5Å value used in formula 1 accommodates for the uncertainty of the positions of both atoms and for the variation of the length of covalent bonds, i.e., the length of a single bond between carbon atoms ranges between 1.2Å to 1.54Å.\nMetal ions usually do not contain electrons in their outer shell. Therefore, if a metal ion forms the covalent bond with another atom, the pair of electrons shared by the metal ion and the second atom should be provided by the other atom. The corresponding covalent bond is defined as the coordination bond. As a result, metal ions and non-metal atoms (on a residue) whose interaction satisfies formula 1 are assumed to form the coordination bond.\n\n\nHydrogen bond\nHydrogen bonds were calculated with HBPLUS [19]. To identify hydrogen bonds, the program finds all proximal donor (D) and acceptor (A) atom pairs that satisfy specified geometrical criteria for the formation of the bond. Theoretical hydrogen atom (H) positions of both protein and ligand are calculated with REDUCE program [20]. The criteria used for the current study are: H–A distance<2.7Å, D–A distance<3.5Å, D–H–A angle>90° and H–A–AA angle>90°, where AA is the atom attached to the acceptor.\n\n\nElectrostatic force\nAmong the 20 amino acids (AAs), the electrostatic force concerns positively charged Arg, His, and Lys residues and negatively charged Asp and Glu residues. The charge of the ligand is annotated using Protein Data Bank (PDB) [21] dictionary located at http://deposit.rcsb.org/public-component​-erf.cif, which provides the charge of each atom of the ligand. An atom of the ligand and an AA in the protein are considered to exert electrostatic force with each other if they have opposite charges and at least one non-hydrogen atom of the AA is less than 3.5Å away from the charged atom of the ligand.\n\n\nVan der Waals force\nA non-hydrogen atom A1 of a protein and a non-hydrogen atom A2 of a ligand form van der Waals contact if the distance d between these two atoms satisfies(2)where vdW(Ai) is the van der Waals radius of Ai and where these two atoms do not form covalent bond, coordination bond, hydrogen bond, and electrostatic force. This is consistent with the definition used in the investigation of protein-protein interactions by Ma and colleagues [8], in which two residues were considered to be in contact if there is at least one pair of atoms, one atom from each residue, at a distance smaller than the sum of their vdW radii plus a threshold of 0.5Å.\n\n\nThe dataset of protein-ligand complexes and distribution of the ligands in PDB\nThe protein chains, which were selected using culledPDB list generated by PISCES server [22], are characterized by the following: 1) the chains share sequence identity of below 25%; 2) the resolution of the protein-ligand complex structure is below 2.0Å; and 3) the Rwork value is below 0.25. These criteria, which resulted in selection of 2320 chains, assure that the selected proteins share low sequence identity (they adequately sample the sequence space) and that the corresponding structures have sufficient quality. The length of these chains varies between 20 and 1083, some short sequences are fragments of protein chains, and both monomers and oligomers are included. The protein and a ligand are assumed to interact with each other when at least one pair of non-hydrogen atoms, one from the protein and one from the ligand, can be found within 3.5Å distance. The minimal distance is consistent with the value used in [8]. If the same ligand binds a given protein in multiple pockets, all pocket-ligand complexes are included. Excluding the water molecule, all molecules annotated as “HET” in PDB, which includes organic compounds and ions, were taken as ligands. This excludes protein chains, peptides and nucleotides. As a result, 7759 pockets which have at least one contact with the considered ligand were extracted from the 2320 chains.\nAmong the 7759 complexes, some of the ligands appear multiple times, some are similar and could be grouped together and the same/similar ligands bind to a variety of pockets. To facilitate analysis of the protein-ligand interactions we select only these ligands that occur frequently and we group them into several categories. The ligands that bind to at least 100 pockets cover 59.4% of the considered complexes. Among these ligands, GOL, EDO, NAG, and ACT are organic compounds, Ca2+, Zn2+, Na+, Mg2+ and Cd2+ are metal ions, and SO42−, PO43−, Cl−, Br− and I− are inorganic anions. Additionally, some inorganic clusters, i.e., Fe-S cluster, also bind to a relatively large number of pockets. Therefore, the considered ligands (including those that occur in less than 100 pockets) are grouped into four categories: organic compounds, metal ions, inorganic anions, and inorganic clusters. We analyze total of 3685 organic compounds (that include 560 distinct types), 1682 metal ions (25 types), 1837 inorganic anions (19 types), and 54 inorganic clusters (9 types), which cover (3685+1682+1837+54)/7759 = 93.5% of all extracted pockets.\n\nResults\nSummary of the interaction patterns\nThe protein pocket-ligand interactions are summarized in Figure 1. The top layer divides the 7759 protein pocket-ligand complexes into 5 categories based on the ligand type. The second layer lists the major forces that are involved in formation of protein-ligand complexes for a given ligand type. For instance, protein pocket-organic compound complexes are formed mainly by the means of covalent bonds, hydrogen bonds, and van der Waals contacts, which accommodate for 99.9% of the interactions. The remaining 0.1% of the contacts between a protein and the organic compound, which are omitted in the Figure 1, is based on the electrostatic force. The bottom layer provides significant patterns that are associated with interactions for a given type of the ligand and a given type of bond/force, which are discussed in detail in the following sections.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  An overview of the protein pocket-ligand interactions.The top layer divides protein-ligand complexes into 5 major groups based on the type of the ligand. The second layer shows the major forces that are involved in formation of protein-ligand complexes for each type of the ligand. The bottom layer summarizes significant (frequently occurring) patterns for each force/bond type and each type of the ligand. The patterns are shown in XR…YL or XR – YL format where X denotes an atom type of residue R in the protein, Y denotes an atom type of the ligand L, strong interactions (covalent and coordination bonds) are depicted by “–”, and weak interactions (hydrogen bond) are represented by “…”.\ndoi:10.1371/journal.pone.0004473.g001The forces that are omitted in Figure 1 are less significant (less frequent or nonexistent) for a given type of the protein-ligand interaction. Our analysis concentrates on the forces that are characterized by frequently occurring patterns for a given ligand category, while omitting some forces which are listed in Figure 1 and for which we could not find strong regularities (patterns). For the protein-organic compound interactions, we focus on the hydrogen and covalent bonds since they exhibit more regular and frequent patterns than the van der Waals contacts. In the case of the protein-metal ion interactions, electrostatic force and coordination bonds, which cover 95% these interactions, are analyzed. The discussion of the protein-inorganic anion interactions concentrates on the electrostatic force and hydrogen bonds; the van der Waals contacts are omitted due to lack of regular interaction patterns. Finally, our analysis of the protein-inorganic cluster interactions concerns only the coordination bonds since they constitute the main driving force for these interactions, i.e., they are involved in all considered protein-inorganic cluster complexes. Although we investigate all four interaction types, in our analysis we concentrate on the protein-organic compound and the protein-metal interactions since they occupy the largest fraction of the considered protein-ligand complexes and they are important for many biological processes [24], [25].\n\n\nInteraction patterns in protein-organic compound complexes\nOrganic compounds bind to proteins mainly by the means of the van der Waals contacts and the hydrogen bonds. Total of 85771 contacts were observed between an organic compound and a protein and they include 77554 van der Waals contacts, 7914 hydrogen bonds, and 246 covalent bonds. The remaining 0.1% of contacts are due to the electrostatic force. Among the 3685 protein pocket-organic compound complexes, 1067 complexes (29%) are based solely on the van der Waals contacts, 2309 (62.7%) involve both hydrogen bonds and van der Waals contacts, 107 (2.9%) incorporate covalent bonds and van der Waals contacts, and 135 (3.7%) include covalent bonds, hydrogen bonds, and van der Waals contacts, see Figure 2. We note that the number of hydrogen bonds is likely underestimated since REDUCE could not supply complete coordinates for hydrogen atoms of some ligands and thus some potential hydrogen bonds could not be counted.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  The summary of forces/bonds that are involved in formation of protein-organic compound complexes.The chart shows that most of the complexes involve multiple contact types with the most frequent contacts involving both van der Waals force and hydrogen bonds.\ndoi:10.1371/journal.pone.0004473.g002Covalent bond.Majority of the 246 covalent bonds formed between organic compounds and proteins are summarized with four patterns: 1) 27 covalent contacts are formed between the thiol of Cys residue and the carbon atom of the organic compound (thioether bond); 2) 139 are formed between the nitrogen atom of Asn residue and the carbon atom of N-Acetyl-D-Glucosamine (NAG); 3) 28 concern the thiol of Cys residue and the sulfur atom of the organic compound (disulfide bond); and 4) 23 involve the nitrogen atom of Lys residue and the carbon atom of organic compound. We observe that the interaction between protein and NAG is established through the process of glycosylation and this interaction is not observed for other ligands. Therefore, this interaction is not included as a pattern for covalent bond. We denote the other three patterns as Scys—Cligand, Scys—Sligand, and Nlys—Cligand respectively. They cover (27+28+23)/107 = 73% of all investigated covalent bonds between proteins and organic compounds; see summary in Table 1. Both the thiol of Cys and the nitrogen atom of Lys could interact with a variety of organic compounds. This result indicates that the covalent bonds could be formed only between a few specific atoms of some AAs and a few specific atoms of the organic compounds.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  A summary of interaction patterns concerning covalent bonds formed between a protein and an organic compound.doi:10.1371/journal.pone.0004473.t001Since some covalent bond patterns concern only a few dozens of complexes, we investigate whether they are specific to a certain protein family or more generic and associated with a variety of families. We note that in contrast to the covalent bonds, in the case of the subsequently discussed coordination and hydrogen bonds, thousands of contacts between the proteins belonging to a wide range of families and the ligands are established. Based on SCOP classification system [23], the Scys—Cligand bonds are formed for proteins belonging to 15 families, which cover four major structural classes, i.e., all-α, all-β, α/β, and α+β. Similarly, the Scys—Sligand and Nlys—Cligand, bonds concern proteins from 15 and 10 families and 4 and 3 structural classes, respectively. This shows that the above patterns span dozens of structurally different protein families, which in turn indicates that they are not specific to a certain protein family or class.\nThioether bond and the bond between the nitrogen atom of Asn residue and the carbon atom of NAG are involved in a number of cellular activities and their formation could be associated with the protein's function. For instance, Ma's study suggests that the mycobacterium tuberculosis LipB enzyme transferase functions as a cysteine/lysine dyad acyltransferase, in which two invariant residues (Lys-142 and Cys-176) are likely to function as acid/base catalysts [24]. We observe that the tuberculosis LipB protein –decanoic acid complex is linked by thioether bond formed between Cys-176 and carbon-3 of decanoic acid. Zoltowski's study shows that formation of thioether bond between Cys thiol and the flavin C4a position is a response upon the blue-light excitation; attack of the thiol at C4a reduces the flavin ring, breaks aromaticity, and bleaches the absorption bands at 450 and 478 nm [25].. The above studies demonstrate the important role of the covalent bonds in catalysis, protein folding, and light-triggered cellular activity.\n\nHydrogen bond.Hydrogen bonds are formed in 2466/3685 = 66.9% of the organic compound based complexes. Although all 20 AAs can establish hydrogen bonds with compounds, their ability to form hydrogen bonds varies. Table 2 shows the distribution of occurrence of the hydrogen bonds formed by each AA and the occurrence of the AAs in the 3685 pockets. Seven hydrophilic residues (based on the low values of their hydropathy index [26]), including Arg, Lys, Asn, Thr, Ser, Gln, and His establish larger number of hydrogen bonds when compared their occurrence in the pockets. Moreover, six hydrophobic residues, i.e., Ala, Cys, Val, Ile, Met, and Leu, occupy 26.1% of the residues in the pockets and they form only 10.7% of the hydrogen bonds. This suggests that the hydrophilic residues form hydrogen bonds with the organic compounds more frequently when compared with the hydrophobic residues. Among the 7914 hydrogen bonds between proteins and organic compounds, AAs serve as donors for 6526 hydrogen bonds, and as acceptors for only 1371 hydrogen bonds; they serve as both donors and acceptors for the remaining bonds.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  A summary of hydrogen bonds formed between specific amino acids and organic compounds.doi:10.1371/journal.pone.0004473.t002The distribution of occurrence of the hydrogen bonds with the organic compounds for the individual AAs is compared with the corresponding results obtained for protein-DNA interactions, which were derived based on 129 protein-DNA complexes [5], see Table 2. In both cases, the distributions are similar, i.e., Arg, Lys, Ser, Thr, and Asn establish the largest number of hydrogen bonds with both the organic compounds and the DNA molecules, while Phe, Met, Cys, and Pro establish the smallest number of hydrogen bonds with both types of ligands. The two AAs that establish the highest number of hydrogen bonds, Arg and Lys, are characterized by a larger relative number of bonds in the case of the binding with DNA, although we emphasize that the order of AAs in both cases is consistent. This suggests that the ability of AAs to establish hydrogen bonds could be an intrinsic characteristic of the AA itself, which is independent of the type of the ligand.\nThe negatively charged residues Asp and Glu did not exhibit strong affinity towards establishing hydrogen bonds in spite of having relatively high solvent accessibility and inclusion of two oxygen atoms in their side chains. We observe that Asp and Glu form the largest number of hydrogen bonds (278 and 300) when the AA serves as acceptor. At the same time they form only 103 and 53 hydrogen bonds when they serve as donors, which is relatively small when contrasted with the number of hydrogen bonds formed by other hydrophilic residues, e.g., 1555 for Arg and 802 for Lys. This low affinity to form hydrogen bonds could be explained by considering that the carboxyl groups of Asp and Glu often lend their H+ to solution, and as a result the two oxygen atoms on the carboxyl group are not bonded to hydrogen atom and cannot serve as donor when forming the hydrogen bond.\nThe most frequently formed hydrogen bond is established between NH- group (as the donor) of an AA and the oxygen atom of an organic compound. This type of the hydrogen bond covers 5206/7914 = 65.8% of all hydrogen bonds. To compare, the NH- group of organic compound serving as the donor and the oxygen atom of AAs account for only 325 hydrogen bonds. The surface patch that is characteristic for NH- group has high potential to form hydrogen bonds with organic compounds. For instance, in the chain A of neuraminidase protein (PDB entry 1F8E) [27], the pocket that binds 4,9-AMINO-2,4-DEOXY-2,3-DEHYDRO-N-ACETYL​-NEURAMINIC(abbreviated to 49A in PDB) includes 4 Arg residues, i.e., Arg118, Arg152, Arg292, and Arg371, see Figure 3. Three of them, Arg118, Arg292, and Arg371, are spatially adjacent and they form 5 hydrogen bonds with the oxygen atoms of 49A, while the other residues in the pocket establish only 2 hydrogen bonds. The cluster of the five hydrogen bonds is crucial for the interaction between the protein chain and the compound.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  An example stereo diagram of hydrogen bonds formed between NH- group of a residue and oxygen atom of an organic compound.The oxygen atom is colored red, nitrogen atom is blue, carbon atom is gray, and hydrogen atom is white. The residues in the pocket are in ball and stick format while the ligand is in stick format. Hydrogen bonds are represented by “…”. The structure is taken from chain A of neuraminidase protein (PDB entry 1F8E), which interacts with 49A. The binding pocket contains four Arg residues and each residue contains 2 NH- groups. Three Arg residues (Arg118, Arg292, Arg371) are spatially adjacent, and they form five hydrogen bonds with the oxygen atoms of the ligand.\ndoi:10.1371/journal.pone.0004473.g003\nVan der Waals contact.Majority of the van der Waals contacts are formed between carbon, oxygen and nitrogen atoms. These three atoms result in nine potential combinations which cover 94.8% of all van der Waals contacts between proteins and organic compounds. The most common van der Waals contacts are established between carbon atom of a residue and carbon atom of a compound, and carbon atom of a residue and oxygen atom of a compound. Each of the above two cases accounts for more than 25% of all van der Waals contacts. In contrast with the covalent and hydrogen bonds, van der Waals contacts are irregular and lack frequently occurring patterns that would indicate involvement of particular residues.\n\n\n\nInteraction patterns in protein-metal ion complexes\nAmong 1682 protein-metal ion complexes, 639 involve both coordination bonds and electrostatic force, 459 are based on electrostatic force but with no coordination bonds, and 499 incorporate coordination bonds with no electrostatic force. Overall, electrostatic force and coordination bonds are involved in (639+459+499) = 1597 complexes, which corresponds to 1597/1682 = 94.9% of all protein-metal ion complexes.\nAsp and Glu residues are negatively charged and could potentially form electrostatic contact with the metal ions. Since the charge is not evenly distributed over the AAs, we analyzed which non-hydrogen atom of Asp/Glu is the closest to the metal ions. Among 1098 complexes involved the electrostatic force, metal ions formed electrostatic interaction with Asp and Glu 1511 times (in some complexes more than 1 electrostatic interaction is formed). In the case of 1385 out of above 1511 interactions, the oxygen atoms of the carboxyl group of Asp and Glu are the closest to the metal ion. This suggests that these two oxygen atoms could be more negatively charged than other atoms in the side chains.\nMetal ions were observed to form coordination bonds with up to 6 atoms of a given protein, i.e., in chain A of 4-chlorobenzoyl coenzyme A dehalogenase protein (PDB entry 1NZY) [28], the calcium ion is coordinated with oxygen atoms of Gly49, Leu202, Ala203, Ala205, Thr207 and Gln210. On the other hand, some metal ions form coordination bonds with just one atom, i.e., in the chain A of human sex hormone-binding globulin protein (PDB entry 1D2S) [29], the calcium ion interacts only with His136. Total of 2345 coordination bonds are formed among the 1138 protein-metal ion complexes that involve this type of bond. The nitrogen atom in the side chain of His forms 787 bonds with the coordinating metal ions, sulfur atom of Cys forms 434 coordination bonds with metal ions, and oxygen atom (of any AA except Asp/Glu since interaction between metal ion and Asp/Glu is considered to be based on the electrostatic force) forms 1039 coordination bonds. The bonds based on these three atoms correspond to (787+434+1039)/2345 = 96.4% of all coordination bonds. The strong affinity of the oxygen to form coordination bonds with metal ion suggests that the interaction between the negatively charged Asp and Glu residues and metal ions could be a combination of both the coordination and the electrostatic force. The interaction between metal ions and Asp/Glu has been considered as coordination in many other studies. For instance, Angkawidjaja and colleagues reported that Ca2+ is coordinated by the side chains of Asp153, Asp157, and Gln120, and the carbonyl oxygens of Thr118 and Ser144 [30]; similarly, Declercq and coworkers show interaction between Ca2+ and the coordinating oxygen atoms of Asp51, Asp53, Ser55, Phe57, Glu59 and Glu62 [31]. As a result, the interactions between metal ions and Asp/Glu should be regarded as both coordination and electrostatic contacts if the distance between the corresponding atoms satisfies the definition of the coordination bond and the electrostatic contact.\nAlthough the generic principles that govern protein-metal ion interactions were discussed in prior works [9]–[12], e.g., interactions concerning Cys-rich Zn2+-binding sites and affinity of interaction between Mg2+ and Asp/Glu in protein cavities [9], we could not find a systematic study that investigates how many residues and what residues types are involved (“preferred”) in the coordination bonds with specific metal ions, and that provides insights concerning similarities in the geometry of the coordination-based interactions with metal ions, which are discussed below.\nAmong the metal ions, Ag+, Ca2+, Cu2+(Cu+), Cd2+, Co2+(Co+), Fe3+(Fe2+), Hg2+, K+, Mg2+, Mn2+, Na+, Ni2+, Pb2+, Sm2+ and Zn2+ form coordination bonds with atoms of residues, see Table 3. Zn2+ forms coordination bonds in the largest number of pockets. This ion is coordinated by atoms of at most 4 residues in a given pocket and it favors to be coordinated by 3 or 4 residues. The second highest number of pockets that involve coordination bonds with a metal ion concerns Ca2+. These ions are coordinated by atoms of up to six residues in a pocket, and they prefer to form the coordination bonds with 4 or 5 residues. Coordination bonds with Mg2+ and Cd2+ ions involve 228 and 109 pockets, respectively. In contrast to Zn2+ and Ca2+, Mg2+ and Cd2+ ions form most of these bonds with atoms of 1 or 2 residues in a given pocket. Na+ ions form coordination bonds in 150 pockets and it favors to be coordinated by atoms of 3 or fewer residues. These 5 ions form coordination bonds in (426+328+228+109+150) = 1241 pockets, which constitutes 1241/1542 = 80.5% of all relevant pockets. The above results suggest that different metal ions prefer to be coordinated by a different number of residues in a given protein pocket.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 3.  A summary of the coordination bonds between metal ions and a given number of residues in a protein pocket that contribute at least one atom to form the bond.doi:10.1371/journal.pone.0004473.t003The residues which are coordinated by the same metal ion are grouped and we denote such groupings as the residue groups. We count the frequencies of the residue groups among different metal ions. For instance, given that Zn2+ forms coordination bonds with 4 Cys residues in 47 pockets, the corresponding frequency of (Cys)4 residue group is 47. The residue groups that are coordinated by at least 10 metal ions are shown in Figures 4, 5 and 6. The frequencies of residue groups that contain 5 or more residues are below 10 and thus they are not included in the above Figures. Total of 5 residue groups, i.e., (Cys)4, (Cys)3(His), (Cys)2(His)2, (Asp)2(His)2, and (Asp)(His)3, include 4 residues, see Figure 4. We observe that the (Cys)4 group is coordinated by the largest number of metal ions (47 metal ions). There are 11 residue groups that incorporate 3 residues, see Figure 5. These groups include (Cys)3, (Cys)1(His)2, (Asp)3, (Asp)2(Glu), (Asp)2(His), (Asp)(Glu)2, (Asp)(Glu)(His), (Asp)(His)2, (Glu)2(His), (Glu)(His)2 and (His)3. The (Asp)(His)2 and (His)3 groups are coordinated by the largest number of 44 and 38 metal ions, respectively. Finally, 6 residue groups, i.e., (Asp)2, (Asp)(Glu), (Asp)(His), (Glu)2, (Glu)(His) and (His)2, that make contact with 2 residues, see Figure 6.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  The residue groups that are coordinated by at least 10 metal ions and consist of 4 residues.doi:10.1371/journal.pone.0004473.g004\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  The residue groups that are coordinated by at least 10 metal ions and consist of 3 residues.doi:10.1371/journal.pone.0004473.g005\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  The residue groups that are coordinated by at least 10 metal ions and consist of 2 residues.doi:10.1371/journal.pone.0004473.g006Cys and His are among the residues that the most frequently form coordination bonds with the metal ions. We observe that although the geometry of (Cys)4–metal ion and (His)3-metal ion interactions is different, each of these residue groups has similar geometry across the set of the corresponding pockets. The prevalent way to form the coordination bond between Cys and a metal ion involves four Cys residues arranged spatially close to each other to form a pocket; the metal ion is located in the center of this pocket. For example, in the chain A of PHD finger protein 21A (PDB entry 2PUY) [32], the zinc ion forms coordination bonds with Cys503, Cys506, Cys529, and Cys532. The distance between zinc ion and the sulfur atom of the four Cys residues varies between 2.26 Å and 2.41 Å. The four sulfur atoms form an approximate regular tetrahedron and the zinc ion is located in its center, see Figure 7A. The length of the tetrahedron edges varies between 3.63 Å and 3.93 Å. The coordination interaction between His and metal involves three His residues arranged to form a pocket with the metal ion located in approximately the same distance to the nitrogen atoms of these three residues. For example, in the chain A of Zn-dependent hydrolase protein (PDB entry 2R2D) [33], the zinc ion forms coordination bonds with nitrogen atoms of His111, His113, and His191. The distance between the zinc ion and the nitrogen atoms varies between 2.06 Å and 2.18 Å, see Figure 7B. The three nitrogen atoms form an approximate equilateral triangle with the length of the sides that varies between 3.14 Å and 3.31Å.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 7.  Examples of typical coordination bonds between metal ions and Cys and His residues.Coordination bonds are represented by solid lines; the dashed lines show the distance between atoms of different residues. Panel A shows the coordination bond between zinc ion and four Cys residues where sulfur atom is shown in gray, carbon atom in white, and zinc ion in black. The sulfur atoms of four Cys residues form an approximate regular tetrahedron and the zinc ion is located in its center. Panel B shows the coordination bond between zinc ion and three His residues. The nitrogen atoms are shown in gray, other atoms of the His side chain are in white, and zinc ion is colored black. The three nitrogen atoms form an approximate equilateral triangle with the length of the sides that varies between 3.14 Å and 3.31 Å. The zinc ion is not located on the triangle plane.\ndoi:10.1371/journal.pone.0004473.g007Metal ion-residue coordination plays a crucial role in stabilizing the protein structure and is involved in a number of catalytic activities [34]–[37]. Traoré's study reveals that the Zn(Cys)4 site locks the dimerization domain and stabilizes the dimer of protein PerR [34]. Ochiai and colleagues show that a calcium ion coordinated by Asp401, Glu422, His363, and His399 is required for the enzyme activity of rhamnogalacturonan lyase YesW [38]. Sankaranarayanan and colleagues point out that a zinc ion is directly involved in threonine recognition, forming a pentacoordinate intermediate with both the amino group and the side chain hydroxyl and mediated AA discrimination by threonyl-tRNA synthetase [39]. Covarrubias and coworkers demonstrate that depletion of a zinc ion, which is coordinated with aspartic acid side chain, leads to the lack of activity of Rv1284 gene [40]. The above example studies demonstrate the important role of metal ions in assisting protein folding and in catalysis of chemical reactions.\n\n\nInteraction patterns in protein-inorganic anion complexes\nInorganic anions bind to proteins mainly through electrostatic force, hydrogen bonds and van der Waals contacts. Among the 1837 anions, 1188 interact with the positively charged AAs such as Arg, His and Lys based on electrostatic interaction and 641 bind to the pocket by the means of hydrogen bonds and van der Waals contacts.\nSimilarly as in the case of metal ions, we studied which atoms of the positively charged residues are the closest to the inorganic anions. Among the 1188 protein-anion complexes that involve electrostatic force, 202 anions bind to His, 327 to Lys, and 659 to Arg. Nitrogen atom in the side chain of these three residues is the closest atom to the anion for 172 anion-His interactions, 222 anion-Lys interactions, and 565 anion-Arg interactions. These numbers suggest that the nitrogen atoms of positively charged residues may be closer to the center of the charge than other non-hydrogen atoms.\nAmong the anions that occur in PDB more than 100 times, 743 SO42− (743/948 = 78.5%) and 109 PO43− (109/148 = 73.6%) bind to positively charged residues, while some other anions less frequently bind with the charged residues. More specifically, 165 Cl− (165/345 = 47.8%), 33 Br− (33/126 = 26.2%), and 22 I− (22/108 = 20.4%) bind to positively charged residue. This could be explained based on the formation of hydrogen bonds between the oxygen atoms of SO42− and PO43− and the NH- group of positively charge residues. For instance, PO43− forms 254 hydrogen bonds with positively charge residues (254/109 = 2.3 hydrogen bonds per pocket) and SO42− forms 1394 hydrogen bonds with positively charge residues (1394/743 = 1.9 hydrogen bonds per pocket). The combination of electrostatic force and hydrogen bonds stabilizes the anion-positively charged residue interaction.\nSimilarly to the protein-organic compound complexes, the most frequent hydrogen bond incorporates the NH- group of a residue that serves as the donor and the oxygen atom of a ligand. This pattern concerns 2777 hydrogen bonds which converts into 2777/3190 = 87.1% of all hydrogen bonds between a protein pocket and an inorganic anion.\n\n\nInteraction patterns in protein-inorganic cluster complexes\nAmidst the nine types of inorganic cluster that could be found in PDB, FS4, FES, SF4, F3S, CLF, and FS3 are Fe-S clusters and contain only iron and sulfur atoms. The remaining three clusters, which include CFN, FSO, and NFS, also mainly contain iron and sulfur atoms.\nWe observe that coordination bonds are involved in all 54 protein-inorganic cluster complexes. These bonds are usually formed between the iron atom of the cluster and the sulfur atom of Cys residue, and the iron atom of the cluster and the nitrogen atom in the side chain of His residue. These two coordination bond patterns cover 201/204 = 98.5% of all coordination bonds between inorganic cluster and a protein pocket. Although FS4, SF4, F3S, and FS3 are positively charged and FSO is negatively charged, these clusters do not interact with charged residues. We did not find the electrostatic force based interactions between the inorganic clusters and proteins.\n\n\nComparison between protein-protein interaction interfaces and protein-organic compound binding pockets\nSeveral statistical studies have investigated the AA frequencies and the pairing preference for the protein-protein interaction interface. Ben-Tal's study indicates the hydrophobic residues are abundant in large interfaces while polar residues are more abundant in small interfaces [41]. They also conclude that contacts between pairs of hydrophobic and polar residues are unfavorable and that the charged residues tend to pair subject to charge complementarity. This conclusion was confirmed in a more recent study by Helms and colleagues [42].\nSince proteins and small organic compounds share similar chemical composition, we examined similarities and difference between AA composition of protein-organic compound binding pockets and protein-protein interaction interfaces, see Figure 8. The AA composition of protein-protein interaction interfaces (gray bars) is taken from [42], where it was calculated based on a non-redundant set of 170 protein-protein complexes. We note that except for Cys, which occurs twice as often in the protein-protein interaction interfaces when compared with the protein-organic compound binding pocket (3.1% vs. 1.5%), other AAs occur with similar frequency for both types of interactions. This similarity indicates that protein-ligand and protein-protein interactions could be determined by same types of forces and thus current protein-ligand docking techniques could be potentially adopted for protein-protein docking.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 8.  Percentage of occurrence of amino acids in the protein-organic compound binding pockets (gray bar) and in protein-protein interaction interfaces (black bar).doi:10.1371/journal.pone.0004473.g008\n\nOverlap and coverage of the interaction patterns\nThe 11 patterns that concern covalent bonds, coordination bonds and hydrogen bonds, see the bottom layer in Figure 1, appear in 2013 protein-organic compound complexes, 1138 protein-metal ion complexes, 1115 protein-anion complexes, and 53 protein-inorganic cluster complexes, which corresponds to (2013+1138+1115+53)/7759 = 55.7% of all protein-ligand complexes. Significant majority of the above complexes incorporates just one of the discussed patterns. More specifically, except for 81 protein-organic compound complexes and 546 protein-metal ion complexes that incorporate two or more interaction patterns, the remaining 4238 protein-ligand complexes include one interaction pattern.\n\n\nPrediction of binding sites of organic compounds based on interaction patterns\nWe show the utility of the discussed patterns in the context of the blind (without the knowledge of the ligand) prediction of binding sites. Since organic compounds are the largest group among the four considered types of ligands and since majority of the oral drugs are based on the organic compounds, we design and test a naïve method to predict the binding sites for the organic compounds that utilizes the knowledge of the four corresponding interaction patterns shown in Figure 1. The predictions are made using a dataset that consists of 901 proteins that was introduced in [43], in which the pairwise sequence identity is below 35%. Over 90% of these chains interact with only one organic compound, and the remaining chains interact with 2 or 3 compounds. Other types of ligands, e.g., metal ions, may bind to some of the chains, however, the binding sites of these ligands are not considered.\nThe prediction procedure follows a sequence of three steps:\n\n\n\n\nCalculate a grid encompassing the protein structure using the LigsiteCSC program [44]. The grid points are divided into those that are inside the protein structure, on the surface, and in the solvent [44]. We only use the solvent grid points.\n\nScan the solvent grid points that are within 5Å from the protein surface and count the interaction patterns that are within R distance from a given grid point. Only the atoms on the protein surface are considered. The interaction patterns for organic compounds include hydrogen bond (formed between NH- group of residue and oxygen atom of ligand) and covalent bonds. The actual counts of the hydrogen bonds and covalent bonds cannot be computed since this is a blind prediction. Instead, we count NH- group within the R radius to reflect potential hydrogen bonds. For the covalent bonds, we count sulfur atoms to reflect potential thioether and disulfide bonds.\n\nSort the grid points in the descending order based on the computed counts. The first prediction corresponds to the top scoring grid point. The subsequent predictions correspond to the points with the largest scores which are at least 5Å away from any accepted prediction.\n\nThe predictions are evaluated based on the distance between the predicted site and the actual position of the ligand, i.e., a prediction is assumed correct if the distance is smaller than a cut-off threshold value, which is varied between 1 and 10Å. For a given protein structure, 5 potential binding sites are predicted, which follows the procedure performed in relevant recent studies [44]. More specifically, if any of the 5 predicted binding sites is less than a certain distance (D) from any atom of the ligand, the prediction for this protein is assumed correct. This is motivated by the fact that the input dataset provides incomplete information, i.e., some actual binding sites could be missing which implies that some predictions that are far from the ligands included in the dataset could be potentially correct. The success rate is defined as the number of the correctly predicted proteins divided by size of the dataset.\nThe pattern-based method is compared with LigsiteCSC that identifies pockets on the protein surface based on a geometrical analysis [44], and which extends Ligsite method [45]. LigsiteCSC is shown to perform comparably well or better when compared with several other binding site predictors such as Ligsite, CAST, PASS, and SURFNET [44]. We also implemented a baseline predictor by random selection of five solvent grid points that are less than 5Å from the protein surface. Two versions of our scanning-based approach are considered, one that uses both hydrogen and covalent bonds patterns, and the other that uses only the hydrogen bond pattern. The radius R is set to 8Å since for this value 1%–5% higher success rate is achieved when compared with other values between 5 and 10Å. Figure 9 compares the predictions. The naïve method based on the interaction patterns is inferior to Ligsite CSC, i.e., our success rate is about 10% lower than that of Ligsite CSC for D = 2 to D = 10. This is not surprising since the four interaction patterns do not cover all protein-organic compound interactions as discussed above and since 29% of the organic compounds bind to protein only through van der Waals contacts. We observe that both methods are superior to the random predictions. We also observe that adding the patterns concerning covalent bonds improves the success rates by 1 to 2% across different D values, which shows that combining multiple patterns is helpful. This improvement is due to the inclusion of thiol of Cys, which potentially forms thioether and disulfide bonds.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 9.  Performance of blind binding site predictors including the pattern-based method, LigsiteCSC, and a random baseline predictor.The y axis shows success rate, i.e., fraction of proteins with minimum distance between the top five predicted binding sites and any atom of a ligand in the native complex that is smaller or equal to the distance displayed on the x axis. The five plots concern the scanning method based solely on the hydrogen bond pattern (named “Scanning (hydrogen)”), the scanning method based on the four patterns concerning both hydrogen and covalent bonds (named “Scanning (hydrogen&covalent)”), the result of LigsiteCSC, the result of baseline method that randomly picks 5 solvent grid points that are within 5Å from the protein surface (named “Random baseline”), and the results that merge the top two predictions of LigsiteCSC and the top three predictions of the scanning method that uses the four patterns (named “Scanning/Ligsite-csc hybrid”).\ndoi:10.1371/journal.pone.0004473.g009We observe that the prediction from the pattern-based method are complementary to the prediction from Ligsite CSC, i.e., for some proteins the binding sites predicted by Ligsite CSC are relatively far away from the actual binding sites while our method provides correct predictions. For example, for the Anguilla anguilla agglutinin protein (PDB entry 1K12), the 5 prediction generated by Ligsite CSC are at least 11 Å away from the ligand, while one of our predictions is only 0.67Å from the compound, see Figure 10. This motivated a hybrid approach in which predictions from the two methods are combined by taking the top two predictions from LigsiteCSC and the top three pattern-based predictions (on average the third best pattern-based prediction is better than third best prediction from LigsiteCSC). Figure 9 shows that the results based on the merged predictions are better than the results from individual methods, especially for low values of D. For instance, in the case of D = 1, both Ligsite CSC and pattern-based methods predict the binding sites that are within 1Å from the ligand for about 35% of the proteins, while the merged predictions are successful at 46% level. This result indicates that interaction patterns could be utilized to improve existing blind geometrical predictions of binding sites.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 10.  The structure of Anguilla anguilla agglutinin protein (PDB entry 1K12).The binding sites predicted by Ligsite CSC are colored in green and the binding sites predicted by the pattern-based method are colored in blue. The protein surface is rendered in gray and the ligand is in the stick form. The LigsiteCSC predictions are over 10Å away from any atom of the ligand, while one of pattern-based predictions is 0.67Å away from one of the ligand's atoms. Only 4 predictions by Ligsite CSC and by the pattern-based method are visible; the remaining predictions are on the other side of the protein.\ndoi:10.1371/journal.pone.0004473.g010\nOne of challenges in contemporary protein research is the discovery of generic rules and interaction patterns from the growing body of structurally characterized protein–ligand complexes. This study presents and investigates several frequently occurring interaction patterns for atomic-level protein-ligand interactions. The considered protein pocket-ligand complexes were grouped into four categories: protein-organic compound, protein-metal ion, protein-anion, and protein-inorganic cluster complexes. These groups cover 93.5% of all protein-ligand complexes from PDB and we show that they are governed by different types of interaction forces. The protein-organic compound complexes are governed by the hydrogen bonds, van der Waals contacts and covalent bond. The protein-metal ion complexes are based on the electrostatic force and coordination bonds while the protein-anion complexes are governed by the electrostatic force, hydrogen bonds and van der Waals contacts. Finally, the protein-inorganic cluster complexes are established mostly due to the coordination bonds.We present several frequently occurring interaction patterns, defined in terms of prevalent interactions between specific atoms of specific residue in the protein's pocket and specific atoms of the ligand, for the abovementioned four groups and for the specific types of interaction forces. We quantify relative abundance of specific interaction types and discuss their characteristic features such as commonly interacting amino acid types. Total of 10 interaction patterns that occur in 56% of all considered complexes were found. For example, we show that 66.9% of the protein-organic compound complexes involve hydrogen bonds and that 65.8% of these hydrogen bonds are formed between the NH- group of the protein's residue and the oxygen atom of the organic compound. As a result, we believe that the geometric and electrostatic complementary, which are used for molecular recognition, should be supplemented by implementation of hydrogen bond(s) in the case of the protein-organic compound complexes. As another example, only four interaction patterns are sufficient to summarize significant majority, i.e., 73%, of normal covalent bond interactions between proteins and ligands; they include the covalent bond between the thiol of Cys residue and the carbon atom of the ligand (thioether bond), the thiol of Cys residue and the sulfur atom of the ligand (disulfide bond), and the nitrogen atom of Lys residue and the carbon atom of the ligand. We also show that the AAs serve as donors for significant majority of these hydrogen bonds. We observe that most of the inorganic anions interact with positively charged AAs including Arg, His, and Lys.We show that the organic compounds form hydrogen bonds more frequently with hydrophilic AAs when compared with hydrophobic AAs, which is consistent with results obtained for protein-DNA interactions [5]. This suggests that the ability of AAs to establish hydrogen bonds could be an intrinsic characteristic of a given AA, which is independent of the ligand type. We also found that protein-organic compound binding pockets and protein-protein interaction interfaces share similar AA composition, which may imply that these interactions are determined by the same types of forces.We also demonstrate a practical application of the abovementioned patterns in the context of blind prediction of binding sites for organic compounds. Our analysis reveals that a scanning method based on simple counts of the occurrence of the patterns provides predictions that complement existing methods that are based on the geometrical analysis of the protein surface.To conclude, we show that for a given type (group) of ligands and a given type of the interaction force, majority of protein-ligand interactions are repetitive and could be summarized with several simple atomic-level patterns. These interaction patterns not only provide a comprehensive overview of protein-ligand interactions, but they also may have profound implications for development of molecular docking procedures and in building of binding site prediction methods."
        },
        "10.1371/journal.pone.0104835": {
            "author_display": [
                "Mengzhe Guo",
                "Youlu Pan",
                "Rong Zhang",
                "Yang Cao",
                "Jianzhong Chen",
                "Yuanjiang Pan"
            ],
            "title_display": "The Specific Cleavage of Lactone Linkage to Open-Loop in Cyclic Lipopeptide during Negative ESI Tandem Mass Spectrometry: The Hydrogen Bond Interaction Effect of 4-Ethyl Guaiacol",
            "abstract": [
                "\nMass spectrometry is a valuable tool for the analysis and identification of chemical compounds, particularly proteins and peptides. Lichenysins G, the major cyclic lipopeptide of lichenysin, and the non-covalent complex of lichenysins G and 4-ethylguaiacol were investigated with negative ion ESI tandem mass spectrometry. The different fragmentation mechanisms for these compounds were investigated. Our study shows the 4-ethylguaiacol hydrogen bond with the carbonyl oxygen of the ester group in the loop of lichenysins G. With the help of this hydrogen bond interaction, the ring structure preferentially opens in lactone linkage rather than O-C bond of the ester-group to produce alcohol and ketene. Isothermal titration 1H-NMR analysis verified the hydrogen bond and determined the proportion of subject and ligand in the non-covalent complex to be 1∶1. Theoretical calculations also suggest that the addition of the ligand can affect the energy of the transition structures (TS) during loop opening.\n"
            ],
            "publication_date": "2014-08-21T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 234,
            "shares": 0,
            "bookmarks": 0,
            "url": "http://www.plosone.org/article/info:doi/10.1371/journal.pone.0104835",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0104835&representation=PDF",
            "fulltext": "IntroductionRapid developments and technological advances in the field of mass spectrometry have made electrospray ionization (ESI)-tandem mass spectrometry (MS/MS) a powerful tool for the analysis of peptides and proteins [1]–[2] with the capability to routinely identify sequences of tens of thousands of proteins [3]–[4]. Most analyses were performed using positive ion mode because the ionization of peptides and proteins in negative ion mode is relatively limited [5]. However experiments in negative ionization mode can also reveal important information in the investigation of peptide and protein structure and function [6]. Hydrophobic amide groups are typically positioned at the C-termini of peptides and are thought to be the key binding sites for biological activity. Negative ion mode can be used to more easily distinguish these amide groups as they are easily deprotonated [8]. Several works have been reported that utilized negative ion mode to study drugs, peptides, and proteins [7]–[8]. Cassady observed different fragmentation patterns in negative ion mode between nearly identical peptides which can provide insight into the structure of the peptides and improved identification of unknown peptides [9]. Moore utilized negative ion mode to investigate the fragmentation chemistry of anionic, hydrogen-deficient and radical peptides [10].\nMicroorganisms are known to produce peptides, especially cyclic peptides, with diverse biological activities that are of great interest to researchers in many fields. These peptides are produced from endophytic organisms, bacteria, and fungi [11]–[12]. The cyclic peptides of interest are formed via reactions with an ester group or a disulfide bond [13]–[14]. Studying the cleavage of cyclic peptides can help to elucidate their structure and function in microorganisms.\nSurfactin is a bacterial cyclic lipopeptide produced by the Gram-positive, endospore-forming bacteria, Bacillus subtilis. It contains a heptapeptide with a hydrolysable ester linkage and a variable aliphatic chain of 13–15 carbon atoms. Because of its amphiphilic properties, which allow it to be stable in both hydrophilic and hydrophobic environments, surfactin is commonly used as an antibiotic. Previous studies reported that surfatin exhibits effective anti-tumour, anti-inflammatory and immunosuppressive activity [15]–[17]. The lichenysin and four homolog compounds produced by Bacillus licheniformis are cyclic lipopeptides in the surfactin family. The primary homolog, lichenysins G (MW 1035 Da), contains the previously mentioned heptapeptide and an aliphatic chain of 15 carbon atoms, which is shown in Figure 1a.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  (a) lichensins G 1035 Da, (b) 4-ethyl guaiacol.doi:10.1371/journal.pone.0104835.g001Mass spectrometry is a powerful technique for the study of cyclic lipopeptide structure and function. Pathak used HPLC coupled to ESI-MS to identify several new fengcin variants and also found four major sites of heterogeneity between them [18]. Samgina utilized collision-induced dissociation (CID) to elucidate the sequences of natural, non-tryptic peptides with C-terminal disulfide cycle [19]. Using mass spectrometry to study non-covalent complexes of peptides and ligands can reveal peptide cleavage sites. O'Hair used mass spectrometry to investigate the gas phase chemistry of proton bound oligosaccharide (S) –ligand (L) non-covalent complexes and found that a ladder series is produced by ligand induced oligosaccharide bond cleavage [20].\nIn this work, lichenysins G, the predominant lichenysin homolog, was selected as the model to investigate the fragmentation mechanism using negative ion tandem mass spectrometry. 4-ethylguaiacol, an important pharmaceutical intermediate, was added as the ligand and the structure is shown in Figure 1b. By comparing the different fragmentation of the lichenysins G and the non-covalent complex of lichenysins G and 4-ethylguaiacol, the detail of the interaction between this two compounds and the effects on fragmentation of lichenysins G induced by the ligand are expounded. The existing work [21]–[23] helping to propose the fragmentation mechanism and theoretical calculations were used to support the proposed mechanism.\nExperimental\nMaterials\nThe four lichenysin targets (purity >90% by MS analysis) were provided by the School of Biotechnology, Jiangnan University. 4-ethylguaiacol (purity >95% by LC-MS analysis) was purchased from Sigma-Aldrich.\n\n\nMass Spectrometry\nAll CID experiments were performed on a Bruker AmaZon ETD mass spectrometer (Bruker-Franzen Analytik GmbH, Bremen, Germany) equipped with a nanospray ionization source and an ion trap mass analyzer using negative ion mode. Nitrogen was used as nebulizing gas at a pressure of 10 psi and drying gas at a flow rate of 5 L·min−1. The drying gas temperature was 250°C and the capillary voltage was 4000 V. Samples were dissolved in acetonitrile/1% salicylic acid solution 99.9/0.1 (v:v) to form a 1×10−8 mol·L−1 and infused to the mass spectrometer with a syringe pump at a flow rate of 3 uL·min−1. The CID mass spectra were obtained with helium as the collision gas at suitable collision energy after isolation of the desired precursor ion. The mass window for precursor ion selection was between 0.8 and 1.0 m/z to ensure 13C isotopic ions were excluded. Data were acquired using the software Esquire 5.0 (Bruker).\n\n\nNMR Analysis\n1H NMR and 2D NOESY experiments were carried out using a Bruker AMX 500 MHz instrument at 298 K with samples in d-DMSO. Typical parameters for 1H NMR experiments consisted of spectral width of 10 ppm, number of scans at 16 and relaxation delay at 1 s. 50–60° pulses with overall delay of 3 s between pulses were used. The time-domain data were exponentially multiplied before Fourier transformation with 0.0–0.5 Hz line broadening functions depending on the resulting S/N ratio of the spectrum. Tetramethylsilane (TMS) was used as internal reference for all spectra (0 ppm). For NOESY spectrum, the data was acquired with a mixing time of 60 ms and a relaxation delay at 2 s [24].\n\n\nTheoretical Calculations\nThe software program Amber was used to perform the conformational search of the non-covalent complex ion. Using molecular dynamics simulation of 100 ns, we divided the series of conformers into twenty groups and obtained twenty representative conformations. The global minimum energy of conformation was chosen by comparing all molecular dynamics simulation energy.\nPotential energy surfaces (PES) were also used to take into consideration the deprotonated lichenysins G and non-covalent complex. The candidate structures of the reactants, products, intermediates and transition states were optimized by calculating the force constants. Calculations were performed to modify the initial structures with a deprotonated carboxyl group and compare the 1, 3 hydrogen transition structure (TS) energy from either the alpha carbon or aliphatic carbon to the oxygen in the ester group. All theoretical calculations were carried out by using the ONIOM method at the B3LYP/6−311++G (d,p) level of theory and PM6 theory in the Gaussian 03 program. No symmetry constrains were imposed on the optimizations. All optimized structures were subjected to vibrational frequency analysis for zero-point energy (ZPE) correction. The sum of electronic and thermal energies of the optimized structures was discussed.\n\nResults and Discussion\nFragmentation of Lichenysins G and Non-covalent Complex\nThe parent ion shown in Figure 2(a) is the non-covalent 1:1 lichenysins G: 4-ethylguaiacol complex, [N-H]− (m/z 1186), and four characteristic fragments for the complex were observed: m/z 1016, 807, 710 and 692. And the Figure 2(b) exhibits the fragmentation of lichenysins G in negative ion mode. The fragmentation of the lichenysins G anion, [M-H]− (m/z 1034), resulted in three characteristic framents: m/z 1016, 794 and 692. In Table 1 which summarizes the primary fragments of M and N we can find the two targets have identical m/z values: m/z 1016 and m/z 692. Performing MS3 of m/z 1016 and m/z 692 can help determining whether these fragments with same molecular weight have the same molecular stucture. The results of MS3 are shown in Figure 3, Figure S1 in File S1 and Table 1. Figure 3(a) shows the MS3 fragmentation of m/z 1016 fragment from the non-covalent complex and Figure 3(b) shows the MS3 fragmentation of m/z 1016 fragment from lichenysins G, respectively. The primary MS3 fragments of 1016 for M are m/z 794 and 692, whereas the primary MS3 fragments of 1016 for N are m/z 807, 710 and 692. The different characteristic MS3 fragmentation patterns observed for M and N suggest that different molecular structures comprise the m/z 1016 MS2 fragment for each analyte. One the other hand, the MS3 fragmentaiton of m/z 692 fragments from N (Figure S1(a) in File S1) and M (Figure S1(b) in File S1) indicate that they have the same molecular structure because of their same fragmentations.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  The fragmentation of non-covalent complex (a) and lichenysins G (b).doi:10.1371/journal.pone.0104835.g002\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  The MS3 fragmentation of 1016 Th ions of non-covalent complex (a) and lichenysins G (b).doi:10.1371/journal.pone.0104835.g003\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Main ions of every fragmentation of Cyclic Lipopetide and Non-covalent complex.doi:10.1371/journal.pone.0104835.t001\n\nFragmentation Mechanisms of Lichenysins G and Non-covalent Complex\nFurther analysis of the MS3 fragmentation of these two m/z 1016 fragment ions from lichenysins G and non-covalent complex, can help elucidate the fragmentation mechanisms for each analyte. The specific cyclic structure of lichenysins G should be taken into consideration; therefore, the open loop of lichenysins G is hypothesized as the initial process of fragmentation.\nFigure 4 shows the pathways rationalizing the fragmentation of M in negative ion mode. At the beginning, the carboxyl of the Asp side chain loses a hydrogen forming the [M-H]− ion. Next the hydrogen from the methylene in the middle of aliphatic chain and carbonyl (colored in red), transfers to the oxygen of ester group via 1,3 hydrogen migration. Thereafter, the carbon (from aliphatic chain) - oxygen (from ester group) bond is broken, sequentially opening the loop and forming a carboxyl and ethylene at both ends respectively [24]–[28]. Finally, the compound, which loses water after ring opening, forms the [M-H-H2O]− (m/z 1016). The loss of water may have two possible pathways. One pathway could be that the tertiary amine of Leu attacks the carbon of the adjacent Asp carboxyl group to form a five-member ring. The hydrogen of the carboxyl group formed by the ring opening can transfer to the hydroxyl formed by the five-member ring as mobile proton, resulting in the water loss [29]–[33]. The second potential pathway is that the carboxyl formed by the ring opening can directly lose water and form the ketene. The first pathway is more probable and supports in the following pages.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  The fragmentation mechanisms of lichenysins G.doi:10.1371/journal.pone.0104835.g004Both MS2 of [M-H]− and MS3 of [M-H-H2O]− generate the fragment m/z 794, shown in Figure 2 and 3. The fragment ion m/z 794 is formed from the open ring structure of [M-H-H2O]−. The ring opening results in linear precursor ions, which undergo subsequent cleavages as shown in figure 4, and then creates a y7 ion which generated the m/z 794 by the neutral loss of an unsaturated ketene group. In addition, this fragmentation may continue and produce the y6 (m/z 666) and y5 (m/z 552) fragmentation ions by losing Gln and Leu respectively. The three fragments listed above can demonstrate that the ring is indeed open in the location of O-C bond between the ester group and aliphatic chain.\nFigure 4 also illustrates the mechanism pathway that rationalizes the neutral loss of m/z 342 from lichenysins G to form the m/z 692 ion. This process proceeds by the N-C bond dissociating from the amine and alpha carbon of the Asp side chain and produces the neutral loss of a tripeptide containing Asp, Leu and Ile. Moreover, the cleavage of the N-C bond rationalizes the mechanism of water loss described above, because the stable five-member ring facilitates the N-C bond dissociation.\nCompared to figure 4 there is prominent difference in the fragmentation mechanisms of [N-H]− in the way of open loop. It is suggests that the ligand has no direct participation in the fragmention of N from Figure 2 and 3 by means of determining all the fragmentations belong to the subject M. Figure 5 illustrates the pathway for formation of fragmentation ion m/z 1016. First the non-covalent complex may lose the neutral micromolecule ligand and next the hydrogen from alpha carbon of Ile transfers to the oxygen of ester group via 1,3 hydrogen migration. A cleavage occurs inside the lactone linkage forming a ketene and an alcohol. Finally the loop opening results in a loss of water and the formation of [N-H-ligand-H2O]− (m/z 1016). The water loss also proceeds by the nucleophilic attack of the Asp carboxyl carbon on the amidogen from the adjacent Leu, forming the five-member ring. The hydrogen of the alcohol formed during ring opening transfers to the hydroxyl formed by the five member ring as mobile proton resulting in the water loss.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  The fragmentation mechanisms of non-covalent complex.doi:10.1371/journal.pone.0104835.g005The ions with m/z 807 and 710 are the fragmentation products of linear precursor ions formed by ring opening. They also have a relationship with the formation of the five-member ring in Asp. When the N-C bond from the amine and alpha carbon of Leu is cleaved, the neutral dipeptide, containing Leu and Ile, may be lost forming the n5 ion with m/z 807. If the five-member ring in the Asp side chain is lost as well, the remaining product is the n4 ion with m/z 710. The n4 ion can easily lose water from the deprotonated alcohol in the terminal of aliphatic chain and form the ion with m/z 692 which is a fragment of [M-H]− shown above. The fragmentation mechanism for forming n5 is also supported by the MS3 results of the n5, which is shown in Figure 6. In this fragmentation the cleavage of the C-C bond in the linear aliphatic chain to lose the aliphatic ketone produce the the ion [y7-y2] (m/z 608) and continue losing the five-member ring of the Asp side chain to produce the ion [y7-y3+NH2] (m/z 511). These two ions are also found in the MS3 results of [N-H-ligand-H2O]− and support the structure of the n5 ion. In addition, the structure of n5 also can show the fragmentation mechanism of [N-H]− and the location of the loop opening.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  The MS3 analysis and fragmentation mechanisms of n5 (m/z 807).doi:10.1371/journal.pone.0104835.g006Through the comparative analysis of the different fragmentation mechanisms of lichenysins G and the non-covalent complex, we infer that the ring opening location of lichenysins G can exchange from oxygen and carbon in the aliphatic bond to the carbonyl and oxygen bond of the ester group caused by the effect of the micromolecule ligand. The linear product ion transfers from the carboxyl and ethylene to the ketene and alcohol. In addition, the effective interaction of the micromolecule ligand may be the hydrogen bond between the phenolic hydroxyl group in the ligand and the oxygen of ester group carbonyl in the lichenysins G.\n\n\n1H-NMR Analysis\nTo verify the ESI-MS result of the non-covalent complex, the types of interactions between lichenysins G and 4-ethylguaiacol were determined by 1H-NMR. The 1H-NMR spectroscopy of lichenysins G was in accordance with the literature [34]. Spectroscopic titration suggested that lichenysins G and 4-ethylguaiacol could form stable 1:1 complex. The αH on alpha-carbon of Ile (δ = 4.12 ppm) had a remarkable shift and change in peak shape. Additional evidence of such interaction was provided by 2D 1H-1H NOESY NMR spectroscopy. The NOE correlations (αH, Hi) between the H on alpha-carbon of Ile and the hydroxyl of 4-ethylguaiacol confirm the interaction (Figure 7, Figure S2 in File S1).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 7.  1H NMR spectra (400 MHz) of non-covalent complex lichenysins G:4-Ethyl Guaiacol in DMSO at 25°C; b) 4:1; c) 2:1; d) 1:1; e) 1:2.doi:10.1371/journal.pone.0104835.g007\n\nTheoretical Calculations\nThe crystal structure of surfactin was used as the reference when performing the theoretical calculations of the lichenysins G, because the amino acid sequence and cis-trans structure of lichenysins G are similar to surfactin. Molecular dynamics simulations of the hydrogen bond interaction between lichenysins G and the ligand provided a steady dynamic conformation (Figure 8).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 8.  The dynamic conformation of lichenysins G and ligand.doi:10.1371/journal.pone.0104835.g008The mechanisms were further explored via theoretical calculations at an ONIOM level of theory. Figure 9 shows that the ligand and the ester group where hydrogen transfer and loop opening occur were approached as the nucleus and used RB3LYP/6−311++G(d,p) level of theory. The rest of the peptide was treated as a periphery region and treated with PM6 level of theory. When the ligand did not participate, the individual lichenysins G was optimized as the potential energy zero. The transition state energy of 1,3 hydrogen migration of the methylene hydrogen in the middle of aliphatic chain carbonyl to the oxygen of ester group (TS1-1) is 56.09 kcal/mol, 25.53 kcal/mol less than the transition state energy of 1,3 hydrogen migration of the alpha-carbon hydrogen of Ile to the oxygen of the ester group (TS1-2, 81.62 kcal/mol) (Table S1 in File S1). This result confirms the fragmentation mechanism of ring opening without the ligand. However, with the hydrogen bond interaction of the ligand, the transition state energy of 1,3 hydrogen migration of the alpha-carbon hydrogen of Ile to the oxygen of the ester group (TS2-2) prominently decreases to 76.76 kcal/mol but the transition state energy of 1,3 hydrogen migration of the methylene hydrogen in the middle of aliphatic chain carbonyl to the oxygen of ester group (TS2-1) was not founded. We observed that the conformation of TS2-1 changed back to the reactant structure when calculating the transfer structure. This could potentially be because the hydrogen bond interaction stabilizes the C-O bond of TS2-1 and makes the C-O bond of TS2-2 easier to cleave. Therefore this result supports the open-loop mechanism of the non-covalent complex.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 9.  The mixed level of theory in which the nucleus calculated at B3LYP/6−31+G(d) is shown as ball & bond type and periphery region calculated at PM6 is shown as wireframe.doi:10.1371/journal.pone.0104835.g009\nIn this work we found a difference in fragmentation mechanisms of lichenysins G when the ligand, which contains phenol hydroxyl group, was present compared to the direct dissociation of lichenysins G alone using ESI tandem mass spectrometry. The hydrogen bond between phenol hydroxyl of the ligand and the carbonyl oxygen of the ester group in the cyclic lipopeptide ring, which was confirmed by NMR, can promote cleavage of the C-O bond of the ester group, resulting in loop opening and ketene and alcohol generation. However with the absence of the hydrogen bond effect, the location of the loop opening of cyclic lipopeptide changes to the O-C bond beside the ester group, producing ethane and a carboxyl as reported in the literature. Molecular dynamics simulation supplies further evidence that there is a stabilized hydrogen bond and the potential energy surface calculations for the hydrogen translation structures energy also support the fragmentation mechanism of the non-covalent complex. The results of this work can provide the theoretical support for the research of metabolites with diverse biological activities produced from endophytic organisms.The online version of this article contains supplementary material, which is available to authorized users."
        },
        "10.1371/journal.pone.0022782": {
            "author_display": [
                "Jeffrey M. Dick",
                "Everett L. Shock"
            ],
            "title_display": "Calculation of the Relative Chemical Stabilities of Proteins as a Function of Temperature and Redox Chemistry in a Hot Spring",
            "abstract": [
                "\n        Uncovering the chemical and physical links between natural environments and microbial communities is becoming increasingly amenable owing to geochemical observations and metagenomic sequencing. At the hot spring known as Bison Pool in Yellowstone National Park, the cooling of the water in the outflow channel is associated with an increase in oxidation potential estimated from multiple field-based measurements. Representative groups of proteins whose sequences were derived from metagenomic data also exhibit an increase in average oxidation state of carbon in the protein molecules with distance from the hot-spring source. The energetic requirements of reactions to form selected proteins used in the model were computed using amino-acid group additivity for the standard molal thermodynamic properties of the proteins, and the relative chemical stabilities of the proteins were investigated by varying temperature, pH and oxidation state, expressed as activity of dissolved hydrogen. The relative stabilities of the proteins were found to track the locations of the sampling sites when the calculations included a function for hydrogen activity that increases with temperature and is higher, or more reducing, than values consistent with measurements of dissolved oxygen, sulfide and oxidation-reduction potential in the field. These findings imply that spatial patterns in the amino acid compositions of proteins can be linked, through energetics of overall chemical reactions representing the formation of the proteins, to the environmental conditions at this hot spring, even if microbial cells maintain considerably different internal conditions. Further applications of the thermodynamic calculations are possible for other natural microbial ecosystems.\n      "
            ],
            "publication_date": "2011-08-11T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 6,
            "views": 2288,
            "shares": 0,
            "bookmarks": 21,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0022782",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0022782&representation=PDF",
            "fulltext": "IntroductionThe imprints of distinct geochemical environments can be found in the molecular compositions of microbial genomes and their protein products. For example, transmembrane proteins of ancestral organisms were likely to be depleted in oxygen, paralleling the low oxygen content of Earth's atmosphere in the past [1]. Environmental imprints on proteins can also be found for spatially separated organisms living contemporaneously; the amino acid composition of proteins differ systematically between organisms living at different temperatures [2], [3]. Together with temperature, the chemical properties of the environment are linked to the compositions of gene sequences in hot-spring microbial communities [4], [5].\nAlthough the sequences of proteins must satisfy a complex array of biological requirements, the different biosynthetic costs of amino acids are viewed as one contributing factor to actual patterns of amino acid usage [6], [7]. In some studies, the biosynthetic costs of amino acids have been estimated from metabolic constraints including numbers of phosphate bonds and hydrogen atoms transferred during synthesis from precursors [6], [8]. Those estimates depend on the growth medium and specific metabolic pathways but otherwise do not involve environmental variables such as temperature and oxidation-reduction conditions. Nevertheless, it can be shown that the Gibbs energy change in overall chemical reactions to synthesize amino acids from from inorganic species depends on environmental conditions [9]. The calculations of energetics of overall synthesis reactions can now be done for proteins, where group additivity methods permit assessing standard Gibbs energies of proteins of any amino acid composition [10], [11].\nThe goal of this study is to use thermodynamic tools to characterize simultaneously the chemical environment and metagenomically derived protein sequences in a hot spring exhibiting large gradients of temperature and oxidation-reduction, or redox, chemistry. The geochemical and biomolecular data are combined using a single model framework based on chemical reactions and their energy changes. The use of a metagenomic dataset in a location where extensive geochemical data are available permits calibration and testing of the model.\nOne setting where in-depth metagenomic and geochemical information are available is the hot spring known as “Bison Pool”, a flowing, moderately alkaline hot spring in Yellowstone National Park [12], [13]. The water at the source is boiling, and rapidly cools along the outflow channel as a result of exposure to the ambient conditions. Extensive chemical analysis of the water also reveals large gradients of chemical composition such as increase in pH, decrease in sulfide concentration, increase in dissolved oxygen and in oxidation-reduction potential of the water. Prior metagenomic sampling of the microbial communities at five sites from the source to approximately 22 meters down the outflow channel offers a window into the biomolecular composition of these communities. Although the metagenomic sequencing is of the DNA molecules, genes present in the metagenome provide a picture of the proteins that are likely to be used by the organisms.\nThe first major theme of this study concerns the changes in chemical composition of proteins along the outflow channel. The stoichiometric quantity we investigate is the average oxidation state of carbon in the proteins, which can be calculated directly from the chemical formulas of the proteins. In general, the average oxidation state of carbon in proteins increases down the outflow channel. This effect is present at the level of the whole metagenome and also within different functional classes of proteins. There is a positive correlation between the average oxidation state of carbon in proteins and the oxidation-reduction potential of the surrounding water.\nThe results of the stoichiometric calculations support a hypothesis that chemical compositions of the proteins reflect processes that tend to minimize the free energy of the system. We applied thermodynamic models to integrate molecular composition with temperature and multiple environmental chemical variables. The second major theme of the paper addresses the relative stabilities of the different classes of model proteins from each sampling site in terms of temperature, pH and oxidation-reduction potential. The major finding of this part of the study is that a redox gradient as a function of temperature traverses the stability fields of the proteins in a way that largely parallels the proteins' spatial distribution. This redox gradient, expressed as activity of dissolved hydrogen, generally parallels estimates derived from measurements of sulfide/sulfate concentrations, oxidation-reduction potential electrodes, and dissolved oxygen, but is more reducing than any of those.\nThese results help to outline the interrelationships between biomolecular composition and geochemistry in the Bison Pool ecosystem. One use of these models is to quantify gradients in oxidation potential between the water and the interiors of cells and/or biofilms at the temperatures found in the hot spring. Another is to establish the extent to which organisms minimize the energy expenditure involved in formation of biomolecules in specific chemical environments. Generalizing the methods and calculations described below can aid in resolving the effects of chemical gradients, energy minimization and other features of this hot spring and other geobiochemical systems.\nMethods\nAverage oxidation state of carbon\nThe average nominal oxidation state of carbon, , is a quantity related to the different electronegativities of elements involved in the covalent structure of an organic molecule.  is equal to the sum of the nominal oxidation states of all the carbon atoms in a molecule divided by the number of carbon atoms. The concept of average oxidation state of carbon has found application in various contexts, ranging from balancing organic oxidation-reduction reactions [14] to characterization of organic matter in aerosols [15] and in terrestrial ecosystems [16]. Moreover, a correlation can be observed between the standard molal Gibbs energies of oxidation half-reactions and the average oxidation state of carbon of the organic molecules involved [17]. As with smaller molecules, it is possible to interpret the chemical composition of proteins using the average oxidation state of carbon.\nThe rules for calculating the formal oxidation states on any carbon atom can be summarized as follows [18]. Each single bond to a more electronegative element (e.g., oxygen, nitrogen, sulfur) contributes  to the oxidation state of a particular carbon atom, while each single bond to a less electronegative element (e.g., hydrogen) contributes  to the oxidation state of a particular carbon atom, and a carbon-carbon bond counts (formally) as zero. Double bonds count doubly. Familiar, though extreme, examples are found with  (two double bonds to oxygen; ) and  (four single bonds to hydrogen; ). The concept of the average oxidation state can be extended to more complex molecules, for example acetic acid, , which has . That value is consistent with an oxidation state of  on the first carbon (having three bonds to hydrogen) and  on the second carbon (having one double bond and one single bond to oxygen).\nThe definition of oxidation state cited in the IUPAC Gold Book [19], [20] states that “… in ions the algebraic sum of the oxidation states of the constituent atoms must be equal to the charge on the ion” (i.e., positive or negative values for cations or anions, or zero for neutral species). We can adopt values for the formal charges of atoms other than carbon,  for oxygen,  for hydrogen,  for nitrogen,  for sulfur, that are consistent with this requirement for amino acids and proteins. The values for nitrogen and sulfur are those that would be assigned to the atoms if they were found in amine groups and sulfide groups, respectively [16]. Writing the formula of glycine as , the oxidation state of the first carbon is  (one bond to nitrogen, two bonds to hydrogen) and that of the second carbon is  (one double bond and one single bond to oxygen), so the average oxidation state of carbon in the molecule is . The sum of the formal charges of the atoms, in the order indicated by the formula, is , which is equal to the net charge of the molecule.\nIn many cases, the value of the average oxidation state of carbon is amenable to calculation using only the chemical formula of a molecule, instead of the more tedious accounting for each carbon. Let us use  to stand for the total charge on an ion (which becomes zero for a neutral molecule) and let the average oxidation state of carbon be represented by . Using formal oxidation states mentioned above for the elements other than carbon, the requirement for algebraic sums of oxidation states of the atoms can be expressed symbolically as(1)where , , ,  and  are the numbers of the respective subscripted elements in the chemical formula. Rearranging Eq. (1) gives(2)This equation shows that the average oxidation state of carbon in proteins is effectively a linear combination of the elemental ratios H/C, N/C, O/C and S/C.\nNote that ionization of the amino acid sidechains in proteins, and other ionization reactions involving only protons, have equal contributions to  and  and produce no net effect on the value of . Similarly, polymerization of amino acids, or other reactions involving only the gain or loss of a water molecule, produce no net effect on the value of  [18]. On the other hand, Eq. (2), and the electronegativity rules outlined above, show that oxidation-reduction reactions in organic compounds are not limited to gain or loss of either hydrogen or oxygen, but that the addition of other heteroatoms (sulfur, nitrogen) to a compound also causes an increase in the overall oxidation state of the molecule [18].\nThe average oxidation states of carbon in the twenty common amino acids range from  (leucine, isoleucine) to  (glycine, aspartic acid, asparagine) and are summarized in Table 1. The values listed for the amino acids in Table 1 span a considerable range but other types of organic molecules are even more or less oxidized [16], [17]. Proteins made up of these amino acids have an average oxidation state of carbon that can be computed as a weighted average of the  values of the amino acids, or equivalently, using Eq. (2) and the chemical formulas of the proteins. It may be noted that other physical-chemical properties of the amino acids can be correlated with differences in average oxidation state of carbon. For example, four highly hydrophobic amino acids (isoleucine, valine, leucine and phenylalanine) [21] have negative average oxidation states of carbon, which is associated with the high H/C ratios of their sidechain groups.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Average oxidation states of carbon and number of carbon atoms of the twenty amino acids commonly occurring in proteins.doi:10.1371/journal.pone.0022782.t001\n\nRelative stabilities of proteins\nChemical thermodynamic methods, borrowed from geochemical modeling applications, can also be used to study the relative stabilities of model proteins from different sampling sites in the hot spring outflow channel. The methods are described conceptually below, followed by description of a specific example.\nFour informal definitions help introduce the modeling strategy. 1) Basis species are a minimum set of chemical constituents that represent all of the chemical elements and the ionization state of proteins. 2) A formation reaction is a chemical reaction to form one mole of a protein from the basis species. The formation reactions of different proteins have different coefficients on basis species because the proteins themselves have different chemical formulas. 3) Chemical affinity is energy change during a reaction; positive values mean energy is released, and negative values mean that energy is consumed. A reaction with a higher chemical affinity is more favored to proceed to the product side. 4) Chemical activity is related fundamentally to chemical potential and can be thought of as the effective concentration of a basis species or protein.\nLet us define one system of interest as a collection of proteins with equal chemical activities interacting with a physical-chemical environment defined by constant values of temperature, pressure, and chemical activities of the basis species. What is the relative stability of one protein compared to another? If the activities of the proteins are equal, the affinities of the formation reactions of the proteins are generally unequal to each other, and the system is not in equilibrium. The most stable protein is identified as the one with the highest chemical affinity of its formation reaction. That is the protein whose formation, at a given chemical activity, releases the most energy, or requires the least energy input.\nNow consider the outcome of hypothetical chemical reactions among the proteins, so that different proteins (chemical species) are formed and destroyed at each others' expense, and as a consequence the chemical activities of the proteins change. The temperature and pressure are maintained, and the system is open so that the activities of the basis species are buffered and therefore remain unchanged. One or more specific outcomes of the hypothetical progression of reactions is an assemblage of proteins in a (possibly metastable) equilibrium distribution. In this equilibrium, or minimum-energy state, the chemical affinities of the formation reactions of the proteins are all equal (but might be non-zero), so the hypothetical transformation of one protein to another involves no overall energy change. If the affinities of the formation reactions of the proteins are all equal but less than zero, then the proteins are less stable than the basis species; that system represents a type of metastable equilibrium and a local, not global, energy minimum. Since the system is at equilibrium, the most stable protein is identified as the one with the highest chemical activity – in terms of concentration it has a higher degree of formation compared to the other proteins.\nThe hypothetical systems described above consist of populations of proteins with either equal activities or equal affinities of formation. To a first approximation (under conditions of ideal mixing) the stabilities of the proteins relative to each other are the same in both cases, since the definition of the chemical environment – temperature, pressure and activities of the basis species – is unchanged. Therefore, it is helpful to conceptualize the systems with equal activities of proteins and equal affinities of protein-formation reactions as being different states of a more generic system, defined only by the chemical environment and the identities of the proteins, but not their chemical activities. The relationship between the equal-activity and equal-affinity reference states is shown schematically in Fig. 1. The relative stabilities of species A, B, and C are the same in both panels of the Figure. If an equal-activity reference state is adopted, greater stability goes with higher affinity (Fig. 1a). If the equal-affinity reference state is adopted, greater stability goes with higher activity (Fig. 1b).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Relative stabilities portrayed in different reference states.In these qualitative diagrams, the same relative stabilities are shown in two different reference states. Species “C” is more stable than “B” is more stable than “A”. Chemical activity is shown on the -axis, and chemical affinity of formation reaction is shown on the -axis. In the equal-activity, or non-equilibrium, reference state (left), the species with the most positive chemical affinity of formation is the most stable. In the equal-affinity, or metastable equilibrium, reference state (right), the species with the most positive chemical activity is the most stable.\ndoi:10.1371/journal.pone.0022782.g001In quantifying the relative stabilities of proteins, a choice can be made between the two reference states; either one is valid, but the relative stabilities of the proteins are revealed through different variables. To generate the figures in this paper relative stabilities were quantified using an equal-affinity, or metastable equilibrium reference state. The primary advantage of doing so is the production of equilibrium activity diagrams [22] that are interpreted as depicting the relative stabilities of the proteins in terms of temperature and activities of the basis species. The specific methods used to calculate relative stabilities of proteins starting with an equal-activity reference state, then using a reaction matrix or equilibrium distribution equation to quantify the activities of the proteins at metastable equilibrium are described below.\nThermodynamic definitions.An equation for the differential of Gibbs energy () that takes account of reaction progress in a system, formulated by de Donder [23], [24] can be written as(3)where , ,  and  are entropy, temperature, volume and pressure,  is chemical affinity, and  is a reaction progress variable. The chemical affinity of the th reaction can be expressed as(4)where  is the gas constant,  represents the natural logarithm of , and  and  are the equilibrium constant and activity product of the th reaction. The chemical affinity is equal to the negative of the overall Gibbs energy change of the reaction ().  and  can be calculated from(5)where  is the standard Gibbs energy of the th reaction, and(6)where  and  are the reaction coefficient (negative for reactants, positive for products) and activity of the th species in the th reaction. In the equations, all operations involving logarithms have a base of 10.\nThe standard state convention adopted for liquids, including , corresponds to unit activity of the pure substance at any temperature and pressure. The standard state convention adopted for aqueous species other than  corresponds to unit activity of a hypothetical one molal solution referenced to infinite dilution at any temperature and pressure [25]. The conventional standard molal thermodynamic properties of both the aqueous electron and proton are taken to be zero at all temperatures and pressures [26].\n\n\n\nCalculating relative stabilities of proteins\nThe case study described below is based on an example described previously [27] for calculating the equilibrium activities of cell-surface glycoproteins (CSG) from Methanococcus voltae and Methanocaldococcus jannaschii. These methanogenic organisms are not likely to be present in detectable quantities at Bison Pool in Yellowstone National Park, but they nevertheless are common model organisms for studying microbial adaptations to differences in temperature and other environmental characteristics [28], [29]. Clues about the organisms' environments may emerge from comparing the sequence and chemical properties of the two functionally homologous proteins.\nThe methods described here for calculating the amino acid and chemical composition as well as the standard molal properties of the proteins [11] are currently restricted to only the unfolded peptide molecules and not the carbohydrate constituent of the glycoproteins. In their uncharged states, the peptide chains of these two molecules have formulas of  and , respectively, with sequence lengths of 553 and 530 amino acid residues (UniProtKB accessions Q50833 for CSG_METVO and Q58232 for CSG_METJA; signal peptides were removed). At 25°C and , the charges of the ionized proteins calculated using standard Gibbs energies of ionization of the amino acid sidechains and protein terminal groups [11] are  and . Although the calculation of protein charge, which is based on group additivity, does not take into consideration the effects of interactions between the ionizable groups, it does have the advantage of being sensitive to changes in temperature.\nWriting formation reactions for residue equivalents.In general, the relative stabilities of proteins of different lengths are of interest. Because of the differences in size, the molal reaction energies can not be directly compared in calculations of relative stability. In many environments, the synthesis of larger molecules, per mole, demands more energy, so for proteins of otherwise equal chemical composition and thermodynamic properties (such as two proteins of different size but with the same relative frequencies of amino acids), the smaller one would generally be thought of as more stable. In more reduced settings, the overall synthesis of organic molecules can actually release energy [30], so the synthesis of larger molecules would be favored. Taking the polymeric nature of the proteins into account, the relative stabilities of proteins of different size can be assessed by first writing formation reactions that are normalized by numbers of amino acid residues. The reactions involve the residue equivalents of the proteins, which have chemical formulas and standard molal properties that are those of the protein divided by the sequence length of the protein. The following two formation reactions are written for the residue equivalents of the two protein homologs:(7)for the protein from M. voltae and(8)for that from M. jannaschii.\nThe reactions above involve the basis species , , , ,  and , which are the same as used in Ref. [27] except that  is used here instead of . The choice of basis species determines the expression for chemical activities, i.e. the way in which the environmental chemical potentials are quantified. Similarly to components, the set of basis species is valid only if they represent a number of independent variables equal to the dimension of chemical variability in the system. There are unlimited combinations of basis species that would qualify, but the actual choice is usually made to facilitate comparisons with the natural system. For the calculations described in this paper  is used instead of  because of the actual formation and metabolic significance of molecular hydrogen in the hot-spring ecosystem [31].\nWriting the formation reactions normalized per residue offers insight into the consequences of changing environmental variables on the relative stabilities of the proteins. Because Reactions 7 and 8 are written per residue of the proteins, comparing the reaction coefficients to infer the effect of changing chemical variables on the relative stabilities of the proteins is consistent with an overall reaction between the proteins that is balanced on the protein backbone group. For example, more moles of  are consumed in Reaction (8) compared to Reaction (7). From specific statements of Eqs. (4) and (6) for both reactions it follows that increasing the activity of  would tend to favor formation of – that is, decrease the energy change of the reaction for – the homolog from M. jannaschii more strongly than that from M. voltae. The effect of changing activity of hydrogen on the relative stabilities of proteins from M. jannaschii and M. voltae parallels differences in oxidation state of the natural environments of these two organisms. A likely range of activities of dissolved hydrogen in the mixing zones of submarine hydrothermal vents and ocean water, representative of the environments inhabited by M. jannaschii, is  to  at °C [30]. In lower-temperature estuarine sediments, typical of the growth setting of M. voltae, lower hydrogen concentrations of  to  have been observed [32].\nInspection of Reactions 7 and 8 implies that increasing activity of  tends to favor formation of the protein from M. jannaschii more strongly than that from M. voltae. This finding is the opposite of what is implied by the difference between the average oxidation state of carbon in CSG_METJA () and CSG_METVO (); the protein from M. jannaschii actually has a higher average oxidation state of carbon. While the average oxidation state of carbon can be derived solely from the chemical composition of the protein, the formation reactions set the stage for understanding relative stabilities of the proteins in terms of reaction stoichiometry, energy, and their relationships to multiple chemical variables represented by the basis species.\n\nCalculation of equilibrium constants.The equilibrium constants of each of the reactions can be calculated using the standard Gibbs energies of formation from the elements of the species in the reactions. In this study, the standard molal thermodynamic properties of aqueous species as a function of temperature and pressure were evaluated using the revised Helgeson-Kirkham-Flowers (HKF) equations of state [25], [33], [34], [35]. The equations of state used for liquid  were taken from Refs. [36], [37], [38] as implemented in a Fortran subroutine in the SUPCRT92 software package [39]. Values of the standard molal thermodynamic properties and of the equations of state parameters for the basis species other than  and  were taken from Refs. [40], [34], [41].\nThe standard molal thermodynamic properties and equations of state parameters of the proteins can be calculated from amino acid group additivity [11]. In the present study, the CHNOSZ package [27] for the R software environment [42], which includes the group additivity equations for the proteins and the equations of state for calculating standard molal thermodynamic properties as a function of temperature, was used for the calculations. Sample code for performing the calculations for this example is included in Supporting Dataset S1. Combining the sources of data outlined above, values of  and  at 25°C and 1 bar can be obtained for these two reactions.\n\nCalculation of chemical affinities.The next step is to calculate the chemical affinities of the formation reactions in an equal-activity reference state. Let us use Eq. (4) to write(9)and(10)\nThe activities of the basis species are set to reference values nominally representative of environmental conditions. The activities of the basis species used in this example are taken from Ref. [27]: , , , ,  () and . The value for  was chosen so that the results would be numerically equivalent to those described in Ref. [27], where  was specified instead. Substituting these values into Eqs. (9) and (10) allows us to write(11)and(12)\nThe activities of the residue equivalents are related to the activities of the proteins as follows. Activity of the th protein () is related to concentration (, for molality) by(13)where  stands for the activity coefficient of the th protein. The total activity of residues in the th protein () is given by(14)where  stands for an activity coefficient. The total molality of residues associated with the th protein is(15)where  stands for the number of amino acid residues, or sequence length of the th protein.\nBecause of the high concentration of metabolites and biomacromolecules in cells, the activity coefficients of proteins in their natural subcellular environments are probably significantly different from unity [43], but available methods for calculating non-ideal behavior of protein solutions are referenced to electrolyte solutions [44] and depend on structural parameters of proteins that would be difficult to deduce from metagenomic sequence fragments. Under these circumstances the activity coefficients of both residues and proteins can be approximated as unity, and Eqs. (13)–(15) can be combined to write(16)To characterize the affinities in an equal-activity reference state, activities of the proteins nominally given by  are used for this example. Using Eq. (16), one then obtains reference activities of the residues given by  and  at 25°C and 1 bar.\nThe reference activities of the residues (computed from equal activities of proteins) can be substituted into Eqs. (11) and (12) to write  and . Therefore, on a per-residue basis, the homolog from M. jannaschii is more stable under the conditions (temperature, pressure, chemical activities of basis species) stated above. Decreasing the activity of hydrogen below a certain value, or changing the values of one or more variables in a specific manner determined by the reaction stoichiometry and Gibbs energy, would change the outcome so the homolog from M. voltae would be the more stable protein.\n\nCalculation of the metastable equilibrium activities of proteins: Reaction-matrix approach.Casting the relative stabilities of the proteins into a metastable equilibrium reference state facilitates comparisons on equilibrium activity diagrams. One approach involves a reaction matrix, where a system of equations is constructed based on the formation reactions of the proteins. In metastable equilibrium, the affinities of the formation reactions are all equal. Let us denote this value by . Combining  with Eqs. (11) and (12) permits writing(17)and(18)So far this is a system of two equations with three unknowns. A third equation arises from the conservation of activity of residues in the system; recall that activities are additive only if the activity coefficients are unity. Assigning both proteins reference activities of , the total activity of residues follows from Eq. (16):(19)The solution to the system of equations (17)–(19) is ,  and . It follows that the metastable equilibrium activities of the proteins (not the residues) are  and . As with the outcome of the equal-activity calculations described previously, CSG_METJA is found to be the more stable protein at the conditions of this example. Changes in temperature, pressure or activities of the basis species would alter these results; in some conditions, for example at more oxidizing conditions specified by lowering the activity of hydrogen, CSG_METVO would instead be the more stable protein.\nEach additional protein that is added to the system represents another unknown and another equation like Eq. (17) or (18), so this method is applicable to systems with any number of proteins. Note however that Eqs. (17)–(19), or others that would be written for different systems of proteins, do not constitute a linear system of equations; the unknown activities are summed in the last equation, but the logarithms of activities appear in the former equations. In software, a root finder can be used to solve these equations, leading to slow performance when the relative stabilities of many proteins (hundreds or thousands) are being considered. This performance penalty would not hinder the calculations described in this paper because at most five model proteins for each of the sampling sites are being considered. However, it is useful to consider a different approach, described in the next section, that is computationally more direct and yields identical results.\n\nCalculation of the metastable equilibrium activities of proteins: Boltzmann distribution.Let us define, for the per-residue formation reaction of the th protein,(20)It can be seen by comparison with Eqs. (4) and (6) that  includes all contributions to the chemical affinity of the th reaction except for the term associated with the activity of the residue equivalent of the protein of interest. For the per-residue formation reaction of the th protein, it follows that(21)where(22)where  enumerates all of the basis species, but not the protein, in the th reaction.\nIn physical applications, the Boltzmann distribution gives the probabilities of occupation of specific energy levels for systems in thermal equilibrium [45]; analogously for chemical systems it can be used to derive the equilibrium distributions of species [46]. An expression for the Boltzmann distribution, written using the current notation, is(23)Since the chemical affinity is the negative of the Gibbs energy of reaction, the exponents in Eq. (23) do not carry negative signs, unlike the energy terms in most common representations of the equation [45].\nFollowing the case study above, it can be deduced from Eqs. (11), (12) and (20) that , and . From Eq. (19) it follows that . Substituting these values into Eq. (23), one can directly calculate  and . These are the same as the values calculated above using the reaction-matrix approach, and can be combined with Eq. (16) to calculate the metastable equilibrium activities of the proteins. Application of Eq. (23) works as well for systems of three or more proteins and, compared to the reaction-matrix approach, leads to a more efficient implementation in software and faster calculations.\n\nEquilibrium activity diagrams.In the example described above, the calculations were carried out at only a single point in temperature-pressure-chemical activity space. The stability calculations can also be performed when one considers the effects of changing temperature, pressure, and/or chemical activities of the basis species, singly or in combination. Interpreting the results of this type of calculation is facilitated by visualizing the relative stabilities of the proteins on equilibrium activity diagrams. In the Results described below, the lines on chemical speciation diagrams show the metastable equilibrium activities of the proteins as a function of a single or composite variable on the -axis. Where two variables are being considered, the fields on predominance diagrams show the protein with the highest metastable equilibrium activity, as a function of the two variables on the - and -axes.\nThe thermodynamic calculations and stability diagrams reported below were made using the CHNOSZ software package [27]. The package encodes the equations of state, thermodynamic data, and the group additivity algorithms for proteins cited above. In recent versions of the software, the Boltzmann distribution was implemented for calculating the relative stabilities of proteins, and the calculations reported below use this method. The source code for the calculations reported in this paper, written in the R language [42] and utilizing the functions available in CHNOSZ, is available in the Supporting Information of this paper; Dataset S1 contains code for the example described above, and Dataset S2 contains the code used to produce the figures in the Results.\n\n\n\nContribution by energy of protein folding to uncertainty in chemical stability calculations\nThe group additivity algorithm adopted here for calculating the standard molal Gibbs energies of proteins is referenced to unfolded aqueous proteins [11]. Most proteins in their active forms adopt a folded conformation. The energy change for the folding reaction, or change of conformation, is commonly referred to as “protein stability” [47]. The latter nevertheless is distinct from the chemical stabilities being considered in this study, which are based on energies of protein formation. However, the energy change in the folding process contributes some uncertainty, assessed below, to the values adopted here for the standard Gibbs energies of the proteins.\nIt was estimated that the uncertainty in standard Gibbs energies of proteins inherent in the group additivity algorithm is of the order of five percent [11]. The values of  of the non-ionized forms CSG_METVO and CSG_METJA calculated using group additivity are  and  kJ mol ( and  kcal mol), respectively [27]; a nominal 5% uncertainty corresponds to  and  kJ mol ( and  kcal mol). For proteins of comparable size, Gibbs energies of folding of 40–80 kJ mol (10–20 kcal mol) are not uncommon, depending on the temperature [48], [47]. These values are approximately one-one hundredth the magnitude of the estimated uncertainties in the additive standard Gibbs energies of the unfolded proteins. Moreover, the effect of any systematic uncertainty that affects the standard Gibbs energies of the proteins in the same direction (as would the folding process) would tend to cancel in the relative stability calculations. Therefore, not accounting for the energy of protein folding contributes little to the overall uncertainty of the relative stability calculations.\n\nResults\nDescription of field site and metagenomic sampling\nChemical and biological sampling was performed in July 2005 at the hot spring known as “Bison Pool” in the Sentinel Meadows in the Lower Geyser Basin of Yellowstone National Park [12]. “Bison Pool” is the unofficial name of a hot spring whose source pool is located at approximately 44.56961°N, 110.86513°W (WGS 84 datum), the closest officially named feature being called Rosette Geyser [49]. A map identifying the sampling sites referred to in this study, based on one found in Ref. [12], is shown in Fig. 2. The spring emits a continuous flow of boiling (°C), moderately alkaline (pH7.5) water, and emerges from within a base of sinter made of silica that has precipitated from the water. The winding outflow channel is occupied by a plethora of biofilms in a striking array of colors. At this and similar springs, the white and pink filaments found at higher temperatures harbor chemotrophic organisms such as Aquificae and some Archaea [50], [49]. Yellow, orange and green biofilms (thick mats) found at lower temperatures are predominantly made up of photosynthetic communities of Cyanobacteria and relatives of Chloroflexi [51], [13], although archaeal organisms can also be found at the lower temperatures [13].\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Map of the Bison Pool hot spring system.The map includes locations of the sites where biofilm and geochemical sampling was performed in the summer of 2005.\ndoi:10.1371/journal.pone.0022782.g002The available metagenomic and geochemical data were obtained from five sampling sites from the source pool of the hot spring to 22 meters down the outflow channel. Site 3 is notable because it is within the “photosynthetic fringe”, or the transition zone (ecotone) where bright colors indicate the onset of photosynthetic potential [52], [12]. A summary of some of the field- and laboratory-based chemical analyses of the water relevant to this study is given in Table 2. Together with a decrease in temperature down the outflow channel, there is an increase in pH. An increase in the oxidation potential of the water is also apparent from the higher dissolved oxygen and lower sulfide concentrations observed in water sampled away from the source.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Sampling site identification, distance from source pool, and summary of chemical and molecular sequence data at five sampling sites at Bison Poola.doi:10.1371/journal.pone.0022782.t002The biofilm samples used for metagenomic sequencing were collected at the same time as the water samples used for chemical analysis [12], except for field measurements of oxidation-reduction potential (ORP), which were obtained in 2009. Environmental DNA in the biofilm samples was shotgun-sequenced by the Joint Genome Institute using the Sanger method. The assembly and annotation of protein coding sequences was carried out through an automated pipeline in the Integrated Microbial Genomes with Microbiome Samples (IMG/M) system [53], and sequences used in this study were downloaded from the IMG/M website (http://img.jgi.doe.gov/m).\n\n\nAmino acid compositions of model proteins\nFor this study, FASTA data files containing predicted protein sequences were downloaded from IMG/M using the taxonomic IDs BISONN, BISONS, etc. The letter codes for all the sampling sites are listed in Table 2, together with the total numbers of metagenomic reads and protein-coding sequences for each site.\nBecause they are derived from shotgun sequencing, most of the inferred protein-coding sequences are actually fragments of whole genes. It would be possible to select specific types of homologs, align the sequence fragments, and use the aligned positions in the stoichiometric and thermodynamic calculations described below. However, the calculation of the average oxidation states of carbon requires only the chemical formula of the proteins, and the calculation of the standard Gibbs energies of the proteins as described in the Methods only requires the amino acid compositions of the proteins. Therefore, in this study, model amino acid compositions were used to represent averages of groups of protein sequences in the metagenome.\nThe five “overall model proteins” have average amino acid compositions that were calculated as the average of all inferred protein sequences, including fragments, identified in the metagenome at each site. The amino acid compositions of the overall model proteins were calculated by summing the amino acid counts of all sequences at each site and dividing by the total number of sequences at each site. Accordingly, the model proteins are not whole proteins, but instead have fractional amino acid frequencies. The amino acid compositions are listed in Supporting Dataset S3.\nThe average amino acid compositions were also calculated for “classified model proteins” in twenty functional classes each corresponding to a keyword in the sequence annotations reported in IMG/M. The keywords were selected based on their frequencies in the annotations and represent a variety of functions and cellular structures, but are neither comprehensive nor mutually exclusive. The keywords and number of identified sequences are listed in Table 3. The classification with the highest number of inferred protein sequences is “transferase”, with a total of 15768 sequences across all five sampling sites, or ca. 5.4% of all of the protein sequences in the metagenome. The classification with the fewest number of sequences is “phosphatase”, with a total of 2260 sequences, or about 0.8% of the metagenome. All the keyword searches were case-insensitive and any match was accepted (e.g., an annotation including the word “transporter” was matched by the “transport” keyword), except for “reductase”, which was only matched to the beginning of a word in the annotation (e.g., annotations including the word “oxidoreductase” were not matched by the “reductase” keyword).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 3.  Annotation terms, total number of sequences used to construct the classified model proteins, and  of the classified model proteins (for site 1 only; entries are ordered by decreasing ).doi:10.1371/journal.pone.0022782.t003\n\nAverage oxidation state of carbon of model proteins\nTo characterize the changes in the compositions of proteins across the sampling sites, we first calculated the elemental ratios and average oxidation number of carbon () of all the protein sequences available in the metagenome for each sampling site. The 95% confidence intervals around the mean values were calculated from a bootstrap analysis (nonparametric, ordinary bootstrap, 1000 replicates) performed using the “boot” package for the R software environment [42]. The results are plotted in Fig. 3 and the numerical values given in Supporting Dataset S4.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Elemental ratios and average oxidation state of carbon in protein sequences.Elemental ratios were calculated from the chemical formulas of all protein sequences available in the Bison Pool environmental genome, and average oxidation state of carbon () was calculated using Eq. (2). The bars are centered on the means, and the heights of the bars represent the 95% confidence intervals derived from a bootstrap analysis.\ndoi:10.1371/journal.pone.0022782.g003The S/C, O/C and N/C ratios shown in Fig. 3 exhibit an overall increase with distance from the hot-spring source, but there is a decrease in S/C and O/C of the proteins from site 3. The H/C ratio rises sharply between sites 1 and 2 and then decreases, with the proteins at site 3 again having a relatively lower value. The combined effect of the elemental ratios accounts for the trend in  appearing in Fig. 3, which can be described as increasing with distance from the hot-spring source. The chemical compositions of the proteins at sites 4 and 5 are more similar to each other than to the other sites, and the overall trends for elemental ratios and  show a slight reversal at these two sites.\nThe average oxidation state of carbon of overall and classified model proteins is shown in Fig. 4a. Because of the number of lines plotted in this figure only selected ones are labeled. The others can be identified by referring to the values of  for the model proteins for site 1 listed in Table 3. Whatever the classification of the model protein, there in an increase in  going from site 1 to site 5, and in most cases an increase between each of sites 1 through 4. Generally, there is a slight decrease in the value of  between sites 4 and 5. The differences between the different classes of model proteins are profound: the oxidoreductases, transport and membrane proteins, and especially permeases, all have lower oxidation states of carbon than the others. This result is not surprising, given the greater abundance of hydrophobic sidechains in these predominantly membrane-associated [54] proteins. The model proteins that have the highest oxidation states of carbon are hydrolase at sites 1–3 and transposase at sites 4 and 5. That the transposase model proteins at sites 4 and 5 are more oxidized than other model proteins is noteworthy because at site 1 the transposase model protein has a value of  that is only a little greater than that of the overall model protein.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Average oxidation state of carbon in model proteins along the outflow channel.The values of  calculated using Eq. (2) for the overall model proteins for each sampling site are shown by the bold green line, and those for each of the 20 classes of model proteins listed in Table 3 are shown by the thin lines. For clarity, only selected classes are labeled; hydrolase and protease are the ones with highest and lowest values of  among the main group of lines at the top.\ndoi:10.1371/journal.pone.0022782.g004Some relationships can be observed between the chemical composition of proteins and the chemical characteristics of the water. On the whole, there is a positive correlation between the values of  of the model proteins and the field measurements of dissolved oxygen, and a negative correlation with total sulfide concentrations listed in Table 2. The correlations suggest that analysis of the energetics of the protein formation reactions could be used to combine protein composition and hot-spring chemistry in a thermodynamic model. In the following sections results are presented from a thermodynamic analysis that describes the relative stabilities of proteins in terms of temperature, pH, oxidation potential and other environmental variables.\n\n\nRelative stabilities of model proteins: Metastable equilibrium\nResidue-normalized formation reactions for the overall model proteins are listed in Table 4. These reactions are written in terms of the basis species , , , ,  and , which correspond to inorganic sources of the major elements in the proteins. Because the number of amino acid residues in each of the model proteins is the average of the lengths of metagenomically derived protein sequences, including many fragments, the number of amino acids in each of the model proteins in Table 4 does not necessarily reflect the actual lengths of protein sequences in the microbial organisms in the hot spring.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 4.  Numbers of amino acid residues, average oxidation state of carbon, and per-residue formation reactions normalized per residue for overall model proteins for different sampling sites.adoi:10.1371/journal.pone.0022782.t004Because the reactions in Table 4 are written for the formation of the residue equivalents of the model proteins, the reactions are effectively balanced with respect to the protein backbone group. Consequently, the stoichiometric reaction coefficients are independent of the sizes of the model proteins and can be compared with each other in a first approximation to assess the effects of changing chemical conditions on the relative stabilities of the model proteins. For example, the coefficients on , appearing on the reactant side of the reactions, decrease in order of increasing distance from the hot spring source, except for the last two sites, where the pattern is reversed. Therefore, increasing the chemical activity of  tends to decrease the energy demand of forming overall model proteins at the high-temperature sites more strongly than the others.\nThe intensive variables used in the equilibrium calculations are temperature (), pressure (), and the chemical activities of the basis species and proteins. Activity coefficients of all species can be set to unity, so, for aqueous species, chemical activities were taken to be equivalent to concentrations in molal units. As noted above, the activity coefficients of proteins in subcellular conditions are currently not amenable to general calculation. In addition, the concentrations of major ions in the water [55] are low enough that setting the activity coefficients of the basis species to unity is a tolerable first approximation. In the calculations reported below, the variables held constant were  bar, , , , . The activities chosen for  and  are based on the measurements of dissolved inorganic carbon and sulfide listed in Table 2, while that of  is a nominal value. The calculations of metastable equilibrium were referenced to unit total activity of the amino acid residues in the system. The other variables (, pH and ) were used as exploratory variables as described below.\nThe results of computations of relative stabilities of the overall model proteins are depicted in Fig. 5. Values of temperature measured at each site in the hot spring (Table 2) are shown in Fig. 5a. Stability calculations were performed for a system composed of the five overall model proteins, using the pH measured at site 3. The most stable overall model proteins as a function of temperature and  are shown in the equilibrium predominance diagram in Fig. 5b. The temperature range in this diagram is somewhat larger than the measured range of temperature in the hot spring, and the range of  was set to encompass the stability fields of the proteins.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Analysis of relative stabilities of overall model proteins for sampling sites.Panel (a) is a plot of the measured temperatures as a function of distance with a smooth curve connecting the points. Panel (b) is a plot of the relative stabilities of the overall model proteins as a function of  and temperature at the pH of site 3 (7.933). Panel (c) is a plot of the measured pHs as a function of distance with a smooth curve connecting the points. Panel (d) is a plot of the relative stabilities of the overall model proteins as a function of  and pH at the temperature of site 3 (67.5°C). The dashed lines in (b) and (d) depict Eq. (24). Panel (e) is a plot of the relative stabilities of the overall model proteins as a function of  at the temperature and pH of site 3. Panel (f) is a plot of the relative stabilities of the overall proteins as a function of the combination of changes with distance of measured temperature and pH and modeled  (using Eq. 24).\ndoi:10.1371/journal.pone.0022782.g005Values of pH measured at each site are shown in Fig. 5c. Stability calculations using the temperature measured at site 3 lead to the equilibrium predominance diagram shown in Fig. 5d. The pH range in this diagram is slightly larger than the measured range of pH in the hot spring.\nNote that in Figs. 5b and d, only the overall model proteins from sites 1, 2 and 4 appear, going from high to low values of  in that order. Increasing pH at constant  and temperature (Fig. 5d) moves toward the relative stability fields for the overall model proteins that are more distal from the hot-spring source; this pattern is congruent with the pH differences between sampling sites in the hot spring. On the other hand, decreasing temperature at constant  and pH (Fig. 5b) moves toward the relative stability fields for the overall model proteins that are more proximal to the hot-spring source; this pattern is incongruent with the temperature gradient in the outflow channel of the hot spring.\nIf it were representative of the chemical gradients in the hot spring, the thermodynamic model used here would generate a pattern of relative stabilities of the model proteins that reflects their geographical distribution. It is apparent from the above findings that this is not possible if  is constant along the outflow channel. Instead, the dashed lines in Figs. 5b and d and the equilibrium chemical activities of the proteins shown in Figs. 5e and f are consistent with changing  in the model together with the measured changes in temperature and pH and , and were derived by considering the relative stabilities of many classes of model proteins as described below.\n\n\nOperational equation for activity of hydrogen\nFigure 6 contains -temperature equilibrium predominance diagrams for the 20 classes of model proteins listed in Table 3. These figures were constructed in a manner analogous to Fig. 5b. For each class of model proteins shown in Fig. 6 the protein from site 1 is relatively stable at higher values of , and the protein from site 4 or 5 occupies the low- portion of the diagram. Other details differ between the various classes of model proteins; for example, the model protein for phosphatase at site 1 is stable relative to the model proteins for phosphatase at other sites only at highly reduced conditions. Compared to other classes of proteins, the predominance field for the model protein for oxidoreductase at site 2 is much smaller, and that for the ribosomal proteins at site 2 does not even appear. In spite of these differences, the overall resemblance of the plots in Fig. 6 to each other and to Fig. 5b indicates that different subsets of the metagenome have similar relative stability relationships.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Equilibrium predominance diagrams for classified model proteins.For each class of model proteins, the fields in these predominance diagram represent the model protein with the most positive equilibrium activity as a function of  and temperature. pH was set to the value listed in Table 2 for site 3 and the chemical activities of the other basis species were set to reference values specified in the Results section. The dashed line in each diagram indicates values of  calculated using Eq. (24).\ndoi:10.1371/journal.pone.0022782.g006The dashed lines in Fig. 5b and d and Fig. 6 denote values of  that are given by the function(24)This equation is the result of a graphical regression of the plots in Fig. 5b and Fig. 6 such that the values of  as a function of temperature give a progression in the stability fields of a majority of the model proteins that is similar to the geographical distribution of the proteins. For example, in Fig. 5b, the line given by Eq. (24) encounters the stability fields for the overall model proteins for sites 1, 2 and 4, in that order, with decreasing temperature.\nNote that the plot in Fig. 5d is drawn for a single temperature (that for site 3 listed in Table 2), so the line for  is horizontal in this figure. The line crosses the predominance field boundary between sites 2 and 4 at pH , which is close to the measured pH of site 3 (Table 2). The absence of a predominance field for the model protein for site 3 in this figure and in most plots shown in Fig. 6 suggests that generally the proteins at site 3 are less stable than the model proteins for other sites, even under the specific conditions of site 3.\n\n\nEffects of other variables\nThe model developed here incorporates spatial gradients of pH and activity of hydrogen, but in principle other chemical variables could also contribute significantly to the relative stabilities of the proteins. For example, the concentration of dissolved sulfide decreases by more than an order of magnitude between the source of the hot spring and the most distal sampling site (Table 2). However, the consequences of that gradient on the relative stabilities of any two proteins is proportional to the difference in the reaction coefficient of sulfide between the two reactions. In Table 4, the largest difference between reaction coefficients on  is only , which is much smaller than the differences in the relative stabilities of the proteins ( for any of the sites listed the Table). Accordingly, the sulfide concentration in the model was set to a constant since the changes seen in the hot spring have a smaller effect on the relative stabilities of the proteins compared to temperature, pH and . For comparison, the difference between reaction coefficients on  in the formation reactions of overall model proteins 1 and 2 is , while the difference in relative stabilities of the proteins is , so a one-order-of-magnitude decrease in the activity of , by itself, is enough to increase the relative stability of the model protein for site 2 over site 1.\nConversely, although the coefficients on  in the reactions shown in Table 4 differ from each other more than those on , the measured concentrations of dissolved inorganic carbon differ by not more than 4 ppm between the sites (Table 2). Therefore, the small changes in the corresponding activities of  would not be expected to significantly alter the chemical affinities of the reactions relative to each other, so, as with sulfide, the activity of  was set to a constant value.\nThe calculations described above used a nominal value of  for the activity of . Total ammonia was not detected above 0.01 mg/L (0.6 mol) in spectrophotometric analysis of water at the hot spring [55], so it is reasonable to ask whether lowering the activity  to  has a major effect on the calculations of relative stabilities. The interdependence of equilibrium activities of the basis species can be assessed by writing a reaction representing the transformation between two proteins, for example the overall model proteins for sites 2 and 4. Adding reaction 4 in Table 4 to the opposite of reaction 2 yields the following reaction:(25)It follows from Eqs. (4)–(6) that, if the chemical affinity of Reaction 25 and the activities of all other reactants and products remain unchanged, decreasing  by 3 results in a decrease in the calculated  of  (i.e., ). Likewise, reconnaissance calculations indicate that decreasing the activity of  to  generally results in a lowering of the equal-activity lines shown in Fig. 6, with a more pronounced effect for the lower equal-activity lines, which in some cases shift downwards by at about half a  unit. Therefore, refinement of the calculations described here may yield results that support modifying Eq. (24) to have a somewhat steeper slope and lower intercept. Nevertheless, without any direct measurements of the total ammonia concentration, such refinements remain speculative.\n\n\nRelative stabilities of model proteins: Chemical affinities\nChemical affinities for the per-residue formation reactions of the overall model proteins in an equal-activity reference state calculated for sites 1 to 5 are listed in Table 5. The calculations used values of temperature and pH listed in Table 2,  for each of these sites taken from Eq. 24, and activities of the other basis species given above. The activities of the residue equivalents of each of the model proteins were set to unity. All of the reactions listed in Table 4 are endergonic reactions, which is apparent from the negative values of chemical affinity that are shown in Table 5. However, the chemical affinities are less negative at the higher-temperature, more reduced conditions, which is consistent with a previous comparison of the energetics of biomass synthesis under oxic and anoxic conditions [56].\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 5.  Chemical affinities for the reactions in Table 4 at the model conditions for each site.adoi:10.1371/journal.pone.0022782.t005Examination of Table 5 shows that the reaction with the greatest chemical affinity at site 1 is that for the overall model protein for that site. In contrast, at the conditions of sites 3–5 the formation, per residue, of the model protein for site 4 is the least energetically demanding. The affinities listed in Table 5 are calculated for the formation reactions of the proteins normalized per residue, but comparison of the relative stabilities of the proteins (as is done on the predominance diagrams) requires further accounting for the relative lengths of the model proteins. Equation (16) was used to account for different lengths when constructing equilibrium activity diagrams for proteins. Analogously, by subtracting the logarithm (base 10) of the number of amino acids present in each of the model proteins from the values listed in Table 4, one obtains length-corrected affinities per residue that can be compared with the equilibrium activity diagrams. For example, performing this operation on the second column of Table 5 shows that the model protein from site 2 is the most stable, even though the per-residue reaction for site 1 has a greater chemical affinity, before applying the length correction. The outcome is in accord with the progression, from sites 1 to 2 to 4, in the relative stabilities of the overall model proteins apparent in Figs. 5b and d.\n\n\nCombined analysis of temperature, pH and oxidation potential\nReturning to the metastable equilibrium (equal-affinity) reference state, the equilibrium activities of the proteins are plotted in Fig. 5e as functions of  at constant temperature and pH (corresponding to site 3) for a total activity of residues equal to one. Nowhere does the equilibrium chemical activity of the overall model protein for site 3 rise above all the others, which is consistent with Fig. 5b, but it is also apparent that the activity for this model protein maximizes at intermediate values of . The relative instability of the model proteins for site 3 throughout the different classes of model proteins is apparent from the low frequency of predominance fields representing this site appearing in Fig. 6. It can also be seen in Fig. 5e that the overall model proteins representing sites 4 and 5 are similar to each other in terms of their relative stabilities.\nTo portray the effects of changing temperature, pH and  simultaneously, all of these variables are projected along the -axis (“distance”) in Fig. 5f. The values of temperature and pH at any point along the distance axis are taken from the curves shown in Figs. 5a and c, and the values of  are calculated using Eq. (24). Fig. 5f has the advantage that the relative stabilities of the model proteins are shown as a function of a spatial variable and can therefore be compared with the physical location of the sampling sites in the hot spring.\nIn order to visualize the relative stabilities of all of the groups of model proteins on a single figure, the equilibrium activities () of the proteins were transformed into equilibrium degrees of formation. The degree of formation of the th model protein () is given by(26)where the summation occurs over all of the model proteins in the calculation, which in this case is five (one for each sampling site). Since , the degree of formation of any protein can be visualized as a fraction of a bar of unit length. The equilibrium degrees of formation of the overall and classified model proteins are shown as a function of distance in Fig. 7. In this figure, the color code refers to the five sampling sites, and the height of the bars represents the equilibrium degree of formation of the indicated model protein. At any point along the distance axis, the bars are stacked with the most relatively stable model protein on the top. The locations and color codes of the sampling sites are indicated by the tick marks.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 7.  Equilibrium degrees of formation of model proteins in 21 classes.The locations of the sampling sites are indicated by the colored tick marks. The degrees of formation of the five model proteins in each class are shown by the heights of the color-coded bars. At any point on the distance axis, the bars are stacked in order of relative stability, so that the most stable (highest equilibrium degree of formation) is at the top.\ndoi:10.1371/journal.pone.0022782.g007Fig. 7 permits a visual test of the overall goodness of fit of Eq. (24) to the geographical relationships of the sampling sites and also helps in identifying outliers. At the high-temperature end, the most stable model protein is usually that from site 1, and rarely from site 2 (phosphatase, periplasmic). At the low-temperature end, the most stable protein is usually that from site 4 or 5 (with approximately equal frequency) and only occasionally from sites 2 (transcription) or 3 (transposase). The only model proteins for site 3 that are the most relatively stable over any part of the combined chemical gradient are those for transposase (over the mid- to low-temperature range), oxidoreductase and phosphatase (only at moderate temperatures). The transition zone occurring toward the middle of the plots is sometimes associated with an increase in the relative stability of the model proteins for site 3 (e.g., transferase, synthase), but in many other cases the equilibrium degree of formation of the model proteins for site 3 minimizes relative to the other sites. The overall relative instability of the proteins from site 3 is can be attributed to constraints on their amino acid compositions that also account for the shifts in O/C, H/C, and S/C for these proteins from the overall trends between sites apparent in Fig. 3. A major exception is transposase, for which the model proteins from site 3 are relatively stable over much of the chemical gradient. As in the general case, the relative stabilities are conditioned by trends in amino acid composition that also affect elemental ratios, apparent in Fig. 4 as a high oxidation state of carbon of the transposase model proteins at sites 4 and 5.\n\n\nMeasurements of oxidation-reduction potential\nEq. (24) represents a proposal for the temperature dependence of the activity of hydrogen derived from the relative stabilities of the model proteins. It can be compared with a variety of other measurements that are indicators of redox conditions of the hydrothermal solution including results of field measurements of redox conditions made using an oxidation-reduction potential (ORP) probe.\nORP, temperature and pH readings obtained in Summer 2009 for Bison Pool (four years after the biofilm sampling for metagenomic analysis and acquisition of chemical data reported in Table 2) and other flowing hot springs in Yellowstone are listed in Table 6 and in Dataset S5. The ORP measurements were obtained at three sites at Bison Pool that approximated the original locations of sites 3, 4 and 5. ORP, pH and temperature measurements at higher temperatures were also obtained at Mound Spring, which is the official name a nearby hot spring in Sentinel Meadows with chemical features similar to Bison Pool [55].\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 6.  Meter readings for selected hot springs and their outflows in Yellowstone National Park, Summer 2009.doi:10.1371/journal.pone.0022782.t006Temperature and pH were measured in the field with hand-held temperature/conductivity (YSI, Yellow Springs, Ohio) and pH meters (WTW, Weilheim, Germany 300i pH meter with SenTix 41 pH electrode). Oxidation-reduction potential was measured using a high-temperature ORP probe (PI-M11-ORP-HT) rated to greater than 80°C and a Thermo Scientific pH/mV meter with a readout sensitivity of 1 mV, both acquired from Pulse Instruments (Van Nuys, CA). The ORP probe contains a silver-silver chloride (Ag/AgCl) reference electrode with saturated KCl solution. Before the field work, the ORP meter was calibrated in the laboratory at 25°C using a stock of Light's Solution (ferrous/ferric sulfate in sulfuric acid) supplied by Pulse Instruments. The stated potential of the solution is  mV at 25°C vs. saturated KCl/AgCl electrode.\nTo convert the ORP readings (referenced to the Ag/AgCl electrode) to Eh (referenced to the standard hydrogen electrode, or SHE) we used(27)where E(Ag/AgCl) is the potential vs. SHE of the Ag/AgCl reference electrode. The potential of the reference electrode was calculated using an equation for Ag/AgCl with 1M KCl electrolyte [57](28)where  is in volts and  is in degrees Celsius. The effect of differences between the saturated and 1M KCl Ag/AgCl electrodes is discussed further below.\nValues of Eh calculated by combining field measurements of ORP with Eqs. (27) and (28) are shown in Fig. 8a. The hot springs are identified by the number codes given in Table 6, and the source pools are indicated by bold symbols. The values for Mound Spring and Bison Pool (points labeled “3” and “4” in the figure) increase with decreasing temperature and are similar to an unnamed hot spring in Sentinel Meadows for which data are available (points labeled “5” in the figure; the GPS locations of this and other springs are given in Table 6).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 8.  Calculated Eh and activity of hydrogen in hot springs.Eh and equilibrium  as functions of temperature for various hot springs were calculated from measured values of pH and ORP listed in Table 6. The different hot springs are identified by numbers (see Table 6 for key), and bold symbols indicate the sources of the hot springs.\ndoi:10.1371/journal.pone.0022782.g008Equilibrium values of  were calculated by combining Eh, pH and temperature. The values of Eh were first converted to pe (the negative of the logarithm of the activity of the electron) using [26](29)where  denotes the Faraday constant. Then,  was calculated from the law of mass action for(30)that is,(31)Values of  calculated using the standard molal properties of the species in Reaction 30 (see Methods) were combined with pe and pH to generate the values of  shown in Fig. 8b. These values correspond to equilibrium with both protons (activities constrained by pH measurements) and electrons (activities inferred from the ORP measurements, which does not capture the full spectrum of reactivity of electrons in the solution).\nAt 25°C the potential of the Ag/AgCl electrode (1M KCl) calculated using Eq. (28) is  V. In contrast, the potential of the Ag/AgCl electrode with saturated KCl, which might be a more appropriate choice for calculations given the specifications of the ORP probe used for the measurements, is about 0.197V [58], or about 0.025V lower. Eqs. 29 and 31 can be used to calculate that a decrease of 0.025V at constant pH would increase the equilibrium  by approximately  at 25°C and  at 100°C. Changes of this magnitude would not drastically affect the relative positions of the points and lines shown in Fig. 8a or the comparisons drawn further below. The difference between the potentials of the 1M and saturated KCl electrode is probably also comparable in size to the total uncertainty in the measurement of oxidation-reduction potential in reactive hydrothermal fluids (see for example Ref. [59]). Therefore, the values of E(Ag/AgCl) used to calculate Eh from ORP (Eq. 27) were taken from Eq. (28) without modification.\nAlthough the water at Mound Spring had a higher pH than reported for Bison Pool (see Table 6) the equilibrium values of  calculated using Reaction 30 for the two hot springs are close to each other at similar temperatures. The values of  calculated in this manner for the outflow channels of Bison Pool and Mound Spring are lower than other hot springs at the same temperatures shown in Fig. 8b, but a more reduced fluid at higher temperatures seems to be the case for any of the measured hot springs.\n\n\nComparison of equilibrium activities of hydrogen\nConcentrations of chemical species such as dissolved oxygen, and species with an element in different oxidation states (e.g. sulfide and sulfate) can be expressed on the  redox scale using relationships derived from chemical equilibrium. Values of  for equilibrium between dissolved sulfide (as ) and sulfate () were obtained using the law of mass action for(32)at the temperature of each sampling site. The law of mass action is the relationship (Eq. 4) between the equilibrium constant () and activity product () of this reaction when the chemical affinity () of the reaction is zero (see Eq. 31 for an example). Activities of  and  were taken to be equal to molalities of the species listed in Table 2, and values of  were again obtained using standard molal thermodynamic properties of the species as a function of temperature calculated as described in the Methods.\nValues of  in equilibrium with dissolved oxygen were obtained using the law of mass action for(33)using activities of  derived from the concentrations listed in Table 2.\nFig. 9 shows a comparison of the activities of hydrogen calculated using Eq. (24) and activities of hydrogen in equilibrium with measured oxidation-reduction potentials (ORP) and the oxygen and sulfur redox indicators described above. The temperature dependence of the equilibrium  is strongest for the ORP measurements and weakest for the sulfur system. Among the three different redox indicators considered here (ORP, sulfide/sulfate, and dissolved oxygen) the lowest values of  come from equilibrium with dissolved oxygen and the highest for equilibrium in the sulfide/sulfate reaction. All of the redox indicators have lower equilibrium values of  than those calculated using Eq. (24).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 9.  Comparison of different estimates of hydrogen activity as a function of temperature.Values shown for sulfide/sulfate and dissolved oxygen were computed using measured concentrations of those species listed in Table 2. Values shown for “Eh” were computed using pHs and ORP readings listed in Table 6. The solid line indicates values calculated using Eq. (24).\ndoi:10.1371/journal.pone.0022782.g009These three proxies are limiting cases, indicating what the values of  would be if each of the reactions dictated the actual hydrogen activity in the system. Therefore, the separation of the lines in Fig. 9 provides evidence that overall redox equilibrium does not characterize the hot spring (see Ref. [55] for further evidence of redox disequilibria in hot springs). Nevertheless, the simplest explanation for the trends shown in Fig. 9 is that the states of various redox reactions in the water are all becoming more oxidized with decreasing temperature, and that the redox gradient derived from the relative stabilities of the proteins (Eq. 24) occurs at more reducing conditions than any of the inorganic oxidation-reduction reactions.\n\nBy computing the standard Gibbs energies of overall reactions representing the formation of proteins, and using geochemical data on the gradients of temperature and chemistry, the relative stabilities of model proteins derived from metagenomic sequences in a hot spring could be calculated. It was possible to compute relative stabilities of the model proteins that reflect their overall spatial distribution in the system. An equation for hydrogen activity as a function of temperature was proposed, as a way of calibrating the model to maximize the correspondence between the geographical distributions of the sampling sites and the progression of relative stabilities of proteins.There is a general increase in average oxidation state of carbon in the model proteins that parallels the rising redox potential of species in the water as it flows away, also cooling, from the hot spring. However, the average oxidation states of carbon in the different classes of model proteins are offset from each other, with membrane-associated proteins (including permeases, transport proteins and oxidoreductases) being more reduced. As noted above, hydrophobic amino acids tend to have lower oxidation states of carbon, so an increase in the number of hydrophobic amino acids can account for a decrease in average oxidation state of carbon in the proteins. An increased frequency of hydrophobic amino acids in proteins is a likely feature of adaptation to higher temperatures, as seen in the genomes of model organisms [60], [61]. Our metagenomically based finding of a negative correlation between temperature and oxidation state of carbon is consistent with those results.The oxidation states of carbon in the different classes of model proteins increase along the outflow channel of the hot spring, except for a slight decrease between sites 4 and 5. The apparent inversion in relative stabilities between sites 4 and 5 might be connected to additional hydrogen input, possibly from oxygenic photosynthesizers [62], as a byproduct of nitrogen fixation [63] or fermentation, and/or from secondary sources of reduced gases in the hot-spring system.A distinctive feature of the chemical compositions of the proteins in the study area is the departure of site 3 from the general trends for H/C, O/C and S/C shown in Fig. 3, the infrequent appearance of the model proteins for site 3 in the diagrams in Fig. 6, and the generally low equilibrium degrees of formation of these proteins in Fig. 7. Therefore, the model proteins for site 3 are on average less stable, or have a greater energy of formation, than those from other sites. This result could imply that there are specific metabolic requirements of the organisms at site 3, near the photosynthetic fringe, that cause them to produce proteins that are relatively energetically demanding.The calculations described here may help elucidate the redox gradients between and cellular interiors and their surroundings. The interiors of bacterial cells such as Escherichia coli are more reduced than the growth medium in laboratory experiments [64], but much remains unknown about the redox conditions of microbial interiors in high-temperature environments. Fig. 9 shows that effective values of  in equilibrium with sulfur/sulfide, dissolved oxygen, and inferred Eh values, all measured on bulk water samples in the field, decrease as as a function of temperature (oxygen and Eh more so), but that the protein-equilibrium-based model equation for  is more reduced than any of these proxies. This finding implies that the interiors of the microbial cells, where the proteins are mainly present, are more reduced than the environmental conditions at the temperatures in the hot spring. There is, however, not a general agreement about the redox conditions inside microbial cells; relatively oxidized conditions would help to account for the probable frequent occurrence of disulfide bonds in proteins in archaeal organisms (including some hyperthermophilic representatives) [65]. It would be useful to have data on subcellular redox indicators (e.g. oxidized and reduced forms of glutathione) at high temperatures to help resolve these questions.Outlining the convergence of physical, chemical and biological forces that shape the information present in metagenomic sequences would benefit from the development of more sophisticated thermodynamic models and a tighter connection with phylogenetic approaches. For example, the construction of the model proteins could be based on identification of housekeeping genes that are conserved across phyla [66]. The use of only aligned sequences therein would help to eliminate some of the noise inherent in averaging shotgun sequence fragments, leading to a closer resolution of the differences between proteins from different environments.Comparing values such as the average oxidation state of carbon with the phylogenetic relationships of gene families that have appeared in different redox conditions [67] might reveal correlations between chemical composition of proteins and evolutionary constraints. Incorporating the energetics of protein-forming chemical reactions in such an analysis would permit even greater integration of available data on the organisms' environments. Comparative calculations of the energetics of overall protein formation reactions is conducive to integrative studies of microbial communities and environments because the energies depend on both molecular sequences and properties of the chemical environment. Extension of such an integrated thermodynamic framework could be a new way forward to quantifying the relationships between chemically distinct environments and their microbial communities."
        },
        "10.1371/journal.pone.0106218": {
            "author_display": [
                "Yuchen Han",
                "Mirjam Perner"
            ],
            "title_display": "The Role of Hydrogen for <i>Sulfurimonas denitrificans’</i> Metabolism",
            "abstract": [
                "\nSulfurimonas denitrificans was originally isolated from coastal marine sediments. It can grow with thiosulfate and nitrate or sulfide and oxygen. Recently sequencing of its genome revealed that it encodes periplasmic and cytoplasmic [NiFe]-hydrogenases but the role of hydrogen for its metabolism has remained unknown. We show the first experimental evidence that S. denitrificans can indeed express a functional hydrogen uptake active hydrogenase and can grow on hydrogen. In fact, under the provided conditions it grew faster and denser on hydrogen than on thiosulfate alone and even grew with hydrogen in the absence of reduced sulfur compounds. In our experiments, at the time points tested, the hydrogen uptake activity appeared to be related to the periplasmic hydrogenase and not to the cytoplasmic hydrogenase. Our data suggest that under the provided conditions S. denitrificans can grow more efficiently with hydrogen than with thiosulfate.\n"
            ],
            "publication_date": "2014-08-29T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 217,
            "shares": 0,
            "bookmarks": 0,
            "url": "http://www.plosone.org/article/info:doi/10.1371/journal.pone.0106218",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0106218&representation=PDF",
            "fulltext": "IntroductionEpsilonproteobacteria have been recognized as a physiologically versatile microbial lineage which appears to be relevant for element cycling in ubiquitous environments (cf. [1]). Many Epsilonproteobacteria have a chemolithoautotrophic lifestyle, where the oxidation of reduced sulfur compounds or hydrogen can be coupled to oxygen or nitrate reduction or where hydrogen can be oxidized with elemental sulfur as electron acceptor (cf. [1]). Among the Epsilonproteobacteria, Sulfurimonas is a genus of which currently only a few cultured representatives are known including S. autotrophica, S. sp. CVO, S. sp. YK-1, S. paralvinella, S. gotlandica, and S. denitrificans [2]–[7]. They have been isolated from a vast array of (sulfidogenic) habitats including marine sediments [5], deep-sea hydrothermal vent habitats [2], [3], pelagic redoxcline [4] and oil fields [6], [7].\nS. autotrophica was described as mesophilic, sulfur-, sulfide- and thiosulfate-oxidizing chemolithoautotroph [2]. It does not grow on hydrogen [2] and lacks hydrogen uptake activity [8]. S. sp. CVO grows only on sulfide and elemental sulfur with acetate as well as CO2 as sole carbon source [7]. S. sp. YK-1 utilizes sulfide, elemental sulfur, thiosulfate as well as hydrogen [6]. S. paralvinella is a mesophilic sulfur- and hydrogen-oxidizing chemolithoautotroph [3] and its crude extracts exhibit hydrogen uptake activity [8]. Only recently the chemolithoautotrophic S. gotlandica was isolated, which can utilize reduced sulfur species (elemental sulfur, sulfide and thiosulfate) as well as hydrogen [4]. Although S. denitrificans was initially grown on reduced sulfur compounds and characterized as a thiosulfate oxidizer [5], recent sequencing of its genome revealed periplasmic and cytoplasmic [NiFe]-hydrogenases [9]. Based on the information gained through genome sequencing, S. denitrificans could be successfully cultivated with hydrogen as the electron donor (cf. [9]), but hydrogen uptake activities are still not available and the role that hydrogen plays for its growth remains unknown.\nAlthough we know that certain Sulfurimonas species can utilize hydrogen [4], [6], [8], in the past this genus has been referred to as being a sulfur-oxidizing lineage [1]. In some hydrogen-rich hydrothermal environments 16S rRNA genes of Sulfurimonas species have been reported to be particularly abundant [10] and environmental hydrogenase gene sequences resembling those of S. denitrificans have been detected [11]. In summary, this data suggests that hydrogen may be more important for the energy metabolism of S. denitrificans and possibly also for other Sulfurimonas species than currently thought. Since several Sulfurimonas strains can utilize sulfur compounds as well as hydrogen, we here investigate the role that hydrogen plays for growth and metabolism in Sulfurimonas and use S. denitrificans as a model organism.\nMaterials and Methods\nBacterial strains and growth conditions\nBacterial strains and plasmids are listed in Table 1. E. coli strains were cultured in Luria–Bertani (LB) broth at 37°C. Shewanella oneidensis ΔhyaB derived stains were grown at 28°C, either aerobically in LB broth or anaerobically in fresh water medium (modified according to Lovley, supplemented with 15 mM pyruvate and 15 mM fumarate) [12] with H2/CO2 (80%/20%, 1 atm) (Westfalen AG, Münster, Germany) in the headspace. When required, antibiotics were used at the following concentrations: kanamycin 20 µg ml−1; gentamycin 10 µg ml−1.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Strains and plasmids used in this study.doi:10.1371/journal.pone.0106218.t001S. denitrificans was routinely grown in serum bottles in DSMZ medium 113 with 18.8 mM thiosulfate at 25°C under anaerobic conditions with H2/CO2 (80%/20%, 1 atm) in the headspace. These cultures served as pre-cultures (1 week growth) for further experiments with different medium. To test cell growth of S. denitrificans under anaerobic conditions, 1 mL of the pre-culture was transferred to fresh medium (125 mL). Four different incubation conditions were tested: (i) H2/CO2 (80/20%) with thiosulfate, (ii) H2/CO2 (80/20%) without thiosulfate, (iii) N2/CO2 (80%/20%, 1 atm) (Westfalen AG, Münster, Germany) with thiosulfate and (iv) N2/CO2 (80%/20%, 1 atm) without thiosulfate. To monitor the growth of S. denitrificans, the amounts of total protein in 1 mL cell culture at each indicated time point were measured (three times) according to Bradford as previously described [13], [14]. The experiments were performed three times and exhibited the same general trend, i.e. all H2/CO2 experiments with thiosulfate exhibited faster growth initially, but a lower growth yield overall relative to the H2/CO2 experiments without thiosulfate. For selected time points p-values were calculated using the student’s t-test.\n\n\nDetermination of nitrate and thiosulfate\nS. denitrificans was grown under the four different conditions as described above. At each indicated time point 1 mL culture was harvested by centrifugation. Supernatants were kept for nitrate and thiosulfate measurements, and cell pellets for total protein measurements (see above). For the measurement of nitrate in the medium the supernatant was diluted 1:20 with Milli-Q water and from this dilution 25 µL was taken for the measurement. Nitrate concentration was determined by high-performance liquid chromatography (HPLC) via ion-pair chromatography with a LiChrospher RP-18 column (5 µm; 125 by 4 mm; Merck KGaA, Darmstadt, Germany) [15] and UV detection in an automated system (HPLC-System LaChrom Elite, VWR International GmbH, Darmstadt, Germany). For the measurement of thiosulfate in the medium the supernatant was diluted 1:250 with Milli-Q water and from this dilution 250 µL was taken for the measurement. The amount of thiosulfate was determined by measuring the discoloration of methylene blue spectrophotometrically at 670 nm [16]. The measurements at the different time points were performed with three independent cultures.\n\n\nIn vivo hydrogen measurements\nFor in vivo hydrogen measurements S. denitrificans was grown anaerobically with H2/CO2 (80%/20%, 1 atm) as described above with thiosulfate (18.8 mM) or without thiosulfate. Controls were set up which consisted of all medium ingredients required for S. denitrificans cultivation but without inoculated bacteria. For every hydrogen measurement 0.25 mL of the headspace was extracted, diluted 1:2,300 with nitrogen (5.0; Westfalen AG, Münster, Germany) and from this dilution 2 mL of gas mixture was injected into the gas chromatograph (GC; Thermo Fischer Scientific Inc, Waltham, MA, USA). The amount of hydrogen in the headspace was measured by pulsed discharge ionization detector in the GC. ShinCarbon ST 100/120 (Restek, Bellefonte, PA, USA) was used as a column and helium as a carrier gas. These experiments were performed in triplicate. Statistics were performed using the student’s t-test.\n\n\nReverse transcription quantitative PCR (RT-qPCR)\nS. denitrificans was grown anaerobically with H2/CO2 (80%/20%, 1 atm) and thiosulfate as described above. Subsamples (10 mL) of the cultures were taken after three, six and twelve days of incubation. The total RNA was isolated with Presto Mini RNA Bacteria Kit (Geneaid Biotech, Taiwan). DNA was removed by using RTS DNase Kit (MO BIO Laboratories, Carlsbad, CA, USA). cDNAs were synthesized from 700 ng RNA with random primers by using SuperScript VILO MasterMix (Life Technologies, Carlsbad, CA, USA) in a final volume of 20 µL. A control reaction lacking reverse transcriptase was performed. PCR amplifications were carried out with cDNA product as template and specific primers (5′-ATATCGTAATGGCGGCAGAG-3′ and 5′-CATCAGGTCCAACAGTATCG-3′ for hydB, Suden_1435, the large subunit of the periplasmic hydrogenase; 5′-TGCGGAATATGTGGACATGC-3′ and 5′-ACTATCGCGTAAGAGGTGTG-3′ for Suden_1437, the large subunit of cytoplasmic hydrogenase; 5′-TGGATTCGCCAAGCAATCTC-3′ and 5′-GCGCCCATCATCTTCACTTC-3′ for rpoD, Suden_1105, RNA polymerase sigma factor) [9] by using SYBR Select Master Mix for CFX (Life Technologies, Carlsbad, CA, USA). The relative gene expression was calculated with Bio-Rad CFX Manager (Bio-Rad Laboratories, Hercules, CA, USA) and related to the gene expression of rpoD (housekeeping sigma factor). The experiments were conducted three times and the student’s t-test was performed for statistics.\n\n\nHydrogenase activity assay\nS. oneidensis strains and S. denitrificans were grown anaerobically with H2/CO2 (80%/20%, 1 atm) as described above. The hydrogenase activity assays were performed in three parallels. Subcellular fractionations were performed in an anaerobic chamber (Coy Laboratory Products, Grass Lake, MI, USA) as previously described [17]. The assays were performed as described before [17]–[19] with oxidized methyl viologen (MV, Sigma-Aldrich). In the presence of an active hydrogenase the MV is reduced to MV+ (blue form), which was quantified spectrophotometrically at 602 nm. All measurements were performed at 25°C and the hydrogen uptake activity was calculated by using an extinction coefficient of 5,401 M−1 cm−1 (personal communication, Nicolas Rychlik). The student’s t-test was performed for assessing the statistical relevance of the findings.\n\n\nConstruction of expression vectors\nThe 1.7-kb Suden_1435 (hydB, the large subunit of the periplasmic hydrogenase) [9] was amplified from the genomic DNA of S. denitrificans by using the primers Suden1435F (5′-ATGTCAAAAAGAGTAATAGTAGA-3′) and Suden1435R (5′-TTAAATAGTGCATCCGCCATA-3′). The fragment was ligated with pGEM-T (Promega, Mannheim, Germany), restricted with SacI and SacII, and religated to SacI/SacII restricted pBBR1MCS-2 [20]. The sequence and the direction of gene Suden_1435 were confirmed by sequencing (Eurofins MWG operon, Hamburg, Germany). The plasmids were transferred into S. oneidensis ΔhyaB by tri-parental conjugation with the helper plasmid pRK2013 [21].\n\nResults and Discussion\nGrowth of S. denitrificans on hydrogen and in vivo hydrogen consumption\nTo monitor the growth of S. denitrificans under different conditions, namely (i) H2/CO2 (80%/20%) with thiosulfate (18.8 mM), (ii) H2/CO2 (80%/20%) without thiosulfate, and as controls (iii) N2/CO2 (80%/20%) with thiosulfate (18.8 mM) and (iv) N2/CO2 (80%/20%) without thiosulfate, the amount of total protein was recorded at different growth phases. S. denitrificans cells grown anaerobically in DSMZ medium 113 grew significantly denser (p-value<0.001) when only hydrogen was present relative to when only thiosulfate was offered (Figure 1, Table S1) clearly indicating that besides thiosulfate oxidation, hydrogen oxidation is also used as an energy source in these cultures (Figure 1). In vivo hydrogen consumption measurements with S. denitrificans confirmed that hydrogen was being actively consumed when the headspace of the culture was filled with hydrogen, regardless whether thiosulfate was present or not (Figure 2).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Growth of S. denitrificans DSM-1251 under different cultivation conditions.S. denitrificans DSM-1251 cells were grown in DSMZ medium 113 containing thiosulfate (18.8 mM, filled marker) or without thiosulfate (open marker). The headspace was completely exchanged with H2/CO2 (80%/20%) (square) or with N2/CO2 (80%/20%) (triangle). Error bars indicate the standard deviations from measurements of three cultures.\ndoi:10.1371/journal.pone.0106218.g001\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  In vivo hydrogen consumption in S. denitrificans DSM-1251.S. denitrificans DSM-1251 cells were grown anaerobically in DSMZ medium 113 either with thiosulfate (18.8 mM) or without thiosulfate. On the first day of inoculation, the headspace of each sample was exchanged with H2/CO2 (80%/20%). The H2 concentration in the headspace was measured with gas chromatography. A control experiment was set up to exclude hydrogen leakage from the vessels. Medium = control, medium without inoculated S. denitrificans DSM-1251; S. den. = inoculation with 0.5 mL of S. denitrificans DSM-1251 pre-culture at day 1. Error bars of “Medium” and “S. den.” indicate the standard deviations from three independent experiments.\ndoi:10.1371/journal.pone.0106218.g002Based on protein contents, cells which were fed with hydrogen and thiosulfate grew significantly faster within the first three days (0.011 µg protein/µl culture per day) (p-value 0.003) than those fed with hydrogen and lacked thiosulfate (0.002 µg protein/µl culture per day) and reached maximum total protein concentrations of 0.041±0.001 and 0.047±0.003 µg/µl, respectively (Figure 1, Table S1). However, cells in medium with hydrogen in the headspace but without thiosulfate grew significantly denser overall (p-value 0.002) (Table S1). The initial rapid growth of cells grown on hydrogen and thiosulfate during the first three days correlated with a steep decrease in nitrate (19 mM was consumed) (Figure 3A) and thiosulfate (13 mM was consumed) (Figure 3B). By the fourth day all the nitrate was used up, thiosulfate oxidation ceased and cells reached the stationary phase by the fifth day. In contrast, cells grown on hydrogen but without thiosulfate exhibited less rapid growth and only consumed ~6.5 mM nitrate (over 14 days) and cells entered the stationary phase late, namely after 11 days. Also, significantly less hydrogen was consumed in cultures where thiosulfate was supplemented (30% of hydrogen consumed after 14 days of cultivation, i.e. ~11 mM) than in those lacking thiosulfate (65% of hydrogen consumed after 14 days of cultivation, i.e. 23 mM) (p-value 0.001) (Figure 2, Table S2).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Nitrate (A) and thiosulfate (B) consumption of S. denitrificans DSM-1251 under different cultivation conditions.S. denitrificans DSM-1251 cells were grown in DSMZ medium 113 containing thiosulfate (18.8 mM, filled marker) or without thiosulfate (open marker). The headspace was completely exchanged with H2/CO2 (80%/20%) (square) or with N2/CO2 (80%/20%) (triangle). Error bars indicate the standard deviations from measurements of three cultures.\ndoi:10.1371/journal.pone.0106218.g003Taken together, these data demonstrate that hydrogen can indeed serve as an electron donor for the growth of S. denitrificans and that under the conditions provided it can grow faster and denser with hydrogen than with thiosulfate. In contrast to experiments with S. paralvinella, where hydrogen is only consumed when reduced sulfur compounds are available [3], S. denitrificans can grow with hydrogen without reduced sulfur sources being present.\n\n\nHydrogenase transcript in S. denitrificans\nGenome sequencing of S. denitrificans revealed hydrogenases on its genome [9]: it encodes the hyd operon with one cytoplasmic hydrogenase, one membrane-bound hydrogenase, and accessory proteins for the hydrogenase assembly. The cytoplasmic hydrogenase has been posited to reduce electron acceptors with very negative redox midpoint potentials and may provide low-potential electrons to the rTCA [9]. If this proves true the need for a reverse electron transport is circumvented because reducing power can be generated directly and as a consequence the efficiency of growth is increased [9]. When cells were grown in the presence of hydrogen and thiosulfate, transcripts of the cytoplasmic hydrogenase were recovered (demonstrated by RT-PCR, data not shown). In many proteobacteria such as Wolinella succinogenes and Shewanella oneidensis, the periplasmic [NiFe]-hydrogenase serves as the major hydrogen uptake hydrogenase (Group I hydrogenase) [22]. This group of hydrogenases is capable of supporting growth with hydrogen as an energy source. They catalyze the oxidation of hydrogen to protons and electrons. The oxidation of hydrogen is coupled to the reduction of electron acceptors with the recovery of energy in the form of a proton motive force [23]. In S. denitrificans Suden_1435 encodes for the catalytic large subunit (HydB) and Suden_1436 for the small subunit (HydA) of the group I periplasmic [NiFe]-hydrogenase [9]. Our RT-qPCR illustrated that Suden_1435 (hydB) is indeed transcribed in S. denitrificans (Figure 4). RT-qPCR performed on the total RNA from S. denitrificans grown in the presence of hydrogen and thiosulfate further demonstrated that hydB was transcribed during the first 10-days of growth but then its transcription level decreased significantly (p-value<0.005) (Figure 4). The transcription level appeared to follow the general trend of growth of S. denitrificans (Figure 1, Figure 4). When S. denitrificans was in the exponential growth phase, the transcription level of the hydrogenases was highest and started to decline when the stationary growth phase was reached.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Transcript of hydB (Suden_1435) in S. denitrificans DSM-1251.S. denitrificans DSM-1251 cells were grown in DSMZ medium 113 with thiosulfate (18.8 mM) and H2/CO2 (80%/20%) in the headspace. The relative transcription levels of hydB were quantified by RT-qPCR and normalized to the reference gene rpoD (housekeeping sigma factor). The transcription level of hydB after 3 days was set as 1. Error bars denote standard deviations from three independent experiments.\ndoi:10.1371/journal.pone.0106218.g004\n\nHydrogenase activity of S. denitrificans HydB\nTo study the hydrogen uptake activity in vitro, we performed the activity assay with the soluble and membrane fractions of S. denitrificans. No hydrogen uptake activity was detected in the soluble fraction at the sampled time point (data not shown), although a cytoplasmic hydrogenase is encoded on the S. denitrificans genome [9] and transcripts of the cytoplasmic hydrogenase were recovered. The hydrogen uptake activity in the membrane fraction was 187±7 nmol of H2 oxidized (mg of protein)−1 min−1 (Figure 5), when S. denitrificans was grown with hydrogen but without thiosulfate in the medium. This is roughly 10-fold lower than hydrogen uptake activity measured in S. paralvinella [8]. However, the hydrogen uptake activity from S. denitrificans was higher than the hydrogen uptake activity detected in other proteobacteria, e.g., Gamaproteobacterium Hydrogenovibrio marinus [49 nmol of H2 oxidized (mg of protein)−1 min−1] and Betaproteobacterium Cupriavidus metallidurans [previously known as Alcaligenes eutrophus, 42 nmol of H2 oxidized (mg of protein)−1 min−1] at 37°C (in the same buffer we used) [18]. When S. denitrificans was grown in the medium with thiosulfate, an activity of 85±12 nmol of H2 oxidized (mg of protein)−1 min−1 was detected in the membrane fraction (Figure 5). This activity is significantly lower (p-value<0.001, roughly 2-times, Table S2) than the hydrogen uptake activity we measured for cells grown with hydrogen but without thiosulfate. These results are in line with the lower in vivo hydrogen consumption for S. denitrificans grown with thiosulfate (Figure 2) and together with thiosulfate consumption measurements (Figure 3B) suggest that if thiosulfate is present, under the provided conditions, S. denitrificans oxidized thiosulfate besides hydrogen which appears to result in lower hydrogen requirement.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Hydrogen uptake activity of the native and recombinant hydrogenase (HydB) from S. denitrificans.Hydrogen uptake activity was measured in the membrane fractions of S. denitrificans grown without and with thiosulfate (18.8 mM) and for the recombinant version of the large subunit (HydB). The large subunit of the periplasmic hydrogenase (hydB) was cloned into the vector pBBR1MCS-2 and expressed heterologously in S. oneidensis ΔhyaB. The empty vector in S. oneidensis ΔhyaB was used as negative control. Error bars denote standard deviations from three independent measurements.\ndoi:10.1371/journal.pone.0106218.g005To ensure that the periplasmic hydrogenase is responsible for the hydrogen uptake activity we cloned the large subunit of the periplasmic hydrogenase (hydB) in the vector pBBR1MCS-2. Recombinant expression of hydB demonstrated an activity of 444±49 nmol of H2 oxidized (mg of protein)−1 min−1 in the membrane fraction (Figure 5). This was in fact twice as high as the non-recombinant expression in S. denitrificans grown without thiosulfate, but can be explained by the copy number of the plasmid: pBBR1MCS-2 is a medium-copy-number plasmid [24] and thus several copies of the plasmid are present in the cell expressing the hydrogenase. In summary, these results indicate that the periplasmic hydrogenase is most likely if not solely responsible for the measured hydrogen uptake activity in the crude extracts.\n\n\nThe role of hydrogen for S. denitrificans metabolism\nS. denitrificans clearly exhibits different growth behaviors, hydrogen and nitrate consumption and hydrogen uptake activity depending on whether the cells are grown on hydrogen with or without thiosulfate. This is expressed in rapid growth, initially, but overall lower cell numbers, higher nitrate consumption (to the point of nitrate limitation), but lower hydrogen consumption (two-fold) and lower hydrogen uptake activity (two-fold) when thiosulfate is supplemented to the hydrogen medium. The reaction 5H2+2NO3−+2H+>>6H2O+N2 yields −959 kJ/reaction whereas 5S2O32−+8NO3−+H2O>>10SO42−+2H++4N2 yields -3926 kJ/reaction [25]. Hence, roughly four times more energy can be gained if thiosulfate is oxidized, but then also four times more nitrate is consumed relative to the reaction involving hydrogen. Organisms without the need of a reverse electron transport require 1060 kJ catabolic energy to fix one mol of carbon in biomass [26]. Assuming that with and without thiosulfate in the hydrogen medium the cytoplasmic hydrogenase catalyzes the reduction of NAD with hydrogen, thereby creating reducing power directly, theoretically considerably more biomass can be synthesized when thiosulfate is oxidized (3.7 mol carbon per five molecules thiosulfate) than when hydrogen is oxidized (0.9 mol carbon per five molecules hydrogen) with nitrate. This likely explains why the cells grown on thiosulfate grow faster initially than the cells grown on hydrogen without thiosulfate.\nIn the growth experiments with hydrogen and thiosulfate, according to the stoichiometry, the 13 mM consumed thiosulfate, if coupled to nitrate reduction, would theoretically account for the consumption of 19 mM nitrate, which is the measured amount of nitrate that was utilized in this experiment (Figure 3A). This would suggest that no nitrate reduction coupled hydrogen oxidation occurs, although 11 mM of hydrogen was used (Figure 2). While some of this hydrogen is likely essential to directly reduce NAD to produce reducing power directly, the hydrogen uptake activity of the periplasmic hydrogenase (although two-fold lower than when cells are grown without thiosulfate) (Figure 5) strongly suggests that energy generation in cells under these conditions is likely also linked to the build up of a proton motive force during hydrogen oxidation. Two scenarios could explain this discrepancy: based on S. denitrificans available genome information, genes encoding proteins that may be involved in the reduction of sulfur compounds including thiosulfate have been identified [9], which may account for some of the used thiosulfate. Hence, some of the nitrate may indeed be available for hydrogen oxidation coupled nitrate reduction. Hydrogen oxidation may also be coupled to sulfate reduction. S. denitrificans has an operon that encodes proteins of sulfate reduction [9]. However, the energy yield of 4H2+ SO42- +2H+ >> H2S+4H2O is considerably lower (154 kJ/reaction, [25]) than for nitrate reduction coupled hydrogen oxidation. However, based on the rapid cell growth in cultures with hydrogen and thiosulfate relative to cultures without thiosulfate we believe that thiosulfate oxidation represents the major mode of energy generation in these hydrogen incubations supplemented with thiosulfate and that under these conditions most of the consumed hydrogen is related to creating reducing power directly.\nWhen no thiosulfate is present, the cells revert to generating energy from hydrogen oxidation and twice as much hydrogen is consumed (Figure 2) and hydrogen uptake activity is doubled (Figure 5). In the experiments where cells were grown on hydrogen without thiosulfate, 6.5 mM nitrate was consumed when cells reached the stationary phase, which, according to the stoichiometry, could fuel the oxidation of ~16 mM hydrogen. In these cultivations around 23 mM hydrogen was used leaving theoretically ~7 mM hydrogen unaccounted for. As suggested above some of this hydrogen is likely used to circumvent the need for reversed electron transport, but the two-fold higher hydrogen uptake activity in these cultures also hint to the larger requirement of energy generation through hydrogen oxidation. Under these conditions some of the hydrogen oxidation may also be coupled to sulfate reduction - it has been postulated that the operon structure encoding the proteins for sulfate reduction can be turned on or off depending on the concentration of reduced inorganic sulfur species in the environment [9]. The lower energy yield through hydrogen oxidation coupled to nitrate or sulfate reduction relative to thiosulfate oxidation explains the slower growth yields. Since these cells are not limited by nitrate, as those in the hydrogen incubations with thiosulfate, growth can proceed to eventually reaching higher biomass than in the cultures with thiosulfate, which started being limited by nitrate after 3 days.\nFinally, based on our experiments we cannot state how much of the consumed nitrate is related to nitrate reduction coupled to thiosulfate oxidation versus hydrogen oxidation or how much hydrogen is used in the experiments for energy generation through nitrate or sulfate reduction or for production of reducing power. Further experiments will be needed to address these questions in detail.\n\nOur study provides the first experimentally shown evidence that S. denitrificans can express a functional hydrogen uptake hydrogenase and use hydrogen as electron donor. In fact, in our experiments S. denitrificans grew faster and denser on hydrogen than on thiosulfate alone. The cells could also gain energy through the oxidation of hydrogen in the absence of reduced sulfur compounds. Although thiosulfate oxidation yields more energy per reaction than hydrogen oxidation does, it also uses four times more nitrate than when hydrogen oxidation is coupled to nitrate reduction. Thus, when the cells oxidize thiosulfate they become limited by nitrate quicker than if they oxidize hydrogen. The many environmental sequences available in the databases resembling those of S. denitrificans may in fact not be solely associated with sulfur metabolism but may also represent microbes that are involved in hydrogen turnover. In fact, in the environment under nitrate limiting conditions hydrogen oxidation may be a favored metabolism."
        },
        "10.1371/journal.pone.0005348": {
            "author_display": [
                "Silvia Schumann",
                "Mineko Terao",
                "Enrico Garattini",
                "Miguel Saggu",
                "Friedhelm Lendzian",
                "Peter Hildebrandt",
                "Silke Leimkühler"
            ],
            "title_display": "Site Directed Mutagenesis of Amino Acid Residues at the Active Site of Mouse Aldehyde Oxidase AOX1",
            "abstract": [
                "\nMouse aldehyde oxidase (mAOX1) forms a homodimer and belongs to the xanthine oxidase family of molybdoenzymes which are characterized by an essential equatorial sulfur ligand coordinated to the molybdenum atom. In general, mammalian AOs are characterized by broad substrate specificity and an yet obscure physiological function. To define the physiological substrates and the enzymatic characteristics of mAOX1, we established a system for the heterologous expression of the enzyme in Eschericia coli. The recombinant protein showed spectral features and a range of substrate specificity similar to the native protein purified from mouse liver. The EPR data of recombinant mAOX1 were similar to those of AO from rabbit liver, but differed from the homologous xanthine oxidoreductase enzymes. Site-directed mutagenesis of amino acids Val806, Met884 and Glu1265 at the active site resulted in a drastic decrease in the oxidation of aldehydes with no increase in the oxidation of purine substrates. The double mutant V806E/M884R and the single mutant E1265Q were catalytically inactive enzymes regardless of the aldehyde or purine substrates tested. Our results show that only Glu1265 is essential for the catalytic activity by initiating the base-catalyzed mechanism of substrate oxidation. In addition, it is concluded that the substrate specificity of molybdo-flavoenzymes is more complex and not only defined by the three characterized amino acids in the active site.\n"
            ],
            "publication_date": "2009-04-29T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 17,
            "views": 3262,
            "shares": 0,
            "bookmarks": 17,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0005348",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0005348&representation=PDF",
            "fulltext": "IntroductionThe molybdenum cofactor (Moco) is found in a class of widely distributed proteins collectively known as molybdoenzymes. Moco containing enzymes are divided into three separate groups on the basis of structure, cofactor and spectroscopic characteristics: the dimethylsulfoxide (DMSO) reductase, the xanthine oxidase, and the sulfite oxidase family [1]. Members of the xanthine oxidase family comprise xanthine dehydrogenase (XDH, EC 1.17.1. 4), xanthine oxidase (XO, EC 1.17.3.2), and aldehyde oxidase (AO, EC 1.2.3.1). All these proteins are characterized by an equatorial sulfur ligand at the Moco essential for the enzymatic activity [2], and belong to the class of complex molybdo-flavoenzymes (MFEs) containing two nonidentical [2Fe-2S] clusters and FAD as additional cofactors [3], [4].\nVertebrate xanthine oxidoreductases (XOR) are the products of single orthologous genes and are the key enzymes in the catabolism of purines, oxidizing hypoxanthine to xanthine and xanthine to the terminal catabolite uric acid, with the concomitant reduction of NAD+ (XDH) or O2 (XO) [5].\nThe gene family of vertebrate AOs is much more complex and the corresponding protein products have been the object of fewer studies. While the human genome is characterized by a single functionally active AO gene (AOX1), the complement of homologous genes in rodents and other mammals is more complex [4]. Marsupials and rodents contain the highest number of functionally active AO genes, which generally cluster on the same chromosomal region at a short distance from one another [4]. The rodent AO gene cluster consists of the human AOX1 orthologue, and three highly related genes named AO homologue-1 (AOH1), -2 (AOH2), and -3 (AOH3) [4], [6]–[8]. In other vertebrates and mammals the number of functionally active loci varies, as a consequence of gene duplication and suppression events [4]. The multiple AO isoforms are expressed tissue-specifically [4], [7], and may recognize distinct substrates and carry out different physiological tasks. The overall level of amino acid identity between AO and XOR proteins is approximately 50%, which indicates that the two proteins originated from a common ancestral precursor [3].\nWhile the biochemical function of XOR is well established, the biochemical and physiological functions of AO are still largely obscure [4]. Monogenic deficits of mammalian AO isoforms have not been reported to date. In humans, AOX1 and XOR do not seem to play a vital role, as genetic deficiencies in the Moco sulfurase (MCSF) gene, causing a defect of both enzymes, are associated with mild symptoms such as the formation of kidney stones [9], [10]. AOs in general are characterized by a broad substrate specificity and play an important role in the metabolism of drugs and xenobiotica [3]. In animals, AOs have a significant toxicological role, detoxifying xenobiotics of wide structural diversity. The enzymes oxidize aromatic aza-heterocycles containing a –CH = N− chemical function (e.g. phtalazine and purines), aromatic or non-aromatic charged aza-heterocycles with a –CH = N+ =  moiety (e.g. N1-methylnicotinamide and N-methylphthalazinium) or aldehydes, such as benzaldehyde, retinal and vanillin [3]. AO and XOR share some common substrates and the relative selectivity of the two types of enzymes has been systematically reviewed [11]. AO may have a role in the degradation of vitamins like nicotinamide and pyridoxal or in the oxidation of all-trans retinaldehyde to all-trans retinoic acid, the active metabolite of vitamin A [3]. Upon oxidation of aldehyde substrates, AO produces significant amounts of highly toxic reactive oxygen species, O2.− and hydrogen peroxide [12], [13].\nThe reaction mechanism of substrate oxidation of MFEs has been only described in detail for Rhodobacter capsulatus XDH [14], a bacterial XDH sharing high similarities to eukaryotic XOR both at the structural level and on the basis of amino acid sequence identity [15], [16]. In the oxidized enzyme, the metal is in the Mo(VI) oxidation state, bearing an oxo ( = O), an hydroxo (-OH) and an equatorial sulfido ( = S) ligand. Site-directed mutagenesis showed that GluB730 is a fundamental residue for the catalytic reaction by abstracting a proton from the Mo-OH group, which then nucleophilically attacks the substrate carbon atom to be hydroxylated [14]. Two other conserved residues at the active site of XORs, GluB232 and ArgB310, are involved in substrate binding and transition state stabilization [14], [17]. In mouse AOX1 (mAOX1), the glutamate acting as an active site base is also highly conserved (E1265), however, the glutamate involved in substrate binding is exchanged by a valine (V806), and the arginine involved in transition state stabilization is exchanged to a methionine (M884) [3].\nTo study the importance of these amino acids in substrate specificity for AOX1, we established a system for heterologous expression of mAOX1 in E. coli. To ensure a high level of incorporation of the sulfido ligand at the Moco site, mouse MCSF (mMCSF) was cloned and coexpressed in this system. The recombinant enzyme was characterized by spectroscopic methods and steady state kinetics. Using site-directed mutagenesis, we exchanged the amino acids present at the active site of mAOX1 by the ones found in XOR. For comparison reasons, we introduced the reverse amino acid exchanges to the active site of R. capsulatus XDH. We determined the kinetic parameters of wild-type and mutant mAOX1 using different aldehyde and purine substrates, gaining insights into the molecular determinants guiding substrate specificity in XOR and AOX1.\nResults\nHeterologous Expression, Purification and Characterization of mAOX1\nSeveral heterologous expression systems for mammalian AOs have been described. However a main drawback of all these systems is the low catalytic acivity of the purified enzymes due to a low Moco content [18]–[20]. To overcome this problem, we designed an expression system that allowed the simultaneous expression of mAOX1 and mMCSF in the E. coli TP1000 strain, and ensured a higher level of sulfurated Moco insertion into the enzyme [21].\nRecombinant mAOX1 was purified from a 12-liter E. coli culture using sequential Ni-NTA chromatography, size exclusion chromatography and benzamidine sepharose affinity chromatography. The purification protocol used for mAOX1 is summarized in Table 1. The overall purification from the soluble fraction was more than 23-fold with a yield of 0.17%, and the final specific activity with benzalyldehyde of the purified protein was 2.3 U/mg. This compares well to the activity reported for mAOX1 purified from mouse liver [7]. We demonstrated that the low increase in specific activity after Ni-NTA is due to the intrinsic aldehyde oxidizing activity of E. coli enzymes of the xanthine oxidase family present in the crude extract (data not shown). Size exclusion chromatography of the purified protein resulted in a single peak with an approximate molecular mass of 300 kDa, showing that the protein existed as a homodimer in solution (data not shown). SDS polyacrylamide gels performed under reducing conditions demonstrated the presence of one major band with a size of 150 kDa after purification (Fig. 1). In these experimental conditions three additional bands with sizes of 120 kDa, 80 kDa, and 50 kDa were also observed (Fig. 1). Electrospray mass spectrometry analysis of the respective gel slices revealed that these bands were degradation products of mAOX1 (data not shown). These bands were also observed in AO purified from other sources, e.g. in AO purified from rat liver [12]. As observed in our data, AO purified from rat liver displayed several bands of sizes of 150, 130, 80, and 45 kDa on SDS- polyacrylamide gels, but not after native-PAGE [12]. Thus we conclude, that the degradaion products occur due the reductive conditions during the SDS-PAGE.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Purification of mAOX1 after heterologous expression in E. coli. 12% SDS-PAGE analysis of purification of mAOX1.The protein was purified after Ni-NTA, Superose 12 and benzamidine sepharose 6B. Purified mAOX1 displays a molecular mass of 150 kDa on SDS-PAGE, the three bands with sizes of 120 kDa, 80 kDa, and 50 kDa correspond to degradation products of mAOX1, as determined by mass spectrometry.\ndoi:10.1371/journal.pone.0005348.g001\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Purification of recombinant mAOX1 after expression in E. coli TP1000 cells.doi:10.1371/journal.pone.0005348.t001The visible absorption spectrum (Fig. 2) of recombinant mAOX1 is similar to those of mAOH1 purified from mouse liver and shows the presence of FeS and FAD as prosthetic groups [8]. The iron content was measured by ICP-OES and showed a saturation of 90% (Table 2). Since the ratio of 450:550 in the UV-Vis spectrum was shown to be 3, this implicates that the protein is fully saturated with FAD [1]. The Moco content of mAOX1 was quantified after its conversion to Form A and related to the molybdenum content of the protein (Table 2), revealing that no demolybdo-mAOX1 was present in the purified fractions. To determine the content of the terminal sulfur ligand required for mAOX1 activity, the cyanolysable sulfur was quantified and in addition, absorption spectra of oxidized mAOX1 and benzaldehyde reduced enzyme were recorded under anaerobic conditions (data not shown). From the reduction spectra the amount of active mAOX1 was calculated to be 20%. While purified mAOX1 was 70% saturated with Moco (Table 2), the purified enzyme was only 20% saturated with the sulfido ligand required for enzyme activity.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Characterization of wild-type mAOX1 by UV-VIS absorption spectroscopy.Spectra of 7 µM of the air-oxidized mAOX1 in 50 mM Tris, 1 mM EDTA, pH 7.5, under anaerobic conditions.\ndoi:10.1371/journal.pone.0005348.g002\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Determination of the Moco and iron content of mAOX1 and R. capsulatus XDH and variants.doi:10.1371/journal.pone.0005348.t002\n\nEPR spectroscopy of the mAOX1 FeS clusters\nFig. 3 shows the EPR spectra of the FeS clusters of dithionite-reduced mAOX1 wild-type (trace a), together with the corresponding simulations (traces b–e). The spectra show signals from the reduced FAD cofactor to the flavin semiquinone and from some remaining Mo(V). Most prominent are however the characteristic EPR signals assigned to the two iron sulphur centers FeSI and FeSII, which are similar for all members of the xanthine oxidase family that have been described to date [1], [22]. FeSI has EPR properties showing an almost axial g-tensor, similar to those of many other [2Fe-2S] proteins, being fully developed at relatively high temperatures (60 K), while FeSII has unusual EPR properties for [2Fe-2S] species with a strongly rhombic g-tensor, showing broad lines and being only observed at much lower temperatures (20 K). The g-values and linewidths were evaluated by simulating the superimposed spectra of FeSI, FeSII and reduced FAD (Fig. 3) using the program EasySpin [23]. The double-integrated simulated spectra for the single iron-sulfur clusters display a ratio of 1:1 indicating the presence of both clusters FeSI and FeSII in the same amount in the protein. The obtained g-values are given in Table 3. The flavin semiquinone (FAD) has been simulated by using an isotropic g-value of 2.0 and a linewidth of 1.9 mT. This linewidth is comparable with that from other flavins [1], [22]. The Moco (MoV) has been neglected in the simulations.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  EPR spectra of mAOX1 wild-type.Experimental cw-EPR spectra of dithionite-reduced mAOX1 wild-type samples at pH 7.0 (trace a) together with the corresponding simulation (trace b). For simulation parameters see Table 3. The flavin semiquinone was simulated with an isotropic g-value of giso = 2.0 and 1.9 mT (trace e). (MoV) was neglected in all simulations. (a) mAOX1 wild-type; (b) simulation of complete spectrum; (c) simulation of FeSI; (d) simulation of FeSII; (e) simulation of FAD. Experimental conditions: T = 20 K, 1 mW microwave power, 1 mT modulation amplitude, 12.5 kHz modulation frequency.\ndoi:10.1371/journal.pone.0005348.g003\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 3.  EPR linewidths and g-values of FeSI and FeSII from mAOX1.doi:10.1371/journal.pone.0005348.t003\n\nCD-Spectroscopy\nCD-spectra were measured in the visible region in both the reduced and oxidized forms (Fig. 4). The spectrum of the oxidized wild-type enzyme exhibited strong negative dichroic bands at approximately 350–400 nm and 520–580 nm, and intensive positive bands between 400 and 500 nm (Fig. 4). From the various maxima and infections, transitions can be identified at 378 (−), 434 (+), 474 (+), and 556 (−) nm. Upon reduction with dithionite, the spectrum changed markedly with less intense transitions at 370 (−), 408 (+), and 478 (−) nm. The visible CD-spectra of reduced and oxidized mAOX1 are very similar in shape and intensity to those of XOR [24], [25], showing that both FeSI and FeSII are present in the recombinant enzyme.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  CD-Spectroscopy of mAOX1 wild-type.Spectra were recorded in 50 mM Tris, 1 mM EDTA, pH 7.5 at 10°C using a Jasco J-715 CD-spectrometer. Spectra of 2.1 mg/mL of mAOX1 were recorded in the oxidized state (solid lines) and after reduction with sodium dithionite (dotted lines).\ndoi:10.1371/journal.pone.0005348.g004\n\nSteady state kinetics of mAOX1 wild-type and active site variants E1265Q, V806E, M884R, and V806E/M884R\nTo determine the role of amino acids Glu1265, Val806, and Met884 for the oxidation of aldehydes by mAOX1, the variants E1265Q, V806E, M884R, and the double variant V806E/M884R were generated and purified. Fig. 5 shows wild-type mAOX1 with the purified variants on a native polyacrylamide gel, showing that all proteins had about the same purity.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Native PAGE of mAOX1 wild-type and variants after purification.Purified enzymes were analyzed by 7% native PAGE. Each lane contained 6 µg of purified enzyme: lane 1, mAOX1 wild-type; lane 2, mAOX1-V806E; lane 3, mAOX1-M884R; lane 4, mAOX1-V806E/M884R; lane 5, mAOX1-E1265Q.\ndoi:10.1371/journal.pone.0005348.g005Since some of the variants were inactive with benzaldehyde, the Moco content of the variants was determined in comparison to wild-type mAOX1, and showed the following range of Moco saturation: E1265Q 60%, V806E 46%, M884R 66%, and V806E/M884R 44% (Table 2). The lower Moco content of the purified variants may be due to the modified purification protocol necessary for the purification of these mAOX1 variants, which did not bind to the benzamidine sepharose. The iron content of the variants varied between 83–108%, reflecting that the FeS clusters were not influenced by the amino acid exchanges at the active site (Table 2). In addition, the variants showed almost identical CD-spectra in comparison to wild-type mAOX (data not shown).\nWith the exception of retinaldehyde, steady state kinetics of wild-type mAOX1 were performed by varying the concentrations of the substrates benzaldehyde, phtalazine, acetaldehyde, xanthine and hypoxanthine, and by using DCPIP as electron acceptor (Experimental procedures). In consideration of the fact that the protein was only 20% active, the enzyme activities compared well to the values obtained for native mAOX1 [7]. In particular the kcat values determined with benzaldehyde were almost superimposable (Table 4). The kcat values determined for retinaldehyde and acetaldehyde were a little higher in comparison to native mAOX1, but were in the same range. The substrates with the highest catalytic efficiency were the aromatic aldehydes phtalazine, benzaldehyde and retinaldehyde. Acetaldehyde showed the lowest catalytic efficiency. The purine substrates xanthine and hypoxanthine were not used by the recombinant mAOX1 under our assay conditions.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 4.  Steady-state kinetic parameters of recombinant mAOX1 and variants with different aldehyde and purine substrates.doi:10.1371/journal.pone.0005348.t004The catalytic parameters for the mAOX1 variants E1265Q, V806E, M884R, and V806E/M884R were also determined. As shown in Table 4, variants E1265Q and V806E/M884R were unable to metabolize any of the aldehyde and purine substrates tested. While the corresponding kcat values were generally decreased, the Km value for benzaldehyde, phtalazine and acetaldehyde were drastically increased by introducing the V806E amino acid exchange in mAOX1. For the variant V806E, the catalytic efficiency decreased by a factor of 10 for benzaldehyde, retinaldehyde and acetaldehyde, while the value was only decreased 3-fold for phtalzine as substrate. Thus, the affinity for small and symmetric aromatic aldehydes are affected by this mutation, while the affinity for more hydrophobic aldehydes like retinaldehyde was increased, albeit with a concomitant decrease in kcat. In contrast, the M884R variant was inactive with phtalazine and acetaldehyde as substrates, and for both benzaldehyde and retinaldehyde kcat and Km were decreased.\n\n\nComparison of steady state parameters of the reverse amino acid exchanges (EB232V, RB310M, and EB232V/RB310M) introduced into R. capsulatus XDH\nThe catalytic mechanism of R. capsulatus XDH was proposed to involve three amino acids at the active site: GluB730 is thought to act as an active site base in the initial step of the reaction, while GluB232 is involved in substrate binding and transition state stabilization and RB310 is involved in transition state stabilization and orientation of the substrate at the active site. Since the catalytic mechanism of R. capsulatus XDH is well characterized, we compared the steady state kinetic data derived for wild-type mAOX1 and active-site variants with amino acid exchanges introduced into the active site of R. capsulatus XDH to reverse the amino acids found at the active site of XDH to the ones conserved in mAOX1. Thus, the XDH variants EB232V, RB310M, and the double variant EB232V/RB310M were generated.\nAs shown in Table 2, the Moco content of the purified XDH proteins was reduced and varied from 43%–63%, depending on the introduced amino acid exchange. Steady state kinetics of XDH were performed by varying the concentrations of the substrates benzaldehyde, phtalazine, retinaldehyde, acetaldehyde, xanthine and hypoxanthine, by using DCPIP as electron acceptor. Compared to wild-type XDH, the activity of the variants with purine substrates was drastically decreased, while oxidation of aldehydes as substrate was increased (Table 5). We were able to sucessfully purify the XDH-EB232V/RB310M double variant. This variant was devoid of purine oxidizing activity (Table 5), however, in contrast, aldehydes were oxidized with a higher turnover number in comparison to wild-type XDH. In general, the kcat values of the three XDH variants for all aldehyde substrates tested was increased, while the Km was decreased for benzadehyde and phtalazine and increased for retinaldehyde and acetaldehyde. Overall, the results obtained for R. capsulatus XDH show that the two amino acids at the active site switch the substrate specificity from purines to aldehydes. However, the reverse was not true for amino acids exchanges in the active site of mAOX1, showing that the oxidation of aldehydes is more complex.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 5.  Steady-state kinetic parameters of recombinant R. capsulatus XDH and variants with different aldehyde and purine substrates.doi:10.1371/journal.pone.0005348.t005\nDiscussionHere, we report a system for the heterologous expression of mAOX1 in E. coli. Our system tried to overcome one of the main problems associated with the expression of recombinant mammalian MFEs in bacteria, i.e. the production of large proportions of inactive demolybdo and desulfo enzymes, as observed in the case of human XOR [20]. To ensure a higher sulfuration level of AOX1, we engineered E. coli for the simultaneous synthesis of the mMCSF and mAOX1 proteins. After coexpression with mMCSF, mAOX1 contained a 50% higher level of the terminal sulfido ligand of Moco (data not shown), although only 20% of the purified enzyme existed in the catalytically active form. This suggested two possibilities: either the conditions for the expression of both proteins have to be further optimized, or the sulfuration of Moco in bacteria and eukaryotes is different. Thus in prokaryotes, sulfuration of Moco precedes insertion into MFEs [26]. In eukaryotes, like Arabidopsis thaliana it was speculated that Moco sulfurase sulfurates Moco already bound to MFEs [27]. Thus it remains possible that due to this difference, heterologous expression of mammalian MFE's in E. coli will not give rise to a complete sulfurated enzyme. However, we also tried expression of AOX1 in Pichia pastoris, and this expression system also did not give rise to a higher sulfuration level of AOX1. Thus, it also remains possible, that part of the sulfido ligand of mAOX1 is exchanged to an oxo-ligand during purification of the enzyme.\nThe recombinant mAOX1 displayed similar catalytic properties in comparison to the enzyme purified from mouse liver [7]. This demonstrates that the protein was correctly folded in mMCSF engineered E. coli cells and could be used for more detailed analyses. The EPR spectra of mAOX1 were found to be very similar to those from the bacterial XDH characterized from R. capsulatus, showing a rather axial signal for FeSI [28]. There are only subtle differences in the g-values and linewidths, in particular for the FeSII center, that may be the consequence of small changes in bond distances and angles of the ligands of the respective FeS centers [28]. Nevertheless, the overall close similarity of the EPR parameters indicated the presence of the same ligands and similar geometries of the two redox centers in mAOX1 and R. capsulatus XDH. The spectroscopic charactaristics of mAOX1 and rabbit liver AO were also similar [29] and are consistent with an axial signal for FeSI.\nEukaryotic XOR and AO are similar in protein structure and prosthetic group composition, but have different characteristics of substrate specificity both at the molybdenum center and at the FAD center [3], [11]. While mAOX1 is a true oxidase using molecular oxygen as electron acceptor at the FAD site, XOR exists in two interconvertible forms: NAD+ is the substrate for the XDH form of XOR, while after proteolytic cleavage or intramolecular disulfide formation, the XO form is generated, which uses O2 instead of NAD+. The hydroxylation of aldehyde and purine substrates is catalyzed at the molybdenum center in both AO and XOR. However, the former catalyzes the hydroxylation of aldehydes more efficiently than the latter [11]. On the other hand, purines such as hypoxanthine and xanthine are good substrates of XOR, but poor substrates of AO. The crystal structure of R. capsulatus XDH with alloxanthine bound to the active site [15], in addtion to site directed mutagenesis studies [14], proposed that the purine substrates are bound and oxidized as follows: Catalysis is initiated by abstraction of a proton from the Mo-OH group by the conserved active site GluB730, followed by the nucleophilic attack on the carbon of the substrate and the concomitant hydride transfer to the Mo = S of the molybdenum center [14]. This reaction yields an intermediate with the hydroxylated product coordinated to the molybdenum via the newly introduced hydroxyl group. The catalytic sequence is completed by displacement of the bound product from the molybdenum coordination sphere by hydroxide from solvent. This is followed by intramolecular electron transfer to the FeS and FAD cofactors, as well as deprotonation of the Mo-SH to give the oxidized form of the molybdenum center. In addition, GluB232 and ArgB310 of R. capsulatus XDH were shown to be involved in the binding, orientation and transition state stabilization of the substrate [14], [17].\nA similar reaction mechanism as the one described for XOR have been proposed for AOs [4], although, this proposal has never been supported by experimental data based on site-directed mutagenesis. Heterologous expression of mAOX1 in E. coli enabled us to study the involvement of specific residues at the active site in more detail for the first time. We exchanged residues Glu1265, Val806, and Met884 to the ones identified in the active site of XORs. For direct comparison reasons, the reverse amino acid exchanges were introduced to the active site of R. capsulatus XDH. Our results showed that the amino acid exchanges in XDH resulted in the complete loss of activity towards purine substrates for the EB232V/RB310M double variant, and a higher activity with aldehyde substrates. In contrast to Yamaguchi et al. [20], we were able to purify the EB232V/RB310M double XDH variant. However, the reverse was not the case for amino acid exchanges in mAOX1. The amino acid exchanges V806E/M884R in mAOX1 resulted in a complete loss of enzyme activity with both purine and aldehyde substrates. Both residues at the active site of mAOX1 seem to be important for the binding of substrates, since the M884R variant either completely lost the activity with phtalazine or acetaldehyde as substrates, or the activity with retinaldehyde and benzaldehyde was drastically reduced, with a concomitant increase of Km for benzaldehyde. In general, the V806E exchange resulted in an increase in Km for most substrates (except retinaldehyde) and in a decrease in kcat. Thus, while the ability to bind and utilize aldehyde substrates was decreased by converting the two residues to the ones found in XDH, purines were still not converted by the mAOX1 variants. While in XDH the two residues seem to determine which substrates are efficiently bound at the active site, in mAOX1 more factors determine the binding and conversion of substrates. mAOX1 seems to be more adapted to aldehyde substrates and the substrate specificity can not be converted back to purine substrates by two amino acid exchanges at the active site. The active site of XDH is deeply buried in the enzyme, but reachable through a funnel-shaped cavity that is wider on the surface. Hydrophobic residues, able to accommodate the ring structures of the aromatic substrates dominate the channel in XOR (e.g. LeuB303, ProB306, PheB344, PheB459 in R. capsulatus XDH) [15]. These residues are exchanged to more bulky, charged or hydrophobic amino acids in mAOX1: Glu877, Trp880, Phe918, and Ile1013 [3]. It is possible that the aromatic purine residues analyzed in this study, are unable to enter the active site of mAOX1.\nFinally, the mAOX1-E1265Q variant was catalytically inactive. This result is consistent with an essential catalytic role for this residue [4]. Like in XOR, Glu1265 of mAOX1 may function as an active-site base to abstract a proton from the Mo-OH leading to the nucleophilic attack on the aldehyde substrate molecule, which is followed by a hydride transfer to give the transition state intermediate (Fig. 6). The roles of residues Met884 and Val806 are proposed to be the stabilization of substate binding. Since the aromatic aldehydes tested in this study are symmetric, the orientation of the substrate at the active site is not as important as in XOR [17].\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Proposed base catalyzed mechanism for mAOX1.E1265 acts as an active site base that abstracts a proton from the Mo-OH group, which in turn undertakes a nucleophilic attack on the substrate benzaldehyde. After hydride transfer to the Mo = S group, the initial intermediate breaks down, with the transient formation of a paramagnetic MoV species, followed by displacement of product by a water molecule to return to the starting LMoVIOS(OH) state. The roles of residues Met884 and Val806 are stabilization of substrate binding.\ndoi:10.1371/journal.pone.0005348.g006In total, the established expression system of mAOX1 in E. coli can further be used for detailed site-directed mutagenesis and structure-function studies of mammalian MFEs. In future studies it is planned to characterize the homologues of mAOX1 in rodents in more detail. Analysis of substrate specificities will give insights into the potential physiological roles of these enzymes.\nE.coli TP1000 (ΔmobAB) cells [30] were used for the coexpression of mAOX1 wild-type and variants with mMCSF. E. coli expression cultures were grown in LB medium under aerobic conditions at 30°C for 24 h. Ampicillin (50 µg/ml) chloramphenicol (150 µg/ml), sodium molybdate (1 mM) and isopropyl-ß-D-thiogalactoside (IPTG) (20 µM) were used when required.The cDNA of mAOX1 was cloned from mouse CD1 liver [31], using primers designed to allow cloning into the NdeI-SalI sites of the expression vector pTrcHis [32]. The resulting plasmid was designated pSL205, and expresses mAOX1 as a N-terminal fusion protein with a His6-tag. By using PCR mutagenesis the amino acid exchanges V806E, M884R, V806E/M884R and E1265Q were introduced into mAOX1. The mMCSF cDNA fragment was cloned from mouse liver using primers for the cloning into the NdeI-XhoI sites of the expression vector pACYCDuet-1 (Novagen), and the resulting plasmid was designated pSS110.For heterologous expression in E. coli, pSL205 and pSS110 were transformed into TP1000 cells [30]. To express recombinant proteins, cells were grown at 30°C in LB medium supplemented with 150 µg/mL ampicillin, 50 µg/ml chloramphenicol, 1 mM molybdate, and 20 µM IPTG. After 24 h, cells were harvested by centrifugation, resuspended in 50 mM sodium phosphate buffer, pH 8.0, containing 300 mM NaCl and disrupted by several passages through a French Press cell. The supernatant was mixed with 3.5 mL of Ni-nitrilotriacetic (NTA) resin per 12 liters of cell growth. The slurry was transferred to a column and washed with 10 column volumes of 50 mM sodium phosphate, 300 mM NaCl, pH 8.0 containing 10 mM imidazole, followed by a wash with 10 column volumes of the same buffer containing 20 mM imidazole. His6-tagged mAOX1 was eluted with 50 mM sodium phosphate, 300 mM NaCl, pH 8.0 containing 250 mM imidazole. This fraction was dialyzed over night in 50 mM Tris-HCl, 200 mM NaCl, pH 7.5, containing 1 mM EDTA. The mAOX1 protein was further applied to a Superose 12 gel filtration column (GE Healthcare). The eluted fractions were analyzed by SDS-PAGE and the ones containing mAOX1 were combined and dialyzed against 100 mM Glycin, 200 mM NaCl, pH 9.0. The mAOX1 protein was mixed with 2 ml of benzamidine sepharose (GE Healthcare), pre-equilibrated in the same buffer. Following 4 h of incubation, the resin was washed 3 times with 10 ml of the equilibration buffer to remove unbound proteins. Absorbed proteins were eluted with 20 ml of equilibration buffer containing 30 mM benzamidine. Subsequently, the eluted protein was dialyzed into 50 mM Tris-HCl, pH 7.5 containing 1 mM EDTA. The protein solution was then concentrated by ultrafiltration. Absorbance spectra were recorded with a Shimadzu UV-2401 dual-wavelength double-beam spectrophotometer. Purified proteins were stored at −70°C until use. The mAOX1 variants were expressed under the same conditions as the mAOX1 wild-type and purified by Ni-NTA and Superose 12 chromatography.R. capsulatus wild-type XDH was purified as described previously [14]. Using PCR mutagenesis amino acid exchanges EB232V, RB310M, and the double variant EB232V/RB310M were introduced into R. capsulatus XDH. The generated XDH variants were purified by Ni-NTA, Q-Sepharose, and Superose 12 chromatography as described previously for other XDH variants [14]. The purified enzymes were concentrated by ultrafiltration, gel filtered using a PD-10 gel filtration column (GE Healthcare) equilibrated with 50 mM Tris, 1 mM EDTA, 2.5 mM DTT, pH 7.5 and stored at −70°C until used.SDS-PAGE was performed as described by Laemmli [33] using 12% polyacrylamide gels. The gels were stained with Coomassie Brilliant Blue R.Enzyme assays were carried out at 30°C in Tris buffer (50 mM, 1 mM EDTA, pH 7.5) in a final volume of 500 µL. Total enzyme concentration was 30 nM for wild-type mAOX1 and 30–200 nM for the variants. Enzyme activity was monitored spectrophotometrically at 600 nm with 100 µM 2,6-dichlorphenolindophenol (DCPIP) as electron acceptor and calculated using the extinction coefficient of 16.1 mM−1 cm−1 for DCPIP [34].To determine retinaldehyde oxidase activity, the assay described by Vila et al. [7] was used with some modifications. Purified mAOX1 (2 µg) was incubated in the dark for 10 min at 30°C in 100 µL of 10 mM potassium phosphate buffer, pH 7.4 and the reaction was started by the addition of 0.01–10 mM all-trans-retinaldehyde. The reaction was stopped with 100 µL of butanol/methanol (95:5 v/v) containing 0.005% w/v of butylated hydroxytoluene (0.005% w/v) (Sigma-Aldrich). The organic phase was separated and 20 µL were subjected to high performance liquid chromatography on a C18 reverse phase column (4.6×250-mm ODS Hypersil, 5 µm). The production of the oxidation product all-trans-retinoic acid was determined at 340 nm and quantified by using a calibration curves.Purified mAOX1 in 150 µL of 50 mM Tris, 1 mM EDTA, pH 7.5, was incubated in an anaerobic chamber (Coy Lab Systems) for 2 h at 4°C before benzaldehyde was added to a final concentration of 300 µM. Complete reduction was achieved by the addition of 20 mM sodium dithionite. Spectra were recorded in 0.15 mL cuvettes using a Shimadzu UV-2401 PC spectrophotometer.The molybdenum and iron contents of the purified proteins were quantified by ICP-OES analysis with a Perkin-Elmer Optima 2100 DV. The samples were wet-ashed at a concentration of 4 µM in a volume of 500 µL by the addition of 500 µM 65% nitric acid and incubated over night at 100°C. The samples were further diluted by the addition of 4 mL of water. As reference, the multi-element standard solution XVI (Merck) was used. The complete Moco content of mAOX1 in comparison to the variants was quantified after conversion to its fluorescent degradation product Form A as described earlier [35].UV/visible CD spectra of 2.1 mg/mL enzyme samples were recorded in 50 mM Tris, 1 mM EDTA, pH 7.5 using a Jasco J-715 CD-spectrophotometer. The scanning mode was set step-wise, each nm a data pitch was recorded, the response time was 2 seconds and each measurement was repeated 4 times.9.5 GHz X-band EPR spectra were recorded on a Bruker ESP300E spectrometer equipped with a TE102 microwave cavity. For temperature control between 5 K und 300 K an Oxford ESR 900 helium flow cryostat with an Oxford ITC4 temperature controller was used. The microwave frequency was detected with an EIP frequency counter (Microwave Inc.). Magnetic field was calibrated using a LiLiF standard with a known g-value of 2.002293±0.000002 [36]. Samples (typically 0.1 mM enzyme) were prepared in quartz tubes with 4 mm outer diameter. Chemical reduction, in order to generate the reduced Fe(II)/Fe(III) in the FeS clusters, was performed by adding a small volume of anaerobic sodium dithionite solution to the protein solution under a weak stream of argon gas (20-fold excess dithionite with respect to the protein). The sample tubes were frozen, after a change of colour was observed (typically 30 s reaction time) in liquid nitrogen. Baseline corrections, when required, were performed by subtracting a background spectrum, obtained under the same experimental conditions from a sample containing only a buffer solution. Simulations of the experimental EPR spectra, based on a spin Hamilton operator approach, were performed with the program EasySpin [23]. Second integrals from the simulated spectra of reduced FeSI and FeSII were used to estimate the relative amount of both clusters in the respective samples."
        },
        "10.1371/journal.pone.0022016": {
            "author_display": [
                "M. Carmen González-Mas",
                "José Luis Rambla",
                "M. Carmen Alamar",
                "Abelardo Gutiérrez",
                "Antonio Granell"
            ],
            "title_display": "Comparative Analysis of the Volatile Fraction of Fruit Juice from Different <i>Citrus</i> Species",
            "abstract": [
                "\n        The volatile composition of fruit from four Citrus varieties (Powell Navel orange, Clemenules mandarine, and Fortune mandarine and Chandler pummelo) covering four different species has been studied. Over one hundred compounds were profiled after HS-SPME-GC-MS analysis, including 27 esters, 23 aldehydes, 21 alcohols, 13 monoterpene hydrocarbons, 10 ketones, 5 sesquiterpene hydrocarbons, 4 monoterpene cyclic ethers, 4 furans, and 2 aromatic hydrocarbons, which were all confirmed with standards. The differences in the volatile profile among juices of these varieties were essentially quantitative and only a few compounds were found exclusively in a single variety, mainly in Chandler. The volatile profile however was able to differentiate all four varieties and revealed complex interactions between them including the participation in the same biosynthetic pathway. Some compounds (6 esters, 2 ketones, 1 furan and 2 aromatic hydrocarbons) had never been reported earlier in Citrus juices. This volatile profiling platform for Citrus juice by HS-SPME-GC-MS and the interrelationship detected among the volatiles can be used as a roadmap for future breeding or biotechnological applications.\n      "
            ],
            "publication_date": "2011-07-19T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 14,
            "views": 4811,
            "shares": 1,
            "bookmarks": 15,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0022016",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0022016&representation=PDF",
            "fulltext": "IntroductionDeveloping powerful platforms for volatile analysis is a prerequisite for further insights into the volatiles biosynthetic pathways and also in the identification of the genetic and environmental effects in volatile production [1], [2]. This information is relevant in the frame of current breeding programs in Citrus which are directed to respond to the market demand for quality fruits and are also important for the biotechnology of fruit and fruit derived product. One of the main characteristics of Citrus fruit quality is defined by the aroma of fruit juice. The aroma of a fresh juice is the product of a complex combination of several odour components that include esters, aldehydes, alcohols, ketones and hydrocarbons, which are collectively defined as volatile organic compounds or VOCs [3]–[6]. Headspace extraction coupled to GC-MS is at present the method of choice for most of the volatile analysis in food/flavour chemistry [7], [8] and particularly in Citrus [9]–[17], having displaced former methods that involved complex sample preparation and large amounts of solvents [18]–[20]. Some studies on the compositional analysis of Citrus juice aroma have been described which used dynamic and static headspace extraction [21], [22]. Different types of fibers have been used for Citrus juice analysis by HS-SPME [10], [13], [14], [23] but the one with three components: DVB/CAR/PDMS (divinylbenzene/ carboxen/polydimethylsiloxane) is the most widely used, because of its ability to extract a larger number of VOCs than other fibers [15], [17], [24], [25].\nSo far, almost all the studies on the aroma of Citrus juices had been conducted on orange juice, normally using one or at most two varieties. The fragmented information available together with the different techniques and fibers used, complicates the comparison of VOCs profiles between different Citrus varieties present in the literature [5], [10], [15], [16], [18], [19], [21], [26]–[28]. In contrast to oranges, only few studies have been conducted on mandarin [10], [11] and grapefruit aroma juices [4], [12], [29]. No studies have been performed for the volatiles in the juice of pummelo, and only one comparative study has been reported comparing mandarin and orange juices [3]. In this paper we describe the optimization of a VOCs capture/profiling method for Citrus and the characterization of the volatile profile for the juice of four Citrus varieties: Powell Navel summer orange, Clemenules clementine mandarine, and Fortune mandarine and Chandler pummelo hybrids. All four varieties are used as parentals in order to obtain new hybrids in breeding programs and at the same time they are themselves important varieties for fresh market in the world [30]. This is the first time that different varieties corresponding to different species are analysed in parallel using the same analytical technique and therefore enable us to describe both the volatile fraction in the juice and the variability in the volatile profile between the materials analysed. It is also the first time that the volatiles in the juice of pummelo are described.\nMaterials and Methods\nCitrus juice\nMature fruits at optimal ripening stage [31], were collected in 2007 from trees of Powell Navel Late sweet orange (Citrus sinensis (L.) Osb.), Clemenules (Citrus clementine Hort. ex Tan.), and two Citrus hybrids: Fortune (C. clementine x C. tangerine) and Chandler pummelo (C. grandis x C. grandis) varieties. All trees were grown in the same orchard and subjected to homogeneous cultural conditions, in order to reduce environmental effects on the volatile profile. The experimental orchard is located at the Experimental Station of Instituto Valenciano de Investigaciones Agrarias, Moncada, Valencia, Spain, under a mediterranean climate (averages rainfall of 515.8 mm and temperature of 15.2°C for 2007). In all cases, three biological replicate samples for each variety were obtained, each one representing at least four different fruits each. Fruit juice was obtained using a hand extractor, in order to avoid squeezing of the flavedo and to prevent contamination of the juice with peel components. After that, 10 mL aliquots of each sample were placed in 22 mL crimp cap headspace vials and kept frozen at −20°C until analyzed. Two aliquots of 10 mL corresponding to technical replicates of each sample were analyzed. The total number of analysis was 24 (3 biological samples x 2 technical replicates for the 4 varieties).\n\n\nHS-SPME extraction conditions\nRight before analysis, samples were thawed at 20°C for ten minutes and then were subjected to headspace solid phase micro-extraction (HS-SPME). Extraction was carried out using 10 mL of sample into a 22 mL crimp cap headspace vial. A 50/30 µm DVB/CAR/PDMS (Supelco, Bellefonte, PA, USA) fiber was used for all the analysis. Pre-incubation and extraction times were 10 and 20 min, respectively. A temperature of 50°C was selected for pre-incubation and extraction because it allowed the detection of a higher number of VOCs than when 30°C was used. Desorption was performed for 1 min at 250°C in splitless mode.\n\n\nGas chromatography–mass spectrometry conditions\nVOCs trapped on the fiber were analysed by GC-MS using an autosampler COMBI PAL CTC Analytics (Zwingen, Switzerland), a 6890N GC Agilent Technologies (Santa Clara, CA, USA) and a 5975B Inert XL MSD Agilent, equipped with an Agilent J&W Scientific DB-5 ms fused silica capillary column (5%-phenyl-95%-dimethylpolysiloxane as stationary phase, 60 m length, 0.25 mm i.d., and 1 µm thickness film). Oven temperature conditions were 40°C for 2 min, 5°C/min ramp until 250°C and then held isothermally at 250°C for 5 min. Helium was used as carrier gas at 1.2 mL/min constant flow. Mass/z detection was obtained by an Agilent mass spectrometer operating in the EI mode (ionization energy, 70 eV; source temperature 230°C). Data acquisition was performed in scanning mode (mass range m/z 35–220; seven scans per second). Chromatograms and spectra were recorded and processed using the Enhanced ChemStation software for GC-MS (Agilent).\n\n\nCompound identification\nCompound identification was based both on the comparison between the MS for each putative compound with those of the NIST 2005 Mass Spectral library and also with the match to our GC retention time and Mass Spectra custom library which have been generated using commercially available compounds. Compounds used as reference were of analytical grade and purchased from Sigma-Aldrich Química (Madrid, Spain), except for 2-carene, thymol and ledene, which were obtained from Extrasynthese (Genay, France). In addition to the commercial compounds, seven esters (methyl pentanoate, ethyl pentanoate, methyl heptanoate, ethyl heptanoate, methyl octanoate, methyl nonanoate, and ethyl nonanoate) were synthesized in our laboratory by acid-catalyzed esterification from analytical grade reagents. For that, 10 µL of the corresponding acid (pentanoic acid, heptanoic acid, octanoic acid, or nonanoic acid, supplied by Sigma-Aldrich) was added to 1 mL of the corresponding alcohol (methanol, ethanol) with 10 µL of H2SO4 96%, and incubated at 40°C overnight. After that, a small amount of sodium carbonate was added and incubated at 4°C for 24 hours, to neutralize any remaining acid. The solution was centrifuged and the supernatant used as a ≈1% standard solution of the ester in the respective alcohol. Also, 1 mL of either 100 ppb or of 1 ppm standard solutions was analyzed in the same conditions as the samples. Only those compounds/peaks confirmed by both mass spectrum and retention time in each and every chromatogram were considered. For relative quantification, the peak area was integrated from the extracted ion chromatogram corresponding to a specific ion previously selected for each compound. A mixture of extracts representing the four varieties analysed was injected regularly as part of the injection series and was used as a reference for correction for temporal variation and fiber aging. Finally, corrected results for each compound were expressed as relative ratios to the average level present in Chandler juice. When a compound was not detected in Chandler, the ratio was calculated to a variety that contained it as indicated in Table 1.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Relative levels (fold changes) of VOCs detected in juices of four Citrus varieties.doi:10.1371/journal.pone.0022016.t001\n\nStatistical analysis\nFor both Principal Component Analysis (PCA) and Hierarchical Cluster Analysis, the complete dataset including all replicates was considered. For both type of analysis, the ratio of the signal relative to that of the average in the four varieties was log 2 transformed. For PCA, the program SIMCA-P version 11 (Umetrics, Umea, Sweden) was used with the centered data. For the Hierarchical Cluster Analysis, the program Acuity 4.0 (Axon Instruments) was used, with the distance measures based on the Pearson correlation. Pearson correlation coefficients were calculated with the SPSS version 15.0 software (SPSS Inc., Chicago, USA). Data from the correlation matrix was represented as a heatmap by means of the Acuity 4.0 program.\n\nResults and DiscussionTable 1 lists the VOCs detected in our HS-SPME-GC-MS platform and the relative levels for the four varieties analyzed. A total of 109 compounds have been identified: 27 esters (19 aliphatic and 8 monoterpenic acetates), 23 aldehydes (18 aliphatic, 4 monoterpenic and 1 norcarotenoid), 21 alcohols (12 aliphatic and 9 monoterpenic), 13 monoterpene hydrocarbons, 10 ketones (8 aliphatic, 1 norcarotenoid and 1 monoterpenic), 5 sesquiterpene hydrocarbons, 4 monoterpene cyclic ethers, 4 furans and 2 aromatic hydrocarbons. It is important to note that although more than 300 VOCs have been reported in other Citrus juice [26], some of them have been identified only tentatively [16], [18], [19], [24]. To unequivocally assign chemical names to the compounds in our dataset, we have used analytical grade commercial compounds. Those compounds that were putatively identified by their mass spectra but were not confirmed with the commercial standard were not included in our dataset.\nAs a result eleven compounds out of a total of 109 are described here for the first time in the juice of Citrus species (6 esters, 2 ketones, 1 furan and 2 aromatic hydrocarbons) (Table 1); the remaining compounds have been described previously in Citrus juice samples [11], [16], [17], [24], [26], [32]–[34]. Almost all the detected compounds showed dramatic changes in the levels of accumulation in at least one of the four varieties (see Table 1). To better understand the usefulness of the volatile profile to define and distinguish the four Citrus varieties, a principal component analysis (PCA) was performed. Figure 1 shows that the first two principal components explain almost 80% of the variance, and clearly separate all four varieties from one another. The first component, explaining 54% of the variance, mainly separates Chandler pummelo from all the other varieties and to a lesser extent also Powell orange from both Clemenules and Fortune. The second component explains about 25% of the variance and clearly separates Clemenules from Powell and Chandler, while Fortune would be intermediate. Finally, the third component (Figure S1) essentially separates Fortune from the rest, and the analysis of the loading plots should reveal the part of the volatile profile which is characteristic of Fortune, and is responsible of roughly 13% of the total variance. These three components together explain as much as 92% of the total variance in the dataset.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Principal Component Analysis score plot (t[1] vs t[2]) for the first and second principal components.doi:10.1371/journal.pone.0022016.g001Analysis of the loadings plot reveals the compounds responsible of the separation between samples (Figure 2). The most relevant for the first component is a group of mostly terpenic compounds (β-caryophyllene, (Z)-ocimene, (E,E)-2,4-nonadienal, (Z)- and (E)-linalool oxides, p-cymene) which is almost exclusive of Chandler pummelo, and the compound octyl acetate, a metabolite present at relatively very high levels in Powell. The second component is defined by a group of compounds, mostly esters, with contrasting relative levels between Clemenules and Powell. The most relevant compounds contributing to the separation of Fortune from the other varieties are revealed by the loadings plot corresponding to the third component (Figure S2), and include propyl acetate, citronellyl acetate and ethyl acetate with higher levels in Fortune, and (E)-2-hexen-1-ol, eucalyptol, 3-carene and 1-decanol with lower levels in this variety.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Principal Component Analysis loading plot (p[1] vs p[2]) for the first and second principal components.Each number corresponds to a particular volatile compound, as indicated in Table 1.\ndoi:10.1371/journal.pone.0022016.g002A hierarchical cluster analysis confirmed that Clemenules and Fortune presented the most similar volatile profile, while Chandler pummelo exhibited the most differential profile of them all (Figure 3). According to the pattern of VOCs presented by these four varieties, volatile compounds can be organized in three clusters, named A, B and C, with some sub-clusters (named A1, A2, C1, C2 and C3). It is therefore revealed that clusters of VOCs with differential accumulation levels rather than a few individual compounds are responsible for the separation between varieties. For the sake of clarity, compounds in Table 1 are displayed according to the same order than in the hierarchical cluster.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Hierarchical cluster analysis of both samples and identified volatile compounds.Samples grouped themselves by varieties: Ch, Chandler; Cl, Clemenules; F, Fortune; P, Powell. Volatiles grouped in clusters A, B and C, and sub-clusters A1, A2, C1, C2 and C3. Colours in the heatmap mean the fold change, in accordance to the scale in the bottom: red for higher levels; green for lower levels. Colour circles before the name of the compounds describe the chemical family each particular compound belongs to: red, aldehyde; brown, ketone; orange, alcohol; yellow, ester; indigo, furan; pink, aromatic hydrocarbon; light green, monoterpene hydrocarbon; dark green, monoterpene cyclic ether; blue, sesquiterpene.\ndoi:10.1371/journal.pone.0022016.g003Correlation analysis of the volatile compounds was also performed, in order to assess how these metabolites were related to each other. When compared to the hierarchical cluster analysis, results are basically consistent. Basically, highly positively correlated volatiles were grouped in the same cluster, and compounds in distant clusters tend to show negative or non-significant correlations (Figure 4, Table S1). When descending to the metabolite to metabolite level, it can be observed a general pattern of high positive correlations of ester compounds to both their alcoholic precursor and other structurally similar esters. This suggests that the levels of these compounds, which show up to 500-fold variations between varieties, could be regulated both by enzymatic activity (by means of relatively specific alcohol acyl transferases) and by substrate availability. A strong negative correlation between ester and aldehyde levels is also observed. This also suggests an important role for alcohol dehydrogenase enzymes activity in the differences detected between the volatile profiles of Chandler, otherwise basically rich in sesquiterpenes and aliphatic aldehydes, and the other varieties with a volatile profile with higher abundance of alcohols and esters.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Heatmap of the correlation matrix of the volatile compounds.Positive correlations are shown in red; negative correlations in green; absence of correlation in black.\ndoi:10.1371/journal.pone.0022016.g004Compounds in the cluster A are present at higher levels in Chandler pummelo than in any of the other three varieties studied. Compounds which are basically exclusive of Chandler belong to sub-cluster A1 and include mostly monoterpene hydrocarbons and derivatives such as 2-carene, (Z)-linalool oxide, (E)-linalool oxide, (Z)-ocimene, p-cymene, and also (E,E)-2,4-nonadienal and nootkatone. Among the compounds in sub-cluster A1, 2-carene had only been identified so far in pummelo peel oil [35] and the sesquiterpene nootkatone has been frequently described in grapefruit juice [4] but rarely in other Citrus juices [18]; the remaining compounds in this subcluster have been identified also in Citrus juices [3], [17], [26]. Sub-cluster A2 includes aliphatic aldehydes from five to nine carbon atoms, and some olefinic aldehydes such as (E)-2-heptenal, (E)-2-octenal, (E)-2-nonenal, and (E,E)-2,4-decadienal, all of which have been described to provide herbal, fruity and floral aroma to Citrus juices [26], [32]. This sub-cluster also includes the compound 2-pentylfuran, reported previously only in tangerine [34], but identified in all four of our varieties in this paper. Cluster A included the only four sesquiterpenes unambiguously identified in our analysis: β-caryophyllene, nootkatone, α-copaene and valencene (β-farnesene was only detected at the level of traces in Powell), all of which had been previously reported in Citrus juices [17], [26]. However, the chromatograms of all varieties, and most notably those of Chandler, presented a large number of unidentified sesquiterpenes (as could be inferred from their MS spectra) which corresponded to the most abundant peaks eluting between 35 and 41 min (Figure S3). The close similarity of the mass spectra of many sesquiterpenes and the lack of standards makes this identification difficult, as it requires the use of purification steps and additional analytical techniques (such as NMR, and chemical synthesis) in order to identify their exact molecular structures. Therefore, although noted here, we did not include them in our approach.\nCluster B is defined by the compounds more abundantly found in Clemenules than in any of the other three varieties. These include a set of highly correlated carotenoid derivatives probably by the action of carotenoid cleavage dioxygenases: β-cyclocitral, β-ionone and geranylacetone (Figure 4, Table S1), and 3-pentanone, a ketone reported here for the first time in a Citrus juice. Other compounds in cluster B have also been previously described in Citrus juice [34] and they include 1-penten-3-one, 2-ethylfuran, 2-methylfuran, eucalyptol and the aldehydes (E)-2-pentenal, decanal, (Z)-3-hexenal, (E)-2-hexenal, and finally β-citronellal, which in our analysis was only detected in Clemenules.\nSub-cluster C1 includes compounds found more abundant in Powell than in the other three varieties. The monoterpene 3-carene and the esters methyl octanoate, methyl decanoate and heptyl acetate are the most important (heptyl acetate is exclusive of Powell variety). Methyl octanoate and methyl decanoate had never been described in Citrus juice, although the presence of many other esters had been previously reported in Citrus [16], [17], [26]. Sub-cluster C2 includes most of the compounds which accumulated generally to higher levels in Fortune than in other varieties, such as linalool or β-citronellol.\nFinally, sub-cluster C3 includes compounds which are present in smaller quantities in Chandler than in the other varieties studied. Included in this sub-cluster are monoterpene hydrocarbons such as α-phellandrene, limonene or γ-terpinene, all of which are generally described in Citrus juices [26]. Also neral and perillaldehyde aldehydes, and 3-methylfuran (the only one of the four furans detected here that had never been described in Citrus juice before) were less abundant in Chandler than in the other three varieties. Some furan compounds are considered to be originated from lipid oxidation [36], but our results suggest independent metabolic pathways for the synthesis of 2- and 3-alkyl furans. This is based in 2-methylfuran showing a very strong positive correlation to 2-ethylfuran and also to 2-pentylfuran in our samples, while no significant correlation was found to 3-methylfuran. Moreover, the majority of compounds included in this sub-cluster showed the highest levels in Powell variety, as it is the case for monoterpenes limonene, α-phellandrene and α-pinene, monoterpene acetates, aliphatic esters octyl-, nonyl-, and decyl acetate, ethyl octanoate, ethyl nonanoate and ethyl decanoate (ethyl nonanoate never been described in Citrus literature before) and the aromatic hydrocarbon styrene. Styrene and pseudocumene (other aromatic hydrocarbon synonymous of 1,2,4-trimethylbenzene) have been identified in all our four varieties for the first time. Of these two, only styrene have been described previously in Citrus commercial juices [21] but not pseudocumene although a compound with a similar structure, 1,4-diethylbenzene, have been reported previously in tangerine juice [37].\nSome volatile compounds commonly described in Citrus juices failed to be detected in our study (Table S2). Thus, no volatile acids were detected in the juices analyzed; in fact it is known that the contribution of the acids to the total aroma of the orange juice is very limited [24]. In addition, some esters usually described in the Citrus juice, such as methyl butanoate [3], [14], [15], [28], ethyl 3-hydroxyhexanoate [5], [16], [28], [33], or methyl o-(methylamino)benzoate [17] were not identified in our samples. Moreover some alcohols such as the aliphatic alcohols 2- and 3-methylbutanol [3] or the monoterpene alcohol borneol and sesquiterpene alcohols β-eudesmol and α-bisabolol [12] described in previous Citrus analysis were not found in our samples. Vanillin was not found in our samples either, although it has been described in many other studies in Citrus juices [5], [27], although this compound usually appears in juices that have undergone degradation due to exposure to high temperature [26]. This is also the case for some aldehydes identified in Citrus aroma, such as cuminaldehyde o (E)-2-undecenal [4], [13], or some C13-norisoprenoids such as β-damascenone or α-ionone identified previously in orange juice [24]. Overall lack of detection of some of those compounds in our samples could be due to these compounds not being present in our samples because of biological/environmental variability, although we cannot discard that differences in extraction and analytical techniques used (i.e. exposure of juices to high temperatures) or misidentification of those compounds in previous reports could be the reason.\nIn summary, over 100 volatile compounds have been unequivocally identified for the first time in the juice of four varieties of Citrus using the same analytical conditions, and therefore allowing us to perform more robust comparisons. Cluster and correlation analyses indicated interesting relationships between compounds and classes of compounds revealing the existence of interesting interactions between the biosynthetic pathways. Our results revealed also that the differences in the volatile profile in Citrus juice are mainly quantitative, and only a few compounds are variety-specific. What appears to be specific is the profile, i.e. relative content of a set of volatiles. Thus, according to the volatile profile, the most different varieties were Chandler and Powell, while Clemenules and Fortune were intermediate and very similar to one another. In Chandler the most characteristic volatiles were principally aliphatic aldehydes, sesquiterpenes such as nootkatone and monoterpenes such as 2-carene. Powell Navel orange showed the highest levels of esters such as nonyl acetate and of monoterpenes such as 3-carene. Clemenules showed the highest levels of ketones 3-pentanone and β-ionone and Fortune showed the highest levels of some acetate esters such as ethyl and propyl acetate, this latter almost Fortune-exclusive.\nVolatile profiling of Citrus juice by HS-SPME-GC-MS has proven therefore to be a highly valuable tool for the characterization of fruit from different varieties. The results and volatile platform described in this paper could be used as a roadmap to guide in the selection process of Citrus breeding programs directed to obtain new varieties with better aroma, to monitor industrial processes that may affect aroma, and also in the study of the pathways leading to volatile production in Citrus.\nFigure S1. \n            Principal Component Analysis score plot (t[1]\n             vs t[3]) for the first and third principal components.\n          doi:10.1371/journal.pone.0022016.s001(TIF)Figure S2. Principal Component Analysis loading plot (p[1] vs p[3]) for the first and third principal components. Each number corresponds to a particular volatile compound, as indicated in Table 1.doi:10.1371/journal.pone.0022016.s002(TIF)Figure S3. \n            Chromatograms representing each of the varieties analyzed: A, Chandler; B, Clemenules; C, Fortune; D, Powell.\n          doi:10.1371/journal.pone.0022016.s003(TIF)Table S1. Pearson correlation coefficients for each of the volatile compounds. Significant correlations (p<0.01) are highlighted in bold.doi:10.1371/journal.pone.0022016.s004(XLS)Table S2. \n            Volatile organic compounds injected as standard but not identified in any juice analyzed.\n          doi:10.1371/journal.pone.0022016.s005(DOC)"
        },
        "10.1371/journal.pone.0030510": {
            "author_display": [
                "Mika Ito",
                "Jan Johansson",
                "Roger Strömberg",
                "Lennart Nilsson"
            ],
            "title_display": "Effects of Ligands on Unfolding of the Amyloid β-Peptide Central Helix: Mechanistic Insights from Molecular Dynamics Simulations",
            "abstract": [
                "\n        Polymerization of the amyloid β-peptide (Aβ), a process which requires that the helical structure of Aβ unfolds beforehand, is suspected to cause neurodegeneration in Alzheimer's disease. According to recent experimental studies, stabilization of the Aβ central helix counteracts Aβ polymerization into toxic assemblies. The effects of two ligands (Dec-DETA and Pep1b), which were designed to bind to and stabilize the Aβ central helix, on unfolding of the Aβ central helix were investigated by molecular dynamics simulations. It was quantitatively demonstrated that the stability of the Aβ central helix is increased by both ligands, and more effectively by Pep1b than by Dec-DETA. In addition, it was shown that Dec-DETA forms parallel conformations with β-strand-like Aβ, whereas Pep1b does not and instead tends to bend unwound Aβ. The molecular dynamics results correlate well with previous experiments for these ligands, which suggest that the simulation method should be useful in predicting the effectiveness of novel ligands in stabilizing the Aβ central helix. Detailed Aβ structural changes upon loss of helicity in the presence of the ligands are also revealed, which gives further insight into which ligand may lead to which path subsequent to unwinding of the Aβ central helix.\n      "
            ],
            "publication_date": "2012-01-23T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 8,
            "views": 1798,
            "shares": 0,
            "bookmarks": 12,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0030510",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0030510&representation=PDF",
            "fulltext": "IntroductionAlzheimer's disease (AD) is one of the most common neurodegenerative disorders in aging people. According to the amyloid cascade hypothesis [1], [2], [3], accumulation of the amyloid β-peptide (Aβ) in the brain is the primary influence driving AD pathogenesis. Originally insoluble fibrils and plaques composed of Aβ were suspected to cause AD [1], [2], but currently prefibrillar aggregates including soluble oligomers composed of Aβ are also considered to be the cause of AD [3]. Aβ is produced mainly as a 40- or 42-residue peptide by proteolysis of an integral membrane protein, the amyloid precursor protein (APP). Nuclear magnetic resonance (NMR) data showed that Aβ(1–40) adopts a folded structure including two α-helical regions (residues 15–24 and 29–35) in water/sodium dodecyl sulfate (SDS) micelles which provide a water-membrane interface mimicking environment [4], [5], and that Aβ(1–42) adopts an unfolded structure including two β-strands (residues 17–21 and 31–36) in aqueous solution [6]. Using NMR it has also been shown that an Aβ(1–42) fibril is a β-sheet composed of two β-strands (residues 18–26 and 31–42) [7]. These structural data indicate that, once Aβ departs from the membrane to the extracellular fluid, its α-helical regions unfold to elongated or β-strand-like forms, and that the β-strands of Aβ enable formation of β-sheets of fibrils and prefibrillar aggregates.\nA wide range of molecules including small compounds and synthetic peptide derivatives have been identified as anti-amyloid agents [8]. Most of these molecules are predicted to bind to elongated or β-strand-like Aβ and to inhibit β-sheet extension, and thus they are expected to prevent Aβ polymerization. However, this strategy may be problematic in that it will favor formation of prefibrillar aggregates such as Aβ oligomers which are cytotoxic [9], and that some of the ligands may act as aggregators [10]. Alternative strategies to develop anti-amyloid agents are needed to overcome these problems. Earlier steps in amyloidogenesis before emergence of β-strand-like Aβ should be targeted to pursue alternative strategies. The emergence of β-strand-like Aβ can be inhibited by trapping Aβ in a state similar to its native structure in membrane embedded APP.\nRecent experimental studies [11], [12] demonstrated that trapping Aβ in a state similar to its native structure by stabilizing the Aβ central helix (residues 15–24) is an effective strategy to reduce Aβ polymerization and Aβ toxicity. Two different classes of ligands were designed to bind and stabilize the Aβ central helix, and it was shown that in the presence of either ligand, Aβ helical content was increased, the amount of Aβ fibrils was reduced, Aβ toxicity to PC12 cells in culture and to hippocampal slice preparations was reduced, and the lifespan of Drosophila model was prolonged [12]. Although many effects of the two ligands (Dec-DETA and Pep1b) are similar, there are also different effects on polymerization. That is, thicker-than-normal Aβ fibrils were detected in the presence of Dec-DETA, and shorter-than-normal Aβ fibrils were detected in the presence of Pep1b, though both ligands substantially reduced the amount of Aβ fibrils. The reason for this was not clarified in the experimental study. We suspect that there are differences in behavior toward Aβ between the two ligands.\nIn order to rationally design new compounds that more effectively stabilize the Aβ central helix and reduce Aβ polymerization into toxic assemblies, detailed molecular mechanisms that underlie unfolding and stabilization of the Aβ central helix should be elucidated. Elucidation of such detailed molecular mechanisms, which are difficult to analyze by using only experimental methods, is possible by taking advantage of computational methods like molecular dynamics (MD). The unfolding process of the Aβ helix has attracted much attention and has been studied by MD simulations [13], [14], [15], [16], [17]. However, effects of ligands on the unfolding process of the Aβ helix have not been fully investigated and detailed molecular mechanisms for the Aβ helix stabilization by ligands have not been uncovered yet, though short MD simulations indicated that the designed ligands stabilize the α-helical conformation of Aβ(13–26) [12].\nIn the present study, effects of the two ligands (Dec-DETA and Pep1b) which were designed in the previous experimental study [12] on the unfolding process of the Aβ central helix (residues 15–24) were investigated by MD simulations. The middle region (residues 15–24) of Aβ is of interest, because a short Aβ(16–20) fragment included in this region is capable of binding to full-length Aβ [18], [19] and to the fragment itself [20], [21], in addition, stabilization of this region in an α-helical conformation by mutations or by ligands counteracts Aβ polymerization into toxic assemblies [11], [12]. For the present study, we performed MD simulations for α-helical Aβ(13–26) in the absence or presence of either ligand, since our previous study [17] showed that MD simulations for the short peptide Aβ(13–26) represent the difference between wild-type Aβ and its mutants in good agreement with experimental data. Here we demonstrate that the two ligands are effective in stabilizing the Aβ central helix in agreement with experiments, and thereupon, we compare effects of the two ligands on unwinding of the Aβ central helix. Furthermore, we suggest a possible explanation to why the lower amount of fibrils formed from unwound Aβ monomers incubated with Dec-DETA and with Pep1b are thicker-than-normal and shorter-than-normal, respectively.\nMethods\nPreparation of Systems\nAn initial model structure of Aβ(13–26), whose sequence is HHQKLVFFAEDVGS, was built as an α-helix using the Insight II program (version 2000) [22], because the middle region (residues 15–24) of the full-length Aβ adopts an α-helical conformation surrounded by flexible unstructured regions in a water-membrane interface mimicking environment as shown by experimental studies [4], [5]. Available NMR structures (entry 1BA4 [4] in the Protein Data Bank [23]) of the full-length Aβ have eight or nine backbone O(i)-HN(i+4) hydrogen bonds in the fourteen-residue region (residues 13–26), including six backbone O(i)-HN(i+4) hydrogen bonds in the middle region (residues 15–24). We therefore built the whole peptide (residues 13–26) as an α-helix which has ten backbone O(i)-HN(i+4) hydrogen bonds. Since Aβ(13–26) is a fragment of the full-length Aβ, the N- and C-termini of our model were capped with N-terminal acetyl and C-terminal amide groups, respectively, mimicking the uncharged amide linkage that is adjacent to Aβ(13–26) on both ends in the full-length Aβ.\nStructures of the two ligand-peptide complexes were manually built using the Insight II program to satisfy the ligand-peptide contacts that were intended in their design: The Dec-DETA complex was designed for electrostatic interaction with E22 and D23 via the two basic functional groups and for van der Waals interaction with L17, V18, and A21 via the hydrocarbon tail (Fig. 1); similarly the Pep1b complex was built for electrostatic interaction with E22 and D23 via the two basic functional groups and with H13 and K16 via the two acidic functional groups, and for van der Waals interaction with F20 via the indole group (Fig. 1).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Initial structures of Aβ and the Aβ-ligand complexes.The initial energy-minimized structures of Aβ (A), the Aβ-Dec-DETA complex (B), and the Aβ-Pep1b complex (C) are shown. The positions of Aβ backbones (ribbons), Aβ sidechains (lines), and the ligands (lines and balls) are displayed. The structural formulae of Dec-DETA (B) and Pep1b (C) are also shown. The numbering of carbon (gray), nitrogen (blue), and oxygen (red) atoms is indicated. The residues of Aβ with which the different groups of the ligands are designed to interact (arrows) are also indicated.\ndoi:10.1371/journal.pone.0030510.g001According to an NMR structure (entry 1HZ3 [24] in the Protein Data Bank [23]) of the unfolded Aβ in water at pH 5.7, all the ionizable residues are in their charged states. Besides, in our previous study [17], we showed that similar results were obtained for the Aβ models regardless of the histidine protonation states. Therefore, all the ionizable residues of Aβ(13–26) were prepared in their charged states, where the basic residues (H13, H14, and K16) were protonated at the sidechain N atoms and the acidic residues (E22 and D23) were deprotonated at the sidechain O atoms. All the ionizable functional groups of Dec-DETA and Pep1b were also prepared in their charged states. The total charges of Aβ(13–26), the Aβ(13–26)-Dec-DETA complex, and the Aβ(13–26)-Pep1b complex are +1e, +3e, and +1e, respectively; the systems were neutralized by adding 1, 3, or 1 chloride counterions. Each model was solvated in a rhombic dodecahedron water box filled with TIP3P [25] water molecules with a minimum solute-wall distance of 10 Å. Water molecules with the oxygen atom less than 2.2 Å from any heavy peptide atom were deleted, and 3028, 3021, and 3007 water molecules remained in the Aβ(13–26), Aβ(13–26)-Dec-DETA, and Aβ(13–26)-Pep1b systems, respectively.\n\n\nMD Simulations\nAll calculations were carried out using the CHARMM22/CMAP force field [26], [27], [28] with the CHARMM program [29], [30]. The force field parameters for the ligands (Table S1) were picked from the CHARMM22 force field parameters for proteins, since the ligands were designed basically using amino acid moieties. The SHAKE [31] algorithm was applied to fix all covalent bonds containing a hydrogen atom allowing a 2 fs timestep to be used in the integration of Newton's equations. The nonbonded (van der Waals and Coulomb) interaction energies and forces were smoothly shifted to zero at 12 Å using the atom-based force-shift method [32], [33], and the nonbonded list was constructed with a cutoff of 16 Å and was updated every time any atom moved by more than 2 Å since the last update. Before MD simulations were carried out, structures of the solvated systems were optimized by 500 steps of steepest descent energy minimization with a harmonic restraint of 20 kcal/mol/Å2 on Aβ followed by 1500 steps of adopted basis Newton-Raphson energy minimization without a harmonic restraint on Aβ (Fig. 1). After the systems were heated up to 360 K gradually for 50 ps, ten independent 20 ns MD simulations at 360 K with different initial velocity assignments were carried out for each system to increase sampling [34]. The MD simulations were performed for the optimized systems under periodic boundary conditions at a constant pressure (1 atm) using the Langevin piston method [35] with piston mass 400 amu, collision frequency 20 ps−1 and bath temperature (360 K). The average temperature was checked every 4 ps, and was found to remain within 5 K of the target temperature after the heating MD run. Fast table lookup routines for non-bonded interactions [36] were used to increase speed of the MD simulations. During the MD simulations, no harmonic restraints were imposed on any molecule in the systems, and coordinates were saved every 1 ps.\nIn our previous study [17], we showed that the Aβ central helix completely unfolded at 360 K in 20 ns MD simulations, though it did not unfold at the lower temperatures (300 and 330 K). Therefore, the simulations for each system were performed at 360 K to accelerate dynamics of Aβ. Additionally, one control 20 ns MD simulation for each system was performed at 310 K with the methods used for the MD simulations at 360 K.\n\n\nAnalyses\nAll analyses were carried out for the trajectories obtained by the MD simulations at 360 K, except as otherwise stated. The data of every 10 ps of the trajectories after the heating time of the MD simulations were used for the analyses. Visualization of the structural change of the Aβ and Aβ-ligand complex models during MD simulations was carried out by using the visual molecular dynamics (VMD) software (version 1.8.6) [37].\nTo examine the structural change of Aβ quantitatively, the root-mean-square deviation (RMSD) and radius of gyration (Rg) were calculated for the middle region (15–24) of Aβ(13–26), thus large fluctuations of the RMSD and Rg due to the mobile N- and C-termini were eliminated. Before the RMSD measurements, overall rotation and translation were removed by least-squares superposition using coordinates of all heavy atoms of the initial energy-minimized Aβ structure obtained prior to the MD simulations. The RMSD was calculated for backbone heavy atoms against the initial energy-minimized coordinates and the Rg was calculated for all atoms along the MD simulation time.\nTo discriminate the type or the pattern of the Aβ structure, the number of α-helical O(i)-HN(i+4) backbone hydrogen bonds (αHBs) in the middle region (15–24) was calculated, using the criterion acceptor-hydrogen distance ≤2.4 Å to define the existence of a hydrogen bond [38].\nTo examine how each ligand interacted with Aβ during the simulations, the probability of the contact between the center of geometry of sidechain heavy atoms of each Aβ residue and each ligand heavy atom was calculated, using the criterion distance ≤6.0 Å. The distance criterion (6.0 Å) was chosen considering the contact distances measured for the initial energy-minimized structures of the Aβ-ligand complexes. A map of the contacts between Aβ and Dec-DETA or Pep1b was created using the calculated probabilities.\nTo determine details of polar interactions between Aβ and each ligand, the number of hydrogen bonds (HBs) between Aβ and Dec-DETA or Pep1b was calculated, using the criterion acceptor-hydrogen distance ≤2.4 Å. For this calculation, both of HBs between Aβ sidechain atoms and ligand atoms and HBs between Aβ backbone atoms and ligand atoms were counted (for each Aβ-ligand complex, the number of the latter HBs was less than 10% of that of all the HBs). When at least one HB between Aβ and the ligand was observed, the ligand was considered to be bound to Aβ.\nAdditionally, to determine details of nonpolar interactions between Aβ and each ligand, the number of C-C and C-N contacts between carbon atoms of the Aβ middle nonpolar part (residues 17–21) and nine heavy atoms of the Dec-DETA hydrocarbon tail (C1–C9) or of the Pep1b indole group (C13–C20 and N3) was calculated, using the criterion C-C or C-N distance ≤5.0 Å. The backbone carbonyl carbon atoms of Aβ were not included in this calculation. The distance criterion (5.0 Å) was chosen considering the radii of carbon (1.8–2.3 Å), nitrogen (1.9 Å), and hydrogen (1.3–1.4 Å) atoms and the C-H and N-H covalent bond lengths (1.0–1.1 Å) used in the CHARMM22 force field [26]. When at least one contact between the Aβ middle nonpolar part and the ligand nonpolar part was observed, the ligand nonpolar part was considered to be in contact with the Aβ middle nonpolar part. In this analysis, contacts between the Aβ middle nonpolar part and the ligand nonpolar parts (the hydrocarbon tail of Dec-DETA and the indole group of Pep1b) were focused on, because it was shown that, during the simulations, the ligand nonpolar parts were mainly in contact with the Aβ middle nonpolar part as they were designed (Fig. 1).\n\nResults\nEffects of the Ligands on Stability of the Aβ Central Helix\nTo examine whether the Aβ central helix eventually unfolded by the end of the simulation, the average backbone RMSD of the Aβ middle region (15–24) and the average number of αHBs of the Aβ middle region calculated for the last 2 ns of the each 20 ns simulation, where fluctuation of the Aβ backbone RMSD is relatively small in every trajectory, were analyzed (Table 1). The trajectories were classified into three groups: group A (RMSD<2.0 Å, 2≤αHB≤6), group B (2.0 Å≤RMSD<4.0 Å, 1≤αHB≤4), and group C (RMSD≥4.0 Å, αHB≈0). By visual inspection, it was ascertained that the Aβ central helix maintained its helical conformation during the whole simulations or refolded after partial unfolding by the end of the simulations in the group A trajectories, that it partially unfolded by the end of the simulations in the group B trajectories, and that it completely unfolded by the end of the simulations in the group C trajectories. The helical Aβ (group A) is observed in only one trajectory in the absence of a ligand, whereas it is observed in five trajectories in the presence of Dec-DETA and is observed in four trajectories in the presence of Pep1b (Table 1). In contrast, the completely unfolded Aβ (group C) is observed in three trajectories in the absence of a ligand, whereas it is observed in only one trajectory in the presence of Dec-DETA and is not observed in any trajectory in the presence of Pep1b (Table 1).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Average RMSD (Å) and average number of αHBs during the last 2 ns of the 20 ns MD simulations calculated for the Aβ middle region in the absence or presence of Dec-DETA or Pep1b.doi:10.1371/journal.pone.0030510.t001To examine behavior of the Aβ middle region during the simulations, the backbone RMSD during the whole simulations (Fig. 2A) and during the second half of the simulations (Fig. 2B) was calculated. By analyzing the backbone RMSD of the whole simulation of each trajectory, it was found that the Aβ helix was relatively stable during the first half of the simulations in five out of ten trajectories even if a ligand was not added to the system. For this reason, the second half of the simulations was used for this analysis. By visual inspection, it was determined that Aβ structures with small (RMSD<2.0 Å), medium (2.0 Å≤RMSD<4.0 Å), and large (RMSD≥4.0 Å) RMSD correspond to helical, moderately unwound, and highly unwound or elongated Aβ structures, respectively. Below we refer to these groups as peptide-conformation classes 1, 2, and 3, respectively. Both ligands, particularly Pep1b, increase the population of class 1 and decrease the population of class 3 (Fig. 2). During the second half of the simulations, the relative frequencies of class 1 and 3 in the presence of Dec-DETA are 1.6 and 0.5 times the frequencies for Aβ alone. In the presence of Pep1b the corresponding numbers are 2.1 and 0.2. Without a ligand class 3 is more populated than class 1 during the second half of the simulations, a situation which is reversed by both ligands (Fig. 2B).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Histograms of RMSD of Aβ in the absence or presence of the ligands.The histograms of the Aβ (black bars), Aβ-Dec-DETA (blue bars), and Aβ-Pep1b (green bars) systems are shown. The histograms were obtained using the data of the whole simulations (A) and the second half of the simulations (B) of all ten trajectories of each system. The relative frequencies of the appearance of the Aβ structures sorted out by the three levels of RMSD (RMSD<2.0 Å, 2.0 Å≤RMSD<4.0 Å, and RMSD≥4.0 Å) of the Aβ middle region are indicated. The relative frequencies were calculated against total time of all ten trajectories of each system.\ndoi:10.1371/journal.pone.0030510.g002The number of αHBs in the helix was calculated to further characterize the behavior of the Aβ middle region (Fig. 3). The relative frequency of Aβ structures with no αHBs is decreased by addition of both ligands, particularly by addition of Pep1b (Fig. 3). This aspect is observed especially in the second half of the simulations (Fig. 3B). The existence of Aβ structures with five or six αHBs is increased by addition of both ligands, particularly by addition of Pep1b. During the second half of the simulations, the probability to find at least five αHBs is 1.3 and 1.5 times higher for Aβ in the presence of Dec-DETA and Pep1b, respectively, compared to Aβ alone.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Histograms of the number of αHBs of Aβ in the absence or presence of the ligands.The histograms of the Aβ (black bars), Aβ-Dec-DETA (blue bars), and Aβ-Pep1b (green bars) systems are shown. The histograms were obtained using the data of the whole simulations (A) and the second half of the simulations (B) of all ten trajectories of each system. The relative frequencies of the appearance of the Aβ structures sorted out by the number of n αHBs (n = 0−6) of the Aβ middle region are indicated. The relative frequencies were calculated against total time of all ten trajectories of each system.\ndoi:10.1371/journal.pone.0030510.g003These results indicate that both addition of Dec-DETA and Pep1b are effective in stabilizing the Aβ central helix and that Pep1b is somewhat more effective than Dec-DETA.\n\n\nInteractions between the Ligands and Aβ\nTo examine whether the ligands were in contact with Aβ as they were designed (Fig. 1), the contact maps (Fig. 4 and 5) were analyzed. All contact probabilities are lower than 0.6, indicating that the ligands sometimes detached from Aβ. By visual inspection of the trajectories, we found both Aβ and the ligands to be quite flexible and that the ligands sometimes detached from Aβ but bound to Aβ again. High contact probabilities (0.4≤P<0.6) are observed for contacts between the basic functional groups (N2 and N3) of Dec-DETA and the acidic residues (E22 and D23) of Aβ and for contacts between the basic functional groups (N5, N7, and N8) of Pep1b and the acidic residues (E22 and D23) of Aβ. Contacts between the acidic functional groups (O1, O2, O4, and O5) of Pep1b and the basic residues (H13 and K16) of Aβ occur with medium probabilities (0.2≤P<0.3). Contacts between the Dec-DETA hydrocarbon tail (C1–C9) and the Aβ middle nonpolar part are distributed from L17 to A21 of Aβ, although the probabilities are low (0.1≤P<0.2). In contrast, contacts between the Pep1b indole group (C13–C20 and N3) and the Aβ middle nonpolar part are localized at F19 and F20 of Aβ, with a preference for F20 (0.2≤P<0.3). Thus, the contact maps show that the ligands were in contact with Aβ as they were designed, even though the ligands sometimes detached from Aβ.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Contact map of the Aβ-Dec-DETA complex.The probability (0.0≤P<0.6) of the contact between the center of geometry of sidechain heavy atoms of each Aβ residue and each Dec-DETA heavy atom is colored (white to blue grids). The probability was calculated using the data obtained from the whole simulations of all ten trajectories. The Aβ residues and Dec-DETA atoms corresponding to the X and Y-axis numbers, respectively, are listed below the map.\ndoi:10.1371/journal.pone.0030510.g004\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Contact map of the Aβ-Pep1b complex.The probability (0.0≤P<0.6) of the contact between the center of geometry of sidechain heavy atoms of each Aβ residue and each Pep1b heavy atom is colored (white to blue grids). The probability was calculated using the data obtained from the whole simulations of all ten trajectories. The Aβ residues and Pep1b atoms corresponding to the X and Y-axis numbers, respectively, are listed below the map.\ndoi:10.1371/journal.pone.0030510.g005Contact maps from simulations of both Aβ-ligand complexes at 310 K (Fig. S1 and S2) show higher probabilities (P≥0.6) than at 360 K, and the distribution of contacts in each Aβ-ligand complex is more localized at 310 K than at 360 K. This is because the conformations of Aβ and the ligands did not change so much and the ligands almost always bound to Aβ at 310 K, in contrast to the motions of Aβ and the ligands at 360 K. However, the pattern of contacts in each Aβ-ligand complex at 310 K is similar to that at 360 K, and the main contacts of each Aβ-ligand complex at 310 K are almost the same as those at 360 K. Although motions of the ligands and Aβ are enhanced due to the increased temperature, interactions between the ligands and Aβ at the relatively high temperature are thus similar to those at the body temperature.\nTo understand polar interactions between the ligands and Aβ, the existence of HBs between the ligands and Aβ was analyzed for the three peptide-conformation classes; the frequency of time when the ligands do not form any HBs with Aβ regardless of the peptide conformation was also calculated (Fig. 6A). In total, Dec-DETA and Pep1b form at least one HB with Aβ for 73% and 91% of the total time, respectively (Fig. 6A). When we consider only the helical class 1 conformations, Pep1b is in polar contact (hydrogen bonding contact) with Aβ 1.7 times as often as Dec-DETA (Fig. 6A). The fraction of the occurrence of the polar contacts for each peptide-conformation class (Table 2) shows that Pep1b binds to the Aβ structures in class 1 with higher probability than to the Aβ structures in classes 2 and 3, whereas Dec-DETA binds to all three peptide-conformation classes with similar probabilities. Besides, the fraction of the occurrence of the polar contacts for the class 1 conformations is higher for Pep1b than for Dec-DETA (Table 2). These data indicate that Pep1b binds more specifically to helical Aβ than Dec-DETA does. Additionally, the Aβ structures in class 1 form one more HB on average with Pep1b than with Dec-DETA (Table 3).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Histograms of polar and nonpolar Aβ-ligand contacts.The histograms of ligand contacts (Dec-DETA, blue bars; Pep1b, green bars) to the three peptide-conformation classes ((1) RMSD<2.0 Å, (2) 2.0 Å≤RMSD<4.0 Å, and (3) RMSD≥4.0 Å) were obtained using the data of the whole simulations of all ten trajectories of each system. In calculations of relative frequencies, the occurrence of the polar or nonpolar contacts for each peptide-conformation class was divided by total time of all ten trajectories of each system. (A) Relative frequencies of the polar contacts with at least one Aβ-ligand HB. (B) Relative frequencies the nonpolar contacts with at least one Aβ-ligand C-C or C-N contact. The contacts between the Aβ middle nonpolar part (residues 17–21) and the hydrocarbon tail of Dec-DETA (C1–C9) or the indole group of Pep1b (C13–C20 and N3) were used for this analysis.\ndoi:10.1371/journal.pone.0030510.g006\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Fractions of polar and nonpolar contacts between Aβ and Dec-DETA or Pep1b for each peptide-conformation classa.doi:10.1371/journal.pone.0030510.t002\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 3.  Average number of polar and nonpolar contacts between Aβ and Dec-DETA or Pep1b for each peptide-conformation classa.doi:10.1371/journal.pone.0030510.t003In a similar way, we analyzed the existence of nonpolar interactions (C-C and C-N contacts) between the nonpolar groups of the ligands (the hydrocarbon tail of Dec-DETA and the indole group of Pep1b) and the middle nonpolar part (residues 17–21) of Aβ for the three peptide-conformation classes; the frequency of time when the ligands do not have any C-C and C-N contacts with Aβ regardless of the peptide conformation was also calculated (Fig. 6B). In total, the nonpolar groups of Dec-DETA and Pep1b have at least one C-C or C-N contact with the middle nonpolar part of Aβ for 64% and 69% of the total time, respectively (Fig. 6B). When we consider only the class 1 conformations, Pep1b is in nonpolar contact with Aβ 1.4 times as often as Dec-DETA (Fig. 6B). The fraction of the occurrence of the nonpolar contacts for the class 1 conformations is higher for Pep1b than for Dec-DETA (Table 2). These data indicate that the indole group of Pep1b has contacts with the middle nonpolar part of helical Aβ more frequently than the hydrocarbon tail of Dec-DETA does. Additionally, the Aβ structures in class 1 have one more C-C or C-N contact on average with Pep1b than with Dec-DETA (Table 3).\nTo further understand interactions between the ligands and Aβ, we also anlyzed the existence of HBs between the ligands and Aβ for the three peptide-conformation classes in each individual trajectory (Fig. 7). The intermittent lines for the Aβ-Dec-DETA (Fig. 7A) and Aβ-Pep1b (Fig. 7B) complexes show that both ligands sometimes detach from Aβ and bind again to Aβ. Long durations of the ligands in hydrogen bonding contact with the class 1 conformations are more frequent for Aβ-Pep1b (Fig. 7B) than for Aβ-Dec-DETA (Fig. 7A). This shows that, compared to Dec-DETA, Pep1b binds to the helical conformations of Aβ more constantly and is thus more effective in stabilizing the Aβ central helix. In contrast, long durations of the ligands in hydrogen bonding contact with the class 3 conformations are more frequent for Aβ-Dec-DETA than for Aβ-Pep1b, indicating that Dec-DETA binds to the highly unwound or elongated conformations of Aβ for longer periods than Pep1b.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 7.  Timelines of Aβ-ligand contacts.Timelines showing the presence of at least one Aβ-ligand hydrogen bonding contact for Aβ-Dec-DETA (A) and Aβ-Pep1b (B), and for Aβ-Pep1b (C) also when both kinds of HBs between Aβ and Pep1b (between the Aβ acidic residue sidechains and the Pep1b basic functional groups, and between the Aβ basic residue sidechains and the Pep1b acidic functional groups) were formed at the same time. The ligand-binding events are distinguished by using different colors for the three peptide-conformation classes 1 (black bars), 2 (gray bars), and 3 (red bars).\ndoi:10.1371/journal.pone.0030510.g007In addition, to examine whether Pep1b binds to Aβ with both acidic and basic functional groups at the same time during the simulation, we analyzed events when both basic and acidic functional groups of Pep1b form HBs with the sidechains of the acidic and basic residues of Aβ, respectively, at the same time (Fig. 7C). All trajectories begin with Aβ in conformation class 1 and Pep1b bound with both acidic and basic groups, and in five trajectories (1, 4, 6, 7, and 8), this is also observed frequently for class 1 during the whole simulation, indicating that Pep1b can bind to helical Aβ with both acidic and basic functional groups at the same time from the beginning to the end of the simulation. In three of these trajectories (1, 4, and 7), Aβ maintained its helical conformation and had not unfolded by the end of the simulation (Table 1, group A).\nAs mentioned above, the group A trajectories exhibit non-unfolding or refolding of Aβ. In one of the group A trajectories of each complex, trajectory 1 of the Aβ-Dec-DETA complex and trajectory 2 of the Aβ-Pep1b complex, Aβ refolded to a helical conformation after being highly unwound during part of the simulations (Fig. 7A and 7B). Dec-DETA was bound to Aβ during the first partial unfolding (8–11 ns) and refolding (11–12 ns) events, and during the first half of the second partial unfolding event (13–16 ns) but not during the second refolding event (16–17 ns) in trajectory Aβ-Dec-DETA-1 (Fig. 7A). Pep1b was bound to Aβ during the partial unfolding event (11–15 ns) except for a short break (12.5–13.5 ns), and was bound to Aβ during the refolding event (15–16 ns) in trajectory Aβ-Pep1b-2 (Fig. 7B). By visual inspection, we found that the charged functional groups of both Dec-DETA and Pep1b formed constant polar contacts with the charged sidechains of Aβ when the ligands were bound to Aβ during the partial unfolding and refolding periods, whereas the nonpolar contacts were intermittent.\nAccording to our previous study [17], the Aβ central helix does not completely unfold in cases where any of the three steps of the three-step mechanism, which was proposed for the complete unfolding of the Aβ central helix, is missing: 1) sufficient loss of α-helical backbone hydrogen bonds, 2) strong interactions between nonpolar sidechains, and 3) strong interactions between polar sidechains. Here we observed that Aβ did not completely unfold due to the lack of steps 3 and 2 in the first and second partial unfolding events, respectively, in trajectory Aβ-Dec-DETA-1, and due to the lack of step 3 in the partial unfolding event in trajectory Aβ-Pep1b-2.\nThese data suggest that strong inter-molecular interactions between the ligand polar groups and the Aβ polar sidechains prevent intra-molecular interactions between the Aβ polar sidechains, thus blocking the third step of the unfolding mechanism in trajectories Aβ-Dec-DETA-1 and Aβ-Pep1b-2. In this way Aβ is inhibited from complete unfolding and instead Aβ refolding is facilitated.\n\n\nLigand-Binding to Unwound Aβ\nAs shown above, both ligands were able to bind to the unwound Aβ structures in the peptide-conformation class 3, and long durations of the ligand-binding for class 3 were more frequent for the Aβ-Dec-DETA complex than for the Aβ-Pep1b complex (Fig. 7). This result suggests that both ligands, particularly Dec-DETA, have the possibility of being involved in the polymerization which occurs after the unfolding of the Aβ central helix. To examine how the ligands interact with unwound Aβ, we analyzed the ligand-binding events for class 3 in each individual trajectory in detail. Details of two Aβ-Dec-DETA trajectories and one Aβ-Pep1b trajectory, which exhibit long durations of the ligand-binding for class 3, are described below. Note that similar features were observed in the other trajectories of each Aβ-ligand simulation.\nIn trajectory 3 of the Aβ-Dec-DETA simulation, Rg of Aβ reaches a peak (Rg≥7.5 Å) at around 17 ns (Fig. 8A), and one or two HBs between Dec-DETA and Aβ are formed at the time (Fig. 8B). At around the time of the Rg peak, β-strand-like forms of Aβ bound by Dec-DETA were observed, and a typical structure of these forms was obtained at 16.96 ns (Fig. 8C). In this structure, two HBs are formed between Aβ and Dec-DETA (Table 4), and the hydrocarbon sidechains of Aβ are located close to the hydrocarbon chain of Dec-DETA (The Cγ1(V18)-C5(Dec-DETA), Cγ1(V18)-C6(Dec-DETA), and Cγ2(V18)-C8(Dec-DETA) distances are 3.97, 3.93, and 4.00 Å, respectively.).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 8.  Structural changes of trajectory 3 of the Aβ-Dec-DETA system.The RMSD and Rg of the Aβ middle region (A) and the number of HBs between Aβ and Dec-DETA (B) are shown. The structure obtained at 16.96 ns (with large RMSD (4.12 Å), large Rg (8.03 Å), and two HBs) is also shown (C).\ndoi:10.1371/journal.pone.0030510.g008\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 4.  HBs formed between Aβ and Dec-DETA or Pep1b at the specific time.doi:10.1371/journal.pone.0030510.t004In trajectory 4 of the Aβ-Dec-DETA simulation, Rg of Aβ reaches a peak at around 14 ns (Fig. 9A), and at least two HBs between Dec-DETA and Aβ are formed at the time (Fig. 9B). β-strand-like forms of Aβ bound by Dec-DETA were observed at around the time of the Rg peak, and a typical structure of these forms was obtained at 14.29 ns (Fig. 9C). In this structure, four HBs are formed between Aβ and Dec-DETA (Table 4), and the hydrocarbon sidechains of Aβ are located close to the hydrocarbon chain of Dec-DETA (The Cγ2(V18)-C2(Dec-DETA), Cγ2(V18)-C3(Dec-DETA), and Cβ(A21)-C11(Dec-DETA) distances are 4.00, 4.00, and 4.13 Å, respectively.).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 9.  Structural changes of trajectory 4 of the Aβ-Dec-DETA system.The RMSD and Rg of the Aβ middle region (A) and the number of HBs between Aβ and Dec-DETA (B) are shown. The structure obtained at 14.29 ns (with large RMSD (4.59 Å), large Rg (8.48 Å), and four HBs) is also shown (C).\ndoi:10.1371/journal.pone.0030510.g009In both Aβ-Dec-DETA structures obtained at the times of the Rg peaks in trajectories 3 and 4, the hydrocarbon chain of Dec-DETA is located along the backbone of β-strand-like Aβ, and thus, β-strand-like Aβ and Dec-DETA form parallel conformations (Fig. 8C and 9C). The parallel conformations of the Aβ-Dec-DETA complex can be formed, due to the non-bulky conformation of Dec-DETA.\nIn trajectory 5 of the Aβ-Pep1b simulation, Rg of Aβ reaches peaks at around 5 and 9 ns (Fig. 10A). The number of HBs between Pep1b and Aβ is more than four at around 5 ns and is less than four at around 9 ns (Fig. 10B). These data show that Pep1b is tightly bound to the highly unwound or elongated Aβ at around the time of the first Rg peak but not at around the time of the second Rg peak. After the first and second Rg peaks, decreases in Rg are observed together with decreases in RMSD (Fig. 10A), and several HBs between Pep1b and Aβ are formed at these times (Fig. 10B), showing that Pep1b is bound to Aβ which adopts compact forms at these times.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 10.  Structural changes of trajectory 5 of the Aβ-Pep1b system.The RMSD and Rg of the Aβ middle region (A) and the number of HBs between Aβ and Pep1b (B) are shown. The structures obtained at 5.15 ns (with large RMSD (4.10 Å), large Rg (7.56 Å), and seven HBs), at 6.36 ns (with medium RMSD (3.64 Å), small Rg (6.16 Å), and six HBs), at 9.12 ns (with large RMSD (4.36 Å), large Rg (7.97 Å), and two HBs), and at 10.47 ns (with medium RMSD (3.45 Å), small Rg (6.56 Å), and four HBs) are also shown (C).\ndoi:10.1371/journal.pone.0030510.g010β-strand-like forms of Aβ bound by Pep1b were not observed at around the times of both Rg peaks, and instead, bent forms of Aβ bound by Pep1b were observed. Typical structures of these forms observed at around the times of the first and second Rg peaks were obtained at 5.15 and 9.12 ns, respectively (Fig. 10C). After the times of the first and second Rg peaks, compact and partially helical forms of Aβ bound by Pep1b were observed, and typical structures of these forms were obtained at 6.36 and 10.47 ns (Fig. 10C).\nAt the time of the first Rg peak (5.15 ns), seven HBs are formed between Aβ and Pep1b (Table 4), and the H14 imidazole ring of Aβ is located close to the indole ring of Pep1b (The Cδ2(H14)-C15(Pep1b) and Cδ2(H14)-C20(Pep1b) distances are 3.27 and 3.42 Å, respectively.). The backbone of Aβ is bent by the electrostatic interactions and by the auxiliary van der Waals interactions (Fig. 10C). After the time of the first Rg peak (6.36 ns), six HBs are formed between Aβ and Pep1b (Table 4), and the K16 sidechain and the F20 benzene ring of Aβ are located close to the indole ring of Pep1b (The Cγ(K16)-C19(Pep1b) and Cγ(F20)-C19(Pep1b) distances are 3.69 and 3.93 Å, respectively.). The helical form of the backbone of Aβ is partially (Q15-A21) reconstructed by the electrostatic interactions and by the auxiliary van der Waals interactions (Fig. 10C). At the time of the second Rg peak (9.12 ns), two HBs are formed between Aβ and Pep1b (Table 4), and the H13 imidazole ring and the F20 benzene ring of Aβ are located close to the indole ring of Pep1b (The Cδ2(H13)-C16(Pep1b) and Cγ(F20)-C20(Pep1b) distances are 3.63 and 3.66 Å, respectively.). The backbone of Aβ is partially (H13-F20) bent by the van der Waals interactions, though the backbone of Aβ is partially (A21-S26) elongated (Fig. 10C). After the time of the second Rg peak (10.47 ns), four HBs are formed between Aβ and Pep1b (Table 4), and the H13 imidazole ring and the F20 benzene ring of Aβ are located close to the indole ring of Pep1b (The Cγ(H13)-C17(Pep1b) and Cε1(F20)-C20(Pep1b) distances are 3.82 and 3.34 Å, respectively.). The helical form of the backbone of Aβ is partially (Q15-A21) reconstructed by the electrostatic interactions and by the auxiliary van der Waals interactions (Fig. 10C).\nAs shown in the Aβ-Pep1b structures obtained in trajectory 5, the basic and acidic functional groups of Pep1b can simultaneously interact with the sidechains of the acidic and basic residues of Aβ, respectively. In addition, the aromatic ring of Pep1b can at the same time interact with the aromatic rings of Aβ. Aβ therefore cannot easily convert to a β-strand-like form because of these electrostatic and van der Waals interactions. Even if Aβ would be assumed to be a β-strand-like form, parallel conformations of the Aβ-Pep1b complex cannot be formed, due to the bulky conformation of Pep1b.\n\nThe effects of the two ligands (Dec-DETA and Pep1b) on the stability of the Aβ central helix (residues 15–24) were investigated by using MD simulations. Detailed information on structural changes upon loss of helicity in the presence of the ligands was also examined, which might explain the observed difference in structures of Aβ fibrils in the presence of Dec-DETA or Pep1b.As indicated mainly by the Aβ backbone RMSD vs the initial structure and by the existence of αHBs of Aβ, the Aβ central helix completely unfolded by the end of the simulation in three out of ten trajectories in the absence of a ligand, whereas it completely unfolded in only one out of ten trajectories in the presence of Dec-DETA and did not completely unfold in any of ten trajectories in the presence of Pep1b. Compared to Aβ alone, the probability of the Aβ helical state (more than 2/3 of all the αHBs are formed) during the second half of the simulations is 1.3 and 1.5 times higher for Aβ in the presence of Dec-DETA and Pep1b, respectively. It was thus indicated that the stability of the Aβ central helix was increased by both ligands, in agreement with the experimental data [12]. It was also indicated that the ability of Pep1b to stabilize the Aβ central helix is higher than that of Dec-DETA, which was not shown in the previous experimental study [12].The analysis of the ligand-binding events clearly showed that Pep1b binds to the Aβ central helix longer time than Dec-DETA does. A main reason for this is that Pep1b has both basic and acidic functional groups which can simultaneously bind to the acidic and basic residues of Aβ, respectively, whereas Dec-DETA has only the basic functional groups. The inter-molecular interactions between the Aβ polar residues and the ligand polar functional groups are important in stabilizing the Aβ central helix, because they can prevent intra-molecular interactions between the Aβ polar residues that induce complete unfolding of the Aβ central helix [17]. An additional reason would be that Pep1b includes a centrally placed aromatic ring which straddles the Aβ middle nonpolar part (residues 17–21) when the basic and acidic functional groups of Pep1b simultaneously bind to the acidic and basic residues of Aβ, respectively. The inter-molecular interactions between the Aβ middle nonpolar part and the ligand nonpolar part are likely to be important in stabilizing the Aβ central helix, since the Aβ middle nonpolar part includes the three nonpolar residues (VFF) that have low α-helical propensities and high β-strand propensities [39], [40].This analysis also showed that both ligands can bind to highly unwound or elongated forms of Aβ. Dec-DETA was found to be able to form parallel conformations with β-strand-like forms of Aβ. In contrast, Pep1b was found not to be able to form parallel conformations with β-strand-like Aβ, due to the bulky conformation of Pep1b, and instead, Pep1b was found to bend unwound Aβ by the charge-charge interactions and by interactions between the aromatic rings. Therefore, it may be suggested that Dec-DETA could be included upon formation and extension of β-sheets to Aβ fibrils while being sandwiched between the two β-strands (residues 18–26 and 31–42) or being associated with the surface of a β-sheet, thus giving rise to fibrils with an alternative structure. On the other hand, Pep1b bound to unwound Aβ may disturb the extension of β-sheets.To summarize, it appears that Pep1b is somewhat more effective in stabilizing the Aβ central helix than Dec-DETA. In addition, the difference in conformations between the unwound-Aβ complexes bound by Dec-DETA and by Pep1b could be a reason why Aβ incubated with Dec-DETA and with Pep1b form thicker-than-normal and shorter-than-normal fibrils, respectively, as reported by the previous experimental study [12], though the physical and physiological consequence of Dec-DETA containing alternative fibrils in vitro and in vivo is unknown. Hence, our study indicates that, compared to Dec-DETA-like ligands, Pep1b-like ligands, which are capable of having charge-charge interactions with both the acidic and basic residues of the Aβ middle region, additional hydrophobic interactions with the Aβ middle nonpolar part, and bulky conformations, appear to be more effective in inhibiting unwinding of helical Aβ and also in preventing subsequent association of unwound Aβ."
        }
    },
    "subject-economics": {
        "10.1371/journal.pmed.0030073": {
            "author_display": [
                "Mpho Selemogo"
            ],
            "title_display": "The Money Issue",
            "abstract": [
                ""
            ],
            "publication_date": "2006-01-31T00:00:00Z",
            "article_type": "Correspondence",
            "journal": "PLoS Medicine",
            "citations": 0,
            "views": 1292,
            "shares": 0,
            "bookmarks": 5,
            "url": "http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0030073",
            "pdf": "http://www.plosmedicine.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pmed.0030073&representation=PDF",
            "fulltext": "Auvert et al. must be commended for showing some appreciation of the ethical issues raised by their research trial [1]. The Research Article itself and the accompanying ethical review by Cleaton-Jones [2], however, curiously seem to take the money issue lightly. The     PLoS Medicine Editorial is quite right in identifying the R300 payment to participants as an issue [3].\n\t\t\t\nRather than just identifying what R300 means in terms of the euro, we need an idea of the sum's effect on the average person enrolled in the study in order to best review issues of autonomy, which are often so problematic in such research. What was its impact on the recruitment process? Was the average income for the participants so low that declining to participate in the study and turning down the money was not an economically feasible option? The absence of such critical socioeconomic data leaves us wondering if this money was meant as a force for recruitment or indeed as a compensation for participation, as the authors assert.\n"
        },
        "10.1371/journal.pone.0056767": {
            "author_display": [
                "Petre Caraiani",
                "Emmanuel Haven"
            ],
            "title_display": "The Role of Recurrence Plots in Characterizing the Output-Unemployment Relationship: An Analysis",
            "abstract": [
                "\n        We analyse the output-unemployment relationship using an approach based on cross-recurrence plots and quantitative recurrence analysis. We use post-war period quarterly U.S. data. The results obtained show the emergence of a complex and interesting relationship.\n      "
            ],
            "publication_date": "2013-02-27T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 1,
            "views": 932,
            "shares": 0,
            "bookmarks": 3,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0056767",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0056767&representation=PDF",
            "fulltext": "IntroductionA key relationship in macroeconomics is the one between unemployment and output. This relationship is of prime importance from a purely pragmatic point of view: it is of a great interest to know how to model the way unemployment changes when a recession hits the economy and also how fast unemployment will recover. The policy implications of the relationship between output and unemployment are extremely important: they steer the longevity of an economy and of its government (for instance how viable is expansionary output in the future) and they (hopefully) make sure that changed levels of output can translate in better levels of employment. As we know, from casual inspection, i.e. by simply looking at today's economy, it is far from obvious what the precise relationship is between output and employment. Hence, studying it further and trying to characterize the relationship, via recurrence plots, as we attempt to do in this paper, can be an important mission. We believe that in this paper we are able to isolate patterns in the dynamics of unemployment and output which, we think, could not be otherwise isolated using standard econometric tools.\nIn macroeconomics the so called ‘Okun's Law’ refers to the regularity observed in the relationship between the changes in production (or the GDP of a country) and its unemployment rate. By ‘output’ we mean production or GDP (in economics, those terms are used in economics interchangeably). Friedman and Wachter [1] define Okun's law [2] as a fixed relationship between levels of unemployment and levels of the output gap. With the ‘gap’ is meant the difference between actual output and so called ‘potential output’ which is the level of output occurring at the natural rate of unemployment. Such a level of unemployment appears when all unemployment is purely of a ‘voluntary’ type.\nInitially, the two variables (i.e. unemployment and output) were estimated with the use of a simple linear regression. However, [1] remark the relationship in early papers, was estimated with the simplifying assumption that the trend rate of change of potential output would be i) exogenously given and ii) steady. In their own paper, [1] relax those two assumptions and many other economists since then have investigated other ways of improving on the original approach to Okun's law. Nalewaik, Diebold and Landefeld [3] point out that since the 1980's Okun's law has taken on a different character. They specifically mention that unemployment has become more responsive to changes in output.\nThe importance of this relationship is underscored by its link with the so called ‘Phillips curve’. In the Keynesian model, an increase in aggregate demand beyond the point of potential output will lead to very high levels of inflation. This is due to the fact that the aggregate supply curve becomes very steeply sloped close to potential output. The so called short run ‘Phillips curve’ [4] attempts to show that a higher (lower) rate of inflation is accompanied by a lower (higher) rate of unemployment. However, in the late 1960's, the Phillips curve became contested [1], [5], [6], [7], [8]. The expectations augmented Phillips curve which emerged out of Friedman and Phelps' work seems to have a close connection to the so called ‘New Keynesian economics’ [9], [10], [11].\nThis paper does not want to pretend to delve further into the economic intricacies of new Keynesian economics. Instead we want to focus on proposing a new approach to explain nonlinearities in the two key variables we mentioned at the beginning of this introduction. The increased awareness of the existence of nonlinearities in economics has surely influenced the way economists model the relationship between output and unemployment. Contributions have started to focus more on asymmetries and nonlinear dynamics mainly because it has been established that the dynamics of unemployment in a period of economic growth differ from those in a recession [12], [13], [14], [15]. However, as is (still) the case with most macroeconomic subjects, this topic has not yet been studied from the perspective of new nonlinear techniques from physics.\nHence, in this paper, we propose a re-evaluation of this key relationship using concepts and techniques from physics, like determinism measures and recurrence plots. Our approach is based on cross and quantitative recurrence analysis. Those methodologies will be presented in the paper.\nWe believe the contributions of this paper are as follows.\nFirstly, we analyse the underlying dynamics of unemployment and output using a novel approach which might lead to answers to well-known issues in the areas of nonlinearities and even determinism. We use recurrence plots and quantitative recurrence analysis to this end. Our findings may hopefully shed more light on whether business cycles are possibly characterized (or not) by a degree of determinism. This is a debate which is still not settled.\nSecondly, we analyse the relationship between the time series on unemployment and output (GDP) using cross recurrence plots and again quantitative recurrence analysis. We believe that through this approach we can reveal some aspects of the relationship between unemployment and output that would otherwise be hidden with a standard econometric approach.\nMethods\nBasic motivation for the use of recurrence analysis in economics\nRecurrence plots have been successfully used in the sciences for some time now and more recently it has found inroads in social science, notably in economics. We provide for relevant references below in the paper. We believe that the adoption of recurrence plots as a tool of analysis is based on the view that it is a useful methodology in understanding better the dynamics of single series or of relationships. We apply this approach to a topic that remains of very high interest to both the macroeconomics academic community and the general public: that is the relationship between production (or also output) and unemployment.\nWhile initial studies focused on the linear aspect of the ‘output’ and ‘unemployment relationship’, there is more and more awareness the relationship between production and unemployment may well be nonlinear in nature. We think the approach proposed by us can better highlight this nonlinear relationship. The nonlinear relationship has deep implications on a macroeconomics level and this was already argued for in the 1960's. The paper by [16] on the nonlinear theory of the employment cycle, for instance argues for a Phillips curve with a loop effect (i.e. a nonlinear effect). However, the quest for uncovering nonlinearities in the above relationship did not produce a steady stream of papers, since the 1960's. As an example, it was only in 2001 that [15] argues for a nonlinear relationship and he indicates that such nonlinearities allow for a better explanation of the varying (in) effectiveness of macroeconomic policies which target unemployment. The paper by [17] discussed in much detail what the consequences can be of policy-induced macro-economic recessions. This is indeed an important consequence given the current economic climate but it is also of importance within the wider European Community context, as [15] indicates, such nonlinear relationships affect aggregation (from a country level to a multiplicity of countries level, such as with the European community). In the work of [18] it is shown that with the use of wavelets, one can test for the presence of lower output volatility in US output (since the late 1940's). This lower volatility [18] claims to be a consequence of changes in a dynamic process. We wonder if this ‘dynamic process’ has a link to what we will call in our paper the periods of ‘dynamic discontinuity’. In summary with the few references cited here, we attempt to show that there is scope to use this methodology to uncover the possible presence of nonlinearities in the relationship between unemployment and output.\nRecurrence plot based techniques have been used to study the nonlinear relationships between different variables. The reference paper by Marwan and Kurths [19], introduces bivariate analysis (with an emphasis on nonlinearity) and it was also shown there that the bivariate recurrence plots can not only deal with linearity but also with the nonlinear aspect of nonlinearity. The possibility of using recurrence plots to pinpoint transitions which from an economics point of view are important is a key contribution from this methodology. As an example, the possibility of arguing for a transition to different states (what we call in our paper a ‘dynamic discontinuity’) during recessions is of great use. This characteristic that recurrence plots are very good at change point detection is highlighted in [20], [21].\n\n\nTheoretical background\nIn 1987 the term ‘recurrence plots’ was proposed in the paper by Eckmann et al. [22] and they emphasize the major benefit of using such plots: i.e. one can derive time information explicitly from a dynamical system.\nIn economics and finance, the methodology began to be applied many years after its inception in papers by [23], [24], [25], [26], [27], [28] and [29]. As one can appreciate the number of applications of this methodology in economics and finance is still small. It also needs to be said that most of those contributions focussed on a univariate based analysis.\nWe discuss in the paragraphs below the main points of this approach and we follow closely Marwan and Kurths [19] and Marwan [30].\nThe phase space trajectory dynamics can be represented as:\n(1)where  is the phase space trajectory;  is a time series with , and  is the sampling time; is the embedding dimension and is the time delay.\nThere are different approaches in setting the parameters corresponding to the embedding dimension  and the delay . See [19] for some suggestions. One defines the recurrence plot based on the following formula:\n\n          (2)\n        \nwhere  is a predefined parameter characterizing the distance between two neighbouring points; ∥ is the norm (normally the Euclidean norm is used); while  is a Heaviside type function.\nSince in this paper, we are interested in the relationship between unemployment and output, we will, besides using classical recurrence plots, also use cross recurrence plots. Amongst the first authors to extend recurrence plots into cross recurrence plots were Zbilut, Giuliani and Webber [31]. Re-consider, equation (2) from above, but alter the ingredients in the norm:\n\n          (3)\n        \nwhere and  are respectively the reconstruction of the first and second series (different dimensions are possible) in the phase space. Again, we can visualize important features of the relationship between the two systems. Long diagonal lines indicate similar behaviour in the phase space. See [19] and [30].\nSeveral measures derived from quantitative recurrence analysis will be used in this paper. Quantitative recurrence analysis is a further development of recurrence plots and is due to contributions from [32] and [33].\nIn what follows below, we present the definitions of the main complexity measures we will employ in this paper. We continue using [19] and [30]. We will also be using the univariate approach (as originally proposed in [32]).\nThe recurrence rate, , is defined as:\n\n          (4)\n        \nwhere  is the distribution of the diagonal line lengths (for a diagonal parallel to the main diagonal);  is the length of the line structure;  is an index of the relative location of the diagonal line; is the dimension of the vector . measures thus the density of recurrence points [30]. The  can also be used as an indicator of changes in the dynamical system.\nWe also consider the ‘determinism’ measure,  which is given by:\n\n          (5)\n        \nLow values indicate stochastic systems, while higher values are an indication of a degree of predictability. The intuition with this measure is more subtle. Although [30] indicates gives the predictability of the system, we need to keep in mind that predictability and determinism are not obvious equivalences. In fact [30] cautions about the reliability of the measure. The and are examples of so called ‘Recurrence Quantification Analysis’ (RQA) (see [32]). RQA measures are statistical measures and [34] makes the important remark that estimating the confidence of such measures is still an open question. Consequently, the and measures have not yet received a lot of attention as to their statistical robustness.\nIn the case of a bivariate analysis, another important measure is the average diagonal length, :\n\n          (6)\n        \nIt indicates the degree of coincidence of two systems.\nIn the case of univariate analysis, a measure of entropy is used. The Shannon entropy, , is:\n\n          (7)\n        \nwhere  is defined in function of the cumulative distribution of the line length.\nThe  measure is also an example of Recurrence Quantification Analysis (RQA). , and  are measures which are based on diagonal lines. The laminarity and trapping time measures in equations (8) and (9) below follow [30] and are based principally on vertical lines in a recurrence plot. As [30] indicates, laminar states refer to ‘chaos-chaos transitions’, while the RQA measures refer to chaotic- periodic state transitions. The laminarity measure, , is defined as:\n\n          (8)\n        \nwhere indicates the distribution of vertical line lengths.\nFinally, the trapping time measure,  is given by:\n\n          (9)\n        \n\nResults and DiscussionWe use one of the longest U.S. data series available for unemployment and output. The unemployment rates are taken from the U.S. Bureau of Labour while the output figures are taken from the U.S. Department of Commerce, Bureau of Economic Analysis. We emphasize we want to use U.S. data in this paper, as most of the studies written on this topic were performed with the help of U.S. data. The sample ranges from the first quarter of 1949 to the last quarter of 2010. We have opted for a quarterly frequency in order to ensure a high number of observations. The data were ensured to be stationary following the procedure in [33].\nA first step in implementing quantitative recurrence analysis requires the determination of the key parameters, namely the embedding parameter , and the delay parameter . Recall that those two parameters were first encountered in equation (1) above. We must pay particular attention to the presence of noise which proves a distorting factor in computing the key parameters and . To aid in this, ‘mutual information’ is used in the case of the delay parameter and the ‘false nearest neighbours technique’ is employed when determining the embedding dimension. See also our brief discussion above equation (2).\nIt has been observed by [24] that a sufficiently large embedding would be sufficient to contain all relevant dynamics. In his application, he chooses an embedding of order 10 in order to take into account the high complexity of human dynamics. However, for the case of the lag he suggested the use of a lag 1 in the context of discrete economic data (financial data in his case).\nWe use the false nearest neighbours approach to determine the embedding dimension for both unemployment and output. We found an embedding dimension of 4 for each of the series. For the delay parameter, given the discussion in the literature, we simply choose a delay parameter  equal to 1.\n\nRecurrence plots\nUsing the values determined above for the key parameters and , we perform a recurrence analysis of the two series (i.e. unemployment and output) we are interested in (see figures 1, 2, 3, 4, 5, and 6 below). We note that we are not using the log-change in output, but instead the annual growth of output as well as the unemployment rate. This is one of the main approaches in selecting the data, which is originally due to [24]. It is also used in [7] and [35]. Only recently in work starting with [36], do we see that the output gap can be used as a complementary alternative to the growth rate of output.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Unthresholded Recurrence Plot for output with Euclidean Distance.doi:10.1371/journal.pone.0056767.g001\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Thresholded recurrence plot for output with Euclidean distance and\n              \n                \n              \n              .\n            \ndoi:10.1371/journal.pone.0056767.g002\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Unthresholded Recurrence Plot for unemployment rate with Euclidean Distance.doi:10.1371/journal.pone.0056767.g003\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Thresholded recurrence plot for unemployment with Euclidean distance and\n              \n                \n              \n              .\n            \ndoi:10.1371/journal.pone.0056767.g004\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Untresholded cross recurrence plot for unemployment and output with Euclidean Distance\n              \n                \n              \n              .\n            \ndoi:10.1371/journal.pone.0056767.g005\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Thresholded cross recurrence plot for unemployment and output with Euclidean distance and\n              \n                \n              \n              .\n            \ndoi:10.1371/journal.pone.0056767.g006A third parameter must be determined, the radius parameter , which also plays a crucial role in the results obtained through the recurrence plots. As [24] observes, choosing a too small value  would lead into quantifying noise only, while a too high value would result in capturing values that are not really recurrent. One strategy [24] suggests is to compute the recurrence percentage for increasing values of the radius until a scaling region is reached. Other approaches in the literature are based on setting the radius based on the idea of obtaining reasonable recurrence rates (see for instance [28] and [33]). In a study on the Taiwanese unemployment rate, [29] used a value for  of 0.65. Since we will be studying unemployment rate series too, we will be using a value close to the one by [29], namely.\nAlthough the interpretation of the figures 1, 2, 3, and 4 (see below) is not straightforward, there are some indications in the literature on how one may read such figures. Eckmann et al. [22] provides for a very intuitive account on how to read some of the plots. Marwan [30] classifies patterns into large scale and small scale and according to his classification; he found four different types of large scale patterns or typologies.\n\n\n\n\nHomogeneous typologies, characterizing white noise;\n\nPeriodic typologies, when recurrence plots present diagonal lines and checkerboard structures, for the case of oscillating systems;\n\nDrift typologies, when there is fading in the upper left and lower-right corners, for systems which are not stationary;\n\nDisrupted typologies, for extreme (and rare) events, when white bands are present, indicating transitions.\n\nThe second class, of small scale structures, can be characterized through the following characteristics:\n\n\n\n\nSingle, isolated points, for rare states;\n\nDiagonal lines, when the system visits the same region of the phase space at different moments;\n\nVertical horizontal lines, when the system either does not change or it changes slowly. This phenomenon is also an indicator of intermittency.\n\nLooking at the recurrence plot figures for unemployment and output, figures 1, 2, 3, and 4, we can clearly reject the idea of random processes as the recurrence plots are not exhibiting a homogeneous typology.\nWe find several white vertical bands that make transitions between different block characterizing periods when the system behaved in a similar way. These bands correspond basically to the more severe recessions from 1973–1975 and 2009 and they can indicate also transitions in the dynamics. The patterns indicate a process which is to a degree predictable. A changing process occurs after 1975.\nFigures 5 and 6 show the results from applying cross recurrence to both unemployment and output series. The dynamics of unemployment and output, as evidenced from the recurrence plots in figures 1, 2, 3, and 4, are again presented in figures 5 and 6. We note that the evidence suggested by figure 6 points to additional unusual patterns, as underlined by the ‘S’ shapes of segments. This might indicate some possible modifications in the relationship at lag levels. These features would be hard to uncover using standard econometric approaches (we thank two referees of this paper for this important insight). We observe again white bands corresponding to the recessions in 1973–1975 and 2009–2010. They mark again transition periods between two periods with similar dynamics. Within these periods, the dynamics are rather similar.\nThese results indicate that the recent literature on the output-unemployment relationship that stressed nonlinearities, time-dependency or state switching is indeed correct. However, our figures go even further, stressing the presence of a degree of predictability and the possibility of transition periods corresponding to the recessions in 1973–1975 and 2009–2010. Such results could be further tested using techniques from econometrics.\nWe note that the possibility of a changing nature of the business cycle is acknowledged in the economic literature. For example, starting with the early 1980s, the economic literature points to the emergence of the Great Moderation, a period of lower volatility that is characterized by specific dynamics, as recessions are smaller and lower in intensity. This period ended with the Great Recession, the last global economic and financial crisis. A key reference to the Great Moderation and how the business cycles changed after 1985 is the paper by [37]. A more general perspective on the historical changes in business cycles is provided for by [17]. There are other recessions, like the ones in the 1970s that are also known to have marked the transition between different types of business cycles. The literature acknowledges that, in general, the macroeconomic relationship between different variables (as is the case of output and unemployment) may change during severe downturns. Thus, we think we can link the evidence through recurrence plots with the idea of a changing nature of business cycles, since through recurrence plots we can identify the changing dynamics of a time series.\n\n\nQuantitative recurrence analysis\nBefore doing a cross analysis, we analyse each series using a windowed quantitative recurrence analysis. We use again an embedding dimension of 4 and a delay parameter of 1. The radius is chosen to be 0.6, while the minimum size for diagonals and vertical lines is chosen to be 2. The moving window is set at 48 periods which, given our quarterly frequency, is equivalent to 12 years. Figure 7 shows the results for the unemployment series, while the results for output are shown in figure 8.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 7.  Quantitative recurrence analysis for unemployment Series.doi:10.1371/journal.pone.0056767.g007\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 8.  Quantitative recurrence analysis for output.doi:10.1371/journal.pone.0056767.g008The results can be read while keeping in mind the output obtained from the application of recurrence plots in figures 3 and 4. In Figure 7, when looking at the recurrence rate, we see a lowering of the recurrence rate corresponding to the two already evidenced recessions, from the mid-1970s and 2009–2010. The degree of predictability, as evidenced through the output, is also decreasing during this period. The results here are similar to those obtained from the recurrence plots: the unemployment series is characterized by a degree of predictability, with two transition periods.\nThe results in figure 8 from the recurrence plots are largely confirming established macroeconomic facts. The variance of output has largely fallen after the 1980's corresponding to the phenomenon called the ‘Great Moderation’. We notice an increase in the recurrence rate of production after period 120 (around 1975) implying an increased level of predictability, which can also be seen from the output. The recurrence rate was generally much lower than that for unemployment during the same period, suggesting a stochastic process for output between the 1950s and mid-1970s. The line length increases also within the second period, with line lengths around 6–7.\nWe discuss in the next two figures (figures 9 and 10), two measures of cross analysis based on quantitative recurrence analysis: quantitative recurrence diagonal analysis in figure 9 and cross quantitative recurrence analysis in Figure 10.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 9.  Quantitative recurrence diagonal analysis for output and unemployment.Red lines correspond to the negative relationship while black lines to the positive one.\ndoi:10.1371/journal.pone.0056767.g009\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 10.  Cross quantitative recurrence analysis for output and unemployment.doi:10.1371/journal.pone.0056767.g010The quantitative recurrence diagonal analysis is based on an embedding dimension of 4 with a delay parameter of 1. The window is 40, corresponding to a 10 year period, with 5 years for negative lags, and 5 years for positive lags. The negative relationship is the strongest one, as expected, and it is slightly biased toward positive lags, suggesting a delayed response by unemployment. The same delay can be found in the peaks reached by the recurrence plots and average line length. We recall the words of [3] (please see the introduction section of our paper) who claimed that the responsiveness of unemployment to output changes was very much a dynamic phenomenon.\nThe relationship, based on figure 9, can be considered as characterized by a degree of predictability, as evidenced from the recurrence rate and determinism rate. Moreover, it also shows there is a more complex picture emerging of the relationship between unemployment rate and output. It may be questioned whether such complex relationship could be easily uncovered with standard econometric techniques.\nIn figure 10 we further investigate the relationship between output and unemployment using quantitative recurrence analysis based on an embedding dimension of four; a delay parameter of one; minimum vertical and diagonal lines of two and a radius distance of 0.6. We use a sliding window of 48 periods (12 years).\nThe results are influenced by the individual dynamics of the series. The recurrence rate increases after the mid-1970s, with the measure increasing also after this period. This signals an increased degree of predictability in the relationship of the series. The average line lengths increased after the ‘70s implying more pronounced common dynamics. We also noticed an increase in the overall complexity of the dynamics, evidence through the measure.\n\nIn conclusion, we analysed the dynamics of output and unemployment in the United States. We considered as well the relations between output and unemployment using recurrence plots and quantitative recurrence analysis. We were able to isolate patterns in the dynamics of unemployment and output which, we believe could not be otherwise isolated using standard econometric tools. The series are found to be characterized by a degree of predictability, with periods of dynamic discontinuity corresponding to the large recessions, like the mid-1970s recession or the 2009–2010 recession. We thank one of the referees of this paper for emphasizing that recessions can be better termed as ‘dynamic discontinuities’.These findings are in line with the main results found from research carried out during the last decades on understanding the relationship between unemployment and output. This research has pointed out nonlinearities, state switching and dynamic relations. In this paper, we go even further, by pointing to a degree of predictability and the possibility of a dynamic discontinuity in different states during the big recessions. Although we do not quantify the relationship, the findings here can be used to expand our understanding of the output -- unemployment relationship. However, it needs to be said that the potential evidence of degrees of predictability should be taken with caution. Marwan [34] makes the following comment on the measure in equation (5): ‘High values of might be an indication of determinism in the studied system, but it is just a necessary condition, not a sufficient one.’ He indicates that even for non-deterministic processes it is possible to find longer diagonal lines with as consequence higher than warranted values."
        },
        "10.1371/journal.pone.0087824": {
            "author_display": [
                "Parisa Samimi",
                "Hashem Salarzadeh Jenatabadi"
            ],
            "title_display": "Globalization and Economic Growth: Empirical Evidence on the Role of Complementarities",
            "abstract": [
                "\nThis study was carried out to investigate the effect of economic globalization on economic growth in OIC countries. Furthermore, the study examined the effect of complementary policies on the growth effect of globalization. It also investigated whether the growth effect of globalization depends on the income level of countries. Utilizing the generalized method of moments (GMM) estimator within the framework of a dynamic panel data approach, we provide evidence which suggests that economic globalization has statistically significant impact on economic growth in OIC countries. The results indicate that this positive effect is increased in the countries with better-educated workers and well-developed financial systems. Our finding shows that the effect of economic globalization also depends on the country’s level of income. High and middle-income countries benefit from globalization whereas low-income countries do not gain from it. In fact, the countries should receive the appropriate income level to be benefited from globalization. Economic globalization not only directly promotes growth but also indirectly does so via complementary reforms.\n"
            ],
            "publication_date": "2014-04-10T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 1025,
            "shares": 0,
            "bookmarks": 0,
            "url": "http://www.plosone.org/article/info:doi/10.1371/journal.pone.0087824",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0087824&representation=PDF",
            "fulltext": "IntroductionGlobalization, as a complicated process, is not a new phenomenon and our world has experienced its effects on different aspects of lives such as economical, social, environmental and political from many years ago [1]–[4]. Economic globalization includes flows of goods and services across borders, international capital flows, reduction in tariffs and trade barriers, immigration, and the spread of technology, and knowledge beyond borders. It is source of much debate and conflict like any source of great power.\nThe broad effects of globalization on different aspects of life grab a great deal of attention over the past three decades. As countries, especially developing countries are speeding up their openness in recent years the concern about globalization and its different effects on economic growth, poverty, inequality, environment and cultural dominance are increased. As a significant subset of the developing world, Organization of Islamic Cooperation (OIC) countries are also faced by opportunities and costs of globalization. Figure 1 shows the upward trend of economic globalization among different income group of OIC countries.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Average economic globalization (KOF index) by income groups.doi:10.1371/journal.pone.0087824.g001Although OICs are rich in natural resources, these resources were not being used efficiently. It seems that finding new ways to use the OICs economic capacity more efficiently are important and necessary for them to improve their economic situation in the world. Among the areas where globalization is thought, the link between economic growth and globalization has been become focus of attention by many researchers. Improving economic growth is the aim of policy makers as it shows the success of nations. Due to the increasing trend of globalization, finding the effect of globalization on economic growth is prominent.\nThe net effect of globalization on economic growth remains puzzling since previous empirical analysis did not support the existent of a systematic positive or negative impact of globalization on growth. Most of these studies suffer from econometrics shortcoming, narrow definition of globalization and small number of countries. The effect of economic globalization on the economic growth in OICs is also ambiguous. Existing empirical studies have not indicated the positive or negative impact of globalization in OICs. The relationship between economic globalization and economic growth is important especially for economic policies.\nRecently, researchers have claimed that the growth effects of globalization depend on the economic structure of the countries during the process of globalization. The impact of globalization on economic growth of countries also could be changed by the set of complementary policies such as improvement in human capital and financial system. In fact, globalization by itself does not increase or decrease economic growth. The effect of complementary policies is very important as it helps countries to be successful in globalization process.\nIn this paper, we examine the relationship between economic globalization and growth in panel of selected OIC countries over the period 1980–2008. Furthermore, we would explore whether the growth effects of economic globalization depend on the set of complementary policies and income level of OIC countries.\nThe paper is organized as follows. The next section consists of a review of relevant studies on the impact of globalization on growth. Afterward the model specification is described. It is followed by the methodology of this study as well as the data sets that are utilized in the estimation of the model and the empirical strategy. Then, the econometric results are reported and discussed. The last section summarizes and concludes the paper with important issues on policy implications.\nLiterature ReviewThe relationship between globalization and growth is a heated and highly debated topic on the growth and development literature. Yet, this issue is far from being resolved. Theoretical growth studies report at best a contradictory and inconclusive discussion on the relationship between globalization and growth. Some of the studies found positive the effect of globalization on growth through effective allocation of domestic resources, diffusion of technology, improvement in factor productivity and augmentation of capital [5], [6]. In contrast, others argued that globalization has harmful effect on growth in countries with weak institutions and political instability and in countries, which specialized in ineffective activities in the process of globalization [5], [7], [8].\nGiven the conflicting theoretical views, many studies have been empirically examined the impact of the globalization on economic growth in developed and developing countries. Generally, the literature on the globalization-economic growth nexus provides at least three schools of thought. First, many studies support the idea that globalization accentuates economic growth [9]–[19]. Pioneering early studies include Dollar [9], Sachs et al. [15] and Edwards [11], who examined the impact of trade openness by using different index on economic growth. The findings of these studies implied that openness is associated with more rapid growth.\nIn 2006, Dreher introduced a new comprehensive index of globalization, KOF, to examine the impact of globalization on growth in an unbalanced dynamic panel of 123 countries between 1970 and 2000. The overall result showed that globalization promotes economic growth. The economic and social dimensions have positive impact on growth whereas political dimension has no effect on growth. The robustness of the results of Dreher [19] is approved by Rao and Vadlamannati [20] which use KOF and examine its impact on growth rate of 21 African countries during 1970–2005. The positive effect of globalization on economic growth is also confirmed by the extreme bounds analysis. The result indicated that the positive effect of globalization on growth is larger than the effect of investment on growth.\nThe second school of thought, which supported by some scholars such as Alesina et al. [21], Rodrik [22] and Rodriguez and Rodrik [23], has been more reserve in supporting the globalization-led growth nexus. Rodriguez and Rodrik [23] challenged the robustness of Dollar (1992), Sachs, Warner et al. (1995) and Edwards [11] studies. They believed that weak evidence support the idea of positive relationship between openness and growth. They mentioned the lack of control for some prominent growth indicators as well as using incomprehensive trade openness index as shortcomings of these works. Warner [24] refuted the results of Rodriguez and Rodrik (2000). He mentioned that Rodriguez and Rodrik (2000) used an uncommon index to measure trade restriction (tariffs revenues divided by imports). Warner (2003) explained that they ignored all other barriers on trade and suggested using only the tariffs and quotas of textbook trade policy to measure trade restriction in countries.\nKrugman [25] strongly disagreed with the argument that international financial integration is a major engine of economic development. This is because capital is not an important factor to increase economic development and the large flows of capital from rich to poor countries have never occurred. Therefore, developing countries are unlikely to increase economic growth through financial openness. Levine [26] was more optimistic about the impact of financial liberalization than Krugman. He concluded, based on theory and empirical evidences, that the domestic financial system has a prominent effect on economic growth through boosting total factor productivity. The factors that improve the functioning of domestic financial markets and banks like financial integration can stimulate improvements in resource allocation and boost economic growth.\nThe third school of thoughts covers the studies that found nonlinear relationship between globalization and growth with emphasis on the effect of complementary policies. Borensztein, De Gregorio et al. (1998) investigated the impact of FDI on economic growth in a cross-country framework by developing a model of endogenous growth to examine the role of FDI in the economic growth in developing countries. They found that FDI, which is measured by the fraction of products produced by foreign firms in the total number of products, reduces the costs of introducing new varieties of capital goods, thus increasing the rate at which new capital goods are introduced. The results showed a strong complementary effect between stock of human capital and FDI to enhance economic growth. They interpreted this finding with the observation that the advanced technology, brought by FDI, increases the growth rate of host economy when the country has sufficient level of human capital. In this situation, the FDI is more productive than domestic investment.\nCalderón and Poggio [27] examined the structural factors that may have impact on growth effect of trade openness. The growth benefits of rising trade openness are conditional on the level of progress in structural areas including education, innovation, infrastructure, institutions, the regulatory framework, and financial development. Indeed, they found that the lack of progress in these areas could restrict the potential benefits of trade openness. Chang et al. [28] found that the growth effects of openness may be significantly improved when the investment in human capital is stronger, financial markets are deeper, price inflation is lower, and public infrastructure is more readily available. Gu and Dong [29] emphasized that the harmful or useful growth effect of financial globalization heavily depends on the level of financial development of economies. In fact, if financial openness happens without any improvement in the financial system of countries, growth will replace by volatility.\nHowever, the review of the empirical literature indicates that the impact of the economic globalization on economic growth is influenced by sample, econometric techniques, period specifications, observed and unobserved country-specific effects. Most of the literature in the field of globalization, concentrates on the effect of trade or foreign capital volume (de facto indices) on economic growth. The problem is that de facto indices do not proportionally capture trade and financial globalization policies. The rate of protections and tariff need to be accounted since they are policy based variables, capturing the severity of trade restrictions in a country. Therefore, globalization index should contain trade and capital restrictions as well as trade and capital volume. Thus, this paper avoids this problem by using a comprehensive index which called KOF [30]. The economic dimension of this index captures the volume and restriction of trade and capital flow of countries.\nDespite the numerous studies, the effect of economic globalization on economic growth in OIC is still scarce. The results of recent studies on the effect of globalization in OICs are not significant, as they have not examined the impact of globalization by empirical model such as Zeinelabdin [31] and Dabour [32]. Those that used empirical model, investigated the effect of globalization for one country such as Ates [33] and Oyvat [34], or did it for some OIC members in different groups such as East Asia by Guillaumin [35] or as group of developing countries by Haddad et al. [36] and Warner [24]. Therefore, the aim of this study is filling the gap in research devoted solely to investigate the effects of economic globalization on growth in selected OICs. In addition, the study will consider the impact of complimentary polices on the growth effects of globalization in selected OIC countries.\nModel SpecificationThis study uses a dynamic panel data model to investigate the effect of globalization on economic growth. The model can be shown as follows:(1)where i is country index, t is time index,  and  are the parameters to be estimated, GDP is the logarithm of real GDP per capita, KOF is economic globalization, CV is a vector of other control variables that affect economic growth,  is unobserved country-specific effect term, and  is the usual error term. The group of control variables is comprised of variables frequently used in the growth literature including government consumption, secondary school enrolment as a proxy for human capital, inflation (consumer price index), domestic investment, liquid liability to capture the financial development and ICRG as an index for institutional quality.\nIn Eq.1, the existence of lag per capita GDP produces the well-known dynamic panel bias due to the correlation between the  and disturbance term, . In other words,  is a function of , as  is time-invariant, therefore,  is also a function of . It means that Eq. 1 has a severe endogeneity problem that happens when the lag of dependent variable, as one of the regressors, is correlated with one component of the error term [37]. In addition, In Eq.1, the fixed effects or time-invariant country characteristics (), might be correlated with the explanatory variables which violates the assumptions underlying the classical linear regression model. In this case, the simple ordinary least squares (OLS) or fixed and random effects approaches can produce highly misleading results.This paper applies the generalized method of moments (GMM) panel estimator first suggested by Anderson and Hsiao [38] and later developed further by Arellano and Bond [39]. This flexible method requires only weak assumption that makes it one of the most widely used econometric techniques especially in growth studies. The dynamic GMM procedure is as follow: first, to eliminate the individual effect form dynamic growth model, the method takes differences. Then, it instruments the right hand side variables by using their lagged values. The last step is to eliminate the inconsistency arising from the endogeneity of the explanatory variables.The consistency of the GMM estimator depends on two specification tests. The first is a Sargan test of over-identifying restrictions, which tests the overall validity of the instruments. Failure to reject the null hypothesis gives support to the model. The second test examines the null hypothesis that the error term is not serially correlated.The GMM can be applied in one- or two-step variants. The one-step estimators use weighting matrices that are independent of estimated parameters, whereas the two-step GMM estimator uses the so-called optimal weighting matrices in which the moment conditions are weighted by a consistent estimate of their covariance matrix. However, the use of the two-step estimator in small samples, as in our study, has problem derived from proliferation of instruments. Furthermore, the estimated standard errors of the two-step GMM estimator tend to be small. Consequently, this paper employs the one-step GMM estimator.In the specification, year dummies are used as instrument variable because other regressors are not strictly exogenous. The maximum lags length of independent variable which used as instrument is 2 to select the optimal lag, the AR(1) and AR(2) statistics are employed. There is convincing evidence that too many moment conditions introduce bias while increasing efficiency. It is, therefore, suggested that a subset of these moment conditions can be used to take advantage of the trade-off between the reduction in bias and the loss in efficiency. We restrict the moment conditions to a maximum of two lags on the dependent variable.We estimated Eq. (1) using the GMM estimator based on a panel of 33 OIC countries. Table S1 in File S1 lists the countries and their income groups in the sample. The choice of countries selected for this study is primarily dictated by availability of reliable data over the sample period among all OIC countries. The panel covers the period 1980–2008 and is unbalanced. Following [40], we use annual data in order to maximize sample size and to identify the parameters of interest more precisely. In fact, averaging out data removes useful variation from the data, which could help to identify the parameters of interest with more precision.The dependent variable in our sample is logged per capita real GDP, using the purchasing power parity (PPP) exchange rates and is obtained from the Penn World Table (PWT 7.0). The economic dimension of KOF index is derived from Dreher et al. [41]. We use some other variables, along with economic globalization to control other factors influenced economic growth. Table S2 in File S2 shows the variables, their proxies and source that they obtain.We relied on the three main approaches to capture the effects of economic globalization on economic growth in OIC countries. The first one is the baseline specification (Eq. (1)) which estimates the effect of economic globalization on economic growth.The second approach is to examine whether the effect of globalization on growth depends on the complementary policies in the form of level of human capital and financial development. To test, the interactions of economic globalization and financial development (KOF*FD) and economic globalization and human capital (KOF*HCS) are included as additional explanatory variables, apart from the standard variables used in the growth equation. The KOF, HCS and FD are included in the model individually as well for two reasons. First, the significance of the interaction term may be the result of the omission of these variables by themselves. Thus, in that way, it can be tested jointly whether these variables affect growth by themselves or through the interaction term. Second, to ensure that the interaction term did not proxy for KOF, HCS or FD, these variables were included in the regression independently.In the third approach, in order to study the role of income level of countries on the growth effect of globalization, the countries are split based on income level. Accordingly, countries were classified into three groups: high-income countries (3), middle-income (21) and low-income (9) countries. Next, dummy variables were created for high-income (Dum 3), middle-income (Dum 2) and low-income (Dum 1) groups. Then interaction terms were created for dummy variables and KOF. These interactions will be added to the baseline specification."
        },
        "10.1371/journal.pone.0066706": {
            "author_display": [
                "Yiyong Cai",
                "David Newth"
            ],
            "title_display": "Oil, Gas and Conflict: A Mathematical Model for the Resource Curse",
            "abstract": [
                "\nOil and natural gas are highly valuable natural resources, but many countries with large untapped reserves suffer from poor economic and social-welfare performance. This conundrum is known as the resource curse. The resource curse is a result of poor governance and wealth distribution structures that allow the elite to monopolize resources for self-gain. When rival social groups compete for natural resources, civil unrest soon follows. While conceptually easy to follow, there have been few formal attempts to study this phenomenon. Thus, we develop a mathematical model that captures the basic elements and dynamics of this dilemma. We show that when resources are monopolized by the elite, increased exportation leads to decreased domestic production. This is due to under-provision of the resource-embedded energy and industrial infrastructure. Decreased domestic production then lowers the marginal return on productive activities, and insurgency emerges. The resultant conflict further displaces human, built, and natural capital. It forces the economy into a vicious downward spiral. Our numerical results highlight the importance of governance reform and productivity growth in reducing oil-and-gas-related conflicts, and thus identify potential points of intervention to break the downward spiral.\n"
            ],
            "publication_date": "2013-06-27T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 1382,
            "shares": 1,
            "bookmarks": 3,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0066706",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0066706&representation=PDF",
            "fulltext": "IntroductionOil and gas are common, high-value commodities in the world market. They are also essential commodities for economic growth and development. Prices for oil and gas have increased dramatically over the last few decades and are expected to continue to do so. Industrial processes, such as electricity generation, machine operation, and petroleum chemical production, require oil and gas. Therefore, areas with abundant oil and gas reserves should be prosperous; however, economists have shown that oil-and-gas-rich countries usually suffer from poor economic performance. The few exceptions include Australia, Canada, and Norway, which are all countries with a democratic regime and a workable tax system that redistributes profits from mining to the rest of the economy and that sustains peaceful development. This economic phenomenon is referred to as the resource curse [1], [2]. Moreover, energy consumption per capita is often far below the world average in oil-and-gas-rich countries, although these exports constitute most of the countries economies (see Figure 1). This is further referred to as the poverty in the midst of plenty.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  World's Proven Oil and Gas Reserves, and Earth's City Lights.Background image courtesy of NASA and data courtesy of CIA-The World Factbook.\ndoi:10.1371/journal.pone.0066706.g001Research on this resource development puzzle tends to focus on oil-and-gas-related civil conflict. The high value and high utility of oil and gas make them points of contest among different social groups. In a weak government, greedy elite may appropriate national patrimony to advance their personal fortunes, while frustrated civilians may use violence to gain control over oil and gas resources. In turn, the elite resort to outright repression to keep the civilians in check. The subsequent escalation of the attack-and-defence cycle displaces human, built, and natural capital [3]–[5]. It also generates political instability, which depresses investment and impedes economic growth [6]–[8]. Therefore, despite years of oil and gas extraction, a resource-rich country in civil conflict remains underdeveloped with an economy that is dangerously reliant on oil and gas exports [9]–[12]. This instability intensifies political competition for control over oil and gas reserves and gives rise to a loop of causalities between resource dependence and conflict [13].\nPolitical economy models generally consider conflicts to be equilibrium behaviors of different interest groups. These models commonly assume that the opportunity costs of attack and defence, or equivalently the productive returns on resources and labor, are exogenously given [3], [14]–[16]. However, these conflict models are insufficient to address the resource curse. It is plausible that a resource-abundant country in conflict is worse off than it is in the absence of conflict [17]. Nevertheless, it is implausible that a country is worse off than it would be without its natural resources, simply because it could neglect its resources and thereby escape from the curse. Therefore, particular attention must be given to the underlying institutions that drive the economy into self-destruction, such as social fractionalization [18]. Furthermore, reduced-form regressions based on these models may be subject to the problem of endogeneity, because of the possible causality loop between oil dependence and conflict. Subsequently, these regressions produce biased estimators, unless a natural experiment is available with relevant content, such as the discovery of an oil field and the subsequent civil conflict. This poses a challenge for empirical studies of the mechanisms that underlie the resource curse and for the formation of related policies.\nThis paper offers a supplementary perspective to the current understanding of the resource curse by using the context of oil, gas, and conflict. It relates poor economic performance to the existence of social fractionalization (elite and civilian), market frictions (monopolistic resource pricing), and resource-related conflict (economic disturbance). When oil and gas are monopolized by the elite, they are often exported rather than sold domestically to support local production. Increased exportation lowers the marginal return to productive activity, and consequently, civil insurgency emerges. The resultant conflict further displaces resources and labor and thus draws the economy into a vicious circle. In the absence of a natural experiment, this research provides a potential alternative structure for econometric identification of the mechanism that drives the resource curse. Additionally, it offers guidance to international organizations on the formation of policies for conflict resolution and poverty reduction.\nAnalysis\nBackground\nWe consider a two-period game that is set up in a small, open economy. The economy has two sectors: extraction and production. The game lasts for two periods . Let  be a measure of the population. At the beginning of period 1, there are two players: an elite  of -measure , who appropriates oil and gas (the resources), and a civilian  of -measure , who has labor force. Resources can be either exported or sold domestically, while labor activity can be either productive or insurgent. The elites represent less than % of the total population (), as shown in Assumption 1\nAssumption 1.\n\nRemark 1.This parametric assumption, the so-called 80–20 rule, is consistent with the World Bank statistics that the richest  hold close to  of the national income in most developing countries (Source: http://data.worldbank.org/indicator/SI.D​ST.05TH.20/countries).\nThe political regime is autocratic, and the elites rule the government. Revolution is broadly defined as any insurgent action or threat against the established political system. We do not distinguish rebellion, which is the attempt to revolt, from revolution, which is a successful rebellion. Accordingly, repression is defined as any counter-insurgency efforts of the elites.\n\n\n\nExtraction and Production\nOil and gas are “point resources” that are fixed in location and thus require sophisticated infrastructures to access, control, and transport. Only the elite can put together the necessary technology for exploration, production, and distribution, with the help of multinational oil and gas companies. The behavior of the multinationals are not modelled in the scope of this paper. According to latest Global Trade Analysis Project database statistics [19], labor in oil and gas extraction constitutes less than 2% of the total labor inputs, or less than 10% of the total inputs into oil and gas extraction in most of the developing world. For simplicity, it is assumed that resource extraction does not require labor input.\nAs is pre-contracted with the multinationals, in each period, the elite extracts one unit of resources and exports  of resources at price , which is exogenously given and constant. The remaining  is sold domestically at price , which is determined by a monopolistic mechanism to be discussed shortly. In total, the elite receives the period resource windfall of(1)\nOn the other hand, in each period, the civilian is endowed with one unit of time. The civilian purchases  unit of resources from the elite in the form of energy and industrial infrastructure and supplies  unit of labor to producewhere  is the size of civilian population in period ,  is the total factor productivity, and  is the output elasticity of resources. Altogether, the civilian has the period net income of(2)\n\n\nResource Market Equilibrium\nTo ensure that the economy has a comparative advantage in exporting, the following condition is assumed:\nAssumption 2.\n\nRemark 2.The following Equation (3) classifies that  is the marginal return on resources when domestic production is at full capacity. If the world price is below , then all resources are consumed domestically.\nGiven a domestic resource price of , the civilians optimal choice is to equalize marginal product and cost of resources as(3)Here, we have used the market clearing condition\n\nThe elite moves simultaneously with the civilian, and can exert monopoly power only to maximize current time profit but not to maximize total survival time profits. Therefore, it is the elites optimal choice to equalize the marginal profits of export and domestic sales, as follows:(4)\nAltogether, the resource market equilibrium is(5)(6)\n\nRemark 3.Assumption 2 ensures that the equality (5) is attainable.\nBy substitution, the elites period windfall is:\nand the civilians net income is\n\nWe can now concentrate on the political dynamics between the elite and the civilian.\n\n\n\nRevolution and Repression\nAt the beginning of period 1, the political statuses of the elites and the civilians are exogenously given. Over the course of the period, civilians can stage a rebellion using their non-productive time . In response, the elites can defend themselves by directing  of the resource windfall to the counter-insurgency expenditure, such as mobilizing military forces, bribing coup leaders, and seeking external intervention. The probability that the elites retain power in period is assumed to be determined by the function , such that(7)Here,  and  are parameters that represent the elites counter-insurgency effectiveness, which captures possible foreign military intervention. Since the early nineteenth century, Britain has played a key role in securing peace and prosperity in the Persian Gulf region. Following World War II, Britain scaled back its military presence around the world because of its economic problems. When Britain announced plans to withdraw troops from the Gulf region, the sheiks of the region asked the British to stay to ensure stability. For more information about oil-related and gas-related foreign intervention, see [20]. The restriction  ensures that  is concave in . Given , the rightmost term of Equation (7) is decreasing in . The spillovers of conflict into neighboring regions and the consequential countermeasures such as military intervention, economic sanctions and humanitarian aid are not explicitly considered in this paper.\nThe elites contest success function, i.e., Equation (7), is a fusion of two streams in the literature. The first presentation is similar to “gun choice,” as seen in [17]. The second presentation has the essence of probabilistic voting , which follows [21]. Because probabilistic voting eliminates the impact of the size of civilian population on political change, we make it comparable by assuming that only , the elites counter-insurgency expenditure as a proportion of total resource windfall, plays a role in .\nRemark 4.By the law of large numbers, the situation in which the civilian revolts with some effort is equivalent to the real-world situation, in which some organized civilians fight against the elites with full effort headed by a coup leader, while the remaining civilians continue to work with full effort. Modeling collective action of civilians is complex [22] and is beyond the scope of this paper.\nThe following properties of  are in order. First, the elite retains power when there is no revolution:\nSecond, the marginal regime-stabilization effect of the elites counter-insurgency efforts is positive and diminishing:\nThird, the marginal regime-stabilization effect of the civilians productive commitment is positive and constant:\nLast, the elites counter-insurgency efforts and the civilians productive commitment are substitutes:\n\n\n\nAftermath of Insurgency\nInsurgency is rewarding but risky. If the insurgency is successful, then the elite dies, and some “lucky” civilian of -measure  becomes the new elite. This leads to an expected loss of civilian population:(8)\nThis indirectly affects productivity in period 2, and can be considered as the expected lethality of revolution. Additionally, violence always causes the civilian to forgo work earnings, no matter who wins.\nRemark 5.Our model does not penalize the civilian if a rebellion is unsuccessful. Modeling this type of penalty requires a discrete function to capture the fact that the elite is penalized only if he or she revolts with an infinitely small effort and still fails, but not if he or she does not revolt. This treatment reduces the continuity and interior differentiability of the model, which are crucial to proving existence and uniqueness of the equilibrium. The expected loss of population already captures the dynamic trade-off of the civilian in relation to insurgency. Thus, we abstract the violence penalty to offer a theoretical model with a unique equilibrium solution that is econometrically identifiable.\n\n\n\nStrategic Interactions\nBoth the elite and the civilian have perfect information and move simultaneously in each period. Knowing the probabilistic regime switching and given the civilians labor supply , the elite chooses defence budget  to obtain(9)where  is the discount factor, and (10)is the elites period payoff net of counter-insurgency expenditures.\nOn the other hand, also knowing the probabilistic regime switching and given the elites defence budget , the civilian chooses labor supply  to obtain(11)where(12)is the civilian's period payoff.\nRemark 6.Although it is more realistic to assume that both the elite and the civilian are risk-averse, this greatly complicates the math. However, letting the elite and the civilian be risk-neutral and assigning linear utilities to their period payoffs does not change the fundamental results of the model. In fact, it gives rise to an equilibrium with no labor supply and thus no domestic production, which better approximates a full-scale civil war.\n\n\n\nEquilibria of Resource-Related Conflict\nLet  be the two-stage game, as defined in the previous section, where  is the list of all model parameters. For the game , the following solution concept is adopted:\nDefinition 1.A sub-game perfect equilibrium is a pure-strategy profile  such that\n\n\n\n\n maximizes , given ,\n\n maximizes , given ,\n\nfor any  that is predetermined,  maximizes , given ,\n\nfor any  that is predetermined,  maximizes , given .\n\nOur first result of this paper now follows:\n\nTheorem 1.Let Assumptions 1 and 2 hold.\n\n\n\n\n1. Let  and  be fixed. There exists a unique sub-game perfect equilibrium to the game , and it can be solved by backward induction.\n\n2. Let  and  be fixed. The solution to the period-2 sub-game is  and , and the solution to the period-1 sub-game must be of one of the two forms below:\n\n                \n\n\n\npeace with  and , or\n\nconflict with  and \n\n             \nWhen , we call the equilibrium civil war.\n\n\n\n\n3. Let other parameters in  and  be fixed. There exists a  such that . That is, the increase of world resource price will eventually lead to a civil war.\n\nProof. See Appendix S1.\n\nRemark 7.The equilibrium of repression and revolution discussed above offers a candidate mechanism for the resource-related conflict. As the elites take a large share of the domestic product, the civilians engage in insurgent activities to uplift their political status and raise the expected economic payoffs. The elites rely on resource windfall from exports and resort to outright repression, rather than economic reforms, to keep the civilians in check and to resolve their discontent. Interested readers can refer to, e.g., [23], for discussions on the effect of resource abundance on the political leaders behavior and political-regime determination. The subsequent escalation of attack and defence displaces labor and destabilizes the domestic environment, which adversely affects production. Consequently, resource exports emerge as the main source of national income, and power struggles over the control of resources prevail. Additionally, the incidence of social conflict parallels the economys increased dependence on resource exports, since higher world resource prices push up the cost of domestic production and intensify the civilians discontent. Because the equilibrium exists and is unique, it is possible to test it against real data. Indeed, our analytical result on the monotonicity of revolution (part 3 of the theorem) is consistent with the empirical finding of Besley and Persson [24], [25] that the oil export price is positively correlated with the incidence of civil war and that civil war is more prevalent among non-democratic oil producers.\n\n\n\nConsequences of Resource-Related Conflict\nIn this section, we investigate the (expected) Pareto inferiority of the equilibrium outcome of our model ( henceforth) as compared to two alternative models. The first-best alternative ( henceforth) has a social planner who solves the following program:(13)\nIn the second-best alternative ( henceforth), the elite is an unchallengeable monarch:\nClearly, the existence of social fractionalization and market monopoly in model  distinguishes it from , while the existence of conflict in  distinguishes it from .\nWe also compare our model to a resource-deficient economy ( henceforth), in which there is no social fractionalization and all resources are imported. In the absence of monopoly, the representative civilian solves the following program:(14)\nThis  economy has a comparative disadvantage in resources but an advantage in market institutions.\nLet  be the collection of models as discussed above. Accordingly, for , let  denote the equilibrium solutions,  denote the equilibrium domestic production, and  denote the equilibrium payoffs of the civilian, the elite, and the economy as a whole. When appropriate, we let  denote the civilians equilibrium payoff, given the world resource price . The following results are in order:\nTheorem 2.Let Assumptions 1 and 2 hold, and let  be fixed.\n\n\n\n\n1. The existence of social fractionalization, market monopoly, and civil conflict leads to the under-provision of labor and resources and thus depresses domestic production:\n\n\nAs a result, resources and labor are displaced from the social optimum:\nThe civilian is better off as a result of insurgency:Being challenged, however, the elite is worse off:All inequalities above are strict when conflict prevails, that is, .\n\n\n\n\n2. Let the conflict technology, i.e, , be constant. If either ,  or  is sufficiently small, then there exists a , such that\n\n\nIn other words, civilians in a resource-abundant economy can be poorer than their counterparts in a resource-deficient economy with better market institutions.\nProof. See Appendix S1.\n\nRemark 8.The theorem above provides a potential explanation for the resource curse. First, domestic production and total social welfare are lower in an economy with resources that are appropriated by the elite rather than in an economy with resources that are allocated by a benevolent social planner. This captures the primary welfare loss, which is caused by market frictions and monopoly. Second, over the course of conflict, resources and labor are displaced from socially profitable activities. This captures the secondary welfare loss, which is caused by attack and defence. For these two reasons, resource-abundant countries may fail to become wealthy. Moreover, if climate conditions are unfavorable (i.e.,  being small), if domestic production is labor-intensive (i.e.,  being small), or if the economy is trapped in a prolonged war (i.e.,  being small), then resource abundance is a curse. This is because the opportunity cost of conflict, as measured by the foregone growth potential, can be immense. Only a minority of civilians will become the new elite and reap the benefits, thus leaving the majority to handle the aftermath of conflict. In contrast, for a resource-deficient economy, domestic-oriented industrialization is a better choice for development. Indeed, according to the recent finding of Caselli and Tesei, resource-deficient countries are more likely to be democratic (Source: http://www.nber.org/papers/w17601). Thus, resource-rich countries may eventually be poorer than resource-poor countries.\n\n\nResults and DiscussionIn this section, we report the numerical results of our mathematical model. We begin by calibrating the baseline to replicate stylized facts of the resource curse and civil conflict. We then conduct sensitivity analyses with parameters in the production function that are related to climate condition and resource dependence. We also investigate the policy implications of resource subsidy and the strategic aspect of domestic-resource sales.\n\nBaseline\nFor the baseline exercise, we assume , which means that civilians constitute % and elites constitute % of the total population. We set total factor productivity  in the production function  equal to . Having a developing country in mind, we set the parameter  equal to , which is much lower than what is commonly used in the literature. The discount factor , is set to , which implies a conflict length of roughly  years, assuming an annual interest rate of 4%. The parameter  in the regime-switching function  is set to , which implies that the elite is three times more likely to win when both the elite and the civilian battle at full strength. According to a recent research of Collier and Hoeffler, only 82 out of 336 rebellions in Africa from 1960 to 2001 were successful (Source: http://users.ox.ac.uk/ econpco/research/pdfs/MilitarySpendingan​dRisksCoups.pdf).The other parameter  is set to , which implies that doubling , the elites counter-insurgency expenditure as a share of total resource windfall, roughly doubles the chance of retaining power when  is within %, but chances are lower when it is greater than.\nThe numerical results confirm our model prediction (Figures 2 to 5). Oil and gas prices are positively correlated to the incidence of conflict. Although conflict worsens social welfare, the civilian is in fact better off. An export boom (roughly a % price wedge) can relatively easily provoke resource-related civil conflict. However, a substantially higher world price (roughly a % price wedge) is required to compensate for the lost growth potential and to make civilians in a resource-abundant economy as successful as their resource-deficient counterparts, who have a more supportive market environment. This shows how easy it is to turn a resource fortune into a curse and how difficult it is to turn a curse into a fortune. In addition, the comparison across the four models, namely, , ,  and , highlights the importance of governance reform in oil-and-gas-related conflict resolution and poverty reduction.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Civilan's Labor Supply at Various Price Levels.doi:10.1371/journal.pone.0066706.g002\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Elite's Counter-Insurgency Expenditure at Various Price Levels.doi:10.1371/journal.pone.0066706.g003\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Social Welfare at Various Price Levels.doi:10.1371/journal.pone.0066706.g004\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Civilan's Total Payoff at Various Price Levels.doi:10.1371/journal.pone.0066706.g005\n\nClimate Change and the Incidence of Conflict\nIn the context of domestic agricultural production, total factor productivity  in the production function  can be understood as climate conditions, such as rainfall variation. Our model can be applied to recent studies of global climate and civil conflict [26]–[28]. Intuitively, adverse climate conditions (i.e.,  being small) lowers the return to legal labor activity, which causes the civilian to challenge the existing elites and to take over control of resources, and vice versa. This is confirmed in the simulation (Figure 6) where we alter the value of  to  and , respectively. Other parameters stay the same as in the baseline. Our simulation also highlights the importance of productivity growth in oil-and-gas-related conflict resolution.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Climate Change and Incidence of Conflict.doi:10.1371/journal.pone.0066706.g006\n\nResource Dependence and the Incidence of Conflict\nBy Euler's Theorem (see, e.g., [29]), the output elasticity of resources  in the production function  is also a measure of the economys dependence on resources. In the context of social fractionalization and monopolistic pricing, as  increases, the share of domestic output that is taken by the resource-owning elite increases, and thus the elite discontent increases. This is confirmed in the simulation (Figure 7) when we alter the value of  to  and , respectively. Other parameters stay the same as in the baseline. Intuitively, this suggests that, in an economy with less demand for oil and gas, high export prices are less likely to trigger conflict.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 7.  Resource Dependence and Incidence of Conflict.doi:10.1371/journal.pone.0066706.g007\n\nResource Subsidy\nOur model predicts that a resource-rich country with weak governance will have higher prices of oil and gas and lower consumption in the domestic market. In fact, many oil-and-gas-rich countries have struggled with sub-par levels of energy consumption, as previously discussed. However, it is more often the case that oil-and-gas-rich countries have lower energy prices, even after accounting for transportation costs (Source: http://www.mytravelcost.com/petrol-price​s/). This is counter-intuitive, as low resource prices should stimulate consumption.\nTo reconcile the observed reality, economic intuitions, and our model prediction, we postulate that domestic resources subside. We assume, ex post, that a fixed shared of the elites counter-insurgency fund is appropriated and spent as an ad valorem resource subsidy. In other words, the subsidies occur and become known to the civilian only after both  and  are determined. This redistribution scheme is never optimal, because it is arbitrary, conspired by the elite, and beyond the civilians strategic consideration. None of the fundamentals of our model is changed. Therefore, the same equilibrium would emerge even if domestic resource consumption were now subsidized. Social conflict may still linger and disrupt production. As a result, the resource consumption of a resource-abundant economy can be lower than that of a resource-deficient economy, despite a lower domestic price of resources.\nOn top of the baseline, we assume that % of the counter-insurgency expenditure is directed to subsidize domestic resource consumption. Figures 8 and 9 show that subsidy is most likely to be superfluous. By assumption, the domestic price is zero when resource consumption is (more than) fully subsidized on a per-unit basis or when there is no demand. As long as the institution of transfer is not open to the involvement of the civilian, insurgency is still the default activity and domestic production ceases. Subsequently, the subsidization budget is squandered. This simulated result is a stronger demonstration of the model prediction of trade-pattern effects of conflict than that of Garfinkel et al. [17]. If subsidy is an ineffective stimulator, then domestic consumption of resources remains low. Civil conflict interrupts production, and thus resources are over-exported.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 8.  Domestic Resource Price at Various Price Levels.doi:10.1371/journal.pone.0066706.g008\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 9.  Domestic Resource Sales at Various Price Levels.doi:10.1371/journal.pone.0066706.g009\n\nStrategic Aspect of Resource Sales\nOur model assumes that the elite moves simultaneously with the civilian in each period. Therefore, the elite can exert monopoly power only to maximize current time profit, rather than to maximize total survival time profits, because the choice of domestic resource sales has no effect on regime switching. This is a reasonable assumption, because many states with an elite class also have weak governance, and each individual elite controls a piece of the oil and gas resources. The consequential conflict between corrupt elites and frustrated civilians leads the economy into a development trap. The first-best solution is to transform the fractionalized state into a democratic nation and to streamline elite and civilian interests, as demonstrated in section with the social-planner model.\nHowever, as a robustness check, we also consider a scenario in which the elites collude and form a strategic alliance to tip resource sales towards the domestic market. This decreases rebellion risk and maximizes the elites€ total survival time profit. In this alternative timing model, with other things being the same as in the baseline, the elite pre-commits a certain amount of resources to domestic use before the civilian commits to labor supply. The elite commits to a level of counter-insurgency expenditure. An extra step of backward induction is to optimize the elites strategic choice of resource sales, considering its effects on the elites later choice of counter-insurgency expenditure and the civilians simultaneous choice of labor supply. The existence and uniqueness of the equilibrium can be established by an approach that is similar to that in this paper, in which we use numerical simulation without rigorous mathematical analysis.\nOur simulated results suggest that the economy is more peaceful in the alternative timing model (Figures 10 to 11). However, the elite is worse off than in the baseline (Figure 12). In other words, it is not profitable (from the elites point of view) or credible (from the elites point of view) for the elite to pre-commit domestic resource sales. Additionally, domestic resource sales are more volatile (Figure 13). When the world prices are low, the elite exports no resources; however, when world prices are sufficiently high, the elite exports all resources. Consequently, the welfare implication is mixed (Figure 14).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 10.  Civilian's Labor Supply under Alternative Timing.doi:10.1371/journal.pone.0066706.g010\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 11.  Elite's Counter-Insurgency Expenditure under Alternative Timing.doi:10.1371/journal.pone.0066706.g011\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 12.  Elite's Total Payoff under Alternative Timing.doi:10.1371/journal.pone.0066706.g012\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 13.  Domestic Resource Sales under Alternative Timing.doi:10.1371/journal.pone.0066706.g013\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 14.  Social Welfare under Alternative Timing.doi:10.1371/journal.pone.0066706.g014\nResource-abundant countries often show poor economic growth. This paper proposes a theoretical framework to study this paradox in the context of oil, gas, and conflict. The model addresses issues of social fractionalization, market friction, and civil conflict. It shows that the equilibrium between elite and civilian individual maximization behaviors can undermine the peaceful environment that is needed for industrialization and development. It can also further provoke resource-related conflict. The model predictions are consistent with observable facts.The setup of the model is simple, and the parameter assumptions are non-stringent. Yet, it features a causality loop between resource dependence, conflict, and poor economic performance. Since the equilibrium of the model is proven to exist, and it is unique and eventually monotone, the model provides a well-defined structure for hypothesis testing and for the econometric identification of the mechanisms that underlie the resource curse."
        },
        "10.1371/journal.pone.0022794": {
            "author_display": [
                "Kun Guo",
                "Wei-Xing Zhou",
                "Si-Wei Cheng",
                "Didier Sornette"
            ],
            "title_display": "The US Stock Market Leads the Federal Funds Rate and Treasury Bond Yields",
            "abstract": [
                "\n        Using a recently introduced method to quantify the time-varying lead-lag dependencies between pairs of economic time series (the thermal optimal path method), we test two fundamental tenets of the theory of fixed income: (i) the stock market variations and the yield changes should be anti-correlated; (ii) the change in central bank rates, as a proxy of the monetary policy of the central bank, should be a predictor of the future stock market direction. Using both monthly and weekly data, we found very similar lead-lag dependence between the S&P 500 stock market index and the yields of bonds inside two groups: bond yields of short-term maturities (Federal funds rate (FFR), 3M, 6M, 1Y, 2Y, and 3Y) and bond yields of long-term maturities (5Y, 7Y, 10Y, and 20Y). In all cases, we observe the opposite of (i) and (ii). First, the stock market and yields move in the same direction. Second, the stock market leads the yields, including especially the FFR. Moreover, we find that the short-term yields in the first group lead the long-term yields in the second group before the financial crisis that started in mid-2007 and the inverse relationship holds afterwards. These results suggest that the Federal Reserve is increasingly mindful of the stock market behavior, seen as key to the recovery and health of the economy. Long-term investors seem also to have been more reactive and mindful of the signals provided by the financial stock markets than the Federal Reserve itself after the start of the financial crisis. The lead of the S&P 500 stock market index over the bond yields of all maturities is confirmed by the traditional lagged cross-correlation analysis.\n      "
            ],
            "publication_date": "2011-08-10T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 2,
            "views": 2512,
            "shares": 2,
            "bookmarks": 9,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0022794",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0022794&representation=PDF",
            "fulltext": "IntroductionFinancial markets play a more and more important role in the economic system. Many financial variables have predictive power for output or inflation of the real economy. Financial markets are becoming increasingly important to the real economy due to their impact on output growth and inflation, among others [1]–[6]. As an important part of financial markets, stock markets can be considered as economy barometers [7], [8]. As a consequence, monetary policy, which is usually based on inflation target and sometimes unemployment goals, is not independent of stock markets. There is a large number of financial economic literature concerned with the impact of and relationship between the monetary policy of central banks and the performance of stock markets. The common wisdom asserts that (i) the stock market variations and bond yield changes should be anti-correlated and (ii) the change in short-term interest rates, as a proxy of the monetary policy of the central bank, should be a predictor of the future stock market direction. The first assertion reflects the impact of capital cost on economic growth. The second statement is a corollary of the causal effect of the former one.\nSome of the most relevant results for our study that were obtained by previous scholars on these two statements include the following. Tobin's portfolio selection theory [9] explained the stock price increases observed in times when the interest rate goes down as due to investors' preference for the higher yield of stock markets. Rigobon and Sack [10] documented that an increase in short-term interest rate results in a decline in stock prices and in an upward shift and flatter yield curve. Bernanke and Kuttner [11] found that a hypothetical unanticipated 25-basis-point cut in the FFR target is associated with approximately a 1% increase in the broad stock indexes. Bjørnland and Laitemo [12] found a significant relationship, which is however the inverse of (i) and (ii): a one percent increase of the stock market leads on average to a 4-basis-point increase of the interest rate. Two of us have also previously found that the stock market seems to influence the FFR, during the 2000–2003 US stock market antibubble [13]–[16].\nHere, using an extension of the so-called TOP technique [13]–[16] for the joint analysis of pairs of time series, we revisit the pertinence of these two assertions (i) and (ii) by estimating the lead-lag structure between the US stock market proxied by the S&P 500 index and a set of Treasury bond yields, including the Federal funds rate (FFR), which constitutes one of the tools implementing monetary policy in the US. Our analysis is applied to monthly and weekly data of Federal funds effective rate (FFR), and nine Treasury bond yields with different maturities: 3M (3 months), 6M, 1Y (1 year), 2Y, 3Y, 5Y, 7Y, 10Y, and 20Y. The period of analysis from August 2000 to February 2010 includes the bearish market up to mid-2003, the bullish bubble-like market regime up to October 2007 followed by the turbulent phases associated with the so-called great Recession [17], [18]. Given the extraordinary developments associated with the financial crises followed by economic crises in different parts of the world, it is particularly interesting to investigate the lead-lag structure between the US stock market and a set of Treasury bond yields.\nMaterials and Methods\nDescription of the thermal optimal path (TOP) method\nThe thermal optimal path (TOP) method has been proposed as a new method to identify and quantify the time-varying lead-lag structure between two time series. The TOP method was successfully applied to several economic cases [14]–[16]. It works as follows.\nConsider two standardized time series  and . The matrix  of distances between  and  is defined as [14], [15](1)The element  of the matrix  thus compares the realization  of  at time  with the realization  of  at time . The value  defines the distance between the realizations of the first time series at time  and the second time series at time . The  matrix  thus embodies all possible point-wise pairwise comparisons between the two time series. Note that the distance matrix could be modified to deal with two non-monotonic time series, for which the TOP algorithm is essentially the same [16].\nOnce the matrix  with elements given by Eq. (1) is obtained, an optimal path is determined that quantifies the lead-lag dependence between the two time series. Figure 1 gives a schematic representation of how lead-lag paths are defined [14]. The first (resp. second) time series is indexed by the time  (resp. ). The nodes of the plane carry the values of the distance for each pair . The path along the diagonal corresponds to taking , i.e., compares the two time series at the same time. Paths above (resp. below) the diagonal correspond to the second time series lagging behind (resp. leading) the first time series. The figure shows three arrows which define the three causal steps (time flows from the past to the future both for  and ) allowed in our construction of the lead-lag paths. A given path selects a contiguous set of nodes from the lower left to the upper right. The relevance or quality of a given path with respect to the detection of the lead-lag relationship between the two time series is quantified by the sum of the distances along its length, called the “cost” of the path. The lead-lag structure is then obtained as the relationship  as a function of , as described shortly. We stress that the two-layer scheme presented in Fig. 1 performs better than multi-layer schemes [15].\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Thermal optimal path method.Representation of the two-layer approach in the lattice  and of the rotated frame  as defined in the text. The three arrows depict the three moves that are allowed to reach any node in one step.\ndoi:10.1371/journal.pone.0022794.g001As shown in Fig. 1, it is convenient to use the rotated coordinate system  such that(2)where  is in the main diagonal direction of the  system and  is perpendicular to . The origin  corresponds to . Then, the standard reference path is the diagonal of equation , and paths which have  define varying lead-lag patterns. Inverting (2), we have(3)This means that a positive  corresponds to , which by definition of the optimal thermal path below means that the second time series  lags behind the first time series , or equivalently  leads .\nThe idea of the TOP method is to identify the lead-lag relationship between two time series as the best path in a certain sense. A natural idea is that the best path is the one which has the minimum sum of its distances along its length (paths are constructed with equal lengths so as to be comparable). This path with minimum cost has thus the minimum average distance between the two time series, i.e., it is such that  resembles the most  along this path . The problem with this idea is that the noises decorating the two time series introduce spurious patterns which may control the determination of the path which minimizes the sum of distances, leading to incorrect inferred lead-lag relationships. It has been shown that a robust lead-lag path is obtained by defining an average over many paths, each weighted according to a Boltzmann-Gibbs factor, hence the name “thermal” optimal path method [14]–[16]. Intuitively, this corresponds to performing an averaging operation over neighboring paths of almost the same cost.\nConcretely, we first calculate the partition functions , for all values of  at a fixed  in the lattice shown in Fig. 1, and their sum  so that  can be interpreted as the probability for a path to be at distance  from the diagonal for a distance  along the diagonal. This probability  is determined as a compromise between minimizing the mismatch or cost as defined above (similar to an “energy”) and maximizing the combinatorial weight of the number of paths with similar mismatches in a neighborhood (similar to an “entropy”). As illustrated in Fig. 1, in order to arrive at , a path can come from  vertically,  horizontally, or  diagonally. The recursive equation on  is therefore(4)where  is defined by Eq. (1). The parameter  plays the role of a “temperature” controlling the relative importance of cost versus combinatorial entropy. The larger  is, the larger the number of paths that contribute to the partition functions. In contrast, as , only the path with minimal cost counts. The recursion relation (4) is derived following the work of Wang et al. [19]. To get  at the -th layer, we need to know and bookkeep the previous two layers from  to . After  is determined, these values are normalized by  so that  does not diverge at large . The boundary condition of  plays an crucial role. For  and , . For , the boundary condition is taken to be , in order to prevent paths to remain on the boundaries.\nOnce the partition functions 's have been calculated, we can obtain any statistical average related to the positions of the paths weighted by the set of 's. For instance, the local time lag  at time  is given by(5)Expression (5) defines  as the thermal average of the local time lag at  over all possible lead-lag configurations suitably weighted according to the exponential of minus the measure  of the similarities of two time series. For a given  and temperature , we determine the thermal optimal path . We can also define an “energy” or cost  to this path, defined as the thermal average of the measure  of the similarities of two time series:(6)\n\n\nBootstrapping tests and statistical significance\nIn order to test whether the extracted lead-lag structure is statistically significant, we introduce a bootstrap approach [20] that is specifically adapted to the present problem. This statistical test extends and makes more robust the method and results, as compared with previous works [14], [15], [16]. Consider two time series  (for instance the logarithmic returns of S&P 500) and  (for instance the time increments of bond yields). We perform the TOP analysis on a fixed time interval at some temperature . Let us assume we obtain the lead-lag function . Recall that  is the diagonal of the  plane. We then shuffle  and  and redo the TOP analysis at the same temperature . We obtain a new lead-lag function . This process is repeated another  times, giving a total of  paths  with . A typical value of  used below is 1000. For each , out of the  reshuffled time series, we determine the 5% quantile  and the 95% quantile , denoted in the following as  and . If  is smaller than  or larger than , we interpret that the lead-lag  at time  is different from zero at the significance level of 95% or larger. Complementarily, given the obtained lead-lag , out of the  reshuffled time series, we obtain the -value as a function of , which thus characterizes the time periods when there is a statistically significant lead-lag structure as those with small -values.\n\n\nData sets\nIn the following, we apply the TOP method respectively to monthly and weekly data of the S&P 500 index, Federal funds effective rate (FFR), and nine Treasury bond yields with different maturities: 3M (3 months), 6M, 1Y (1 year), 2Y, 3Y, 5Y, 7Y, 10Y, and 20Y. Each time series spans from August 2000 to February 2010. The Treasury bond at 30-year maturity is not considered because it was discontinued in January 2002 and then reintroduced in February 2006.\nFigure 2 shows the weekly sampling of the FFR, the nine Treasury bond yields with different maturities, and the S&P 500 index. In the left panel of Fig. 2, very interesting patterns emerge in the term structure. In general, the yields of Treasury bonds with short maturities are more sensitive to the economic circumstance and change to a larger extent. In 2000, 2006 and 2007, the spread is very narrow and the FFR is even higher than the Treasury bond yields, corresponding to an anomalous inverted yield curve. These time periods correspond respectively to the early stages of the 2000 US stock market crash and to the current financial crisis. The spread reaches local maxima in 2004 and 2010. In addition, the right panel of Fig. 2 suggests that the FFR and the S&P 500 index change roughly in the same direction. It is thus interesting to refine this visual impression and determine rigorously using the TOP method described above what is the lead-lag structure between the evolution of the US stock market and the FFR, which embodies an important part of the policy of the Federal Reserve.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Data sets.(a) Weekly sampling of the Federal effective funds rate (FFR) and nine Treasury bond yields. (b) S&P 500 and FFR together with the 20Y for comparison.\ndoi:10.1371/journal.pone.0022794.g002In this paper, we use as inputs the logarithmic returns of the S&P 500 index and the increments of the FFR and of all the yields, rather than the non-stationary original time series. We define the logarithmic returns of the S&P 500 index as follows(7)and the logarithmic increments of yields curves as follows(8)where the time unit for  is one week for weekly data and one month for monthly data. We then normalize the two time series  and  so that their mean is zero and their standard deviation is equal to  [14]. This ensures that they are comparable and can be used meaningfully in the TOP analysis to extract their lead-lag structure.\n\n\nUnit root tests\nWe perform unit root tests on the logarithms of the original time series and their first-order differences ( and ) to check for their stationary. The augmented Dickey-Fuller (ADF) [21], Phillips-Perron (PP) [22], and Kwiatkowski-Phillips-Schmidt-Shin (KPSS) [23] tests are adopted. For the ADF and PP tests, the null hypothesis is that the time series has a unit root, which utilizes the -statistic. In contrast, the null hypothesis of the KPSS method is that the time series is stationary and uses the LM-statistic. The results are presented in Table 1.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Unit root tests of the logarithmic monthly and weekly data and their first-order differences.doi:10.1371/journal.pone.0022794.t001For the logarithmic monthly data and logarithmic weekly data, the ADF and PP tests show that these time series are not stationary and have a unit root since the -values are greater than 10%, except for the 20Y yield. In contrast, the KPSS test suggests that four time series are stationary since the -values are much greater than 10%.\nFor the differences of the logarithmic monthly data and logarithmic weekly data, the ADF and PP tests show that all time series are stationary at the 1% significance level, and the KPSS test also confirms that these time series are stationary at the 10% level. These results justify our use of the logarithmic returns in the TOP analysis in order to avoid possible spurious signals in the estimated lead-lag structure that could result from large excursions exhibited by the non-stationary time series.\n\nResults\nThe S&P500 leads all yields: Evidence from the TOP method\nEmpirical results.Figure 3 shows the instantaneous evolution of the lead-lag  between the returns of the S&P500 index taken as the first time series and the logarithmic variation of each of the yields for the monthly data at temperature . We have been careful to investigate the impact of the locations of the starting and ending extremities of the paths. There are indeed a total of  thermal optimal paths, because there are 19 starting points  and 19 ending points , denoted using the  system instead of the  system for simplicity. The 19 starting points are , , and  for . The 19 ending points are ,  and  for , where  is the length of the time series. The overall thermal optimal path  is chosen as the one with minimal energy (or total cost) among the  thermal paths. As for the choice of the temperature , we investigated other values and found our results to be robust and qualitatively similar with respect to variations of  between  and . To present our results, we choose this value  as it seems to represent a reasonable optimal, confirmed by cross-correlation analyses performed on the steady periods found with fixed lag times for various 's.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Lead-lag  for monthly data.Dependence of the lead-lag  between the returns of the S&P500 index taken as the first time series and the logarithmic variation of each of the yields for the monthly data: (a) FFR, 3M, 6M, 1Y, 2Y, and 3Y Treasury bond yields as the first group; (b) 5Y, 7Y, 10Y, and 20Y bond yields as the second group. The unit of  is one month.\ndoi:10.1371/journal.pone.0022794.g003Figure 3 is organized in two panels, each panel plotting one group. The first group includes FFR, 3M, 6M, 1Y, 2Y, and 3Y Treasury bonds as shown in Fig. 3(a). The second group includes 5Y, 7Y, 10Y, and 20Y Treasury bonds as shown in Fig. 3(b). The evolution of  in each group are quantitatively similar.\nFigure 4 is the same as Fig. 3 for weekly data. Apart from largest fluctuations of the lead functions, the results are very similar and robust to this change of time scale from monthly to weekly.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Lead-lag  for weekly data.Dependence of the lead-lag  between the returns of the S&P500 index taken as the first time series and the logarithmic variation of each of the yields for the weekly data: (a) FFR, 3M, 6M, 1Y, 2Y, and 3Y Treasury bond yields as the first group; (b) 5Y, 7Y, 10Y, and 20Y bond yields as the second group. The unit of  is one week.\ndoi:10.1371/journal.pone.0022794.g004\nStatistical significance.Before commenting and exploiting the information presented in Figs. 3 and 4, it is important to ascertain their statistical significance. For this, we use the bootstrap method described above. Figure 5 illustrates the obtained results from the monthly data for two maturities, namely the shortest one (FFR) and the longest one (20-year Treasury bond yield). It shows that the two lead function  are well above the 95% quantile curves, that is, . The conclusion is the same for other Treasury bond yields. We conclude that the obtained lead-lag structure for the monthly data cannot be produced by chance at the 95% significance level.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Bootstrap test for the significance of the lead-lag structure.(a) the monthly FFR and (b) the monthly 20Y Treasury bond yield.\ndoi:10.1371/journal.pone.0022794.g005Figure 6 illustrates the obtained results from the weekly data for two maturities, namely the shortest one (FFR) and the longest one (20-year Treasury bond yield). The conclusion is the same for other Treasury bond yields. Therefore, the  functions for the weekly data are positive at the 95% significance level, which unveils the nontrivial intrinsic lead-lag structure of the S&P 500 index and the yield time series.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Bootstrap test for the significance of the lead-lag structure.(a) the weekly FFR and (b) the weekly 20Y Treasury bond yield.\ndoi:10.1371/journal.pone.0022794.g006\nTwo shocking stylized facts.The first and most important observation extracted from Figs 3 and 4 is that, for all yields and at all times, the S&P500 index leads the yield changes, since  is always positive, which by definition (3), means that  for the optimal thermal path. Since the index  corresponds to the S&P500 index and the index  corresponds to one of the yields, this conclusion follows. This result confirms and extends considerably that reported previously by two of us [13] using standard measures of correlations over a restricted period from 2001 to 2003, under the somewhat provocative title “Causal slaving of the U.S. Treasury Bond Yield … by the Stock Market…” Indeed, as this title suggests, this result  is particularly striking and rich of implication. This result collides against the common wisdom that usually asserts the following two rules:\n\n\n\n\nthe stock market variations and the yield changes should be anti-correlated;\n\nthe change in FFR, as a proxy of the monetary policy of the central bank, should be a predictor of the future stock market direction.\n\nIndeed, according to the standard story, a lower interest rate means lower costs of borrowing for the private sector, implying that the private sector is going to profit from this opportunity by increased investments in innovations and entrepreneurial opportunities, leading (with some lag) to an improved outlook for the future growth of the economy. Since stock market prices reflect the anticipation of investors, this better outlook for the future economy should be soon reflected in the appreciation of the stock market. Reciprocally, an increase of the yields and in particular of the FFR should, according to the standard story, translate soon into a drag on the growth of stock markets.\nWe observe the opposite of (i) and (ii). First, we find that the stock market and yields move in the same direction, as pointed out independently by R. Werner [24]. Second, the stock market leads the yields, including and especially the FFR. The implication is clear: the central bank policy is (1) reacting to the stock market and (2) is following it. When the stock market exhibits a rally, the Fed tends to progressively increase its rates as an attempt to calm down the “overheating engine”, as occurred towards the end of the ICT bubble when the Fed rate was increased to 6.5%. A similar increase of the Fed rate occurred from 2004 to 2007. When the stock market plunges, the Fed tends to decrease its rates, in the hope of putting a brake on the stock market losses that negatively feedback onto the real economy via the wealth effect.\nBoth previous and present Fed chairmen Greenspan and Bernanke have increasingly made clear that the Federal Reserve does care more and more about the evolution of the stock markets. On Dec. 3rd, 2010, former Federal Reserve Chairman Alan Greenspan told CNBC that rising stock values have played a critical role in the economic recovery. The stock market got a boost from the Fed policy to boost liquidity, which drove interest rates down and pushed investors toward riskier investments like stocks. “I think we are underestimating and continuing to underestimate how important asset prices, very specifically equity prices, are not only to shareholders but the economy as a whole,” he said. Equities have risen more than 80% from the lows set during the financial crisis, noted Greenspan, benefiting investors and helping fuel the recovery (Source: http://www.dailyfinance.com/story/invest​ing/greenspan-rising-stock-markets-are-k​ey-to-recovery/19743325/?icid=sphere_cop​yright). On Nov. 3rd., 2010, Bernanke issued the following statement in an opinion article for the Washington Post released hours after the Fed announced the $600 billion of Treasury buying through June in a second round of unconventional monetary stimulus: “Resuming large-scale asset purchases should boost economic growth through lower borrowing costs and higher stock prices… Stock prices rose and long-term interest rates fell when investors began to anticipate this additional action… Easier financial conditions will promote economic growth.” Being content to see the stock market growing, this suggests a hidden mandate of the Federal Reserve to steer the stock markets.\nIt seems that the dynamics of the Fed policy, as translated in the Fed rates and the longer maturity yields (which of course are far from being controlled by the central bank), is much more straightforward than articulated in fancy models [25]. The evidence presented here suggests that Fed policy appears to be as if a straightforward reaction to financial markets was the main factor.\n\nComparison between different yields.Comparing the lead functions  for the various yields with different maturities, we find that the short-term yields in the first group (left panel of Figs. 3 and 4) move approximately in synchrony with the long-term yields in the second group (right panel of Figs. 3 and 4) until 2007. And this synchrony is almost perfect from the yields spanning FFR to 3Y in the first group until mid-2007. Thereafter, during the time period following the financial crisis that started in mid-2007, we can observe that the short-term yields clearly lead the long-term yields and we have the sequence of inequalities(9)This is seen from the fact that  tends to be larger for the short-term yields, since they are all compared with the same S&P500 stock market index. It is also interesting to observe the increasing lag  between the yield rates and the S&P500 index from around  month in 2000 to about one year in 2007. This is followed by a plateau for all yields from FFR to 3Y, that lasts about 2 years and is then followed by a decay of the lag thereafter to about half its maximum, i.e., around 6 months.\nFor the second group of yields with maturities from 5Y to 20Y whose 's are plotted in the right panel of Figs. 3 and 4, the picture is somewhat different. Before early 2003, the four curves are close to each other with no clear lead-lag structure between them. Then, from 2003 to mid-2007, a period corresponding to a very bullish upward trend of the stock market boosted by the favorable low rate of the Fed policy and a booming real-estate bubble, one can observe that the longer term yields lead clearly the shorter term yields:(10)Thereafter, in the reaction to the financial crisis, one observes as for the FFR-3Y yields that the shorter-term yields lead the long-term yields:(11)There is much less evidence for a plateau of the lead structure with respect to the S&P500.\nWe would also like to mention that a reversal such as the one from (10) to (11) does not seem to have been documented before.\n\n\n\nThe S&P500 leads all yields: Evidence from cross-correlation analysis\nBy construction, the traditional cross-correlation analysis [26] is not adapted to time-varying lead-lag structures. It is however useful to investigate how it performs in the present context in which the TOP method has diagnosed a significant time-varying structure. For centered random variables, the cross-correlation function can be calculated as follows:(12)where  denotes the sample average and  is the sample variance.\nTwo representative time series (FFR and 20Y) are presented for illustration. The significance levels of the cross-correlations are evaluated using bootstrapping tests through shuffling the return time series, similar to the analysis for the TOP method. We use the monthly data in this analysis. For each pair of time series, we analyze the whole time series and two non-overlapping time periods. The results are shown in Fig. 7. It is obvious that the lagged cross-correlation analysis is not able to characterize the instantaneous evolution of the lead-lag structure evidenced in the previous TOP analysis. This is not a surprise.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 7.  Lagged cross-correlation analysis.Lagged cross-correlation between the logarithmic return of the S&P 500 index and the logarithmic difference of the FFR (a–c) and between the logarithmic return of the S&P 500 index and the logarithmic difference of the 20Y Treasury bond yield (d–f) during different time periods: (a,d) the whole time period from August 2000 to February 2010, (b,e) the time interval from August 2000 to April 2007, and (c,f) the time interval from May 2007 to February 2010. The ordinate axis shows the cross-correlation coefficients . The unit of the lag time  along the abscissa is month.\ndoi:10.1371/journal.pone.0022794.g007The (S&P 500, FFR) pair.For the (S&P 500, FFR) pair in the whole time period, the highest peak found in Fig. 7(a), with a positive lag, shows that the FFR lags behind the S&P 500 index by about 3 months, with a cross-correlation coefficient , which is significantly positive at the confidence level of 95%. There are two other peaks that are also significant, one at a negative lag of  month with  and another at the positive lag  month with . The largest peak with positive lag and highest cross-correlation coefficient  can be considered as confirming the main results of the previous section that the stock market changes precede the FFR variations. Due to the fixed lead-lag structure of the method, the cross-correlation provides only an average coarse representation of the real much richer and dynamical nature of the lead-lag structure.\nFor the time period before April 2007, we see many peaks at positive and negative lags  that are significantly different from zero, as shown in Fig. 7(b). It is hard to extract from this plot a clear picture about the lead-lag structure between the S&P 500 and FFR. In the presence of large variations of the lead-lag structure, it is not surprising that the cross-correlation analysis is not informative.\nFor the time period after April 2007, we see in Fig. 7(c) a significant peak at the positive lag  month with . This lag is consistent in magnitude with the average value of the  curve shown in Fig. 5(a). This clear signal in the cross-correlation analysis can be explained from the fact that the lead-lag has stabilized approximately above a value of 6 months, according to the analysis of the  function shown in Fig. 5(a) during the time period under investigation.\n\nThe (S&P 500, 20Y) pair.For the (S&P 500, 20Y) pair in the whole time period, there are two significant peaks around zero lag  in Fig. 7(d).\nFor the time period before April 2007, the signal is ambiguous although we can see several significant peaks in Fig. 7(e).\nFor the time period after April 2007, we see in Fig. 7(f) only one significant peak at  month with . According to Fig. 5(b), the lead-lag  decreases from about  to  month. Therefore, these two analyses give consistent results: on average, the S&P 500 index leads the 20Y Treasury bond yield by about 3 months.\nComparing Fig. 7 for the cross-correlation analysis and Fig. 5 for the TOP analysis, we can conclude that the cross-correlation analysis can extract only part of the information and the TOP method is clearly superior.\n\n\nIn this work, we have adopted the thermal optimal path method to investigate the dynamics lead-lag structure between the S&P 500 index of the US stock market and Federal Funds rate, as well as several Treasury bond yields with different maturities. The time period that has been investigated runs from August 2000 to February 2010. Both monthly and weekly data have been used and we obtained consistent results. In all cases, the S&P 500 index is found to lead the FFR and the bond yields. This is quantified by the lead function  found to be positive at a high statistical confidence level determined by bootstrapping tests. This finding is consistent with and extends significantly a previous work reporting that the US Federal Reserve was “slaved” to the stock market during the 2000–2003 US stock market antibubble [13].According to the TOP analysis, we observed that the FFR and the Treasury bond yields can be divided into two groups. The first group contains FFR, 3M, 6M, 1Y, 2Y, and 3Y bond yields with short-term maturities and the second group contains 5Y, 7Y, 10Y, and 20Y bond yields with long-term maturities. The lead functions  between the S&P 500 index and the yields in each group have very similar quantitative shapes, while they are different at a quantitative level across the two groups. We found that the short-term yields in the first group lead the long-term yields in the second group before the current financial crisis around 2007 and the inverse relationship holds afterwards, namely the long-term yields lead the short-term yields after 2007.For the first group, the lead function  increases during the time period from 2000 to 2007, followed by a two-year-long plateau, and then plummets in late 2009. We also found that the yields (including FFR) with shorter maturity in the first group have a longer lag behind the S&P 500 index than for the longer maturities. In contrast, for the second group, the lead function  increases till 2006 and then decreases. We observed a reversal of the order of the lead functions  among the different maturities in 2007: a yield with shorter maturity has a shorter lag to the S&P 500 index before the reversal point and a yield with longer maturity has a shorter lag to the S&P 500 index after the reversal point. Qualitatively, the reversal phenomenon is coincident with the outbreak of the current financial crisis.The lag of the FFR to the S&P 500 index can be interpreted in the light of comments of the previous and present Fed chairmen Greenspan and Bernanke that the growth of stock markets is “key” to the recovery and health of the economy. The evidence provided here suggests indeed that the FFR policy is in a significant part influenced by the recent past behavior of the stock markets (stock market  Federal Funds rate). In plain words, the fact that the FFR follows the stock market direction can be interpreted as a direct attempt to limit its losses and revive it in times of bearish markets or to stabilize it in times of overly buoyant bubbling markets.As for the longer maturities, the lag structure with respect to the S&P 500 index reflects (i) a natural link in the term-structure that attach the longer maturities to the shortest maturity and (ii) the aggregate strategies of investors facing uncertainties over the long term behavior of the economy [27], [28]. In the first sub-stage before 04/2007, we observe the causal relational flow from the stock market  Federal Funds rate  short-term yields  long-term yields, and afterwards, we find the flow from the stock market  long-term yields  short-term yields  Federal funds rate. Thus, the lead-lag structures between the different yields changed after the financial crisis starting in 2007. This change can be rationalized by the strategies implemented by long-term investors in the face of growing global market uncertainties, such as central banks of major Asian countries and pension funds which are heavily invested in the US long-term Treasury bonds [29]. The stern challenges faced by the US economy escalated the uncertainty which cascaded to exchange rate and inflation. Consequently, the long-term Treasury bonds became quite reactive to the behavior of stock markets, reflecting the actions of these long-term investors “flying to safety”: a plunge in the stock markets led to strong demand for the supposedly safe US Treasury bonds, pushing down mechanically the corresponding yields. This suggests that the long-term investors have been more reactive and mindful of the signals provided by the financial stock markets than the Federal Reserve itself after the start of the financial crisis. This may be due to the more complex agenda as well as the delicate role of the Federal Reserve, which has to take into account the impact of its interventions [25]. Caution and prudence on the part of the Fed in a time of high uncertainty may thus be the reason for this inversion of the lead-lag relationship between changes of yields of different maturities. However, the robust lead of the S&P 500 stock market index with respect to yields of all maturities remains the most important stylized fact unearthed by our study."
        },
        "10.1371/journal.pone.0052749": {
            "author_display": [
                "Gabriele Tedeschi",
                "Amin Mazloumian",
                "Mauro Gallegati",
                "Dirk Helbing"
            ],
            "title_display": "Bankruptcy Cascades in Interbank Markets",
            "abstract": [
                "\n        We study a credit network and, in particular, an interbank system with an agent-based model. To understand the relationship between business cycles and cascades of bankruptcies, we model a three-sector economy with goods, credit and interbank market. In the interbank market, the participating banks share the risk of bad debits, which may potentially spread a bank’s liquidity problems through the network of banks. Our agent-based model sheds light on the correlation between bankruptcy cascades and the endogenous economic cycle of booms and recessions. It also demonstrates the serious trade-off between, on the one hand, reducing risks of individual banks by sharing them and, on the other hand, creating systemic risks through credit-related interlinkages of banks. As a result of our study, the dynamics underlying the meltdown of financial markets in 2008 becomes much better understandable.\n      "
            ],
            "publication_date": "2012-12-31T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 3,
            "views": 1622,
            "shares": 5,
            "bookmarks": 19,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0052749",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0052749&representation=PDF",
            "fulltext": "IntroductionAs economic literature has taught us in more than one occasion, there are many economic examples of situations in which mainstream theory, i.e., the Arrow-Debreu general equilibrium model, does not explain interactions between economic agents well. In particular, we believe that if we want to understand the dynamics of interactive market processes, and the emergent properties of the evolving market structures, it might pay to analyze explicitly how agents interact with each other, how information spreads through the market and how adjustments in disequilibrium take place.\nTo model how the agents’ decisions are influenced by their mutual interactions and the repercussions that these may have on the economic system, we use a “communication structure” based on network theory, in which nodes can represent agents and edges connective links measuring the intensity of interaction between agents.\nThe recent vicissitudes of the credit market are a natural research issue to be analyzed with graph theory. If the banks were “isolated units”, the bankruptcy of a borrower would be almost unimportant in the credit system. However, given the strong interdependence in the interbank market, the default of one bank can bring about phenomena of financial contagion.\nIn the last thirty years, in most advanced and developing economies, the financial sector has assumed an increasing relevance with respect to the production sector; furthermore, the role of the banking system has gradually shifted from the loan based financing of non-financial corporations to more market-based activities and speculative operations. This deep transformation, usually named as financialization of the economy, has not only increased the interdependence among financial institutions, but also determined an increase of “easy credit”. This has created asset bubbles and debt-induced economic booms, with the consequent rising of corporate debt-equity ratios and bank leverage that have made the economy increasingly fragile and potentially unstable. Following the severe financial and economic crisis that started in 2007 in US, the phenomenon of growing financialization is increasingly under critical discussion as some of the major causes of the crisis. Although different important interpretations of the current crisis have been proposed (see, for instance, [1]), the effect of the increasing globalization and financialisation of the economic system is, certainly, one of the key elements to understand the current crisis.\nThree types of propagation of systematic failure have been studied in the literature. First, the bank runs, known as self-fulfilling panic [2]–[6]. Second, the asset price contagion [7], [8]. Third, the inter-locking exposures among financial institutions [8]–[13].\nFollowing this last line of research, in this paper we are explicitly concerned with the potential of the interbank market to act as a contagion mechanism for liquidity crises and to determine macroeconomics outcomes such as bankruptcies. Allen and Gale (2000), Thurner et al. (2003) and Iori et al. (2006) have shown that, modeling the credit system as a random graph, when increasing the degree of connectivity of the network, the probability of bankruptcy avalanches decreases. However, when the credit network is completely connected, these authors have proven that the probability of bankruptcy cascades goes to zero. The explanation for this result is that, in credit networks, two opposite effects interact. On the one hand, increasing the network connectivity decreases the banks’ risk, thanks to risk sharing. On the other hand, increasing the connectivity rises the systemic risk, due to the higher numbers of connected agents which, in case of default, may be compromised. According to the three cited models, the impact of the risk sharing plays a leading role. So, in these models there is a benefit in creating links between agents, because they allow to diversify risk.\nAn exception to this view is the recent contribution by Lorenz and Battiston (2008), where the authors show that the introduction of a trend reinforcement in the stochastic process, describing the fragility of the nodes, generates a trade-off. Rising the connectivity, the network is less exposed to systemic risk, in the beginning, thanks to risk sharing. However, when the connectivity becomes too high, the systematic risk eventually increases.\nA forerunner of this trade-off between risk sharing and systemic risk was already present by Iori et al. (2006), where the authors showed that, in the presence of heterogeneity, a non-monotonic relationship between connectivity and systemic risk exists.\nIn the present paper, we deal with the correlation between risk sharing and connectivity in the interbank system. In view of the recent economic crisis, in fact, the linear relationship between connectivity and systemic risk should be reassessed. Spreading the risk around the globe may indeed improve stability in good times thanks to risk sharing. However, in times of crisis, we believe that the effect of critical perturbations can spread across the whole system. Therefore, the credit market as a network with interdependent units, is exposed to the risk of joint failures of a significant fraction of the system, which may create a domino effect such as bankruptcy cascades.\nA recent model that is related to ours is that of Battiston et al. (2012a). The authors show that, in the presence of financial acceleration - i.e., when variations in the level of financial robustness of institutions tend to persist in time or to get amplified - the probability of default does not decrease monotonically with connectivity. Along this line, several authors have started to analyze the correlation between connectivity and probability of bankruptcy in credit networks. Many theoretical studies have found a non-monotonic relationship between these two variables. In particular, many recent models [14], [15] have shown that the diversification of credit risk across many agents has ambiguous effects on systemic risk.\nThe problems arising from financial market interconnectedness have also been highlighted by empirical studies which have emphasized structural properties of lending networks before and after the current financial crisis [16]–[20] and defined new analytical tools able to better identify and monitor systemic risk and crisis transmission[21]–[23].\nOur model represents a simple three-sector economic system (considering goods, credit and an interbank market), involving firms and banks. Two types of credit are considered: loan and interbank credit. According to the economic situation, companies may ask for money from financial institutions to increase their output. In this case, firms enter the credit market and consult with a fixed number of randomly chosen banks. Banks consider the investment risk and finally decide whether to offer the requested loan and define interest rates. After this first consultation meeting, each firm asks the banks it links with for credit, starting with the one with the lowest interest rate. If this bank faces liquidity shortage when trying to cover the firms’ requirements, it may borrow from a surplus bank.\nIn the interbank market, we assume a random connectivity among banks. If one or more firms are not able to pay back their debts to the bank, the bank’s balance sheet decreases. To improve its own situation, the bank rises the interest rate offered to other firms, eventually causing other defaults among firms. The bad debt of companies, affecting the equity of financial institutions, can lead to bank failures as well. Since banks, in case of shortage of liquidity, may enter the interbank market, the failure of borrower banks could lead to failures of lender banks. The interest rate, thus, can bring about a cascade of bankruptcies among banks. The source of the domino effect may, on one side, be due to indirect interactions between bankrupt firms and their lending banks through the credit market and, on the other side, due to direct interactions between lender and borrower banks through the interbank system.\nThe originality of this work compared to Battiston et al. (2012a) is the introduction of three interacting markets influencing each other. In this way, we can study the impact of systemic risk not only on the agents’ dynamics such as their financial fragility, but also on the business cycle and economic growth. In this regard, we study the effect of an exogenous shock on a specific firm by increasing the connectivity in the interbank system, and we observe that the systemic risk prevails over the advantages of risk sharing. Although the demand of loans and the number of granted loans stay almost the same by changing the connectivity in the inter-bank system, surprisingly, with higher connectivity we observe larger cascades of bankruptcies among banks. As shown in Iori et al. (2006), we find that the root of avalanches lies in the agents’ heterogeneity. In particular, our results show that the degree of contagion depends on the size of losses imposed by failing debtor banks on creditor banks in the system (see [24]–[27] for empirical analysis). Moreover, in line with other works [28], [29], we show that financial crises are characterized by the procyclicality of leverage across financial institutions.\nFurthermore, we also find that the holding of large liquid reserves, while generally stabilizing in the interbank market, reduces the growth of aggregate output by decreasing granted loans and therefore firm investments.\nThe remainder of the paper is organized as follows. First, we describe the model with the behavior of firms and banks. Then, we discuss the results of computer simulations for the baseline model and for the model with the interbank system. Finally, the last section presents conclusions.\nStructure of the ModelOur model represents a three-sector economy: goods, credit and the interbank market.\nWe consider a sequential economy populated by a large number of firms  and banks , which undertake decisions at discrete time, denoted by t = 0,1,2,…,T.\nIn the goods market, output is demand-driven, that is firms, given their production constraints, sell as much output as the market can absorb. However, incomplete information about the market potential can generate a gap between the firms’ expected and realized demand. In this disequilibrium scenario, supply does not (necessarily) match aggregate demand, so the goods market may be out of equilibrium. In this way, the model is able to generate an unexpected shock to the revenues of firms, so that their profit may become negative.\nTo meet their expected demand, companies make investments using the credit market. Therefore, in each time period, a subset of firms enter in the credit market asking for credit. The amount of credit requested by firms is related to their investment expenditure, which is therefore dependent on their expected demand, interest rate and firm’s economic situation.\nThe primary function of banks activity is to lend their funds through loans to firms, as this is their way to make money via interest rates. Banks consulted by companies, after analyzing their credit risk, may grant the requested loan, when they have enough supply of liquidity. However, since banks adopt a system of risk management based upon an equity ratio, companies may not receive requested loans even if banks have enough supply of liquidity. If consulted banks do not have liquidity to lend, they can enter the interbank market, in order not to lose the opportunity of earning on investing firms. The interbank market has the same structure as the credit market.\n\nFirms\nIn each time period t, we have a large finite population of competitive firms indexed by . The overall population  of firms is time dependent because of endogenous entry and exit processes to be described below. Firms are profit seekers. Therefore, at any time period t, they try to maximize their expected profits, by forecasting the market demand.\nFollowing some of the key elements of behavioral agent-based models, closely related to Keynes’ view that ‘expectations matter’, to Simon’s view that economic man is boundedly rational and to the view of Kahneman and Tversky that individual behavior under uncertainty can best be described by simple heuristics and biases [30]–[34], we model a gap between a firm’s actual demand  and its expected demand . Demand  is defined as(1)where  is a constant,  is a normally distributed variable and the expected demand is .\nTo produce a homogeneous output , the firm f uses its capital  as the only input. The firm’s production function is(2)where the capital productivity  is assumed to be constant and uniform across firms for simplicity. However, given the incomplete information about the demand, firm f decides to produce as much as it expects the market to be able to absorb. In this light, the production function mirrors the maximum output that firm f can produce at any time t. This amount, however, can shrink due to a lack of the expected demand.\nTo clarify, assume that  and . This means that the firm can produce up to 100 goods. If its expected demand , it will just produce 10, as it is the maximum amount that the company forecasts to be able to sell. However, if its expected demand is , the firm will produce 100, as it cannot produce more with its capital. In the latter case, the firm will ask for a loan from the credit market to increase its productivity and satisfy expected demand in the future.\nThe only external source of finance that firms have is the loan from banks [35], [36]. The firm’s demand of loan to reach the expected demand is(3)\nEq. (3) reproduces an empirical evidence: lending often increases significantly during business cycle expansions, and then falls considerably during subsequent downturns [37], [38]. Consistent with this stylized fact, Federal Reserve Chairman Alan Greenspan (Chicago Bank Structure Conference, May 10, 2001) noted that at the bottom of the cycle, ``the problem is not making bad loans […] it is not making any loans, whether good or bad, to credit-worthy customers”, consistent with the sometimes dramatic fall in lending during cyclical downturns [39]–[41]. Eq. (3) therefore should be interpreted as a new micro-foundation, and its relevance and reliability is grounded by empirical evidence. Nevertheless, since borrowing is risky, the company considers its probability of bankruptcy and its risk aversion (see, for instance [42], [43]). To incorporate these elements into the model, we assume that the firm adjusts its demand of loan according to:(4)where  is a constant which mirrors the risk aversion coefficient and may be higher, lower, or equal to one, reflecting risk lover, adverse, and neutral respectively and  reflects the firm’s financial fragility based upon its debt commitments  and expected profit  ratio. If firm f expects its next profit not to be enough to pay back its installments, it will ask for less loan.\nAt each time t, the debt commitments  (interest & installment) for the firm f are , where  is the real interest rate that firm f pays to bank b. We assume that a loan given at time t to the firm f has to be payed back by the next  periods.\nFor simplicity, we furthermore assume that each firm has total variable costs equal to financing costs. Therefore, profits in real term are(5)where the selling price of one good is set to 1. Assuming that all the profits are retained [36], the firm’s capital stock changes are updated according to(6)\n\n\nBanks\nSimilar to companies, we have a time dependent finite population of competitive banks indexed with .\nWhen a firm needs loan, it contacts a number of randomly chosen banks. This means that a firm knows the credit conditions of few banks in each time step. Each contacted bank is assumed to offer an interest rate of(7)where  is set by the Central Bank and  is the supply of liquidity of bank b. So the interest rate is decreasing as the bank’s financial robustness.\nAfter exploring the lending conditions of the contacted banks, each firm asks the consulted banks for credit starting with the one offering the lowest interest rate. Banks deal with firms in a “first come, first served” basis. If a firm asks for a loan from a bank, either it receives the complete amount of the requested loan or it receives no money (where the bank may use the interbank market or not).\nThe regulation of financial intermediaries (Basel I and II) forces banks to hold a capital caution of  of liquidity to prevent bankruptcies due to unexpected losses. For the sake of simplicity, we model this regulatory parameter assuming that banks give the requested loan with a certain probability.(8)\nThis means, for example, out of 10 different requested loans with , one loan will be given. By increasing , banks are forced to hold in reserve a larger percentage of their liquidity.  has to be interpreted as the fraction of risk that a bank is allowed to take within a given time step, as compared to its own liquidity. This threshold may be viewed as a regulatory parameter, since it imposes an upper limit for a bank’s risk dependent on its cash. It is a helpful tool to limit the bank’s risk, in particular the credit risk. Moreover, according to Eq. (8), the volume of credit given by a bank is proportional to its present liquidity. The smaller the bank the smaller its transactions.\nIf the bank regulatory parameter is satisfied and the bank has enough supply of liquidity, then it grants the requested loan.\nIf the contacted bank has not enough supply of liquidity to fully satisfy the firm’s loan, then the bank considers to use the interbank market. Our goal is to understand how the interbank structure can influence the economic cycle and the bankruptcy among banks. As in the credit market, the requiring bank asks the lacking fraction of the loan requested by the firm from x randomly chosen banks. Among the contacted banks, the banks satisfying the risk threshold in Eq. (8) and having enough supply of liquidity offer the loan to the asking bank for an interbank interest rate, which equals the credit market interest rate in Eq. (7). Among this subset of offering banks, the bank  (borrower) chooses the bank, starting with the one offering the lowest interest rate. When it receives the requested loan, the bank lend it to the asking firm.\nBank supply of liquidity , evolves according to:(9)where the second term (right side) shows the total loan of bank b at time t, the third term denotes the installment and the interest that the bank receives from the ‘safe’ firms, to which it has given a loan not before  time steps ago, and the last term, , reflects the lending by bank b from other banks at time t. Note that  can be negative or positive, depending on whether the bank is creditor or debtor. In case of interbank borrowing, as for the firms, interests and installments must be paid back within the next  periods. When, for instance, we consider the borrower bank ,  is(10)where  is the credit that the bank  obtain from . It is important to underline that  is immediately used by  to lent firm f.\nLike companies, banks are profit seekers. A bank’s profits in time t is:(11)\nThe bank’s profit depends on the interests payed by firms (first term), on the firms’ bad debt, with  to be the share of loan that firms could not pay back because they went bankrupt (second term) and on the interbank credit (third term). Note that  is positive if the bank lends in the interbank system, otherwise zero. Considering the lending bank ,  is(12)\nAs in Eq (11), the first term mirrors interests payed by debtor banks and the second term is the banks’ bad debt (losses).\n\n\nBankruptcy Conditions and Demography of Firms and Banks\nBecause of the uncertain environment, agents may go bankrupt. In this model, bankruptcy happens to firms or banks when they do not have enough ‘cash’ (revenues) to pay their loans back. In this sense, we are much closer to the idea of liquidity crisis than to the financial fragility conditions of Greenwald and Stiglitz framework. When agents go bankrupt, they leave the market. We also assume that an agents leave the market if it fails to receive requested loans for s consecutive time steps.\nRegarding entries, we follow the approach of Delli Gatti et al. (2005). The economic literature has suggested models ranging from exogenously stochastic processes [44], where authors assume a simple mechanism of entrance based on a one-to-one replacement, to models with an endogenous entry process, which depends on expected profit opportunities [45], [46]. These last theories argue that the entrance of new firms in an industry will be influenced by the amount of sunk costs in the sector. A greater degree of sunk costs should reduce the likelihood of entry (see [47], [48] for empirical evidence).\nOur modeling strategy aims at reproducing this evidence. The number of new entrants () is obtained by multiplying a constant  with a probability, which depends negatively in the case of firms and positively in the case of banks on the average lending interest rate:(13)where d and e are constants. The higher is the interest rate, the higher are firms debt commitments, and the lower (higher for banks’ side) are expected profits, with entries being lower (higher for banks’ side) in number.\nMoreover, in line with the empirical literature on firm entry ([49]; [50]), we assume that entrants are on average smaller than incumbents, with the stock of capital of new firms and the supply of liquidity of new banks being a fraction of the average stocks of the incumbents. So, entrants’ size in terms of their capital stock is drawn from a uniform distribution centered around the mode of the size distribution of incumbent firms/banks.\n\nSimulation ResultsWe explore the dynamic properties of the economic system modelled above by means of computer simulations. We consider an economy initially consisting of  firms and  banks and study it over a time span of  periods. Each firm is initially given the same amount of capital  and demand . We fix , , , . Firm entrance parameters are , , and .\nEach bank is initially given the same amount of liquidity . We fix the Central Bank interest rate , , , , and . Despite the homogeneous initial conditions, the economy develops heterogeneous distributions through the interaction of noise and feedback effects.\nIn order to get rid of transients we evaluate only the last 1600 simulated periods. Simulations are repeated 100 times with different random seeds.\n\nStylized Facts of the Benchmark Model\nLet we start from a sort of “benchmark” setup, for which the model jointly accounts for an ensemble of stylized facts regarding both “micro/meso” aggregates such as indicators of industrial structures (e.g. firm size distributions and firm growth rates) together with macro statistical properties (including rates of output growth and output volatility).\nFirst of all, the model robustly generates endogenous self-sustained growth patterns characterized by the presence of persistent fluctuations, as shown in Figure 1 (left side).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Evolution of the aggregate output (left side) and growth rates of the aggregate output (right side), as a function of time.doi:10.1371/journal.pone.0052749.g001Indeed, aggregate fluctuations, measured by output growth rates (right side of Figure 1), are path dependent (i.e., nominal shocks have real and permanent effects). Moreover, they are characterized by cluster volatility, a well-known property in the financial literature (see for instance [51]). This implies that large changes in variable values tend to cluster together, resulting in a persistence in the amplitudes of these changes. A quantitative manifestation of this fact is that, absolute growth rates display a positive, significant and slowly decaying autocorrelation function. In our case, the autocorrelation parameter is equal to 0.95, a value very close to that found for the quarterly empirical data for the G7 countries, which is 0.93 [52].\nIn addition to fluctuations resembling business cycles, the simulated time path of aggregate activity is characterized by a broken-trend behavior. The model is able to generate an alternation of aggregate booms and recessions as a non-linear combination of idiosyncratic shocks affecting individual decision-making processes. The account of business cycles offered by the agent based model thus contrasts sharply with DSGE theory, according to which fluctuations in aggregate activity are explained by random variations in aggregate TFP growth. In our simulations, depressions are due to the failure of big firms. Indeed, since we do not impose any aggregate equilibrium relationship between the firms actual demand and their expected demand, our simulated market generates individual out-of-equilibrium dynamics. Due to the absence of any exogenously imposed market-clearing mechanism, the economy is allowed to self-organize towards a spontaneous order with persistent excess demands, which have important consequences on the dynamic of firms. In fact, the gap between the expected and actual demand may generate an unexpected shock to firms’ profits, able to trigger bankruptcies of firms. If one or more companies are not able to pay back their debts to banks, then also banks suffer with a decrease in their equity level. Consequently, in order to improve their own situation, banks rise the interest rate to all the firms in their portfolio, eventually causing other defaults among companies. Figure 2 (left side) displays the time series of firm defaults, which are roughly constant during the simulation even when the system experiences severe breakdowns. This feature of the model underlines the important role of heterogeneity. In fact, in Figure 2 (right side), we show that crises do not depend on the quantity of bankrupted agents, but on their ‘quality’. The same economic process can thus produce small or large recessions depending to the size of failed companies.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Time evolution of firm bankruptcies (left side) and decumulative distribution function of failed firms’ size (right side).doi:10.1371/journal.pone.0052749.g002In addition, it is important to note that the model provides an useful tool to predict crises. In line with Minsky’s Financial Instability Hypothesis (1992), we show that over periods of prolonged prosperity and optimism about future prospects, financial institutions grant more loans without considering borrowers financial fragility. A natural way to assess the co-movement between the increase (decrease) in aggregate output and increase (decrease) in the number of granted loans is to study their correlation. The Pearson correlation coefficient significant at 1% level between positive aggregate output changes and the number of granted loan reaches a value above 0.63, confirming higher credit levels in prosperous periods. However, it can happen that banks underestimate their credit risk, making the economic system more vulnerable when default materializes. In this case, we observe a negative correlation of 0.71 between aggregate production in time t and the leverage of firms in the previous time step.\nFigure 3 shows time series of granted loan (left side) and the inverse of firms leverage. The balance sheet identity implies that firms can finance their capital stock by recurring either to net worth () or to bank loans (), . From Eq (6) we can easily calculate firm equity . So, the leverage is equal to . In the graph (3) (left side), we plot . Comparing Figure 3 and Figure 1 (left), we observe that these three time series co-evolve. In particular, the simulated aggregate output suffers a severe crisis in , which is anticipated by a rapid increase in the financial fragility in the previous time steps (in fact the inverse of leverage decreases rapidly, as shown in Figure 3 (left)). Our findings support Minsky’s view. Expectations exceeding the actual demand are the main driving force behind over-leveraging and investing in riskier projects. When firms expect to be able to sell higher levels of output, they increase their loans. Banks, facing incomplete information about the true probability of good and bad outcomes, increase their borrowing to expand their balance sheet. This results in much higher defaults and financial instability once a bad state occurs (see [53]–[57] for empirical evidence).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Time series of granted loan (left side) and the inverse of the firms’ leverage (right side).doi:10.1371/journal.pone.0052749.g003Although companies in our model initially start with the same amount of capital and cash, trading generates a fat tail distribution of agents’ size, in accordance with the empirical evidence that, in real industrialized economies, market participants are very heterogeneous in dimension (see for example, [58]–[63]). Small and medium size firms -here we use firm production as proxy of firm size - dominate the economy. Large firms are relatively rare, but they represent a large part of total supply. When the firms size distribution is skewed, the mean firm size is larger than the median one, and both are larger than the modal firm size. Clearly, in this case the very notion of a representative firm is meaningless.\nFigure 4 (left side) displays this evidence and the distribution is well fitted by a power law distribution , with intercept 12.19 and slope −0.23. The result is robust to the Kolmogorov-Smirnov test.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Decumulative distribution function of firm sizes (left side) and bank sizes (right side).doi:10.1371/journal.pone.0052749.g004Our analysis on banks sizes (see Figure 4 (right)) reveals a similar skewed distribution. In this case, the Kolmogorov-Smirnov test is consistent with the null hypothesis of a lognormal distribution of bank sizes [64], [65].\n\n\nDefault Cascades in the Interbank Market\nIn this section we explore the impact of the interbank market, in which each bank can be borrower and lender, at the same time, on the macroeconomic dynamics. In particular, we investigate the effect of credit risk and systemic risk on the aggregate fluctuations and on the dynamic of default cascades of banks.\nSince the purpose of this exercise is to study the evolution of a self-contained system with a given initial number of banks, we exclude the possibility that failing banks would be replaced by new entrants.\nThe first question concerns the role of reserve requirements, reflected by the  parameter in Eq (8). Figure 5 shows how different reserve ratios affect the fraction of surviving banks for the case of no interbank credit market (Higher  means higher reserves). As the reserve ratio  increases, the rate of bank failures clearly falls. This result is in line with other publications regarding the role of reserves (see Thurner et al. (2003) and Iori et al. (2006)). Obviously, increasing reserves contribute to the stability of individual banks, as shown by a lower value of average bank leverage (see center of Figure 5). However, increasing reserves reduces the output growth rate, since many firms do not get loans in the credit market (see right side of Figure 5).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Time evolution of the number of surviving banks for different levels of reserve ratios:  (solid line),  (dotted line) and  (long dashed line) (left side).Average bank’s leverage over time and simulation as a function of  (center). Average output growth rate over time and simulation as a function of  (right side).\ndoi:10.1371/journal.pone.0052749.g005We now analyze how different degrees of linkage in the interbank market affect the bankruptcy of financial institutions.\nThe left panel of Figure 6 displays the number of surviving banks as function of time, for various numbers x of financial institutions each bank randomly links with. By increasing linkage, the systemic risk raises in the sense that in any period, more banks fail. Indeed, with 100 percent linkage, the system collapses completely, analogously to a tragedy of the commons [66]. This result is further analyzed by Figure 6 (center), which shows the average number of surviving banks, over all times and all simulations as a function of the number of interbank linkages. While the earlier empirical literature on the systemic risk, in line with Allen and Gale’s result on the risk sharing role, found a very little evidence of global vulnerability [26], [67]–[69]. Strong evidence has been collected after the default of Lehman Brothers, showing that interbank linkages strongly impact systemic risk (see Battiston et al. (2012a), [70], Wagner (2010)) through a high probability of domino effects. So, in line with these new empirical and theoretical works, we find that the default of an agent may increase the systemic risk by increasing the connectivity.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Time evolution of the number of surviving banks with  for different interbank linkages:  (solid line),  (dotted line),  (dashed line),  (long dashed line),  (dot-dashed line) (left side).Average number of surviving banks as a function of x (center). Average absolute slope of the curve representing the number of surviving banks (right side) as a function of x.\ndoi:10.1371/journal.pone.0052749.g006Moreover, increasing x, not only the number of bankruptcies increases, but the time path of surviving banks also declines much more rapidly over time. This result is shown in the right panel of Figure 6, where we plot the average absolute slope of the number of surviving banks curve as a function of x. This graph provides a first evidence of contagious failures, that is periods in which many banks collapse together.\nIn line with our hypothesis that a higher connectivity generates a higher systemic risk, not offset by a lower credit risk, Figure 7 shows, on the left, that the banks’ financial fragility increases with interbank linkages.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 7.  Average bank’s leverage (left side).Average output growth rate (right side), over time and simulation as a function of x.\ndoi:10.1371/journal.pone.0052749.g007To understand if different linkages in the interbank market have some effect on the real economy, Figure 7 displays on the right hand side the average output growth rate as a function of x before bankruptcy cascades occur. One can immediately see that increasing the interbank connectivity has no effect on system growth. Companies have no benefits from a more strongly linked interbank market. In fact, it does not facilitate the granting of loans to enterprises, but it merely transfers liquidity among financial institutions.\nWe now turn to the issue of contagious failures. Banks are prone to default by bad debits of both the firm-bank credit market and the interbank market. To ensure that the higher number of bank bankruptcies in the case of a highly connected interbank market is not only the result of bad debits in the firm-bank market, but also is the result of more bad debits in the interbank market, we run the following experiment: we calculate the size of the largest connected component of the failed banks, which are connected by bad debits, in 100 simulations for each value of linkage in the interbank market (see Figure 8). As expected, a more inter-connected interbank market results in larger cascades of bankruptcies due the larger systemic risk.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 8.  Size of the largest bankruptcy cascades, which are connected by bad debits for a bank market of size 50, determined from 100 simulations for interbank linkages of 1, 5, 10, and 49.A highly connected interbank market results in large cascades of bankruptcies.\ndoi:10.1371/journal.pone.0052749.g008As for firms, we can infer that bankruptcy cascades depend on the size of failed banks -here we use bank liquidity S as proxy of bank size -. In fact, the distribution of failed banks for different interbank linkages is skewed (see left panel in Figure 9). Moreover, increasing the interbank connectivity creates fatter tails in the distribution of failed banks, as evidenced by a higher kurtosis (see center of Figure 9). A more precise measure of fat tails is provided by the Hill exponent. In the right panel of Figure 9, we plot the Hill exponent as a function of x. Empirically the tail exponent is found to take values between 2 and 4. Changing the parameters of the model our simulations generates values of the Hill exponent in the same range. When , that is for low connectivity in the interbank market, the tail exponent is closer to the “normal” value of 4. However, increasing , the model generates fatter tails.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 9.  Decumulative distribution function of failed bank’s size S, for  (solid line),  (dotted line),  (dashed line),  (long dashed line) and  (dot-dashed line) (left side).Kurtosis (center) and Hill exponents (right side) of failed banks distribution as a function of x.\ndoi:10.1371/journal.pone.0052749.g009\n\nConclusions\nIn this paper we have investigated systemic risk and the impact of sharing risk and in an interbank market. We have studied the agents’ financial fragility and the macroeconomic performance. The focus has been on how the emergent heterogeneity of market participants and the nature of their interconnectedness affect the trade off between mutual insurance and the potential for contagion.\nWe have shown that a higher banks connectivity not only increases the agent’s financial fragility, but also generates larger bankruptcy cascades due the larger systemic risk. Interestingly, high interbank linkages have no effect on economic output, even during boost/boom. The interbank market, in fact, just has a marginal effect on firms’ investments and on the granted loans. In contrast, higher bank reserve requirements stabilize the economic system, not only by decreasing financial fragility but also dampening avalanches. However, holding in reserve a larger percentage of banks’ equity affects the aggregate output growth by reducing credit to companies.\nOur simulation results also indicate that heterogeneity contribute to instability. Although this result is strictly related to the dynamic of our model, other theoretical studies [10], [71] have shown that the possible emergence of contagion depends crucially on the degree of heterogeneity. Indeed, when the agents’ balance sheets are heterogeneous, banks are not uniformly exposed to their counterparty. Therefore, if the contagion is triggered by the failure of a big bank, which represents the highest source of exposure for its creditors, the situation is certainly worse than when agents are homogeneous. One policy implication is that interbank lending relationships should be restricted to banks that share similar liquidity characteristics. These results may be specific to our model, but they offer stimulating insights into the nature of contagion.\nThe main limitation of this study is that our model is fully demand-driven, i.e. firms can sell all the output that market exogenously can absorb at a fixed price. In a future paper, we will extend this analysis by including endogenous prices, which will allow us to investigate the demand side as well. Furthermore, we will introduce a more realistic mechanism of interbank linkages, by modeling network structures in an evolutionary way.\n\n"
        },
        "10.1371/journal.pone.0069792": {
            "author_display": [
                "Sara Evans-Lacko",
                "Martin Knapp",
                "Paul McCrone",
                "Graham Thornicroft",
                "Ramin Mojtabai"
            ],
            "title_display": "The Mental Health Consequences of the Recession: Economic Hardship and Employment of People with Mental Health Problems in 27 European Countries",
            "abstract": [
                "Objectives: A period of economic recession may be particularly difficult for people with mental health problems as they may be at higher risk of losing their jobs, and more competitive labour markets can also make it more difficult to find a new job. This study assesses unemployment rates among individuals with mental health problems before and during the current economic recession. Methods: Using individual and aggregate level data collected from 27 EU countries in the Eurobarometer surveys of 2006 and 2010, we examined changes in unemployment rates over this period among individuals with and without mental health problems. Results: Following the onset of the recession, the gap in unemployment rates between individuals with and without mental health problems significantly widened (odds ratio: 1.12, 95% confidence interval: 1.03, 1.34). This disparity became even greater for males, and individuals with low levels of education. Individuals with mental health problems living in countries with higher levels of stigmatizing attitudes regarding dangerousness of people with mental illness were more vulnerable to unemployment in 2010, but not 2006. Greater agreement that people with mental health problems have themselves to blame, was associated with lower likelihood of unemployment for individuals with and without mental health problems. Conclusion: These findings study suggest that times of economic hardship may intensify social exclusion of people with mental health problems, especially males and individuals with lower education. Interventions to combat economic exclusion and to promote social participation of individuals with mental health problems are even more important during times of economic crisis, and these efforts should target support to the most vulnerable groups. "
            ],
            "publication_date": "2013-07-26T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 7,
            "views": 6580,
            "shares": 106,
            "bookmarks": 33,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0069792",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0069792&representation=PDF",
            "fulltext": "IntroductionSeveral studies have demonstrated large disparities in unemployment rates between people with and without mental illness. Although most people with mental illness want to work [1], they have higher unemployment rates than people without mental illness and compared to people with other chronic diseases [2]–[4]. High unemployment rates among individuals with mental illness are a major contributor to the substantial societal impact of these disorders [4]–[6]. Unemployment has an impact upon the course and outcome of mental illness [7] and excludes individuals from social participation. A period of macro-economic recession may be particularly difficult for people with mental health problems as they may be at higher risk of losing their jobs and more competitive labour market conditions may make it more difficult for them to find a new job in the first place [8]. This is especially important as research suggests that unemployment could present a specific barrier to recovery from mental illness [9], [10].\nUnemployment among people with mental illness may be aggravated during times of economic hardship [7], [11], [12]. Negative attitudes towards marginalized groups (e.g., ethnic minorities or immigrant groups) which often increase during an economic recession [13] are one possible factor influencing this trend. Recent research from Germany suggests that the German public's unwillingness to recommend an individual with depression for a job increased between 2000 and 2011 (i.e., during the period of the economic recession) compared to 1990–2000 [14]. A synthesis of public attitude trends in the US between the 1950s and 1990s showed improvements and declines which mirrored the economic and employment context of the country [7]. Findings regarding the impact of economic recession on disparities [15] and the mechanisms involved, however, are mixed [15]–[18] and we need to better understand the complexity of this relationship. Interestingly, one study [16] did not show that individuals with severe mental illness were at earlier risk of unemployment during times of economic contraction; however, this study specifically investigated individuals with severe mental illness who received occupational rehabilitation services and these results may not be broadly generalizable to the wider population of people with mental illness. Furthermore, the effects of the recession since 2008 on disparities are yet to be determined.\nIn addition to research which suggests that mental health problems increase during times of economic recession, we investigate the impact of the economic recession on people with mental health problems and how this may be mediated by stigma. This paper investigates the impact of economic hardship on unemployment rates of people with mental health problems using Eurobarometer survey data collected from 27 EU countries in 2006 and 2010. We test the hypothesis that the European macro-economic crisis since 2008 has had a greater impact on employment of people with mental health problems compared to people without mental health problems. We also hypothesise that the impact on individuals with mental health problems is greater for people living in regions with greater public stigma towards people with mental illness, after controlling for regional unemployment rates. Additionally, as some research suggests that certain population subgroups, such as men or individuals with low levels of education [19], may be particularly vulnerable during times of economic recession, we investigate whether there is a differential impact of the recession on these subgroups in relation to unemployment.\nMaterials and Methods\nData Source\nFull details of the design and sampling for the Eurobarometer surveys (Eurobarometer Mental Well-being 2006 and Eurobarometer Mental Health 2010) are given elsewhere [20], [21]. Data were collected via face-to-face interviews among European Union (EU) citizens (n = 29,248 in 2006 and n = 26,800 in 2010) residing in the 27 member states (approximately 1,000 individuals per country per year). For our analysis we restricted the sample to adults of working age (i.e., 18–64) (n = 20,368 in 2006 and n = 20,124 in 2010).\nThe initial mental health Eurobarometer survey was conducted in 2006 (fieldwork carried out between 7 December 2005 and 11 January 2006). A second survey assessing attitudes toward mental illness and treatment-seeking was administered to a new sample of respondents in 2010 (between 26 February and 17 March 2010). All participants were recruited via multistage random probability sampling. Participants were representative of residents aged 15 or older in the participating countries.\n\n\nAssessments\nMental health problems were assessed via the Mental Health Inventory (MHI-5), a well-validated and reliable measure derived from the Short Form 36 (SF-36) [22], [23] As a validated cut-point has not been established for the MHI-5 [22], for the purposes of this study, individuals scoring one standard deviation higher than the standardised mean score were categorised as having mental health problems.\nStigmatising attitudes towards individuals with mental health problems were assessed in Eurobarometer 2006 using four questions about various stigmatizing beliefs: (1) People with psychological or emotional health problems constitute a danger to others; (2) People with psychological or emotional health problems are unpredictable; (3) People with psychological or emotional health problems have themselves to blame and (4) People with psychological or emotional health problems never recover. Participants were asked how much they agreed with each statement. Response options were on a 4-point Likert scale from ‘totally disagree’ to ‘totally agree’. Participants who responded ‘totally agree’ or ‘tend to agree’ to each statement were considered as agreeing with that statement. Responses were aggregated within each country to obtain a country-level measure of stigmatizing attitudes.\nSocio-demographic information included age band (18–29, 30–39, 40–49 and 50–64 years), gender, education level (age at which individuals finished full-time education), and urbanicity (i.e., size of locality of respondent residence: large town, small or middle sized town or rural area/village). Current employment was assessed via the question: ‘What is your current occupation?’ Individuals could endorse the following categories: (1) responsible for ordinary shopping and looking after the home, or without any current occupation, not working (referred to throughout the paper as ‘home-maker’), (2) student, (3) unemployed or temporarily not working, (4) retired or unable to work through illness, or (5) in paid employment.\n\n\nNational level unemployment rates\nNational unemployment figures for the years 2006 and 2010 were taken from the Eurostat yearbook (http://epp.eurostat.ec.europa.eu/statist​ics_explained/index.php/Europe_in_figures_-_Eurostat_yearbook). Eurostat is a Directorate-General of the European Commission and the statistical office of the European Union. The Eurostat figures for 2006 were moderately highly correlated with the national unemployment rates calculated from the Eurobarometer data (r = 0.76 and 0.70, respectively).\n\n\nStatistical Analysis\nFour separate multivariable logistic regression models were used to examine predictors of unemployment for individuals with and without mental health problems in 2006 and 2010. Independent variables included age, gender, urbanicity, country-level attitudes regarding dangerousness, recovery, blameworthiness, and unpredictability of people with mental illness. Country-level variables were computed as an average rating for each country and each variable was standardized. Eurobarometer post-stratification weights, based on sex, age, region and size of locality, were used in all analyses to estimate the country-level averages. We used generalized estimating equations (GEE) with the robust variance estimates to model within-country correlations. In the absence of theoretical reasons for specifying a correlation matrix structure, we used an unstructured correlation matrix [24]. In order to investigate whether individual unemployment status differed by population subgroups of interest (i.e., men, individuals with low levels of education and younger individuals) following the recession, we first tested the interaction between survey year and these variables and then tested a three-way interaction between survey year, mental health problems and these variables. All analyses were carried out using SAS version 9.3.\n\n\nEthics statement\nEthical approval was not required as this was secondary data analysis.\n\nResults\nSocio-demographic characteristics (Table 1)\nCompared to individuals without mental health problems, individuals with mental health problems were disproportionately women (χ2 = 125.2, df = 1, p<0•001 in 2006 and χ2 = 87.9, df = 1, p<0•001 in 2010) and older (χ2 = 316.9, df = 3, p<0•001 in 2006 and χ2 = 93.9, df = 3, p<0•001 in 2010). The majority of people with and without mental health problems had completed education at least to 16 years of age; however, more of those without mental health problems finished education at age 20+ or were still studying (χ2 = 210.1, df = 1, p<0•001 in 2006 and χ2 = 237.8, df = 1, p<0•001 in 2010). A higher proportion of people with mental health problems had no formal education or only finished education at 15 years of age (χ2 = 313.8, df = 1, p = p<0•001 in 2006 and χ2 = 213.7, df = 1, p<•0001 in 2010). Individuals with mental health problems were less likely to be in paid employment or to be a student or home-makers and more likely to be unemployed or disabled/retired, (χ2 = 452.6, df = 4, p<0•0001 in 2006 and χ2 = 109.4, df = 4, p<0•0001 in 2010).\n\n\nTrends in unemployment among people with mental health problems\nUnemployment rates were higher among people with mental health problems compared to those without in both survey years (Table 1). Overall unemployment rates were also higher in 2010 compared to 2006. The gap in unemployment rates between individuals with and without mental health problems widened in 2010 compared to 2006 (Figure 1). The differential trend was statistically significant (odds ratio [OR] for the interaction term for mental health problems by year = 1.12, 95% confidence interval [CI]: 1.03, 1.34. We performed several types of sensitivity analyses to test the robustness of this relationship. We investigated additional cutpoints for individuals scoring in the top ten and the top five percent of mental health problems and their likelihood of unemployment relative to the rest of the population. The differential trend was also statistically significant for these groups: the p-value for the interaction term for mental health problems by year for the top ten percent was 0.020 and the top five percent was 0.018. We also conducted additional sensitivity analyses applying an instrumental variable approach in which individual mental health problems were considered to be endogenous to the model and this also showed a significant relationship and the interaction term for year and mental health problems was also significant (p<0.001).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Average unemployment rates among individuals in Eurobarometer 2006 and 2010, stratified by presence of mental health problems (aged 18–65).doi:10.1371/journal.pone.0069792.g001\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Descriptive statistics among people with and without mental health problems in Eurobarometer 2006 and 2010.doi:10.1371/journal.pone.0069792.t001\n\nRelationship between unemployment and mental health status\nIn each of the survey years, local unemployment rates ascertained by Eurostat were strongly associated with the odds of being unemployed among participants both with and without mental health problems in Eurobarometer (Table 2). Among people with mental health problems, males were more likely to be unemployed than females in 2010 (OR: 1.58, 95% CI: 1.30, 1.92, p<0.001) and marginally more likely to be unemployed than females in 2006 (OR: 1.24, 95%CI: 0.99, 1.57, p = 0.067). The interaction term for gender and year was statistically significant for the entire sample (p<0.001) and among individuals with mental health problems (p<0.01), but not among those without mental health problems. In 2010, 21.7% of men with mental health problems were unemployed, compared to 13.7% in 2006. For women with mental health problems, the difference in unemployment rate between 2010 (15.6%) and 2006 (11.9%) in 2006 was narrower.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Results of multivariable logistic regression analyses for predictors of unemployment stratified by presence of mental health problems in Eurobarometer 2006 and 2010.doi:10.1371/journal.pone.0069792.t002In both 2006 and 2010 individuals in the youngest age band (18–29 years), with and without mental health problems, were more likely to be unemployed than individuals in the oldest age band (50–64 years). However, age patterns of unemployment in both survey years varied among those with and without mental health problems in that the younger age was more strongly associated with unemployment among those without mental health problems (p<0.001). Indeed, the unemployed with mental health problems were significantly older than those without mental health problems (mean age = 4.3 vs. 36.1, t-test = 10.16, p = 0.001).\nFewer years of education was significantly associated with unemployment among individuals with and without mental health problems; however, education was more strongly associated with unemployment among individuals with mental health problems compared to those without these problems (p = 0.001). The impact of education on employment was also more substantial during 2010 compared with 2006 among individuals with mental health problems only (p = 0.010). This interaction was also significant among the entire sample (p = 0.020), but not among those without mental health problems.\nUrbanicity (i.e., size of the town where participants were recruited) did not play a major role in likelihood of unemployment except that individuals with mental health problems who lived in a large town relative to a rural area were more likely to be unemployed (Table 2), which could be interpreted as implying that a larger labour market disadvantages those with mental health problems.\nDuring 2010, but not 2006, among individuals with mental health problems only, living in a country where a higher proportion of the general public agreed that people with mental health problems are dangerous was associated with a higher likelihood of being unemployed (Table 2). During 2006, individuals with mental health problems living in a country where a lower proportion of the general public agreed that people with mental health problems have themselves to blame were more likely to be unemployed. This relationship was maintained in 2010. Living in a country where a higher proportion of the general public agreed that people with mental health problems will never recover was associated with a marginally higher likelihood of being unemployed among individuals with mental health problems (p = 0.097).\n\nEconomic recession has had enormous impacts across much of Europe; however, little information is available about the specific impact of the recession on groups who are already vulnerable to social exclusion, specifically individuals with mental illness. This is the first study to demonstrate that the European economic crisis had a greater impact on people with mental health problems, compared to people without mental health problems, as measured by exclusion from employment. Our study also identified important sub-groups which experienced greater impacts of the economic recession in terms of unemployment, specifically men and individuals with low levels of education. Overall, males and individuals with lower levels of education appear to have been affected disproportionately by the recession; both groups had a significantly greater increase in likelihood of being unemployed following the recession. Moreover, for individuals with mental health problems, gender and level of education were particularly important determinants of employment status as the recession seemed to have a disproportionately higher negative impact on their likelihood of being employed for men and those with less education. This may be due to shifts in labour markets: other studies have suggested that men may be more vulnerable to unemployment during the current recession in Europe as they are more likely to work in construction and manufacturing jobs which are more vulnerable to decreases in demand and job loss [25], while other research suggests that this disparity is only evident during the initial stages of a recession [11].This study also showed that stigmatizing attitudes, specifically beliefs regarding dangerousness of individuals with mental health problems, could be an important mediator in the relationship between unemployment and mental health problems following the recession. Living in a country where a higher proportion of individuals believe that individuals with mental illness are dangerous was associated with a higher likelihood of unemployment for people with mental health problems, but did not influence employment rates for those without mental health problems. Moreover, this became significant in 2010, following the economic recession. Other studies have emphasised the persistence of attitudes related to dangerousness and their association with community rejection [26]. Research on racial discrimination suggests that stereotype amplification in relation to risk and fear of victimisation plays an important role in the persistence of racial inequalities and community segregation [27]. These attitudes may be internalised by the stigmatised group. Recent international work underscores the prevalence of experienced and anticipated discrimination among people with depression in relation to employment, suggesting that this is a critical barrier to achieving employment integration [28]. A recent analysis of trends in public attitudes toward people with mental health problems in England and older research from the U.S. also suggested that attitudes to people with mental health problems may harden during periods of economic crisis [7], [29]; however, there is a gap in research around this topic. Surprisingly, a higher proportion of the public endorsing blameworthiness was consistently associated with lower rates of unemployment among people with mental health problems. Previous research has found that stigmatizing attitudes are highly specific in their relation to impact on people with mental health problems. For example, living in a community with stronger beliefs about blameworthiness of individuals with mental illness is associated with lower rates of willingness to seek professional help [30] but also lower levels of perceived discrimination among people with mental health problems [31]. Other research has shown that world views such as stronger just world beliefs for self may be a double edged sword as they are associated with greater blameworthiness; but also lower self stigma among people with mental illness [32]. It could be that environments with greater endorsement of blame and controllability of symptoms and/or illness also engender a context where the guilt and blame associated with those who are not working is increased. Thus, any intervention would need to carefully consider the complexity of cultural factors and beliefs underlying individual and public attitudes.Previous studies have demonstrated the impact of the recession on public health more generally [33]–[35], however, the selective impact of recession on people with mental health problems, especially males or individuals with lower levels of education, should be acknowledged through both research and policy. Analysis of general government policy responses in Europe following the crisis reveals deficiencies and problems and suggests that governments should allocate resources toward keeping and reintegrating people into employment in addition to initiating programmes that help people cope with the negative effects of job loss to counteract the adverse health effects of the recession [33]. Highlighting the population subgroups who are most vulnerable to economic shocks and identifying ways to mitigate the effects of these shocks is also important. It may be that investment in targeted programmes such as debt advice for people with mental health problems may improve their mental health and financial circumstances [36], [37]. Given the cuts in mental health services across Europe, the impact of the recession is likely to be felt among a growing number of individuals alongside dwindling resources. Lack of resources may strain mental health services during times of higher need leading to decreased access in the face of increased need. In Spain where the impact of the recent recession has been among the greatest, the prevalence of mental disorders diagnosed in primary care settings is increasing. These increases are associated with increases in unemployment and also present among individuals whose employment is threatened and also those who are struggling to make payments on their mortgage [38]. Recent findings from both England and Spain suggest that the recession is associated with a deterioration in population mental health [19], [38]. In addition to people with mental health problems generally, it is important to acknowledge specific subgroups with mental health problems, such as males and those with lower education. In addition to having a higher likelihood of unemployment, these subgroups have lower rates of help-seeking and more negative attitudes about mental illness [29], [39] and thus, may require specific forms of outreach.This study presents new and important information about the impact of macroeconomic downturn on people with and without mental health problems in Europe using nationally representative data from 27 countries in Europe surveyed over two time points, before and after the onset of the current recession. Nevertheless, the data were not collected with the specific aims of this study in mind and were not longitudinal in nature as the same individuals were not interviewed in the two surveys. Mental health status was determined via a brief self-report measure and thus mental health problems were not verified by a clinician. Additionally, type and severity of problems were not assessed. Most previous research on employment of individuals with mental health problems and also on mental illness stigma has focused on those with severe mental disorders which could not be identified in the Eurobarometer data. Additionally, data on potentially important characteristics such as ethnicity and immigration status or survey response rates were not available. The investigation was limited to two time points only and although the impact of economic recession was clearly evident in 2010, long term effects could not be investigated. Relatedly, as these are observational data, our analyses could not rule out reverse causality, and the potential that people who were unemployed were more likely to develop mental health problems in 2010. Other research has suggested that a large proportion of the consequences of unemployment such as mortality are due to mental health related selection prior to becoming unemployed [40] suggesting that this is an important mechanism to investigate. Our main outcome of interest was unemployment; however, there may be other important effects of the economic crisis in terms of social exclusion which we were not able to examine. As Eurobarometer recruited individuals by household, we were not able to investigate individuals who may have transitioned into more extreme types of exclusion i.e., individuals who became homeless, were in care or hospital settings or were imprisoned. Finally, attitudes about people with mental illness were only collected at one time point in 2006 which precludes assessment of changes in public attitudes over time and its potential impact on unemployment trends. However, the assessment of attitudes preceded the economic crisis and so was not confounded by the effects of the recession.Past research has consistently shown that people with mental health problems tend to be excluded from employment, housing and social relationships, and that this exclusion has negative social and economic consequences [8]. This study suggests that times of economic hardship are likely to heighten such exclusion for people with mental health problems. The study also provides some preliminary clues as to which groups of individuals with mental health problems are especially vulnerable during times of economic hardship, and what societal factors might moderate this adverse relationship. Use of both individual-level and aggregate-level data to explore this relationship provides new and important evidence about the impact of the macro-social context on individuals during times of economic recession and facilitates micro-macro research in relation to mental health and exclusion [41], [42]. Findings suggest that programmes to combat exclusion and to promote mental health may be more important during times of economic crisis. Future research should examine the long term effects of the economic recession on people with mental health problems and the relationship between different types of employment and social welfare policies and unemployment rates for people with mental health problems."
        },
        "10.1371/journal.pone.0040693": {
            "author_display": [
                "Petre Caraiani"
            ],
            "title_display": "Evidence of Multifractality from Emerging European Stock Markets",
            "abstract": [
                "\n        We test for the presence of multifractality in the daily returns of the three most important stock market indices from Central and Eastern Europe, Czech PX, Hungarian BUX and Polish WIG using the Empirical Mode Decomposition based Multifractal Detrended Fluctuation Analysis. We found that the global Hurst coefficient varies with the q coefficient and that there is multifractality evidenced through the multifractal spectrum. The exercise is replicated for the sample around the high volatility period corresponding to the last global financial crisis. Although no direct link has been found between the crisis and the multifractal spectrum, the crisis was found to influence the overall shape as quantified through the norm of the multifractal spectrum.\n      "
            ],
            "publication_date": "2012-07-17T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 4,
            "views": 1379,
            "shares": 0,
            "bookmarks": 0,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0040693",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0040693&representation=PDF",
            "fulltext": "IntroductionThere is a long interest in modeling financial markets that span well beyond the disciplines of finance and economics attracting mathematicians, physicists and many others from different fields. The attractiveness of financial markets comes not only from its complex dynamics that result from the interactions of a multitude of agents but also from its presence and influence in our daily life as the last financial crisis has proved it. One of the questions that emerged in the last decades was whether financial markets are characterized by chaos and fractality.\nBefore going further, we clarify a few key concepts for the general audience. By efficient financial market we understand, following [1], a market where prices reflect in a full manner all the information available and, moreover, they adjust in a quick manner when new information becomes available. We also use the concept of daily (index) returns by which in this paper we understand the logarithmic difference of a stock market index between its closing price in a certain day and its closing price a day earlier.\nThe discipline of economics has not remained indifferent to the rapid emerging field of fractal and chaos theory. The development of testing techniques in the fields of mathematics and physics has started to be felt in economics in the early `80’s when early tests for the presence of fractal dimension and chaotic behavior in economic and financial processes were applied, see [2] and [3] for a review of early results. Until now, the idea of chaos and fractal behavior remains debatable in the field of economics and finance, mostly due to the specific of economic time series characterized by relatively short samples (the accurate computations of correlation dimension or the maximum Lyapunov exponent require large samples) and the presence of noise. [4] summarized the research taken in the `80’s and `90’s by pointing that there is no evidence of “within the structure of the economic system” as current tests cannot determine the source of detected chaos.\nAt the same time, as some of the research points out, [3] and [4], there is a further need to further develop tests and deepen the topic of chaos and fractality in the field of economics. The need is even more urgent in the discipline of finance. The still dominant paradigm of efficient stock markets as outlined by [1] has serious weaknesses, among which we can enumerate time dependent self similarity, see [5] and [6] for a larger review. Such weaknesses called for alternative theories, one of which is worth mentioning in the context of present paper, namely the fractal market hypothesis due to [6]. According to [5], the fractal market hypothesis assumes that asset returns are dependent on both frequency and time horizon and that there is global dependency manifested through its fractality. This hypothesis has been reinforced by the discovering of multifractals in the asset returns, see [7] for early findings, which develops earlier ideas by [8] as well as [9].\nAlthough there is a growing work on multifractality for either developed stock markets, see [7], or emerging stock markets, [10] or [11] , the literature not only on multifractality, but in general in testing for chaotic and fractals behavior in CEE stock markets is very limited. Nevertheless, some papers are worth mentioning. [12], using a Hurst coefficient derived on the basis of the wavelet decomposition, found evidence for long run dependence on some of the CEE stock markets. They also found evidence of a time dependent value for the Hurst coefficient. In a recent paper, [13], using the Hurst coefficient determined on the basis of the Detrended Fluctuation Analysis, analyzed the dynamics of daily returns of share prices of 126 selected companies from the Warsaw Stock Exchange. He found that the after the drop in the Hurst exponent, the change in either long-term trend or in the long-term rate of return has an increased probability than for points randomly selected from the whole sample.\nThis paper proposes itself to answer to several questions, namely whether the daily returns in the selected CEE stock market indices are characterized by multifractality, how much using a surrogate data series, shuffled ones, leads to changes in the results. Not at last, we also investigate whether the crisis period has lead to different strenghts of multifractal spectrum, as suggested in an earlier work on the 1987 financial crisis by [14].\nThe paper is organized as follows. The methodology used throughout the paper is explained in the second section. The third section presents the empirical results and discusses the results. The last section draws the conclusions and outlines some possible extensions of this paper.\nMethodsThe methodology is based on the Empirical Mode Decomposition, EMD hereafter, based Multifractal Detrended Fluctuation Analysis, (EMD based MFDFA hereafter). There are a number of techniques to derive the multifractal spectrum of a time series, some based on wavelets, other based on detrended fluctuation analysis.\nBasically, EMD based MFDFA is a development of the now well established technique of Multifractal Detrended Fluctuation Analysis, MFDFA hereafter, due to [15]. We discuss first the EMD approach in decomposing time series, and present then in a comparative way the standard MFDFA as well as the EMD variation.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Standard MF-DFA Analysis of Czech Stock Market Index PX.a) Daily returns for Czech Stock Market Index PX; b) Log Scaling Function; c) q-generalized Hurst Exponent; d) Multifractal spectral scaling exponent τ(q) versus q; e) Multifractal spectrum.\ndoi:10.1371/journal.pone.0040693.g001\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Standard MF-DFA Analysis of Hungarian Stock Market Index BUX.a) Daily returns for Hungarian Stock Market Index BUX; b) Log Scaling Function; c) q-generalized Hurst Exponent; d) Multifractal spectral scaling exponent τ(q) versus q; e) Multifractal spectrum.\ndoi:10.1371/journal.pone.0040693.g002\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Standard MF-DFA Analysis of Polish Stock Market Index WIG.a) Daily returns for Polish Stock Market Index WIG; b) Log Scaling Function; c) q-generalized Hurst Exponent; d) Multifractal spectral scaling exponent τ(q) versus q; e) Multifractal spectrum.\ndoi:10.1371/journal.pone.0040693.g003\nThe Empirical Mode Decomposition\nThe Empirical Mode Decomposition, is a new technique in signal theory due to [16]. Several papers have outlined its advantages with respect to other filtering techniques, see) [17] or [18]. As [16] showed, essentially, the EMD consists in decomposing a certain time series into a finite number of so-called intrinsic mode functions. These functions have to fulfill two essential conditions. The first one says that the numbers of local extreme and the numbers of zero crossings, for the entire sample of data, must be equal or differ by 1 at most. The second condition states that at any point in time, the mean value of the “upper envelope”, as given by the local maxima, and the “lower envelope”, given by the local minima, must be zero.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Multifractal strength for standard MFDFA.doi:10.1371/journal.pone.0040693.t001When one compares it the wavelet approach or the Fourier approach, one notices that it enjoys several advantages. Compared with the Fourier approach, it gives a representation in both time and frequency and it also allows working with nonstationary data while compared with wavelets it also can work with nonlinear time series. We detail the algorithm below:\n\n\n\n\nFor a given time series y(t), one identifies all extrema;\n\nUsing an interpolation procedure, the local maxima result in an upper envelope U(y);\n\nIn a similar manner, from the minima, a lower envelope results, L(y);\n\nOne derives the mean envelope as:\n\n\n          \n        \n\n\n\n\nThis mean is extracted from the signal, so that a new series results:\n\n\n          \n        \n\n\n\n\nFinally, one verifies whether the new series g(t) satisfies the two above mentioned conditions.\n\nIf the conditions are met, the algorithm is stopped, if they are not, the algorithm continues. In the end, the trend is given by:Where  represents the trend of the series.\n\n\nThe MFDFA Based on Empirical Mode Decomposition\nThe introduction of EMD based MFDFA can be traced back to [19]. The development assumes that the first two steps of MFDFA remain the same. In the third step, instead of a polynomial detrending, specific to detrended fluctuation analysis, the EMD is used to decompose the series. The method used in this paper is described below, following [15] and [19].\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  EMD based MF-DFA Analysis of Czech Stock Market Index PX.a) Daily returns for Czech Stock Market Index PX; b) Log Scaling Function; c) q-generalized Hurst Exponent; d) Multifractal spectral scaling exponent τ(q) versus q; e) Multifractal spectrum.\ndoi:10.1371/journal.pone.0040693.g004\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  EMD based MF-DFA Analysis of Hungarian Stock Market Index BUX.a) Daily returns for Hungarian Stock Market Index BUX; b) Log Scaling Function; c) q-generalized Hurst Exponent; d) Multifractal spectral scaling exponent τ(q) versus q; e) Multifractal spectrum.\ndoi:10.1371/journal.pone.0040693.g005\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  EMD based MF-DFA Analysis of Polish Stock Market Index WIG.a) Daily returns for Polish Stock Market Index WIG; b) Log Scaling Function; c) q-generalized Hurst Exponent; d) Multifractal spectral scaling exponent τ(q) versus q; e) Multifractal spectrum.\ndoi:10.1371/journal.pone.0040693.g006We start from a given time series. In the first step we derive a profile of the series which is nothing more than a cumulative sum:(1)\nNext, in the following step, we partition the profile ,, in segments each one of equal size s, with the property of being disjoint, where. Here is determined as the ratio between N and the scale factor s.\nEach of the segments  has the following property:(2)\nHere l is determined from: .\nThe step three of the algorithm in the baseline MFDFA implies the detrending of the segments using a polynomial fitting. In the version based on the empirical mode decomposition, one computes an EMD local trend for each segment  as , where is the local trend and  is local trend based on the EMD approach, see the previous section.\nOne constructs then the series of residuals using the trend function as follows:(3)\nUsing the residuals determined in equation (3), the detrended fluctuation function  for a segment  is given by:(4)\nBased on this we derive the q-th order overall detrended fluctuation function as:(5)\nWhere q can take any real value except q = 0. In case q = 0, the formula becomes:(6)\nFinally, based on different timescales s, a power-law relationship can be established between  and the time scale s:(7)\nHere  stands for the generalized Hurst index.\nFurther, for a each q a corresponding function  can be determined by:(8)\nWith  representing the multifractal spectrum.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Multifractal strength for EMD based MFDFA.doi:10.1371/journal.pone.0040693.t002\nResults\nData Used\nThe data consist in daily returns of main stock market indices in Czech Republic, Hungary and Poland. All the data were taken from DataStream. The data were transformed, as usual in the literature, in US dollar denominated values. Before applying the statistical techniques, the price indices were transformed in log-returns.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 7.  The Impact of the Crisis Analyzed Using Standard MF-DFA.Multifractal spectrum for the whole sample compared with 2008–2009 sample that includes the high volatility period using standard MFDFA: a) Czech case: PX for the whole sample and PX crisis for 2008–2009 period; b) Hungarian case:BUX for the whole sample and BUX crisis for 2008–2009 period; c) WIG for the whole sample and WIG crisis for a 2008–2009 subsample.\ndoi:10.1371/journal.pone.0040693.g007\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 8.  The Impact of the Crisis Analyzed Using EMD based MF-DFA.Multifractal spectrum for the whole sample compared with 2008–2009 sample that includes the high volatility period using EMD based MFDFA: a) Czech case: PX for the whole sample and PX crisis for 2008–2009 period; b) Hungarian case:BUX for the whole sample and BUX crisis for 2008–2009 period; c) WIG for the whole sample and WIG crisis for a 2008–2009 subsample.\ndoi:10.1371/journal.pone.0040693.g008The data for Czech Republic consists in daily observations for PX index from April 1994 to December 2010. Overall, 4369 observations are used. For Hungary, we used daily data on BUX index, dating from June 1993 to December 2010, with a total number of observations of 4577. The last index, the one for Poland, consists in daily observation for the WIG index, dated between June 1993 and December 2010, overall 4575 observations being used.\n\n\nStandard MF-DFA\nThe results for the standard MFDFA for the three stock market indices are presented in Figures 1, 2 and 3. The procedures used an upper bound for q of 5, a lower bound of −5 and considered 31 elements in the vector q. For each case, results for the case of the shuffled time series are also presented. Shuffled time series are obtained from the original series after eliminating the serial correlation.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 3.  Multifractal strength in crisis compared to full sample using standard MFDFA.doi:10.1371/journal.pone.0040693.t003\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 4.  Multifractal strength in crisis compared to full sample using EMD based MFDFA.doi:10.1371/journal.pone.0040693.t004The Hurst coefficient for each series, as shown in the literature, is given by H(q) for q equal to 2. I obtained a Hurst coefficient for Czech PX of 0.57, for Hungarian BUX of 0.55, while for the last case of Polish WIG, Hurst coefficient was estimated at 0.54. These estimations indicate persistence of the time series and are usually interpreted as an indicator of an emerging financial market, see [10] and they are consistent with the results from other studies, see [13].\nFirst of all, there is evidence of multifractality from the dependence of the H(q) from the q moment, as presented in Figures 1c, 2c and 3c. Moreover, there is a decreasing trend for H(q) which is a clear sign of multifractality as indicated by the literature. In order to characterize the multifractality, the multifractal spectra are presented in Figures 1e, 2e and 3e.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 5.  Norms of the multifractal spectra in crisis compared to full sample based on MFDFA.doi:10.1371/journal.pone.0040693.t005\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 6.  Norms of the multifractal spectra in crisis compared to full sample using EMD based MFDFA.doi:10.1371/journal.pone.0040693.t006There are some variations with respect to the amplitude of the fractal spectrum, given by the formula, see Table 1. We would like to know whether the shuffled series have a different multifractal strength. We apply the χ2-test for association with which we can test whether there is any influence of shuffling the series on the multifractal strength. When running the test we get there is no influence of shuffling the series (p-value is of 0.47).\n\n\nEMD Based MF-DFA\nWe follow the same procedure in applying the EMD version of the MFDFA, using values for q between −5 and 5 and 31 elements for q. We also apply the procedure for the shuffled series. The results are presented in Figures 4, 5 and 6.\nWe look again at the results for the Hurst coefficient which are given by H(q) for q equal to 2. We obtained similar results, namely a Hurst coefficient for Czech PX of 0.57, for Hungarian BUX of 0.53, while for the last case of Polish WIG, Hurst coefficient was estimated at 0.53.\nWe also present the multifractal strength for each case including the shuffled series, Table 2. We test again if shuffling the series led to changes in the strength of the multifractal series. The χ2-test for association is used and the results indicate as in the standard case that shuffling the series did not modify the multifractal strength (p-value is of 0.39).\n\n\nThe Impact of the Crisis\nAnother question that I answer to in this paper is whether the global financial crisis has led to increased multifractality in the selected stock markets. As showed by [14], the financial crisis from 1987 led to changes in the diameter of the multifractal spectra, signaling an increased complexity in financial data. We discuss in this section whether a similar phenomenon occurred in the emerging financial markets from Europe. Again we apply both approaches in deriving the multifractal spectra of the time series in cause.\nFigure 7 and 8 shows the multifractal spectra computed for the whole period as well as for a subsample corresponding to the financial crisis period. We computed the multifractal spectrum for a subsample of two years, January 2008 to December 2009, roughly corresponding to the crisis period, also Figures 1a, 2a and 3a. Two entire years were selected as the precise date when the crisis spilled to a particular financial market is hard to determine.\n\n\nIs the Multifractal Strength Different during the Crisis?\nThe multifractal strengths are presented in Tables 3 and 4. When testing for any influence of the crisis on the multifractality using the χ2-test for association we cannot find any statistical influence of the crisis on the multifractality of the series as synthesized in the multifractal strength (the p-values are of 0.35 for the standard MFDFA and of 0.40 for the EMD version).\n\n\nHas the Multifractality Changed during the Crisis?\nWe discuss here further evidence regarding the shape and distribution of the multifractal spectrum for the selected emerging European stock markets. While in the previous section we focused on the multifractal strengths, here we take a look at the multifractal spectra taken as a whole. While the approach in the previous section was justified on the grounds that most of the research on the multifractality of the financial time series has been interested first of all in the multifractal strength of the series. However, given the fact that the multifractal strength of a time series has not only a maximum but also width and a parabolic distribution, we quantify each of the multifractal strengths through a Euclidean norm.\nWe use the p-norm, the 2-norm to be more precise, to characterize the multifractal spectrum of the series given the fact that the multifractal spectrum is a line in a two dimensional space. The 2-norm is also known as Hilbert – Schmidt norm and it is given as:\nWe compare the norms of the multifractal spectra for each series for the whole sample as well as for the crisis period.\nThe results are presented in Table 5 and 6. We apply again the χ2-test for association to see if there is any influence from the crisis on the multifractal spectra. In this case the values of the computed χ2-test (0.000245 for the baseline MFDFA approach as well as 0.000244 for the EMD based MFDFA) indicate that the crisis has clearly influenced the overall shape of the multifractal spectrum.\n\nThe accumulation of evidence in the favor of chaotic patterns, fractality and multifractality in economic and financial time series is an important contribution in the understanding of the complexity of economic and financial processes. In this paper, we add to the existing evidence on multifractality in financial time series by using daily returns from three of the key stock market indices in Central and Eastern Europe.We showed that the global Hurst coefficient varies with the moment q, and that the series are characterized by a multifractal spectrum. We compared the results from the initial time series with those obtained on the basis of shuffling the time series which we found that did not influence the results. We also studied the impact of the financial crisis the multifractal spectrum for the overall periods for each stock market index with those for a subsample of two years, 2008 to 2009, the years of the last big financial crisis. The overall evidences found here, although not a clear argument in the favor of an increased multifractal strength, point to a more complex change in the shape of the multifractal spectrum.Further studies could deepen the topic by analyzing the factors that drive the strength of the multifractal spectrum, its relationship to the degree of financial development or its behavior during the periods of financial crisis."
        },
        "10.1371/journal.pone.0087820": {
            "author_display": [
                "Yuriy Mishchenko"
            ],
            "title_display": "Oscillations in Rational Economies",
            "abstract": [
                "\nEconomic (business) cycles are some of the most noted features of market economies, also ranked among the most serious of economic problems. Despite long historical persistence, the nature and the origin of business cycles remain controversial. In this paper we investigate the problem of the nature of business cycles from the positions of the market systems viewed as complex systems of many interacting market agents. We show that the development of cyclic instabilities in these settings can be traced down to just two fundamental factors – the competition of market agents for market shares in the settings of an open market, and the depression of market caused by accumulation of durable overproduced commodities on the market. These findings present the problem of business cycles in a new light as a systemic property of efficient market systems emerging directly from the free market competition itself, and existing in market economies at a very fundamental level.\n"
            ],
            "publication_date": "2014-02-05T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 564,
            "shares": 2,
            "bookmarks": 1,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0087820",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0087820&representation=PDF",
            "fulltext": "IntroductionEconomic or business cycles are some of the most noted features of market economies, spanning historically over 200 years and ranking among the most serious of economic problems [1], [2]. Despite a number of economic theories proposed to explain the nature of economic cycles, including the theories of multiplier-accelerator [3], inventory cycles [2], [4], politically based cycles [5], [6], credit/debt cycles [7], [8], the real business cycles [9]–[12], and many others [2], [13]–[16], the nature of such cycles remains highly controversial. Notably, most existing economic theories associate economic cycles with various economically suboptimal and irrational behaviors such as speculative and crowd effects [17], inefficiencies in business decision making [4], exogenous shocks such as new technologies, political crises and wars [10]–[12], political interventions [5], [6], etc. However, the dramatic persistence of economic cycles throughout the 200 years of recorded economic history leads one to question the soundness of such views.\nIn mainstream economic theory, business cycles are associated with the fluctuations in aggregate demand coupled with so called accelerator and multiplier effects [1], [18]–[21]. Accelerator effect is the tendency of businesses to increase their investment spendings beyond usual levels in growing economies and to lower that in shrinking economies. Multiplier effect is the tendency of increased investment spendings to additionally stimulate economy as the result of the money turnover. The multiplier-accelerator model, if represented in a mathematical form [3], [22], does give rise to oscillatory patterns in the fluctuations of aggregate demand; however, for that it relies on economically “irrational” tendency of businesses to continue expanding their investments even in already oversaturated but still growing economy, as embodied by the accelerator effect, and leaves without explanation the nature of the initial fluctuation that gives rise to subsequent oscillations. A completely different perspective on business cycles have been assumed by the more recent real business cycles theory [9]–[12]. The real business cycles theory supposes that business cycles always have an exogenous cause such as disruptive new technologies, geo-economical changes, political crises, wars, etc. and, in that sense, are just a response to the changes in real markets’ conditions. Credit/debt cycles theory [7], [8], on the other hand, attributes business cycles to the dynamics of over-borrowing by businesses during the times of economic booms, followed by economic slowdown and, finally, a debt crisis and a recession. Political cycles theory [5], [6] attributes business cycles directly to political manipulations and improper government interventions. Some of the oldest views on business cycles in Marxian economics [13], [14], [23] associate business cycles with the intrinsic property of businesses to lose profitability and fail with time, translating into recessions accompanied by mass unemployment, wealth inequality and economical restructuring aimed at recovering profitability.\nIn recent years a number of works, especially in the context of the new physics of complex systems, had emerged pursuing the understanding of market phenomena from the perspective of market systems viewed as complex systems of interacting agents [24]–[33]. Such works had offered new insights into phenomena such as financial fluctuations [24], [26], [34]–[38], market panics [29], [39]–[41], financial contagion [42]–[45], and many others. In this work, we present new findings for the problem of business cycles assuming a similar perspective on the business cycles as a systemic property of market systems originating from the collective behavior of rational market agents. We show that the development of business cycles in such settings can be traced down to just two factors – systemic overproduction caused by the competition of rational market agents for market shares in the settings of an open market economy and the depression of the market caused by sustained accumulation of thus overproduced durable commodities.\nSubsequently, we focus on an example of extremely basic and fundamental economic model of a single commodity market with several competitive producers. We show that this economic setting is characterized by the property known otherwise as the “Tragedy of the commons” [46]. The tragedy of the commons is an instance of a public goods dilemma that arises when several agents are allowed to collectively exploit a shared resource. It is known that in such settings the individually optimal decisions of the agents can lead to collectively disastrous outcomes in the form of the resource’s overexploitation and even its complete destruction [47]. For open markets, we show here that the markets themselves can be viewed as such a common “resource” being “exploited” by producers, and that the “overexploitation” of this resource in the circumstances similar to that of the tragedy of the commons manifests itself as overproduction crises. Such overproduction coupled with the depressing effect on the market of the accumulation of overproduced durable commodities can cause the market to crash and initiate an economic cycle.\nThe development of economic cycles is thus linked to the free market competition and the ability of overproduced commodity to accumulate on the market, that is, we observe that the cycles develop in the markets of durable goods but do not appear in the markets of nondurable goods. Interestingly, this is otherwise a well-known property of real economic cycles [21], [48]. For instance, in Fig. 1A we show the U.S. economic output by industries in 1947–2010 (U.S. Bureau for Economic Analysis). While the business cycles affect profoundly the durable goods manufacturing and construction, the nondurable goods and services remain practically unaffected by the business cycles throughout the entire period. Our model is also found to produce characteristic patterns in the evolution of commodity’s inventories, with an excess accumulation of inventories immediately prior to and drop during and after the recession segment of the cycle. Indeed, such pattern is also present in real economies. For instance, in Fig. 1B we graph durable and nondurable goods inventories in the U.S. economy in 1967–2010 (U.S. Bureau for Economic Analysis), with special attention to the last 7 recessions. The pattern of inventories’ over-accumulation immediately prior to the recessions and drop during the recessions is clearly visible in the durable goods inventories. In fact, co-cyclic pattern of inventories in business cycles is a well-known feature of real economies [21], [48].\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Business cycles appear prominently throughout economic history and display certain prominent patterns.A) Business cycles are known to affect primarily durable goods manufacturing and construction, while nondurable goods and services remain essentially unaffected. Graph A shows the value added by different industries in the U.S. economy since 1947, normalized to the year 2010. The differences between durable goods and construction and nondurable goods and services are clearly visible. B) Business cycle exhibits the pattern of inventories accumulation prior to and reduction during the recession part of the cycle. Graph B shows the changes in durable and nondurable goods inventories in the U.S. economy (in trillions of chained 2005 US dollars) during the last 7 recessions. The beginning of each recession is marked with a triangle. The pattern is clearly visible in durable goods but not nondurable goods inventories. Dashed triangle shows one case of the pattern appearing without an official recession. (Source: U.S. Bureau for Economic Analysis).\ndoi:10.1371/journal.pone.0087820.g001The present findings, therefore, cast the problem of economic cycles in a new light as an emergent property of efficient market systems originating directly from the free competition in the settings of open markets, and inherent to open market systems at a very fundamental level.\nMaterials and Methods\nSystemic Overproduction Crises in Open Market Economies\nWe consider a model of a single commodity market with several fully informed and rational competitive producers. In the model, each producer chooses the amount of the commodity  that she wants to produce, while the demand X is assumed to be a constant. The producers choose the production levels individually and rationally so as to maximize individual profits defined as,(1)\nHere,  is the production of the ith producer (i = 1,2,…,N),  is the production cost of the ith producer, and  is the market’s return. The return function  depends on the total production output  and the market size X, and is a non-increasing function of Y and a non-decreasing function of X, following the standard supply-demand arguments [21]. For simplicity, we shall assume here that all producers are equal, that is  for all i.\nThe tragedy of the commons is a public goods dilemma in which a group of players is allowed to exploit a common resource (a “commons”), commonly exemplified by a shared pasture, fishery, or forest [46]. Each player is free to choose a level of the resource exploitation (for example, the number of cattle to put on the pasture etc.) and does so independently and rationally according to one’s self-interest. The payoff of each decision is given by an equation identical to Eq. (1), in which  is understood as the level of the resource exploitation by player i and  is the associated cost. An essential property of the tragedy of the commons is that the resource’s return function, , is decreasing with exploitation Y; this is a typical situation for most shared resources [47]. Given that assumption, it can be shown that the Nash equilibrium of the players in this situation causes the resource to be necessarily overexploited [49]–[52].\nBriefly, the Nash equilibrium in a non-cooperative game is defined as such an equilibrium point  in which none of the players can further increase their payoffs by any unilateral action [52]. Here, such unilateral actions correspond to increase or decrease of ; therefore, this condition translates into . At the same time, for the total return , the maximum is achieved at . Noting that , it is easy to see then that  necessarily implies  if , in other words, the Nash equilibrium corresponds to the players’ configuration where the returns are degrading, that is, the resource is overexploited.\nWe point out that the open market model described above is identical in its mathematical structure to the above tragedy of the commons. Specifically, the producers’ gains are defined by the same Eq. (1) and the market returns  are likewise a decreasing function of Y. Then, similarly to the classical tragedy of the commons, it can be shown that in the Nash equilibrium of this model as well necessarily , that is, the commodity is overproduced and the market is oversaturated. Intuitively, this result can be understood from the fact that the collectively “optimal” configuration, in which the supply and the demand meet, that is,, is unstable to unilateral increases in the production  by any one of the producers, which allow that producer to increase her returns due to an associated increase in the market share . Of course, such an increase comes at the cost of the market shares and the profits of all the other producers. As a result of that, merely to maintain a parity in the market, all of the producers are subsequently led to increase their production outputs beyond the optimal point , in order to counteract potential increases by their competitors. As a result,  stops being an equilibrium point of the system and overproduction naturally develops as an outcome of such producers’ competitive behavior.\n\n\nOscillatory Patterns in Open Market Economies\nAlthough overproduction crises are commonly stated as the leading cause of economic recessions [1], [14], [21], [23], in here we do not observe that the overproduction by itself necessarily causes a recession. In fact, in a dynamical simulation of the model (1) we observe that the model outputs converge to equilibrium monotonically and no recession occurs, Fig. 2. In Fig. 2 we graph a solution of the model (1) for different values of the return function g. Although the situation of overproduction indeed develops quite rapidly in these settings, the model does not exhibit subsequent crises and instead settles on an equilibrium monotonically. We, therefore, are led to conclude that overproduction and loss of profitability by businesses by itself is not sufficient for economic crises. If a recession is to emerge, a different mechanism is required to trigger drop in production.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  The market share competition of producers in a competitive open market economy should always result in overproduction of the commodity and oversaturation of the market, as shown in these market model dynamics.Overproduction by itself, however, does not necessarily trigger an economic recession, as the model production outputs observed here approach equilibrium point monotonically. In the graph, “g/c” stands for the profit margin used in each model and “Y = X” corresponds to the classical equilibrium point of equal supply Y and demand X.\ndoi:10.1371/journal.pone.0087820.g002We find such a mechanism by observing that allowing overproduced commodities to simply accumulate on the market over extended periods of time suffices to trigger oscillatory patterns in production outputs. More specifically, we describe the dynamic behavior of the producers in an open market by following relationships,(2)\nHere, the adjustments in the producers’ outputs  are driven by the expected profit , but the market size is taken in the form , where S is the commodity that had been overproduced and is currently remaining on the market. The latter reflects the fact that previously produced and now persisting on the market commodity depresses the demand and the market for the new produce. The second equation describes the commodity’s accumulation on the market with the  term modeling the commodity’s persistence on the market, whereas α represents the fraction of the overstock commodity lost naturally over one period of time.\nTo inspect the possible solutions of the model (2), we consider a simple instance of the model (2) in which the commodity’s price is assumed to be constant g. (Note that even in that case the return function  is not constant because in the market oversaturation regime, , the amount of sold commodity saturates at X and the return per Y effectively drops as ). In that case, Eqs. (2) describe a linear dynamical system controlled by the following characteristic equation,(3)where . Depending on the value of the persistence constant α, therefore, Eq.(3) allows three different types of solutions. For large , both roots λ1,2 of Eq. (3) are real and smaller than one, and the corresponding solutions of Eqs. (2) are non-periodic, α = 1 in Fig. 3. For small , the roots become complex and the dynamical system (2) becomes periodic. Depending on the magnitude of λ1,2, however, one of two cases can realize here. For , |λ1,2| are smaller than one and the dynamics is damped oscillations, α = 0.1 in Fig. 3. For , |λ1,2| are greater than one and the dynamical system (2) becomes unstable. The consequences of this instability are two-fold. Firstly, the corresponding market model ceases to have a stable equilibrium, that is, the cycles develop from any however small deviations from the exact equilibrium. Secondly, the oscillations become nonlinear – the excess commodity S is always reduced to zero at some point during the cycle and the cycle becomes self-sustained and non-decaying, α = 0.01 in Fig. 3. The cycle additionally becomes chaotic, as can be observed in the respective Y–S phase space trajectories of this dynamical system.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Cyclic recessions in model production outputs develop if overproduced commodity is allowed to accumulate on the market.The recessions, therefore, are triggered by the ability of overproduced commodity to accumulate on the market, that is, the model cycles develop in the markets of durable goods (small α) but not in nondurable goods (large α), similar to the business cycles in real economies, Fig. 1A. The dashed line shows the co-evolution of the commodity’s inventories during the cycle. Note the co-cyclic pattern similar to that observed in real economies, Fig. 1B. The simulation parameters: coefficient a = 1/20, profit margin g/c = 2, market size X = 1, the number of independent producers N = 4, the commodity’s persistence constants α = 0.01, 0.1 and 1.\ndoi:10.1371/journal.pone.0087820.g003Depending on the persistence constant α, therefore, we observe three possible behaviors of the model (2). For large α, the production outputs approach the Nash equilibrium monotonically and no oscillations develop. In this case, the commodity does not accumulate on the market fast enough to trigger a recession and equilibrium is achieved directly. For α below a certain threshold, however, damped oscillations begin to develop, and yet for smaller α the model becomes unstable. In that latter case, persistent cyclic instabilities emerge from any however small deviations from exact equilibrium and develop into a self-sustained business cycle.\nThe situation of large α (low commodity persistency), evidently, can be taken to correspond to the situation with nondurable goods and services, while the case of small α (high commodity persistency) would correspond to the situation with durable goods and construction. Remarkably, these features of the model emerge also as a well-known property of real business cycles [21], [48], which are known to affect primarily durable goods manufacturing and construction, while leaving nondurable goods and services unaffected, Fig. 1A. Second striking feature of the model (2) is the co-cyclic behavior of the inventories, with excess accumulation of the inventories immediately prior to and drop during the recession part of the cycles, also well known for real business cycles, Fig. 1B.\n\nDiscussionIn this work, we elucidate the development of cyclic instabilities in a fundamental economic model of an ideal single commodity open market with several producers. We observe that the root cause of these instabilities is a systemic overproduction caused by the competition of rational producers for market shares, followed by market depression due to an accumulation on the market of overproduced durable commodities. The possibility and the severity of such model cycles is found to be directly related to the ability of the commodities to accumulate on the market, that is, the cycles are observed for durable goods but not for non-durable goods or services. This feature of the model’s cycles is an otherwise well-known property of real business cycles [21]. The cycles are observed also to be associated with specific co-cyclic patterns in commodity inventories, with excess accumulation of inventories prior to and drop during the recession segment of the cycle, which is also a known property of real business cycles [48].\nThe emergence of cyclic instabilities with the key signatures of real business cycles in the above model is extremely striking. Single commodity market is one of the simplest and the most fundamental models in economics. Furthermore, we had to make no assumptions or artificial adjustments in order to observe the development of the cycles – the oscillations developed naturally from the fundamental properties of the model itself, namely, the strategic competition of the producers for market shares and the depressing effect of durable goods overstocks on the sales of the new produce.\nAs such, the described model invoked only pure market-driving forces, in the form of the strategic desire of market agents to maximize their profits and stocks-overstocks dynamics. It is clear, therefore, that the model’s behavior can change substantially in the presence of any additional regulatory mechanisms. In particular, the regulatory mechanisms affecting the types of behaviors touched upon in this paper can be expected to most significantly affect the persistence of business cycles. Such regulatory mechanisms, for instance, might include incentives for durable goods manufacturers that discourage them from attempting market share expansions in already saturated markets, or incentives for businesses aimed at discarding durable overproduced stocks at higher rates. Of course, any such regulatory options bring with them an entire array of complex technical, legal, social, and economic issues that cannot be possibly comprehensively examined in this work and shall warrant thorough investigation.\nWhile one cannot expect the long-standing problem of business cycles to be resolved with any simple model of two variables such as described here, the simple findings presented in this work offer new insights into the long-standing issue of business cycles as a systemic property of efficient market systems emerging directly from free market competition itself and, therefore, intrinsic to open markets at a very fundamental level.\n"
        },
        "10.1371/journal.pmed.1001043": {
            "author_display": [
                "Joan Benach",
                "Carles Muntaner",
                "Carlos Delclos",
                "María Menéndez",
                "Charlene Ronquillo"
            ],
            "title_display": "Migration and \"Low-Skilled\" Workers in Destination Countries",
            "abstract": [
                "\n        In the fourth article in a six-part PLoS Medicine series on Migration & Health, Joan Benach and colleagues discuss the specific health risks and policy needs associated with migration in destination countries, especially for low-skilled and illegal migrant workers.\n      "
            ],
            "publication_date": "2011-06-07T00:00:00Z",
            "article_type": "Policy Forum",
            "journal": "PLoS Medicine",
            "citations": 11,
            "views": 6656,
            "shares": 4,
            "bookmarks": 24,
            "url": "http://www.plosmedicine.org/article/info%3Adoi%2F10.1371%2Fjournal.pmed.1001043",
            "pdf": "http://www.plosmedicine.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pmed.1001043&representation=PDF",
            "fulltext": "This is one article in a six-part PLoS Medicine series on Migration & Health.\nIntroductionOver the last few decades, increases in international migration have transformed the lives of hundreds of millions of people around the globe. Since 1975, the number of international migrants has more than doubled, with most living in Europe (56 million), Asia (50 million), and North America (41 million). Nowadays, 3.1% of the world population resides in a country other than where they were born [1], and when one includes visitors on business or personal trips, roughly one million people move between the high-income and mid- and low-income countries each week [1].\nAlthough people cite many reasons for why they move from their home country to another, there is little doubt that a large increase in international migration has been driven by economic globalisation. Roughly half of all international migrants are economically active migrant workers [2]. Generally, these workers who move from low-income to middle- and high-income countries are searching for ways to provide for their families and to escape unemployment, war, or poverty in their countries of origin [2]. Through their high-skill and low-skill labour, migrant workers contribute to growth and development in destination countries by creating new demands for housing and other products and services. Changes in global production systems, along with demographic factors and labour market dynamics in countries of origin, have relegated millions of workers from poor countries to serving as a source of cheap, flexible, low-skill labour and direct and as indirect taxpayers in wealthier countries. Meanwhile, labour markets in destination countries do not often provide workers who are willing to accept precarious employment with long working hours for low pay [3].\nIn destination countries, migration has important implications for public health and health care. While high-skilled migrant workers may suffer from some potential risks, they also receive various benefits. For instance, the negative effects of migration on health tend to spare migrants of high socioeconomic position [4], whereas studies have shown that low-skilled migrant workers, for example, are at risk of contracting diseases unknown to their region of origin [5]. The health of migrant workers is also affected by their exploitation. That is, migrant workers often serve as the low-skill labour force that fills the “3-D” jobs (“dangerous, dirty and degrading”) that national workers are reluctant to perform, despite often being over-qualified for these positions [6]. Thus, migrants are more often exposed to potentially health-damaging work environments than native workers.\nThe aim of this article is to examine the relationship between migration, employment, and health in destination countries by focussing on the employment and working conditions experienced by low-skilled migrant workers. We discuss the policy implications stemming from these complex relationships.\nWork and Health of Migrant Workers in Destination CountriesIn most destination countries, migrant workers are found in the agricultural, food processing, and construction sectors of the economy, in semi-skilled or low-skill manufacturing jobs, and in low-wage service jobs. Foreign-born workers are vulnerable to coercion into precarious employment conditions within those sectors as a result of their irregular and undocumented legal status [7]. A large number of migrant workers work on the lowest rung of the destination country's employment ladder in low-skill day labour, which holds no guarantee of future work and mainly employs recently arrived immigrants [7]. While the characteristics and size of populations of migrant day labourers have been difficult to establish, the number of migrants in an irregular situation was estimated by the International Labour Organization (ILO) in 2006 to be roughly 20–30 million globally [8]. Migrant status can be an important source of global occupational health inequalities, regardless of whether the individual is considered low-skilled or highly skilled. Recent reviews on employment and health inequalities suggest that migrant status is a key cross-cutting mechanism linking employment and working conditions to health inequalities through diverse exposures and mechanisms [3],[9].\nMigrant day labourers who cannot obtain work permits are especially vulnerable to exploitation, since they fear job loss, incarceration, and deportation; they can be hired at extremely low wages and are often underpaid or not paid at all [10]. Furthermore, migrant day labourers are often exposed to a variety of work-related hazards (such as chemicals, pesticides, dust, and other toxic substances) without proper protective equipment, compensation insurance, or on-the-job training [7],[11]. The health issues to be considered for these workers are many including, among others, occupational safety and injury prevention, work-related diseases, and barriers to accessing mental health services. For instance, because of the high-risk conditions in which many migrant day labourers work, injury is an ever-present threat. Worldwide, ILO has estimated that migrant day labourers experience 335,000 accidents per year in the most dangerous of industries—agriculture, mining, and construction [12]. In the US, Valenzuela et al. reported that one in five migrant day labourers has suffered an injury, and agricultural hazards accounted for 7.4% of these work-related deaths [13].\nLow-skill migrant workers tend to experience more serious situations of discrimination and exploitation in destination countries than native workers. In particular, bonded labour most directly affects migrant women working in insecure informal economy jobs [14],[15]. As the most vulnerable workers in destination countries, female migrant labourers work primarily in retail, domestic work, or consumer services [16]. Foreign-born women in irregular jobs can become trapped into smuggling or servitude by migration agents, criminal organisations, or sex traffickers. It has also been shown that women do not resort to smugglers as often as men, which is in fact a result of their knowledge of how much more hazardous smuggling is for them than for men [17]. Another important issue is the enormous health risks to migrant women and children. A systematic review found that pregnant migrant women are at risk for worse pregnancy outcomes, particularly in destination countries that do not have strong policies for the integration of migrant communities [18]. For children whose parents are migrant workers, being excluded from medical services and the educational system in the destination country can lead to serious mental and physical problems [19],[20]. The effects of job insecurity on psychological distress and overall health constitute yet another burden for migrant workers and their families [21],[22].\nWhile most studies of migrant workers in destination countries refer to occupational injuries, many have also analysed and found evidence of a large number of work-related social problems, including social exclusion, lack of health and safety training, fear of reprisals for demanding better working conditions, concealing their need for medical care from employers, lack of knowledge regarding their rights as workers, linguistic barriers that minimize the effectiveness of training, and difficulty accessing care and compensation when injured [7]. Their unstable working conditions and associated social isolation also induce a risk of poor mental health such as chronic stress, anxiety, and depression [23]. Although a few of these labourers, when injured, receive medical treatments covered by employer-sponsored insurance, the vast majority obtain no treatment at all [24].\nThe immediate future does not bode well for the health of migrant workers, as the global economic recession is likely to heavily impact migrants, who constitute the most vulnerable and deprived segment of the workforce [17],[25]. Previous economic downturns, such as the East Asian economic crisis in the 1990s, were found to aggravate already negative conditions for these deprived and vulnerable workers [26], and it is reasonable to suggest that the current global economic crisis has similarly aggravated their harsh employment conditions and their health [27].\nPolicies to achieve better employment and working conditions among immigrants in destination countries require the implementation and evaluation of programs outside the health care sector. To identify what works across different historical and political contexts is an urgent and essential task in addition to a critical examination of existing policies as determinants of health for immigrants.Arguably, institutional policies in destination countries also contribute to imposing further restrictions on migrant workers that can directly or indirectly affect their health. One example is Canada's Live-In Caregiver Program, often criticised for facilitating exploitation of migrant women. The live-in requirement and compulsory completion of a minimum number of hours to become eligible for permanent residency, all the while providing no monitoring or regulation of work conditions or payment, are features of this program that leave participating migrants particularly vulnerable [28],[29]. Often, these features result in newly arrived immigrants without supportive networks or resources being overworked, completing domestic duties that are not within the scope of caregiving, being underpaid, and facing a substantial imbalance of power that leave them at the hands of their employers for fear of not completing the program requirements and having to face deportation [29],[30]—all factors that impact both somatic and mental health. Such policies by host countries need to be critically evaluated in terms of their consequences, whether these are intended or not.For migrant workers, the excessive risk of injury and disease linked to dangerous employment and working conditions in their destination countries is a global phenomenon. To reduce employment-related health inequality for these labourers in their countries of destination, future legislation backed by research, evaluation, and monitoring, on a country-by-country basis, is urgently needed so that better understanding is gained of (1) what true magnitude, mechanism, and pathways underlie the relation between migrant health inequality and employment conditions, (2) how health effects vary according to the hosting county's labour regulations and policy, and (3) how a global economic recession amplifies health inequality for legal and illegal migrant day labourers [31]. To ensure their health and safety, governments, unions, and international organizations should collaborate to implement fair labour standards by (1) administering adequate supervision, safety training, health surveillance, and work-related insurance for legal and illegal labourers that are on par with citizen workers, and (2) standardising labour migration policies while instituting legal support for these undocumented labourers to help eradicate human trafficking and other forms of extreme labour exploitation [3],[32].Immigration policies of sending countries similarly require closer scrutiny, particularly regarding weighing the balance of positive and negative consequences of migration for the individual versus the country. Although governments may present the premise of migration as positive to the public, it is important to scrutinize the motives behind pro-migration policies in addition to other policies in place, if any, that provide support and protection for migrant workers. For example, return migration (i.e., return of migrants to the home country) is often argued to be a positive consequence of migration. Yet, it is important to consider that reintegration of migrant workers into their country of origin with skills acquired through their migration experiences are often complicated by hindrances such as high unemployment [3]. The need to address such issues by the governments of sending nations become especially poignant during periods of economic downturn in host countries, when migrants may have to return “home.” The loss of economic and other investments in migration, coupled with unemployment and the inability to re-enter the home country's labour market, place returning workers and their families at risk of poverty and ill health [3].Return migration is less likely to occur in countries where migrants enjoy secure and stable residency [17]. Examples of the return migration “failure” have been observed in the Philippines' outflow of nurses, the majority of whom do not return to their home country [33],[34]. So far, strategies such as providing financial incentives for return migration, which have been introduced in some countries (e.g., in the Czech Republic and Spain), have had limited success [17]. As programs like the Migration for Development in Africa (MIDA) are implemented to encourage and facilitate return migration, a consideration of non-economic factors influencing return (e.g., political instability in the home country) is crucial for understanding the potential for success of such programs.Although specific mechanisms are poorly understood, current evidence shows that the employment and working conditions faced by most migrant workers are dangerous to their health. Ultimately, more global health surveillance and socio-epidemiologic analyses of migration will be needed to render employment conditions prominent in migration policy."
        }
    },
    "subject-geography": {
        "10.1371/journal.pbio.0030339": {
            "author_display": [
                "John Novembre",
                "Alison P Galvani",
                "Montgomery Slatkin"
            ],
            "title_display": "The Geographic Spread of the CCR5 Δ32 HIV-Resistance Allele",
            "abstract": [
                "\n        The Δ32 mutation at the CCR5 locus is a well-studied example of natural selection acting in humans. The mutation is found principally in Europe and western Asia, with higher frequencies generally in the north. Homozygous carriers of the Δ32 mutation are resistant to HIV-1 infection because the mutation prevents functional expression of the CCR5 chemokine receptor normally used by HIV-1 to enter CD4+ T cells. HIV has emerged only recently, but population genetic data strongly suggest Δ32 has been under intense selection for much of its evolutionary history. To understand how selection and dispersal have interacted during the history of the Δ32 allele, we implemented a spatially explicit model of the spread of Δ32. The model includes the effects of sampling, which we show can give rise to local peaks in observed allele frequencies. In addition, we show that with modest gradients in selection intensity, the origin of the Δ32 allele may be relatively far from the current areas of highest allele frequency. The geographic distribution of the Δ32 allele is consistent with previous reports of a strong selective advantage (>10%) for Δ32 carriers and of dispersal over relatively long distances (>100 km/generation). When selection is assumed to be uniform across Europe and western Asia, we find support for a northern European origin and long-range dispersal consistent with the Viking-mediated dispersal of Δ32 proposed by G. Lucotte and G. Mercier. However, when we allow for gradients in selection intensity, we estimate the origin to be outside of northern Europe and selection intensities to be strongest in the northwest. Our results describe the evolutionary history of the Δ32 allele and establish a general methodology for studying the geographic distribution of selected alleles.\n      \n        A spatially explicit model of the Δ32 mutation, which confers resistance to HIV-1 infection, reveals its spread across Europe and provides a general method for tracking the geographic spread of selected alleles.\n      "
            ],
            "publication_date": "2005-10-18T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS Biology",
            "citations": 37,
            "views": 41491,
            "shares": 175,
            "bookmarks": 152,
            "url": "http://biology.plosjournals.org/perlserv/?request=get-document&doi=10.1371%2Fjournal.pbio.0030339",
            "pdf": "http://fetchObject.action?uri=http://biology.plosjournals.org/perlserv/?request=get-document&doi=10.1371%2Fjournal.pbio.0030339&representation=PDF",
            "fulltext": "IntroductionThe geographic spread of advantageous alleles is fundamental to evolutionary processes, including the geographic distribution of adaptive traits, the cohesiveness of species, and the spatial dynamics of coevolution between pathogens and their hosts. Various theoretical models describe the dynamics of how advantageous alleles spread within a population, but few well-studied examples exist, particularly in humans, of how advantageous alleles spread geographically.\nThe CCR5 Δ32 mutation is a good example of an advantageous allele with a well-characterized geographic distribution. The Δ32 mutation currently plays an important role in HIV resistance because heterozygous carriers have reduced susceptibility to infection and delayed onset of AIDS, while homozygous carriers are resistant to HIV infection [1]. The mutation is found principally in Europe and western Asia, where average frequencies are approximately 10%, although the frequency varies within this geographic area. HIV only recently emerged as a human pathogen, so researchers were surprised when various sources of evidence showed strong selection in favor of Δ32 throughout its history. The age of the Δ32 allele has been estimated to be between 700 and 3,500 y based on linkage disequilibrium data [2,3], and recent ancient DNA evidence suggests the allele is at least 2,900 y old [4]. If Δ32 were neutral, population genetics theory predicts it would have to be much older given its frequency. The alternative explanation is that the Δ32 mutation occurred recently and then increased rapidly in frequency because of a strong selective advantage [2,5]. Quantitative studies have concluded that heterozygous carriers of Δ32 in the past had a fitness advantage of at least 5% and possibly as high as 35% [2,3]. Bubonic plague was initially proposed as the selective agent [2], but subsequent analysis suggested that a disease like smallpox is a more plausible candidate ([6–8], with reviews in [9–11]).\nTo understand the origin and spread of Δ32, we modeled the effects of selection and dispersal on the allele. The Δ32 mutation is found only in European, West Asian, and North African populations. The allele frequency exhibits a north–south cline with frequencies ranging from 16% in northern Europe to 6% in Italy and 4% in Greece (Figure 1; [2,5,12–17]). The broadest area of high frequency is located in northeastern Europe, particularly the Baltic region, as represented by samples from Sweden, Finland, Belarus, Estonia, and Lithuania. There are additional peaks of frequency in samples from the northern coast of France, the Russian cities of Moscow and Ryazan, and portions of the Volga–Ural region of Russia. Ashkenazi Jews have high frequencies of Δ32, but this is likely due to founder effects unique to their history rather than the general process of dispersal that spread the allele in other populations [18].\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Shaded Contour Map of Δ32 Allele Frequency DataThe sampling locations are marked by black points. The interpolation is masked in regions where data are unavailable.\ndoi:10.1371/journal.pbio.0030339.g001Previous discussion of the geographic distribution of Δ32 has focused on the north–south cline in frequency. Lucotte and Mercier [12] suggested that the cline and other features of the geographic distribution imply a Viking origin. In particular they proposed that the allele was present in Scandinavia before 1,000 to 1,200 y ago and then was carried by Vikings northward to Iceland, eastward to Russia, and southward to central and southern Europe. The age and geographic distribution of the allele are consistent with the qualitative predictions of the Viking hypothesis [12,17], but there has been no quantitative analysis of the Viking hypothesis or alternative hypotheses.\nOne alternative is that a northern origin coupled with typical levels of dispersal in Europe is adequate to explain the geographic distribution of Δ32. Under this hypothesis, rare long-distance dispersal events, such as Viking dispersal, play a minor role in the spread of the advantageous allele. Another alternative is that the allele may have arisen in central Europe and increased to a higher frequency in the north because of a geographical gradient in selection intensity [19]. There are two plausible biological causes for a gradient in selection. First, the selective advantage of Δ32 may have been larger in the north. This hypothesis stems from anecdotal evidence that indicates smallpox epidemics were more intense in northern Europe [20]. A second mechanism is that a selective cost associated with the Δ32 allele may have been stronger in the south, and thus the overall selection intensity in favor of Δ32 may have been weaker in the south and stronger in the north. While there is little direct evidence that Δ32 carriers are more susceptible to general infection [10,11], the plausibility of a selective cost of Δ32 is supported by evidence that chemokines play an important role in inflammatory responses to infection [21,22] and by studies with mice that show that CCR5 knockouts have poor immune responses to various pathogenic infections [23–25]. These results suggest some pathogens may have an advantage infecting Δ32 carriers because the immune response is impaired by the absence of functional CCR5 chemokine receptors. If such pathogens tended to be more prevalent in the temperate climates of southern Europe, then a selection gradient would arise. It is even plausible that Δ32 could be disadvantageous in certain areas where the protective effect is outweighed by the disadvantage of a weakened immune response. In a model with selection gradients, Viking dispersal may still contribute to the spread of the allele, but the geographic origin of the allele and the influence of spatially variable selection differ from that in the Viking hypothesis.\nA further question regarding the geographic spread of Δ32 is whether the historical selective agent acted only in Europe and western Asia or on a larger geographic scale. In the former case, the restriction of Δ32 to Europe and western Asia is explained by spatially varying selection, and in the latter, by insufficient time for the allele to have dispersed farther.\nHere we fit a simple population genetic model to the geographic distribution of Δ32 in order to infer features of the processes of dispersal and selection that shaped the historical spread of the allele. In particular we conclude that given current estimates of the age of the Δ32 allele, the allele must have spread rapidly via long-range dispersal and intense selection to attain its current range. We find the Δ32 allele is likely restricted geographically because of limited time to disperse rather than local selection pressures. In addition, we show that the data are consistent with origins of the mutation outside of northern Europe and modest gradients in selection.\nResultsTo examine the geographic distribution of Δ32, we adapted Fisher's deterministic “wave of advance” [26] model of selection and dispersal to a geographically explicit representation of Europe and western Asia. The wave-of-advance model is a continuous-time, continuous-space, partial-differential-equation (PDE) model that describes the change in allele frequency at any point in the range in terms of the effects of dispersal and selection. The model treats dispersal as a diffusion process, which implicitly assumes the effects of dispersal can be approximated by considering only the mean and variance of the dispersal distribution. Furthermore, dispersal is assumed to be homogeneous across the range and isotropic (i.e., the mean of the dispersal distribution is zero). In the simplest form of the model, the allele under selection is additive in effect and selection intensity is assumed to be homogeneous across the range. We then extended the model to allow for both east–west and north–south gradients in selection intensity. The parameters of the model are the initial population density (D), the initial position of the mutation in terms of latitude and longitude (x0 and y0, respectively), and the ratio (R) of the variance in the parent–offspring dispersal distance distribution (σ2) to the additive selection coefficient s. When a gradient in selection intensity was incorporated, sc was the selection coefficient in the center of the gradient and GNS and GEW were the percent changes in the selection coefficient per kilometer of north–south and east–west distance, respectively.\nTo apply the model to allele frequency data sampled from different locations, we combined the spatially explicit PDE model with a binomial sampling scheme (see Materials and Methods). With this approach the data are viewed as being a set of binomial samples from an underlying, unobserved allele frequency surface that is generated by the PDE model. One feature of this model is that while the underlying allele frequency surface produced by the PDE model may be unimodal and smooth, the resulting data are expected to be noisy and multimodal owing to the binomial sampling step. The multimodal nature of data expected under the model approximates that found in the dataset of Δ32 allele frequencies we collated from 71 locations across Europe, northern Africa, and Asia (Figure 1; data described in Materials and Methods). Indeed when simulations are used to generate allele frequency data with identical sampling locations and sample sizes as in the collated dataset, multiple peaks are often found that are similar to those observed in real data (Figure 2). This result demonstrates that specific regions of high frequency are not necessarily generated by unusual local conditions or specific migration events, but can arise because of sampling in models with homogeneous dispersal.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  An Example of the Allele Frequency Surface and Simulated Data(A) The underlying allele frequency surface generated by the PDE model using MLEs for the parameters. The coarseness of the surface and irregular coastlines are due to the resolution of the simulated habitat (see Figure S1).\n(B and C) Two replicates of simulated data obtained using the same sampling locales and sample sizes as in the dataset and displayed using the same interpolation methods and contours as in Figure 1. The results show that underlying smooth, unimodal allele frequency surfaces can give rise to irregular, multimodal observed allele frequency surfaces.\ndoi:10.1371/journal.pbio.0030339.g002To estimate the parameters of the model, we derived a likelihood function based on binomial sampling from the deterministic allele frequency surface. Estimating parameters via maximum likelihood requires an optimization step, and here we use a simple grid search. We found in applications to simulated data that the likelihood method with a grid search is able to estimate parameters with reasonable accuracy (Table 1).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  The Effect of the Dispersal Distribution on the Estimates of R and σdoi:10.1371/journal.pbio.0030339.t001The result of the maximum likelihood estimation is that values of R = σ2/s on the order of 105 and 106 km2 have the highest likelihood. The maximum likelihood estimate (MLE) of R depends on whether selection gradients are allowed. When GNS = GEW = 0, the estimate of R is 2.77 × 105 km2 with the profile likelihood falling off nearly symmetrically for higher and lower values (Figure 3). When GNS and GEW are treated as free parameters, the MLE of R is 1.03 × 106 km2 with a steep drop in likelihood for values less than 105 and a gradual decline for values greater than 106 (Figure 3). Values of R on the order of 104 km2 or smaller result in an expected geographic distribution of Δ32 that is too restricted to fit the data, and values of R on the order of 107 result in a distribution that is far too broad. Figure 4 demonstrates the values of σ and s implied by these estimates. The value of σ is consistently larger than 100 km for s ≥ 0.05. These values of σ are larger than estimates based on studies of historical and modern dispersal, which indicate that σ ranges from 1 to 75 km in European populations [27]. Conversely, values of σ < 75 km, imply values of s ≤ 0.02, which are lower than estimates of s ≈ 0.05–0.35 obtained in previous studies based on Δ32 frequency and linkage disequilibrium data [2,3].\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Profile Likelihood for RThe grey line shows the log profile likelihood for R when selection is assumed to be uniform spatially (GNS = GEW = 0). The MLE of R in this case is 2.77 × 105 with a log-likelihood of −263.0. The black line shows the profile likelihood when selection gradients are incorporated into the model (GNS and GEW are free parameters). The corresponding MLE of R is 1.03 × 106 with a log likelihood of −247.7.\ndoi:10.1371/journal.pbio.0030339.g003\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  The Dispersal Parameter σ as a Function of the Selection Intensity sThe curves are drawn for the two MLEs of R and labeled accordingly. 1Based on estimates in [2] and [3]. 2From Table 1 of [27].\ndoi:10.1371/journal.pbio.0030339.g004The time required in the model to reach a frequency of 16% for a fixed value of s is indicative of the age of the Δ32 allele. Using this information we find that strong selection (s ≥ 0.1) is necessary to reconcile the spatially explicit model of Δ32 with estimates of the allele age. The linkage disequilibrium data of Stephens et al. [2] and Libert et al. [5] suggest the age of the Δ32 allele falls in the range of 700–3,500 y, and recent ancient DNA data suggest the allele arose at least 2,900 y ago[4]. Assuming s = 0.1, we found across the range of origins we investigated that 130–156 generations (3,250–3,900 y) are required to reach current frequencies, which is consistent with the upper range of allele age estimates from linkage disequilibrium data and the ancient DNA data. To generate younger allele ages would require selection coefficients larger than 0.1. For weaker selection coefficients, the ages must be much larger. As an example, 650–780 generations (16,250–19,500 y) are required to reach current frequencies assuming s = 0.02.\nWe next investigated whether the data reject the hypothesis of uniform selection gradients. We used a likelihood ratio test that compares the maximum likelihood achieved when GNS and GEW are both restricted to zero to the maximum likelihood obtained when GNS and GEW are estimated from the data. As shown in Figure 3, when GNS and GEW are free parameters, the maximum log-likelihood is approximately −248. With GNS and GEW fixed to zero, we find the maximum likelihood at −263. As a result, the likelihood ratio statistic is strongly significant (likelihood ratio statistic = 30, df = 2, p < 10−5), such that we can reject the null hypothesis of uniform selection.\nIn the model with selection gradients, selection was inferred to be stronger in the north and in the west, with the north–south selection gradient being steeper than the east–west selection gradient. In particular, the estimates of GNS and GEW were GNS = 1.3 × 10−4 and GEW = −0.25 × 10−4. The profile likelihood surface for GEW and GNS shows a peak such that the likelihood drops off steeply from the maximum in both directions along the GEW axis while only declining gradually from the maximum in the GNS axis (Figure 5). The magnitude of the gradients is not extreme. For instance, the gradient of GNS = 1.3 × 10−4 km−1 implies a selection intensity at the latitude of Oslo that is a 21% increase on the selection intensity at the latitude of Milan (e.g., s = 0.23 in Oslo versus s = 0.19 in Milan). Similarly, GEW = −0.25 × 10−4 km−1 generates a selection intensity in Copenhagen that is 5% greater than in Moscow (e.g., s = 0.22 in Copenhagen and s = 0.21 in Moscow).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Profile Likelihood Surface for GNS and GEWThe plus signs indicate locations where the likelihood was evaluated. The dark contour at −250 marks the −2 log-likelihood support region for the estimates of GNS and GEW.\ndoi:10.1371/journal.pbio.0030339.g005Regarding the geographic origin of Δ32, we found that if selection is constrained to be spatially uniform, the origin is localized to a region east of the Baltic (Figure 6A; parameter set NE in Table 2). Moreover, the likelihood surface drops off dramatically outside of this region. If gradients in selection are incorporated, the origins with the highest likelihood are in southern Europe, with the maximum likelihood origin west of the Caspian (Figure 6B; parameter set C in Table 2). An origin in Spain (parameter set SW in Table 2) and an origin east of the Caspian (parameter set SE in Table 2) are within two log-likelihood units of the maximum. All three origins are coupled with an inference of a south-to-north and east-to-west increase in selection intensity. Origins in northern Europe have low likelihood, although the likelihood surface is fairly flat so that origins with a latitude as high as 58° are within six log-likelihood units of the maximum.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Likelihood Surfaces for the Origin of the Allele(A) Assuming selection intensity is uniform spatially (i.e., GNS = GEW= 0).\n(B) Allowing for north–south and east–west spatial gradients in selection (i.e., GNS and GEW are free parameters).\nLikelihoods were calculated at each of the black points and the surface was obtained by interpolation.\ndoi:10.1371/journal.pbio.0030339.g006\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Detailed Results for Parameter Sets with High Likelihoodsdoi:10.1371/journal.pbio.0030339.t002The underlying allele frequency surface generated by the MLE parameters is a qualitative indicator of the goodness of fit of the model (see Figure 2A). The frequency surface is broadly consistent with the observed allele frequency distribution (see Figure 1), although the estimated underlying distribution shows allele frequencies to be higher than observed in the northernmost part of Scandinavia and in parts of southern Europe. In addition, the estimated surface shows a region in the northeast of Russia where no data points exist and Δ32 is predicted to be present in frequencies of 8%–10%. Simulated data from the MLE allele frequency surface, with sample sizes and locations that are identical to those in the original dataset, have features such as multiple peaks that are present in the original data (for two examples of simulated data see Figure 2B and 2C). When we quantitatively tested the goodness of fit using a G-test, we rejected the null hypothesis that the data are derived from the model (G = 205.6, df = 66, p < 10−5). We also found that the data are overdispersed relative to the variance expected under a binomial distribution. The overdispersion parameter ϕ (see Materials and Methods) was estimated to be 3.1 where a value of one is expected under the model.\nFinally, to better understand the history of Δ32, we considered an extension of the model that included the dispersal of the allele to Iceland. Iceland was colonized principally from Scandinavia at approximately 900 CE [28] and present-day Iceland samples show a high frequency of Δ32 [13]. To model the frequency of Δ32 in Iceland, we used a single-population selection model for Iceland with initial conditions based on the mainland PDE model (see Materials and Methods). We used this simple model to calculate an expected frequency of Δ32 in Iceland at the present day. Data from present-day Iceland show a frequency of 14.7% in a sample of 204 chromosomes [13]. Hypothetical origins outside of northern Europe result in underestimates of the frequency in Iceland, and a northern origin results in a slight overestimate (for examples, see Table 2). To integrate the Iceland results into the analysis of the mainland frequency data, we used the fact that the observation in Iceland is an independent data point and added the log-likelihood of the Iceland data to that obtained from the mainland allele frequency data. An origin in Spain (parameter set SW in Table 2) is found to be the MLE with a log-likelihood of −261.0. We also found that an origin at 53°N 13°E in northern Germany (parameter set G in Table 2) has a log-likelihood of −262.4 and is the only other origin within two log-likelihood units of parameter set SW. Thus, including historical information about Iceland shifts the estimated origin to western Europe, with likely origins in Spain or northern Germany.\nDiscussionWe can draw several conclusions from our analysis of the geographic distribution of the Δ32 allele. First, the results suggest that strong selection (s ≥ 0.10) and long-distance dispersal of humans (σ > 100 km) are necessary to explain the current geographic distribution of the Δ32 allele. Values of σ > 100 km are larger than the estimates of 1–75 km found in various studies using historical records for Europeans [27]. If, however, the Δ32 allele is older and less advantageous than previously estimated [3,4,6], which has been suggested recently by [29], then our analysis of the geographic distribution becomes more consistent with previously published estimates of dispersal distances in European populations. Because our results depend on the ratio R = σ2/s, a smaller s implies estimates of σ that are closer to those based on historical records (see Figure 4), although still somewhat larger. For example, for our MLE of R = 1.03 × 106, a value of s = 0.0075 corresponds to σ = 88 km. One explanation for the discrepancy between genetic and historical estimates of dispersal distance is that the estimates based on historical records may be too low because they do not reflect longer-distance dispersal events such as those associated with trade routes or population movements. Our larger estimate of σ may reflect such long-distance dispersal events.\nSecond, we conclude that if selection is spatially uniform, Δ32 arose by mutation in northeast Europe as suggested by Libert et al. [5]. This hypothesis is parsimonious because it does not require gradients in selection intensity. If selection is not spatially uniform, we find the geographic origin could be far from locations where Δ32 is currently in high frequency. We reach this conclusion because, in our model, dispersal dominates initially and spreads the new allele over a large geographic area before selection can increase the allele frequency locally. When we allow for selection gradients and take account of the data from Iceland, we conclude that Δ32 most likely originated either in Spain or northern Germany. The gradients in selection intensity needed are not extreme and are on the order of only a 20% relative difference between southern and northern Europe and a 5% relative difference between eastern and western Europe. Although allowing for selection gradients is less parsimonious, the model with selection gradients had a significantly higher likelihood than the model with uniform selection. The north–south gradient detected here is consistent with anecdotal evidence that smallpox was more prevalent in the north [20].\nThird, our results show that the geographically restricted distribution of Δ32 is a result of Δ32 not having had time to disperse more widely, rather than resulting from a geographic restriction of selection favoring it. Given more time and no change in selection affecting Δ32, the allele would have spread over a wider area.\nOur large estimates of dispersal are consistent with the Viking hypothesis of Lucotte and Mercier [12]. Moreover, when selection is assumed to be spatially uniform, the maximum likelihood origin is in southern Finland. However, incorporating gradients in selection provided significantly better fits to the data, and in models with gradients, origins in Scandinavia did not have high likelihoods. Thus, our likelihood-based analysis provides some support for the Viking hypothesis in that we detect a strong signature of long-range dispersal events, but it also raises the possibility that the allele arose outside of Scandinavia and spread into the region via dispersers from the south.\nOur analysis makes a number of simplifying assumptions. Our model does not incorporate genetic drift. To examine the effect of ignoring drift, we simulated a stepping stone model with local deme sizes of Ne = 2,500 and a selection coefficient of s = 0.05. We found that with drift, the allele frequency surface becomes somewhat more jagged than without drift, but the underlying shapes are still the same (results not shown). Concluding that drift is negligible based on the simulation results is conservative as the selection coefficient of Δ32 is most likely at least 0.05 [2,3] and the effective population size of a deme (where 1° latitude by 1° longitude may be considered a deme) would be greater than 2,500 even assuming 2000 BCE population densities (2.5 km−2; [30]) and Ne as one-quarter of the census population size. Additional support for the idea that drift is unimportant for the large-scale patterns in this type of model comes from the analytical results of Kot et al. [31] for a similar model of branching random walks. They show that the average rate of expansion in a stochastic model is similar to that in a deterministic model.\nAnother assumption of our approach is that the allele under selection has an additive effect. We tested the robustness of our results to deviations from additivity by generating allele frequency surfaces in which the fitness advantage of Δ32 heterozygotes is kept constant and a range of fitness advantages of the Δ32/Δ32 genotype was assumed. We found that varying the degree of dominance had little effect on the geographic distribution of Δ32 (results not shown). The negligible importance of the fitness advantage of Δ32 homozygotes arises because the proportion of Δ32 homozygotes is sufficiently small throughout the history of Δ32 that the assumption regarding the fitness of the homozygote only has a minor effect.\nOur use of diffusion equations assumes that only the mean and variance of the dispersal distribution are needed to model the effects of dispersal and that higher central moments such as kurtosis are negligible. Studies of Fisher's wave of advance in the ecology literature have shown that if kurtosis is non-negligible, as in the case of “fat-tailed” dispersal distributions (distributions whose tails are not exponentially bounded), the asymptotic behavior of the wave of advance changes so that the speed of the wave continually accelerates [32,33]. Because observed dispersal distributions for humans have been found to be leptokurtic (i.e., large kurtosis) [34], we considered the effect of a leptokurtic dispersal distribution on our results. We simulated the spread of an advantageous allele in a two-dimensional stepping stone model on a torus using three different dispersal distributions with varying degrees of kurtosis: a normal distribution, a double gamma distribution used by Cavalli-Sforza et al. [34] to fit human dispersal data, and a modified double exponential distribution used by Clark et al. [35] to describe fat-tailed seed dispersal data (see Table 1). These distributions were scaled to have a standard deviation of 100 km. We sampled data from the resulting spatial distributions of allele frequency, and estimated the ratio of dispersal to selection using the same method we applied to the Δ32 data. The results (Table 1) show that the effect of kurtosis on the estimates is very small: between all three distributions the estimates of R vary in a narrow range and the corresponding values of σ vary only between 98 and 110 km.\nWhile violations of each of these simplifying assumptions (no genetic drift, additivity of the selective effect, a diffusion approximation for dispersal) are unlikely to have important effects on our estimates, the variance introduced by violations may contribute to the overdispersion observed in the data and the significant G-test statistic we computed. Another likely cause of the unexplained variance is that our model does not explicitly incorporate specific historical events. Information about particularly important dispersal events will help refine quantitative models of the evolution of Δ32. However, a challenge to developing such models is the difficulty of keeping them from becoming too parameter-rich or overburdened with assumptions regarding historical demographic events [36].\nIn summary, we present an approach to analyzing the geographic distribution of a selected allele. The approach allows us to estimate the ratio of dispersal to selection as well as fit gradients in selection to the observed allele frequency data. Our analysis confirms Δ32 has been under strong selection, and furthermore shows that long-range dispersal and selection gradients have been important processes in determining the spread of this advantageous allele. The results provide an insight into the history of Δ32 and into the processes that affect the geographic spread of advantageous alleles in humans.\nWe focused our analysis on the region extending from 22°N to 75°N and 10°W to 154°E. Topographic data were obtained from the ETOPO5 data assembled by the National Geophysical Data Center. The exact dataset used was a version with 1° latitude/longitude resolution that is provided as a standard dataset in MATLAB 7. The coastline data were extracted by taking all values above sea level to be land. A land bridge between Denmark and Sweden was added to model migration between the two closely separated land masses. An image of the habitat is available as Figure S1.A summary of Δ32 allele frequency data was constructed by pooling data from multiple published papers [12,14,19,37–43]. Latitude and longitude for 58 of the 71 samples were kindly provided by S. Limborska. For the remainder of samples, we used either the latitude and longitude of the city where the sample was collected or the latitude and longitude of a major city in the region sampled. We excluded any data points that were obtained from land masses not connected to the European mainland in our model, as well as allele frequency estimates from Ashkenazi Jews because of the unique founder effects in their history [18]. The allele frequency data are provided in Table S1.To model the frequency of the Δ32 allele across Europe we used an approach based on Fisher's wave-of-advance theory [26]. The model is deterministic and based on a two-dimensional, nonlinear PDE. The PDE describes the function p(x,y,t), which represents the distribution of allele frequency across the xy plane at time t:\n\t\t\t\t\t\n\t\t\t\twhere Δ(p) is a nonlinear function of p that represents the change in allele frequency due to selection. The coefficient σ2 denotes the variance of the parent–offspring dispersal distance distribution. To calculate Δ(p), we assume fitnesses of 1 + d, 1 + s, and 1 for the Δ32/Δ32, Δ32/+, and +/+ genotypes, respectively. For this parameterization of selection, standard deterministic theory [44] gives the following result:\n\t\t\t\t\t\n\t\t\t\tTo incorporate gradients in selection, s and d are replaced with linear functions, denoted s(x,y) and d(x,y), respectively. In particular\n\t\t\t\t\t\n\t\t\t\twhere sc, xc, and yc represent the selection coefficient and the x and y coordinates of the center of the habitat, respectively. This approach does not limit s(x,y) to being positive; that is, if the gradient in selection is strong enough, portions of the range may have negative selection coefficients. The results presented here are limited to the assumption of additivity, so d(x,y) = 2s(x,y), although the results are not sensitive to the assumption regarding d (see Discussion).To represent the occurrence of the mutation at a single location in space with an initial local frequency of p0, we specified the initial conditions of the PDE solution to be\n\t\t\t\t\t\n\t\t\t\twhere δ(x,y) is a two-dimensional Dirac delta function, which takes on values close to one at x = 0 and y = 0, and values near zero for all other values of x and y. The value of p0 was calculated by the formula 1/D, where D is the initial population density. Population density only enters the model by determining the initial frequency. We generated results for D = 2.5 and D = 20. The two conditions correspond to published estimates of the population density in Europe at 1000 BCE and 1300 CE [30] and thus represent population density at the two orders of magnitude that are relevant for the origin of Δ32. The general results presented did not differ between D = 2.5 and D = 20, so we report results only for D = 2.5. For the boundary conditions, a model of reflecting boundaries was imposed. The assumption of reflecting boundaries is an implicit assumption that alleles are not lost or gained at the habitat boundaries.For the application of the equations to a geographic habitat, we set the x-axis to be latitude and the y-axis to be longitude. In the results we report σ in units of kilometers. For rendering the habitat we work in coordinates of degrees latitude and longitude, so a simple conversion is needed to change σ in units of kilometers to units of degrees latitude and longitude. To convert from units of latitude we employ the number of kilometers per degree latitude as the scaling coefficient, which is a constant 111 km per degree latitude. To account for the decreasing amount of geographic distance represented by 1° longitude as one moves north, σ in the longitudinal axis is converted by taking σ/m(x) where m(x) is the number of kilometers per degree longitude at latitude x. Without this correction, there would be a nearly 4-fold difference in latitudinal dispersal between the lower edge (22°N) and upper edge (75°N) of the range we considered. A similar correction is applied to s(x,y), d(x,y), and p0.To solve the PDE for p(x,y,t), we used an alternating-direction implicit approach with Crank–Nicholson updates at each time step [45]. In this approach, the continuous habitat is discretized into elements of length Δx and width Δy. Time is discretized into segments of length Δt, so that p(x,y,t) is represented by a three-dimensional matrix P(n) with elements P(n)j,k representing the value of p(x,y,t) at time nΔt at a point (jΔx,kΔy) relative to the origin. Δx and Δy were set to 1° of latitude and longitude, respectively. For the results presented here Δt was fixed at 0.005. Results were qualitatively similar for different values of Δx, Δy, and Δt, provided that all three were sufficiently small. The accuracy of numerical solutions was confirmed by comparison to analytical results that exist for simple geometries and linear selection pressures (results not shown).To model the frequency of Δ32 in Iceland, we used a standard single-population deterministic model of selection in which the additive selection intensity was set to s = 0.2 and the initial frequency was obtained from the PDE model. Specifically, the initial frequency was obtained by setting a selection Intensity of sc = 0.2 in the PDE model and recording the allele frequency at a representative location in Scandinavia (60°N 11°E) 44 generations (≈1,100 y) before the allele reaches 16% frequency. This approach neglects any possible founder effects associated with the founding of Iceland and any recurring migration between Iceland and mainland Europe. It also assumes the selection intensity in Iceland to be similar to that on the mainland.To obtain MLEs for the parameters of the model, we used a fixed allele age ta and supposed the number of Δ32 alleles observed at each sample locale arose as an independent binomial sample where the success probability at point (x,y) is determined by p(x,y,ta). The resulting likelihood function is\n\t\t\t\t\t\n\t\t\t\tThis likelihood approach benefits from taking into account the sample size at each sampling locale, so that the discrepancy between predicted and observed allele frequencies is penalized less at locations with smaller sample sizes. The value of ta used for the results was the time at which, given the other parameters, the maximum allele frequency first reached 16%, although using a value of 20% provided qualitatively similar results.A grid-based method was used to produce a joint likelihood surface over R, GNS, GEW, x0, and y0. We used a grid for R that had eight values between 2 × 104 and 2 × 106 spaced evenly on a logarithmic scale; a grid for the geographic origins x0 and y0 that contained the 29 locations indicated by the points in Figure 6; a grid for GNS that started at 0 and then covered from 1 × 10−5 to 22 × 10−5 with increments of 3 × 10−5; and a grid for GEW that included zero as well as a range from −17.5 × 10−5 to 17.5 × 10−5 with increments of 5 × 10−5. The resulting grid contained 14,848 points in the five dimensions of R, GNS, GEW, x0, and y0.To asses the goodness of fit of the model we performed a standard G-test. The G-test statistic can be formulated as 2(Ln −L5) where L5 is the log-likelihood computed using the MLE values in our full five-parameter diffusion-based model, and Ln is the log-likelihood computed using the observed sample frequencies as the respective population frequencies for the binomial distributions in the likelihood function (equation 5). For our dataset, Ln = −144.9 and L5 = −247.7. We also estimated the overdispersion parameter ϕ by the ratio of the G-test statistic to the number of degrees of freedom. Under the null hypothesis the estimate of the overdispersion parameter is expected to be one, and for large samples, values greater than one are indicative of overdispersion in the data.To evaluate the effect of kurtosis we used simulations on a two-dimensional stepping stone habitat of 121 × 121 demes placed on a torus-shaped habitat arranged in a uniform distribution on (−6,000 km, 6,000 km) along the x and y axes. The origin of the allele was chosen to be at x0 = 0 and y0 = 0, and the initial frequency of the allele was set to 10−5. The simulations were stopped when the allele frequency became greater than 16%. Selection was incorporated with an additive allele with s = 0.2. Dispersal was modeled using three dispersal distributions. Here, for simplicity, we present the one-dimensional version of each dispersal kernel. The two-dimensional distribution was found by assuming dispersal along each axis was probabilistically independent and taking the products of the corresponding one-dimensional distributions. The first distribution was a mean-zero Gaussian distribution:\n\t\t\t\t\t\n\t\t\t\tThe second was a modified double exponential that when c < 1 is not exponentially bounded, and thus qualifies as a fat-tailed dispersal distribution:\n\t\t\t\t\t\n\t\t\t\tThe third was a double gamma distribution that was used by Cavalli-Sforza et al. [34] to fit historical data on human dispersal:\n\t\t\t\t\t\n\t\t\t\tAll three distributions were parameterized to have a standard deviation equal to 100 km, so that the effect of kurtosis alone could be assessed. For the shape parameter of the double gamma distribution we used the value of 0.0419 estimated by Cavalli-Sforza et al. [34] for human historical data. The resulting double gamma distribution had a kurtosis of 146.2. For the modified double exponential distribution, we used a shape parameter (c = 1/2) that gives reasonable kurtosis (K = 25.2) and guarantees the tails of the distribution are not exponentially bounded. The resulting allele frequency surfaces were binomially sampled at 49 evenly spaced locations with samples of size 120 to construct a simulated allele frequency dataset. The data were then passed to the likelihood-based method used on the Δ32 data but with GNS, GEW, x0, and y0 all fixed to zero, so that R was the only parameter to estimate. For the grid search we used a grid of R values with nine points from 3.3 × 104 to 1 × 105. The mean and standard error for estimates of R and σ reported in Table 1 are the average of ten replicates for each dispersal distribution."
        },
        "10.1371/journal.pone.0088666": {
            "author_display": [
                "Wei Luo",
                "Peifeng Yin",
                "Qian Di",
                "Frank Hardisty",
                "Alan M. MacEachren"
            ],
            "title_display": "A Geovisual Analytic Approach to Understanding Geo-Social Relationships in the International Trade Network",
            "abstract": [
                "\nThe world has become a complex set of geo-social systems interconnected by networks, including transportation networks, telecommunications, and the internet. Understanding the interactions between spatial and social relationships within such geo-social systems is a challenge. This research aims to address this challenge through the framework of geovisual analytics. We present the GeoSocialApp which implements traditional network analysis methods in the context of explicitly spatial and social representations. We then apply it to an exploration of international trade networks in terms of the complex interactions between spatial and social relationships. This exploration using the GeoSocialApp helps us develop a two-part hypothesis: international trade network clusters with structural equivalence are strongly ‘balkanized’ (fragmented) according to the geography of trading partners, and the geographical distance weighted by population within each network cluster has a positive relationship with the development level of countries. In addition to demonstrating the potential of visual analytics to provide insight concerning complex geo-social relationships at a global scale, the research also addresses the challenge of validating insights derived through interactive geovisual analytics. We develop two indicators to quantify the observed patterns, and then use a Monte-Carlo approach to support the hypothesis developed above.\n"
            ],
            "publication_date": "2014-02-18T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 1451,
            "shares": 43,
            "bookmarks": 6,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0088666",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0088666&representation=PDF",
            "fulltext": "IntroductionThe world has become an increasingly interconnected system with multi-scale geographically embedded networks (i.e., transportation, internet). Spatial analysis aims to understand such systems in terms of spatial patterns, relationships, processes, and change within and among geographical spaces [1]. Social network analysis has been used to understand how systems emerge through the interaction of individual agents (i.e., humans, companies). Both approaches have advantages and limitations as methods through which to understand the complex geo-social interconnected world. Many geo-social interconnected systems mainly grow from the bottom-up, but traditional spatial analysis is a top-down approach that cannot deal with the evolution of the systems over space and time [2], [3]. Social network analysis, a bottom-up approach, can link individual-level behaviors and interactions to the emergence of social phenomena [4], but the approach typically ignores geographical constraints [5]. An effective integration of both approaches has the potential to aid understanding of geo-social systems from a more comprehensive perspective. For example, the integration of spatial consideration into a social network approach enables understanding of why and how an air-borne disease diffuses within an urban area in a manner that can generate disease hot spots as well as cold spots [6]. The integration of spatial analysis and social network analysis has the potential to link individual-level behaviors and interactions (i.e., human, vehicle, organization) to understand urban sprawl over space and time [4]. Although spatial analysis and social network analysis have the potential to complement each other, the formal integration of two approaches remains relatively underdeveloped in the literature [7].\nThis paper therefore integrates spatial analysis and social network analysis into a unified framework through a geovisual analytics approach. Geovisual analytics tools integrate computational methods with interactive visualization, in order to enable insights on large and complex geospatial datasets [8], [9], [10], [11]. Specifically, we present and apply a geovisual analytics tool, GeoSocialApp [12], that consists of three major analytical “spaces” implemented as linked components: a geographic space, a network space, and an attribute space. Each performs a specific task and can coordinate with other components to facilitate a process through which insights are enabled. We illustrate how the GeoSocialApp facilitates development of hypotheses, with the international trade network (ITN) as a case study. The explicit geographical and network representations in the GeoSocialApp facilitate and enable insight in terms of different roles that spatial and social relationships have in the ITN across geographical regions with network hierarchies at different scales. One major goal of geovisual analytics is to develop hypotheses on how space matters based on the patterns identified from geo-spatial data [13]; but the validation of geovisual analytics results is still regarded as a challenge [14]. Here, we propose a Monte-Carlo approach as a statistical validation to support the hypothesis developed through visual-computational exploration of spatial and social interaction in the ITN.\nThe paper begins below by reviewing the development of geo-social visual analytics methods in geography and network domains (Section 2). We then present an overview of the methods (Section 3) and the international trade network data used in this study (Section 4). The results obtained through applying the methods to the data (Section 4) provide insights on the different roles that spatial and social relationships play in relation to trade across geographical regions (Section 5). We next introduce the Monte-Carlo approach as a statistical validation to support the insights discussed in section 5 (Section 6). Finally, we present conclusions and an outlook for future research (Section 7).\nLiterature ReviewCurrent geo-social visual analytics tools can be classified into two major groups: the first group, rooted in geography, focuses on geographical analysis with an implicitly network representation; the second group, rooted in social network science, has an explicitly network representation with geography as a background to visualize the results. This section reviews the geo-social visual analytics tools from geography and social network science domains, and argues for a more balanced approach that emphasizes spatial relationships and social networks simultaneously.\nSpatial interactions/flows associated with topics such as human migration and disease transmission are major research domains for integrating network representation into geovisual analytics. For example, Andrienko and Andrienko [15] develop a spatial generalization method to transform trajectories with common origins and destinations into aggregated flows maintaining essential characteristics of the movement between areas. In complementary research, Guo [16] proposes an integrated interactive visualization framework that is applied to county-to-county migration data in the U.S. in order to visualize and discover network structures, multivariate relations, and their geographic patterns simultaneously. Additional relevant research can be found in recent papers by Andrienko et al. [17], Demšar and Virrantaus [18], Guo, Liu and Jin [19], and Wood, Dykes and Slingsby [20].\nAll of the above studies consider the geo-social processes from a primarily geographical perspective. Spatial interactions/flows in research taking this perspective are typically visualized on maps, which provide important information on spatial context. The observed spatial patterns can be related to the spatial context (e.g., big cities tend to be hotspots for human interaction). The methods for geo-social interaction discussed so far assume that geographic locations define the geo-social process, but new communication and transportation technologies clearly spread social networks beyond traditional geographical constraints (i.e., distance) [21]. Therefore, understanding the social meaning behind the geo-social processes is equally important.\nGeo-social visual analytics from a social network science perspective tends to have an explicit network context with an implicitly geographical representation. Ahmed et al. [22] introduce new visual analysis methods with dynamic network views (e.g., wheel layout, radial layout, and hierarchical layout) to explore the 2006 International Federation of Association Football (FIFA) World Cup competition in which countries are clustered based on their geographical locations in the dynamic graph representation. The visual analysis methods allow users to analyze and compare each country's performance within the geo-social context. The explicit network representation and implicitly geographical representation require analysts to relate the explicit network representation to his or her unrepresented geographic background knowledge in the visually interactive process [8]. Thiemann [23] developed the SPaTo Visual Explorer, which implements multiple explicitly geographical and network representations. Using a case study focused on global air flight networks, he illustrates how SPaTo can allow users to develop hypotheses about the interaction between geographical distance and social network distance. For example, they derive evidence showing that geographical proximity of cities corresponds with short social distance among the cities. Beyond the above, four additional research efforts have focused on specific components of methods to involve explicitly geographical representations into a traditional social network approach: 1) spatial point pattern exploration approach (e.g., kernel density) can be used to understand spatial impacts on the development of social networks [24]; 2) spatial autocorrelation coefficient (e.g., Moran's I) has been applied to social networks to measure the statistical similarity of individuals [25]; 3) explicitly spatial representations facilitate practical implementation of decision-making in certain social network application domains (e.g., infectious disease control) [26]; and 4) certain geo-social systems (e.g., human migration, international trade network) can be better understood or predicted through mathematical models considering physical and social space [27], [28].\nAs discussed above, understanding geo-social systems requires consideration of both geographical relationships and social network relationships. Therefore, it is necessary to involve explicitly geographical and social network representations. Andris [29] lists five benefits to having an explicit network representation within a geo-spatial framework: 1) the group of connected geographical regions can be studied as a unit with social closeness based on a network community detection approach; 2) the social power of places can be represented by node measures (i.e., degree, betweenness); 3) the social role of interconnected places over the whole system can be represented by network system measures (i.e., degree distribution, betweenness distribution); 4) the complex social interaction between places can be understood through adding multiple social flow layers on Geographical Information System (GIS); and 5) the geo-social systems in which spatial closeness and social closeness do not match can be better modeled with an explicit network representation.\nThe above discussion illustrates that there is the lack of explicitly spatial and social network representations in current geovisual analytics and the importance of such representations to understand geo-social systems [30]. It is also still a challenge to statistically support the hypotheses developed through visual exploration [31], particularly the hypotheses directed to geo-social interaction. To fill the gap, this paper introduces the GeoSocialApp with the 2005 international trade network as a case study to understand the interaction between spatial and social relationships, and introduces the use of a Monte-Carlo approach to validate the hypothesis developed in our geo-social visual exploration.\nMethodsIn this paper, we extend and apply the GeoSocialApp, a geovisual analytics tool initially introduced in preliminary form in Luo et al. [12]. The GeoSocialApp implements traditional network analysis methods within the context of an environment that links explicitly spatial and social representations to understand the interaction of spatial and social relationships in the ITN. The GeoSocialApp is an extension of the GeoViz Toolkit (GVT) developed in the GeoVISTA Center at Penn State [32]. The research presented here makes use of the existing choropleth mapping capabilities of GVT to support geographical analysis as well as the component coordination methods that enable dynamic linking and brushing across views, and adds a dendrogram component that supports multiple graph-based views to represent a varying network hierarchy. Details about other GVT components that could be used to extend the analysis presented here can be found in http://www.geovista.psu.edu/GeoSocialApp​/ (The source code for the GeoSocialApp is open source under the Library General Public License, version 2 (LGPL 2.0). We plan a public release of a binary version usable by non-programmers in the future).\nAs noted above, we use two components in the GeoSocialApp for this study: a dendrogram view and a choropleth view. The dendrogram view implements the convergence of the iterated correlations (CONCOR) algorithm [33], [34] to group nodes with equivalent positions in a single network or multiple social networks together. Equivalent positions refer to collections of actors that have similar ties to and from all other actors in the network. The implication of actors having equivalent positions is that they play similar social roles in a relational network. We can describe the relational network by an adjacency matrix A, which can generate a position similarity matrix R to measure the equivalent positions, whose element value rij is defined as:(1)where is the mean of the values in row i (j) of the matrix A and is the mean of the values in column i (j) of the matrix A. At the initial level of analysis, CONCOR performs the above equation calculations iteratively on the position similarity matrix R until all values converge to either 1 or −1, resulting in all nodes being grouped into one of two categories. Two groups can be too generalized for some studies, so hierarchical structures can be achieved by running CONCOR on each subgroup. In this way, CONCOR can continue to split nodes into successively smaller groups: two become four, four become eight, and so on. Although this algorithm was developed originally for application to social networks of individuals, it has been demonstrated to be an effective method to empirically locate structural positions in terms of the ITN [12], [35].Equivalent positions in terms of the ITN refer to collections of countries that have a similar import and export trade relationships with all other countries [36]. The implication of countries having equivalent positions is that they play similar social roles in the ITN. According to world system theory, the economic development of different countries is affected by their structural positions: core, semi-periphery, and periphery through unequal economic exchanges among them [37]. Core countries focus on capital-intensive production, periphery countries provide low-skill labor and raw materials, and semi-periphery countries are the industrializing countries positioned between the periphery and core countries. The CONCOR algorithm can classify the ITN into these three structural equivalence positions [38], [39].A tree layout and a radial layout are implemented in the dendrogram view to visualize the hierarchical structure of CONCOR results (Figure 1). The tree layout organizes the graph in a hierarchical way by placing child nodes under their common ancestors. An informationally equivalent radial view can be transformed from the tree by putting child nodes in the enclosing circle of their common ancestors [40], [41]. The dendrogram view in the GeoSocialApp also provides a slider to control the hierarchical level of CONCOR results.The dendrogram view of social space is dynamically linked to a choropleth map view used for visual exploration in geographical space. Each node in the dendrogram view corresponds to a geographical unit (i.e., states, countries) in the choropleth map. The choropleth map allows users to choose the number of classes, the classification method (i.e., equal intervals, quantiles), the variable to display, and the ColorBrewer palette [42] for color selection. Thus, the linked dendrogram and map views allow exploration of social positions and social groups and their corresponding spatial positions and spatial groups simultaneously. With the hierarchical level control in the dendrogram view, the linked views further support the explicit exploration of interaction between social space and geographical space and its impact on outcomes of interest at different network hierarchy (Figure 2). This capability will be illustrated in the case study presented below, after the data used in that case study are first described."
        },
        "10.1371/journal.pone.0072970": {
            "author_display": [
                "Juan A. Añel",
                "Douglas R. Allen",
                "Guadalupe Sáenz",
                "Luis Gimeno",
                "Laura de la Torre"
            ],
            "title_display": "Equivalent Latitude Computation Using Regions of Interest (ROI)",
            "abstract": [
                "\nThis paper introduces a novel algorithm to compute equivalent latitude by applying regions of interest (ROI). The technique is illustrated using code written in Interactive Data Language (IDL). The ROI method is compared with the “piecewise-constant” method, the approach commonly used in atmospheric sciences, using global fields of atmospheric potential vorticity. The ROI method is considerably more accurate and computationally faster than the piecewise-constant method, and it also works well with irregular grids. Both the ROI and piecewise-constant IDL codes for equivalent latitude are included as a useful reference for the research community.\n"
            ],
            "publication_date": "2013-09-25T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 931,
            "shares": 5,
            "bookmarks": 0,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0072970",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0072970&representation=PDF",
            "fulltext": "IntroductionIn atmospheric sciences Equivalent Latitude () is a Lagrangian coordinate defined as the geographical latitude enclosing the same area as the isoline of a given atmospheric field on a 2D (longitude by latitude) surface [1], [2]. It is computed as:: radius of the sphere (e.g. Earth's radius).\nTherefore computing  is simply the problem of computing the area  enclosed by the isoline of the studied property, which is essentially the determination of a function from a subset of data points.\nOne common technique to compute  works by assuming the value of the atmospheric field to be constant within each grid cell (hereafter the ‘piecewise-constant’ method). To determine  for a given threshold value of the field, you simply sum the areas for all grid cells with field values less than the threshold value.: value of the field. : value to be evaluated. : grid cell.\nThe piecewise-constant approach is only first-order accurate, not allowing the atmospheric field to vary across the cell. Increased accuracy could be attained by higher-order approximations for the field variation (e.g., linear, parabolic, cubic interpolation). In this paper, we develop a higher-order solution to the problem of calculating  by using contour mapping techniques. This approach, known also as regions of interest (ROI), is a well known concept in medical imaging and is also used for geographical information systems. The general approach involves defining a region from a field (up to 4D), which is selected for a posteriori analysis. That is, given a field or sample, a subsample is selected which meets certain properties and that is the region in which we are interested for further analysis. Here we limit the field variation to the surface of a sphere (2D). The use of ROI based on interpolated contours is more accurate than the piecewise-constant method, since it is a better approximation to the real function (exact area). This is illustrated in figure 1, which maps the areas enclosed by a threshold value of potential vorticity (PV) using both the piecewise-constant and ROI methods. This is an advantage when considering, for example, small isolated and closed contours, since the size of the grid becomes more important in order to take them into account or not. For example, the ROI method is more accurate to assess the real area of a closed contour when it is slightly smaller or bigger than a cell or subset of cells of the grid.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Comparison between the area used for the computation of  by the piecewise-constant method (gray surface) and the ROI technique (surface inside red contours) for a grid size of 2.5°×2.5°.The case corresponds to 1 January 1990 at OO UTC from NCEP1 for 2 PVU on an isentropic surface of 380 K.\ndoi:10.1371/journal.pone.0072970.g001In this paper we explore the ROI technique for the computation of  and compare the performance and accuracy with that obtained using the piecewise-constant method. For the purposes of this paper the ROI will be a 2D field of atmospheric PV [3], usually measured in PV units (PVU), where . The reason for this choice is that computation of  from PV is commonly used in atmospheric sciences, applied for example to the study of the stratospheric polar vortex [4], [5], and the goal for which we originally developed this approach. The differences between both techniques and, explained in a basic way, the steps that we follow are:\n\n\n\n\npiecewise-constant technique (traditional):\n\n\n\nPV is assumed to be constant within each grid cell;\n\ncompute the area of each cell which value is less than the PV threshold that we are interested in;\n\nsum the area of the cells from step 2\n\n\n\nROI technique:\n\n\n\nPV is ‘not’ constant within each grid cell;\n\nmake the contour mapping function ‘draw’ (interpolate) the isolines of PV;\n\ncompute the area enclosed by the isoline corresponding to the PV threshold value that we are interested in;\n\n\n\nWe developed the ROI code using the Interactive Data Language (IDL), a programming language extensively used for research in atmospheric sciences [6], and compare with piecewise-constant code also written in IDL. In this way we put the focus on the code and the technique, and do not take into account any dependence on programming language, corresponding libraries, or dependencies on hardware.\nThe following section describes the data used followed by the design and implementation of the ROI routine, addressing the ways in which we have solved the shortcomings of the computer language and subroutines affecting our implementation. This is followed by a results section that details the accuracy and performance of the ROI method relative to the piecewise-constant method. To conclude, we briefly discuss several ways of improving the solution here proposed, including using another programming languages and language interpreters.\nMaterials and Methods\nPrevious code and data\nRoutines to compute  are available through internet, e.g. the ones using the NCAR Command Language (http://www.ncl.ucar.edu/Applications/equ​iv_lat.shtml) or pascal (http://www.bodekerscientific.com/how-do-​i/calculate-equivalent-latitude). To check our new methodology against the more traditional one, we use the code (not previously published) supplied as (file S1) that applies the piecewise-constant method. This code takes as input a global gridded field with supplied latitude and longitude grids along with a desired threshold value of the field at which  is calculated. The grid cells are sorted by the value of the field in each cell, and  is calculated for every grid point on the globe.  is then interpolated to the desired threshold value and is output to the user. A flow diagram is provided in figure 2.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Flow diagram corresponding to the piecewise-constant code.doi:10.1371/journal.pone.0072970.g002We have tested our new ROI routine using fields of PV computed for five sets of reanalyzed data (NCEP/NCAR (hereafter NCEP1) [7], NCEP/DOE (hereafter NCEP2) [8], JRA-25 [9], ERA-Interim [10]), MERRA [11] and one chemistry-climate model (WACCM3 [12]). Additional information about the computation of the fields of PV, the datasets and results obtained with this routine can be obtained from Añel et al. [13].\n\n\nDesign and Implementation\nThe method here proposed is based on a simple idea: given the field that we want to study (PV), we just need to plot it using a global contour map and then to make use of the IDL tools to detect what parts of the field meet a criterion (in our case that the value is below a given PVU threshold). Then the area that meets the criterion is automatically computed by the ROI detection tools avilable in IDL and we just need to apply the corresponding equation to get .\nThe first problem to be solved for the implementation of the computation of the area using ROI is to determine the configuration of a given isoline of a global field projected on a sphere (the planet). Depicting the field with a cylindrical projection, an isosurface of PV can show three different configurations, directly related to the shape of the isolines over this projection (see figure 3):\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Upper left: global isosurface of 4 PVU on 1 January 1990 from ERA-Interim reanalysis data.Upper right:  corresponding to the area covered by the 4 PVU isosurface (in this case 30.41°). Lower panel: cylindrical projection corresponding to the upper left panel.\ndoi:10.1371/journal.pone.0072970.g003\n\n\n\ncase 1: contours closed for any region of the planet, the easiest problem as it requires just applying the computation of the area using the available routines for ROI;\n\ncase 2: contours going through the meridian 180 and not enclosing the pole are not closed in a cylindrical projection, so they need to be split in two different contours (one in each hemisphere) and the area has to be computed as the sum of the two different areas;\n\ncase 3: the contours around the poles are not closed in a cylindrical projection. They go from the the lowest to the highest value in longitude. They are closed using a large number of points (resolution of 0.1°) at the latitude near the pole (North or South) and then the enclosing area is calculated using routines for ROI.\n\nAnother problem addressed during the design of this code was that the computation of ROI in IDL gives erroneous negative values if one line of the isolines of PV drawn by the function ‘contour’ crosses itself. This happens mainly for latitudes near to the poles in rectangular grids where cell size and isolines get narrow and the IDL ROI implementation fails for unknown reasons. We solve this problem by increasing the latitudinal resolution in these regions, studying the ROI with a resolution of 0.1° to avoid the problem of crossing isolines.\nIt was also necessary to create new matrix of data in order to close the contours not closed in the cylindrical projection, just because for example they cut the meridian zero as we mentioned before. An additional problem was discovered for the computation of cosines for 90° and −90°. IDL presents a bug and the results are negative values instead of zero. To avoid this problem we close the contours in a latitude of 89.99° instead of 90°.\nThe IDLanROI class is used to determine the regions enclosing specified threshold values of PV, and it is combined with the application of the ‘path_info’ keyword in the Contour procedure. Also we had to take into account the difference between computation of the geometric area (the real value of the mathematical calculation) and the masked region (value from the pixels in the displayed area) when using ROI in IDL. Therefore we applied the ComputeGeometry function to obtain the geometric area. ComputeGeometry relies on the Contour procedure to get the function.\nFinally, the total area to be passed to the routine to compute the equivalent latitude is considered as the addition of the values for the different partial areas obtained. We use −9999.99 for the total area when (according to ‘path_info’) there is not a corresponding contour for the threshold PV value that we try to evaluate.\nThe process is explained in a flow diagram in the figure 4 and the ROI routine is included in file S2.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Flow diagram corresponding to the ROI code.doi:10.1371/journal.pone.0072970.g004\nResults\nPrecision\nFirst, we checked that the ROI methodology produces reasonable results for  at all latitudes and for different isentropic surfaces. Figure 5 shows  calculated from ERA-Interim PV data on 1 January 1990 using both the piecewise-constant and ROI methods from 340 to 420 K potential temperature. It is clear that the two methods produce very similar results, although there are some differences (e.g., near 40°S at 340 K).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Comparison of PVU values for several isentropic surfaces and corresponding to different .Values computed using the piecewise-constant and ROI methods on 1 January 1990 from ERA-Interim data.\ndoi:10.1371/journal.pone.0072970.g005Next, in order to check the accuracy of both methods we computated  for two different cases in which the PV field is not based on real data, but rather on an analytic function ( and , so that we know the exact relationship between  and PV. For the first case, the PV isolines form perfect circles around the pole and  is simply equal to PV*10. In the second case we simply obtain a wavy field, but the relationship between  and PV is the same. The PV fields generated in this way are then used as input for both methods and the results are compared. Figure 6 shows the results using the piecewise-constant and ROI routines for the first method and figure 7 for both of them. Also the tables 1 and 2 quantify the variation of the error for different grid sizes. As it can be seen for a grid of 1.5°×1.5° the piecewise-constant method shows a bias respect to the corresponding  around a half degree for the first analytic function and 0.75 for the second one. This is greater than the bias for the ROI technique. Moreover the accuracy of the piecewise-constant method depends much more on the size of the grid (see both tables and figure 7).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Red and blue circles:  circles computed applying the piecewise-constant method and the ROI technique, respectively, from an ideal field of PV represented on a 2.5°×2.5° grid.Black circles: real values of  for the PV field studied (not seen for the case of the ROI technique as the blue ones are superimposed as they match perfectly the ideal field).\ndoi:10.1371/journal.pone.0072970.g006\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 7.  Accuracy of the ROI technique and the piecewise-constant method for different values of PV and latitude and for the two analytical functions.doi:10.1371/journal.pone.0072970.g007\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Precision dependence with technique and grid size in degrees for the PV field given by the first analytical function.doi:10.1371/journal.pone.0072970.t001\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Precision dependence with technique and grid size in degrees for the PV field given by the second analytical function.doi:10.1371/journal.pone.0072970.t002\n\nPerformance\nAs we can see in tables 3 and 4 the ROI technique computed in this way is faster than the piecewise-constant method for a grid of 1.5°×1.5°, and moreover the finer the grid the greater the advantage in computational time for the ROI technique. This is something to have into account as in the future we probably will be moving to smaller grid sizes. This result is probably because the piecewise-constant method relies for the evaluation of the values of the function in each point on several FOR loops, which makes especially slow any computation in IDL. However the ROI technique simply computes the function once.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 3.  Computation time dependence with technique and grid size in  seconds for the PV field given by the first analytical function.doi:10.1371/journal.pone.0072970.t003\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 4.  Computation time dependence with technique and grid size in  seconds for the PV field given by the second analytical function.doi:10.1371/journal.pone.0072970.t004The test was performed on a computer with the following hardware: Intel Quad 2 Core with cpu frequency of 2.66 GHz and 3 MB of cache memory, 3 GB of RAM memory. The operative system was Ubuntu GNU/Linux i686 with Linux kernel 2.6.32–37-generic. The IDL interpreter was provided by EXELIS and the version was 8.0.\n\nThe ROI code described in this paper is publicly available from the URL https://github.com/eqlat/roi. As expressed in the code of the routine the code is under the GNU General Public License (GPL) (http://www.gnu.org/licenses). In this way we comply with the criteria on public availability of code for research purposes [14]. To work with it, it is just necessary to get the code and to have and IDL interpreter.We suggest as potential future directions for development the possibility of writing an equivalent ROI code in Python or Fortran, two programming languages very used in the geociences community, but it should been had into account that very limited support for ROI computation seems to exist in both languages. To the authors knowledge just the NiPy library (http://nipy.sourceforge.net) for Python, currently under development, and some punctual implementations in Fortran. Also, currently is not possible to use the code based on ROI with GNUdatalanguage (http://gnudatalanguage.sourceforge.net), a free IDL compiler, as computation of ROI is not implemented in it."
        },
        "10.1371/journal.pone.0066293": {
            "author_display": [
                "Salvatore Eugenio Pappalardo",
                "Massimo De Marchi",
                "Francesco Ferrarese"
            ],
            "title_display": "Uncontacted Waorani in the Yasuní Biosphere Reserve: Geographical Validation of the <i>Zona Intangible Tagaeri Taromenane (ZITT)</i>",
            "abstract": [
                "\nThe Tagaeri Taromenane People are two indigenous groups belonging to the Waorani first nation living in voluntary isolation within the Napo region of the western Amazon rainforest. To protect their territory the Ecuadorean State has declared and geographically defined, by Decrees, the Zona Intangible Tagaeri Taromenane (ZITT). This zone is located within the UNESCO Yasuní Biosphere Reserve (1989), one of the most biodiverse areas in the world. Due to several hydrocarbon reserve exploitation projects running in the area and the advancing of a large-scale deforestation front, the survival of these groups is presently at risk. The general aim was to validate the ZITT boundary using the geographical references included in the Decree 2187 (2007) by analyzing the geomorphological characteristics of the area. Remote sensing data such as Digital Elevation Models (DEM), Landsat imagery, topographic cartography of IGM-Ecuador, and fieldwork geographical data have been integrated and processed by Geographical Information System (GIS). The ZITT presents two levels of geographic inconsistencies. The first dimension is about the serious cartographical weaknesses in the perimeter delimitation related to the impossibility of linking two rivers belonging to different basins while the second deals with the perimeter line not respecting the hydrographic network. The GIS analysis results clearly show that ZITT boundary is cartographically nonsense due to the impossibility of mapping out the perimeter. Furthermore, GIS analysis of anthropological data shows presence of Tagaeri Taromenane clans outside the ZITT perimeter, within oil production areas and in nearby farmer settlements, reflecting the limits of protection policies for non-contacted indigenous territory. The delimitation of the ZITT followed a traditional pattern of geometric boundary not taking into account the nomadic characteristic of Tagaeri Taromenane: it is necessary to adopt geographical approaches to recognize the indigenous right to their liveable territories in the complex territorialities enacted by different stakeholders.\n"
            ],
            "publication_date": "2013-06-19T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 1,
            "views": 8931,
            "shares": 152,
            "bookmarks": 17,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0066293",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0066293&representation=PDF",
            "fulltext": "IntroductionThe Tagaeri Taromenane are two indigenous groups living in “Voluntary Isolation” in the Napo Moist terrestrial ecoregion, located in the western Amazon rainforest [1]–[5]. This ecoregion is one of the most biologically and culturally diverse areas on the planet [6], [7] representing an extraordinary richness across several taxa (amphibian, mammal, bird and plants), a high level of regional endemism [8]–[10], and is home to several indigenous ethnic groups including some of the world's last uncontacted peoples [11].\nThe Tagaeri Taromenane are settled in the Ecuadorean Amazon Region between the Yasuní and Curaray rivers and within the ancestral territory of the Waorani (or Huaorani) indigenous first nation [12], [13].\nTo protect the territory of these uncontacted indigenous people in the Yasuní Biosphere Reserve, the national government declared (1999) and mapped (2007), through two Presidential Decrees, the “No-Go-Zone” called Intangible Zone or Zona Intangible Tagaeri Taromenane (ZITT) [14], [15].\nDecree 2187 of 2007 represents the first official text that uniquely gives, using a geographical language, spatially explicit information to define the perimeter of ZITT.\nThe boundary of the ZITT is clamped onto 17 univocal points geographically expressed by pairs of metrical coordinates declared in the Decree 2187 (Figure 1A). To map the borders of the ZITT it is required to join the pairs of coordinates by rectilinear segments and by sections of river courses according to the geomorphological descriptions included in the Decree 2187.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Yasuní Biosphere Reserve and Zona Intangible Tagaeri Taromenane (ZITT): geographical framework.A) Delimitation of the Zona Intangible by given geographical coordinates and critical hotspots (red circles); B) Oil production in the Ecuadorian Amazon Region (9th concession licensing round, 2001).\ndoi:10.1371/journal.pone.0066293.g001Due to different interpretations of the geographical references expressed in the Decree 2187 and to the complex geomorphological characteristics of the Amazon sub-basins, different map representations of the ZITT are demarcated both in the official and unofficial cartography of the Yasuní Biosphere Reserve [16]–[20].\nSeveral studies about the ZITT and the issue on the uncontacted indigenous groups have been produced using mainly social and anthropological approaches [2], [3], [12], [13], [21]–[25]. No geographical studies and GIS spatial analysis on the ZITT have been produced in the scientific literature.\nFieldwork activities in the Yasuní Biosphere Reserve, geographical analysis of the Decree 2187, and studies on geomorphological characteristics of the area suggested the hypothesis that the official boundary of the ZITT is cartographically inaccurate with serious geographical inconsistencies along the perimeter.\nCrucial geographical inconsistencies seem localized between points No. 6 and No. 7 and between No.7 and No. 8.\nThe general aim of this paper is to validate the boundary using the geographical references expressed in the Decree 2187 of 2007 by analyzing the geomorphological characteristics of the area in order to re-map the perimeter of the ZITT through spatial and remote sensing analysis.\nThe GIS analysis results clearly show geographical incoherencies in two hotspots of the Yasuní and unequivocally demonstrate the impossibility to map the perimeter of the ZITT.\nAnalyses of both anthropological geospatial data about the Tagaeri Taromenane and their spatial relationships with extractive activities and roads confirm the existence of uncontacted groups in a wider area crossing oil fields and farmer settlements outside the same boundary of the Zona Intangible.\nFurthermore, violent contacts between Tagaeri Taromenane group and external mestizo farmers within the territory district of Dayuma (close to the Hormiguero extractive platform and the Armadillo oil field in 2009) [26]–[28] and maybe with Waorani relative (in 2013 within oil block 16, near the Maxus Road) happened in the area of Yasuní. Between the 4th and 5th of March 2013 two old Waorani were speared to death. The attack is under investigation by Ecuadorian Authorities to confirm responsibility of Tagaeri Taromenane groups [29]. These facts confirmed once more the presence of the Tagaeri Taromenane also outside the ZITT perimeter (Figures S1, S2), fuelling a hot debate inside national society questioning the relationship among the majority and minorities and asking for a different way of conceiving territories: from the Westphalian rigid geometry to a geographic network of cultural and environmental relationships.\n\nThe overlapping systems of territorial complexities\nThrough the Man and Biosphere Program, the United Nation's Agency UNESCO included this geographical area into the Biosphere Reserve network (1989), declaring the Yasuní National Park and the land-titled of Waorani territory (known as Waorani Ethnic Reserve) as Yasuní Biosphere Reserve. It was added in order to prioritize and to conciliate biodiversity conservation with sustainable development in the territory planning [30], [31].\nHowever, due to huge non-renewable energy reserves and to the crucial role they play in the national economy [32]–[34], the Ecuadorean State zoned specific geographical areas in the Amazon Region for hydrocarbon industrial activities – the so called “oil blocks” – so that, according to the 9th licensing round established in 2001, roughly the 79% of the YBR [35]–[37] was overlapped by concessions for oil extraction and production (Figure 1B) [38].\nOil development in the Ecuadorian Amazon (Figure 1B) has already directly and indirectly caused major social and environmental change to the Amazon people and ecosystems. Direct effects include deforestation for drilling platforms, pipelines, access roads, seismic prospecting activities, and chemical contamination of water bodies from wastewater discharges, oil spills and gas emissions [39]–[43]. Indirect effects are related to the opening of roads for oil exploration and transportation which turns terrestrial communications infrastructures into the main vector for colonization of primary forest [44]–[46], represented by the Moist Tropical Forest. Colonization of Moist Tropical Forest is causing increased deforestation as well as logging and hunting from human settlements [47]–[50]. As it is interpreted by the satellite image of the Via Auca Road (Landsat ETM+, 2002; Figure S1) – the most important oil terrestrial infrastructure at the western sector of the Yasuní Biosphere Reserve – colonization processes of the forests are usually driven by the main road axis [51]. They represent the main driver of extensive deforestation activity traced out by orthogonal and parallel processes resulting in land cover/land use change dynamics in the typical deforestation spatial pattern called fish-bone (Figure S1) [52]–[56]. Moreover, rapid expansion of Via Auca road system to the west underlies direct anthropogenic pressure to Tagaeri Taromenane mobility space [38]. Previous GIS analysis by processing GPS data of the Via Auca road network showed, between 2009 and 2012, a spatial evolution to the west of 100 linear km (Figure S2) [38], [57]. Land use and land cover change dynamics result in agricultural activities both at intensive cultivation level (i.e. African Palm, Elaeis guineensis bot. sp.) and non-traditional farming (Figures S1, S2).\nTherefore, hydrocarbon reserves exploitation, by its direct and indirect impacts on tropical forest ecosystems, played a pivotal role in turning the Napo ecoregion into one of the 14 mayor deforestation fronts in the world [58]–[60].\nThis set of socio-environmental processes taking place in the Ecuadorian Amazon (especially in the Auca territory system) is also affecting the social reproduction and the same survival of the Tagaeri Taromenane, clans of uncontacted hunters, and collectors living by semi-nomadic lifestyle in a wide territory of the Yasuní Biosphere Reserve's western and southern sector [3], [13], [61].\nAdditionally, in the last decades, several violent contacts between uncontacted indigenous people and external stakeholders induced the Inter-American Commission on Human Rights (IACHR, 2006) to grant precautionary measures in favor of the Tagaeri Taromenane demanding that “the Ecuadorian State adopt measures necessary to protect the territory inhabited by beneficiaries from third parties” [62].\nThe institutional history of the Intangible Zones began in the 1999 by the Decree 552, declaring the Zona Intangible to protect the territory of the uncontacted Tagaeri Taromenane indigenous groups. This Decree stated to identify and geographically define the ZITT within a time range of 120 days from the 2nd of February 1999 [14]. After 4 years, by the Inter-ministerial Agreement 092, a Technical Commission was saddled with establishing the perimeter of the ZITT (Quito, 2004) along with the task of identifying criteria including the integration of monitoring and controlling systems of the area [63]. The delimitation of the ZITT has been a long and controversial process [2], [64] complicated by the fact that this area is “geographically embedded” within the industrial concession areas for oil exploitation [65]. On the north and west sides, there are oil companies operating in industrial production for the past three decades while on the south side there are the exploratory blocks which represent, by the 11th licensing round opened in November 2012 [66], a spatial shift to the Southern Amazonian sector of the extractive frontier (Figure 1B) [67]–[70].\nFinally, a geographic delimitation of the ZITT was issued on the 16th of January 2007 through the Decree 2187 signed by the President of the Republic Alfredo Palacio [15]. To mitigate external anthropic influences, a buffer zone of 10 km has been defined around its perimeter. Within this additional protected area, wood extraction activities and new oil concessions are forbidden, while traditional activities such as hunting, fishing, and traditional use of biodiversity is allowed to the ancestral indigenous communities (the Waorani first nation) (Art. 2, Decree 2187) [15].\nAccording to the Ecuadorean Decree the Intangible Zones are defined as “protected areas of high cultural and biological importance in which whatever extractive activities are forbidden due to its high value for the Amazon rainforest, Ecuador, the World and the present and future generations” (1999, Presidencia de la Repubblica) [71]. It is therefore an “untouchable zone” in which oil, gas, and other external anthropic activities must be off limits.\nThe ZITT is also a crucial issue because it is related to the “Yasuní-ITT initiative” undertaken by the Ecuadorean Government (2007) [72]–[74], an international initiative oriented to protect biodiversity, recognize indigenous peoples' territory, and contrast climate change. The “Yasuní-ITT Initiative” proposal aims to keep locked underground, in perpetuity, at least 850 million barrels of heavy crude oil. This will prevent the emission of CO2 associated with burning fossil fuels of about 410 million metric tons of out of the atmosphere, in exchange of financial compensation from the developed countries concerned about tropical deforestation and climate change [75], [76].\n\nResults and Discussion\nCartographic analysis of the 17 vertex coordinates\nThe boundary of the ZITT is cartographically defined by Art. 1 of Decree 2187. The perimeter is spatially determined by 17 given points expressed by metric coordinates (UTM projection, zone 18, PSAD1956 geographic projection system) which have been cartographically validated. They are all geographically plausible and do not have any transcription errors. On the one side, all of the 17 vertex coordinates are consistent both with the geomorphological description and the measurements of straight lines included in the text of Decree 2187; however, on the other side sections of river courses linking vertex coordinates are in some segments ambiguous. In the latter case, mapping out the boundary becomes more complicated.\nMarking out river segments of the ZITT boundary imply the use large scale maps (1:50,000 or 1:25,000), including a detailed representation of hydrographic networks, the relief system, and toponyms. Due to a complex and vast territory characterized by the typical Moist Tropical Forest land cover and by a hydrographic system evolving into a very dense network, geographic interpretation could be difficult for cartographers since it is easily prone to cartographic errors.\nCartographic delimitation of the ZITT starts from point No. 1 which reaches by a straight line point No. 2. From here the boundary overlaps the Cononaco Chico River to the north until point No. 3. This segment of the limit is short (4,550 meters) and it does not create any problems of interpretation due to the importance of this river which is well represented in the cartography production of IGM. From point No. 3 to No. 4, the boundary is marked out by a straight line of 6,140 meters according to the given coordinates. The border continues from point No. 4 to No. 5 again by a straight line. From point No. 5 to No. 6 the perimeter line follows once again a straight line “till reaching the confluence of two tributaries of the Rio Bahameno in point No. 6, having 9892355 North and 339450 Este” (2nd indent, Art. 1, Decree. 2187) (Document S1). We noticed that, although the linear distance between these points is consistent to the values of the given coordinates (Table S1) and point No. 6 exactly lies on a river confluence, the two tributaries do not belong to the Rio Bahameno but to the Rio Dicaro (Figure 2, 3). As a matter of fact, it is the same geomorphological description in the official text stating that from point No. 6 the boundary follows the Rio Dicaro downstream in the east direction.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Perimeter of Zona Intangible Tagaeri Taromenane (ZITT) including points No. 5, No. 6, No. 7. (IGM Ecuador, scale 1:250,000).Points No. 6 and No. 7 belong to different rivers: the former to the Rio Dicaro, the latter to the Rio Nashiño.\ndoi:10.1371/journal.pone.0066293.g002\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Digital Elevation Model (SRTM) in the area of points No. 5 and 6.DEM analysis of river basins and catchment's divides: Rio Dicaro and Rio Nashiño belong to different basins: the first one flows into the Yasuní River, the second one into the Curaray River. Therefore “following downstream the Rio Dicaro till point No. 7 by the Rio Nashiño” (Art. 1, indent 3) [15] becomes a “geographical nonsense” in mapping out the perimeter section from points No. 6 to that No. 7.\ndoi:10.1371/journal.pone.0066293.g003The most controversial part has been identified in the perimeter section between point No. 6 and No. 7: “(the boundary) follows to East direction by the Rio Dicaro course, then it runs downstream by the Rio Nashiño till reaching point No. 7” (3rd indent, Decree 2187) (Document S1) [15]. As mentioned above, the ZITT limit runs downstream along the Rio Dicaro in the east direction. Once proved that point No. 6 matches up with the riverbed of the Rio Dicaro (not the Rio Bahameno) and point No. 7 properly overlies the Rio Bahameno, it is important to understand how it has been operated the leap of river basins. It is clear that the Rio Dicaro and Rio Nashiño belong to two different river basins. The first one flows into the Rio Yasuní and the second into the Rio Curaray.\nAssuming the linear distance between point No. 6 and No. 7 is about 23,300 meters (Table S1), this particular step of the Decree 2187 technically gives infinite solutions. Therefore, if there is no better explanation, this perimeter section becomes cartographically unresolved. In any case, it is not possible to follow downstream the Rio Dicaro until point No. 7. This discrepancy results in geomorphological inconsistencies if it is not made explicit where to leave the Rio Dicaro course.\nFrom point No. 7 to No. 8 the boundary follows the river course of Rio Nashiño. The distance corresponds to more than 50 km (Table S1). From this point the limit continues until point No. 9 by a straight line approximately 20.6 km to the north-east direction, according to the values of coordinates declared in the Decree 2187.\nFrom point No. 9 to No. 10 the ZITT perimeter follows the river course of Rio Yasuní and from that point by a straight line of 6,850 meters reaches point No. 11. This point coincides to the same state border between Ecuador and Peru.\nFrom point No. 11 to that No. 12 the boundary is traced out by a straight line of 53,978 meters oriented in the south-west direction. The given coordinates of point No. 12 matches the river course of Rio Cononaco which has to be followed upstream to point No. 13. From here, by a straight line of 5,438 meters, the ZITT limit reach point No. 14 which overlaps the course of Rio Curaray. From this point the perimeter section follows the Curaray River upstream in the west direction until point No. 15. From point No. 15 to No. 16 the boundary is trace out by a straight line 10,190 meters long orientated in the north direction. It then heads north-east by another straight segment of 10,500 meters to reach point No. 17. From this point to that No. 1 the limit line close the perimeter by another straight line headed north.\nAccording to geographic interpretation of spatially explicit data contained in Art. 1 of the Decree 2187, the ZITT perimeter from point No. 7 to No. 17 does not show cartographic inconsistencies nor geomorphological ones.\n\n\nRio Dicaro and Rio Nashiño: the river basins analysis\nBasins and river dynamics analyses have been carried out to clarify the most critical perimeter section of the ZITT expressed between points No. 6 and No. 7.\nThe GIS analysis is based on comparing three different Digital Elevation Models (DEM) that clearly represent the terrain morphology: an IGM DEM map [77], an Aster GDEM (second version) map [78], and a SRTM map [79].\nThe DEM analysis of river basins spatial relationships clearly show the catchment's divides of the Rio Dicaro and the Rio Nashiño river basins. Using the results from three different DEM, maps demonstrate that it is not possible to operate a leap of basin to link out points No. 6 to that No. 7 (Figures 3, S3); however, according to the Decree 2187 text, it is clear that somewhere this leap has to be done.\nTherefore, “following downstream the Rio Dicaro till point No. 7 by the Rio Nashiño” (Art. 1, indent 3, Decree 2187) (Document S1) [15] becomes “geographical nonsense” in mapping out the perimeter section from points No. 6 to that No. 7. Due to this geographic ambiguity, different representations of the ZITT boundary in this area have been mapped out in official and unofficial cartography production on the Yasuní Biosphere Reserve [16]–[20].\n\n\nPerimeter section between points No. 7 and No. 8: a critical mapping out\nAccording to the Decree 2187 the ZITT boundary section between points No. 7 and No. 8 has to run exactly along the riverbed of the Rio Nashiño and consequently following the bottom valley line. On the contrary, the official cartographic representation of the ZITT boundary [17], [71], [80] seems to have mapped out a different limit line on the section close to point No. 7.\nTo validate this section of the limit by remote sensing technologies, a geospatial analysis has been developed using both DEM cartography (SRTM and GDEM maps) and satellite scenes (Landsat 5 TM) in order to deeply investigate the terrain morphology and the river dynamics in this area.\nTo deploy a crossed spatial analysis we have examined four Landsat 5 TM satellites scenes in the years 1990, 2004, 2005, and 2008. The DEM analysis of river dynamics clearly shows that the boundary section drifts away from the Rio Nashiño riverbed running over a hill's ridge (Figures 4, S4).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Digital Elevation Model (SRTM) analysis between points No. 7 and 8: a critical mapping out of the Zona Intangible Tagaeri Taromenane (ZITT).The DEM analysis of river dynamic clearly shows that the boundary section, rather than overlapping the Rio Nashiño river bed, drifts away from the river course running over a hills line upper slope. This mapping out violates both the terrain morphology and the same official text (Decree 2187, 2007).\ndoi:10.1371/journal.pone.0066293.g004The Rio Nashiño river course is consistent with all the analyzed time series satellite scenes. Despite the previous cartographic base used to trace out the ZITT limits, this boundary section does not match with results obtained by remote sensing analysis of Landsat 5 TM satellite imagery (Figures 5, S5), SRTM DEM, nor Aster GDEM. Following its own trace violates both the terrain morphology and the same official text (Decree 2187).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Spatial analysis of perimeter section between points No. 7 and No. 8 processing satellite scenes from Landsat TM 5.Landsat Thematic Mapper (TM) false color composite (4 blue, 5 green, 7 red) of the territory encompassed between points No. 7 and No. 8 in the southern sector of the Oil Block 31 (PetroAmazonas). Boundary section (red color line) does not match the Rio Nashiño river bed (yellow color line).\ndoi:10.1371/journal.pone.0066293.g005\n\nThe Zona Intagible Tagaeri Taromenane and the Yasuní Biosphere Reserve: a synthesis map\nThe nine thematic variables assembled in the synthesis map give rise to the multiple territories setting up the Yasuní Biosphere Reserve space: the hydrocarbon dimension represented by concession, oil fields and wells for extraction and re-injection, protected areas recognized as IUCN category II for conservation of biodiversity by Yasuní National Park [65] and the Waorani indigenous territory by the titled-land (Waorani Ethnic Reserve), and finally the Zona Intangible for the Tagaeri Taromenane. Moreover, geodata about the historic incidents between Tagaeri Taromenane and external actors has been included in the map.\nThe hydrographic network (unfortunately the map presents a spatial gap in the south-east sector of the Yasuní Biosphere Reserve due to the unavailability of data) has been included in the map background because sections of river are often used as a “natural boundary” to delimitate protected areas (Yasuní National Park, Waorani Ethnic Reserve and ZITT) and they also play a crucial role in the indigenous territory building [56], [81]–[83]. Furthermore, according to anthropological studies, we considered Tagaeri Taromenane as indigenous groups who traditionally establish their territory along “inter-fluvial” spaces [3], [65], [84].\nThe road network is represented by the complex of the Via Auca (at the west sector, built in 1972), Via Maxus (north-west sector, built in 1994), Via “Oxy” (built in 2003 by the Occidental Petroleum company), and the Via “Petrobras” (built in 2004 by the homonymous Brazilian oil company). All the roads in the Yasuní Biosphere Reserve are important terrestrial infrastructures for hydrocarbon reserves exploitation [38], [85], [86].\nAs it is shown in the synthesis map (Figure 6), presently the ZITT is geographically adjoined with productive oil concessions confirming the overlapping territorial policies. GIS analysis of spatial relationships between oil block areas and the Zona Intangible demonstrates that – except with blocks No. 16, No. 14 and No. 17 which seem to cut out the same ZITT perimeter – hydrocarbon concessions No. 31 (Petroamazonas) and the ITT block (managed by Secretaria de Hidrocarburos of Ecuador) result in, at the present time, an overlap of 8,542 and 9,952 ha (Figure 6; Table 1). It is relevant to clarify that the actual spatial dimension of oil blocks reflects the most recent changes operated by SHE in 2011 [87]. Before these relevant changes, previous GIS analysis had calculated a wider overlap of 84,789 ha by the overall oil blocks, corresponding to 10% of the ZITT. Areal overlap becomes greater if we consider the 10 km buffer zone of ZITT – 190,929 hectares, corresponding to 22,7% (Table 1) [20].\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Synthesis map: Yasuní National Park (YNP), Waorani Ethnic Reserve (WER), Zona Intangible Tagaeri Taromenane (ZITT) and oil blocks.The overlapping systems of territorial complexity in and around the Yasuní Biosphere Reserve: the Zona Intangible (ZITT) is geographically adjoined with productive oil concessions area at the north-west side; on the contrary the north-east sector overlaps both with oil blocks No. 31 (operated by the Petroamazonas national company) and that ITT block (Ishipingo-Tiputini-Tambococha block, Secreteria de Hidrocarburos of Ecuador). The spatial distribution of the Tagaeri Taromenane clans is partially outside the ZITT perimeter in the cases of the Nashiño and the Maxus groups; the Armadillo group is completely outside the ZITT boundary overlapping oil blocks No. 14 and No. 17. Their home range overlaps several titled and untitled land of mestizo (colonos) farmers and non-autochthonous indigenous settlements in the Dayuma and Ines Arango districts, implicating a sensible proximity with roads system related to the Via Auca main axis. The presence of the Armadillo group in this area is also confirmed by recent violent contacts between the Hormiguero oil platform and the Armadillo oil field.\ndoi:10.1371/journal.pone.0066293.g006\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Oil Blocks overlaps on the Zona Intangible Tagaeri Taromenane (ZITT) and the Buffer Zone (10 km).doi:10.1371/journal.pone.0066293.t001GIS analysis of the spatial distribution of the four Tagaeri Taromenane clans [88] shows geographical weaknesses of the ZITT perimeter and the Buffer Zone in protecting the PVIs territory (Figure 6). As it is disclosed in the synthesis map, the ZITT area does not completely fit with the home range of Tagaeri Taromenane. The Cunchiyacu group is settled within the Curaray and Cononaco rivers within the ZITT. In the case of the Maxus and Nashino groups, the home range of these two clans is partially out of the ZITT area. On the contrary, the Tagaeri Taromenane Armadillo clan is definitely out of the same perimeter of the ZITT, completely overlapping oil blocks No. 14 and No. 17 (Figure 6) which are confirmed hydrocarbon reserve areas to be exploited by the Petrooriental oil company [89], [90].\nFurthermore, the Armadillo clan home range overlaps several titled and untitled lands of mestizo (colonos) farmers and non-autochthonous indigenous settlements in the Dayuma and Ines Arango districts, implicating a sensible proximity with roads system (Figures S1, S2) related to the Via Auca main axis [38].\nThe fact is a crucial zone in which different actors and different territory projects are at stake is also confirmed by the dramatic contacts between colonos and uncontacted people in 2009. Only 400 meters from the Hormiguero extraction platform (oil blocks No.17), a Tagaeri Taromenane group speared to death three local farmers [3], [27], [28], [91].\nThe Nashiño and the Maxus groups are partially outside the ZITT perimeter overlapping three oil blocks (No. 14, No. 17, No. 16, No. 31 and the ITT). In the case of the Maxus group most of their territory, having a home range of roughly 30 km, is outside the ZITT within the oil blocks No. 16 and No. 14. Within block No. 16 the news documented a possible contact in March 5th (2013) (under verification by Ecuadorian Authorities) confirming it is a critical area [29], [92], [93]. This spatial distribution of these clans is also confirmed by previous anthropological analysis on the Tagaeri Taromenane issue [2], [3], [28], [84], [94], in which several witnesses by the Waorani communities, settled within the same oil blocks No. 16, report the presence of uncontacted people in this precise area. Moreover, qualitative field based research by Colleoni and Proaño refer to signs of presence by footprints and typical symbolic territory demarcations (i.e. crossed broken branches) in the area of the Rio Dicaro sources, between oil blocks No. 14 and No. 17., identifying a mobility space of uncontacted people that are used to cross from Rio Dicaro to Rio Yasuní (Figure 6). This behaviour is also confirmed by the database of Ministerio de Justicia y derechos Humanos related to the operation derived from direct survey and patrols of the ZITT and surrounding areas.\nA synthesis map of the critical zone defined between points No. 5, 6, and 7 of the ZITT has been produced. Geospatial data on hydrocarbon concessions (10th licensing round), oil fields and wells, Waorani communities, and the home range of Tagaeri Taromenane have been assembled using a SRTM DEM map to also show geomorphological characteristics. This map shows spatial relationships between oil production areas, the ZITT perimeter, and the Maxus uncontacted group (Figure 7).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 7.  Synthesis map of the critical zone around points No. 5, 6 and 7 of the perimeter section: spatial relationships between oil production (block 16), home range of uncontacted Maxus group, and the spot of the Tagaeri Taromenane attack occurred on the 5th March 2013.This synthesis map of the critical zone defined by points No. 5, 6, 7 of the Zona Intangible show sensible spatial relationships between oil production (blocks, fields and wells), the home range of uncontacted Maxus clan, the ZITT perimeter and the Waorani communities settlements. Spatial relationships between oil production areas and the Zona Intangible perimeter section are clearly shown in the map. The attack has major significance in terms of its location since it occurred near the geographic inconsistencies highlighted by this paper: about 100 meters from the Wipsi-1 oil platform, 10 km from point 7, almost 20 km from point 7, along the border of the Buffer Zone. Background raster map has been assembled using a SRTM DEM model to also show geomorphological characteristics of the area.\ndoi:10.1371/journal.pone.0066293.g007\n\nDrawing a new map: recognizing the spatial patterns of Tagaeri Taromenane\nFrom the perspective of the geography discipline, the territory is not just a polygon closed by a perimeter linking the adjoining areas. Territories are based on junctions, corridors, multiple and non-continuous polygons, centres and peripheries, intensity and directions of functions, multiple uses of natural resources, and co-existence of actors [56], [81], [82], [95]–[97]. As it as shown by the geographical analysis of the ZITT, every geometrical space may include multiple territories, often conflicting each other.\nIn a geographical perspective, territories are produced not only by physical intervention, but also (and before physical intervention) by images and particularly maps since they are the strongest way to produce imaginary geographies [98]–[102]. Normally maps presenting the ZITT are produced in the framework of the Westphalian territorial idea based on a rigid geometry of surface and perimeter which is the classical and most common representation of the ZITT. Figure 8 tries to produce a map representation closer to human right policies and the spatial patterns of Tagaeri Taromenane. First of all limits are abolished. Second, there is a representation of the buffer zone (normally not communicated). Third, a territorial continuity between the nuclei of the four groups and the official ZITT has been represented. This representation shows that the traditional geometric ZITT seems unable to contain the territory reconstructed from the traces of practical territorialization implemented by Tagaeri Taromenane. The decision to use a texture instead of a strong tick colour or an empty area with a marked perimeter is a typical choice from a wide array of tools available to cartographic science. Its usage stresses the communication message of a cultural geography of network relationships among societies and ecosystems [103]–[106].\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 8.  Synthesis map of the Zona Intangible. Recognizing the complex territory of uncontacted Tagaeri Taromenane: from geometry to geography.The map results from the combination of human right policies (Zona Intangible Tagaeri Taromenane) representing also the Buffer Zone (normally avoided in official maps), the oil concessions for hydrocarbon production and the spatial patterns of Tagaeri Taromenane (combining the results of data about paths and presences). Considering the power of images in shaping territorial representation, cartography science has the responsibility in formulating more complex discourses about controversial territories.\ndoi:10.1371/journal.pone.0066293.g008This map is only a first attempt to highlight how the recognition of indigenous territory in the Amazon basin needs to implement geographical approaches in order to deal with the complex dynamics of territories and territorialities in relation to actors and conflicts.\n\nMaterials and MethodsTo develop geographical analysis of the ZITT limits we primarily examined the Decrees No. 552 (1999) and No. 2187 (2007). The first one officially declares the No-Go-Zone as “an Intangible Zone for conservation of the homeland of the Waorani groups known as Tagaeri Taromenane in which it is forbidden in perpetuity whatever sort of extractive activities” (see the Spanish original text in supporting materials) while the second one determines the boundary of the ZITT providing geospatial information and detailed geomorphological description of the area [14], [15]. In the second Decree (2007) the ZITT is cartographically defined by 17 pairs of metrical coordinates conjoined to each other by straight lines or sections of river courses (Figure 1A). It also declares the Geographical System adopted to mark off the ZITT: Provisional South America Datum 1956 (PSAD-1956), 18 South Zone. The total area declared is 758,051 ha.\nIn order to validate the perimeter of the ZITT, we analyzed topographic sheets by the Military Geographic Institute of Ecuador (Instituto Geografico Militar, IGM) at different scales (1:50,000, 1:250,000). The IGM cartography at a 1:50,000 scale consists of four sheets named as Rio Bahameno [107], Rio Yasuní Este [108], Rio Nashiño [109], and Rio Yamino [110]. The sheets adjoin each other to form four quadrants including, in their common corner, the perimeter between points No. 5, No. 6, and No. 7. The sheets are provided of geographical metadata defining as projected into a UTM 18 South, Datum WGS 1984 except the Rio Nashiño map that is projected in the Zone 17 South. All four sheets have been assembled using aerial photography mainly taken in the year 1986 while the “Yasuní” sheet is derived by photographic coverage from 1963 and the “Rio Bahameno” sheet from 1976. Contour lines show an equidistance mainly of 20 meters and occasionally of 10 meters. These values are obtained by subtracting a constant of 30 meters of altitude assuming an average canopy of the MTF of 30 meters high.\nAltimetry points show a distribution average of 0.6 values per km2, calculated by three information aliquots of 66 km2. Geodetic and trigonometric points are rare most likely due difficulty of accessing to the Amazon territory.\nTo perform a comparative cartographic analysis we also used the 1:250,000 IGM sheets named Shushufindi [111] and Curaray [112]. Unfortunately, the Military Geographic Institute does not cover the eastern territory of the ZITT by the other two adjoining quadrants. However, the “Shushufindi” sheet represents a more detailed cartographic landscape compared to the ones at 1:50,000 scale. On the contrary, the Curaray sheet is definitely poor, without any basic metadata. These topographic sheets were produced in the year 1998 and are basically derived from the 1:50,000 scale sheets. Therefore, we do not expect different information according to the larger scale. In any case, both the 1:250,000 scale sheets represent a poor and incomplete hydrographic network. For instance, there are some river sources ending at the border of the sheet and not continuing onto the adjoining one.\nIn order to analyze the spatial relationships both of catchment's divides and hydrographic network related the ZITT perimeter we acquired raster thematic cartography (Table S2). Three types of Digital Elevation Models (DEM) maps have been used to deeply investigate both the terrain morphology and river dynamics [113]–[116]: an IGM-DEM map [117], a Shuttle Radar Topographic Mission (SRTM) map [79], and a GDEM derived by the Aster satellite [78].\nThe IGM-DEM map presents a geometric resolution of 30 meters (one spot elevation every 900 m2), the SRTM map of 92 meters (one spot elevation every 8,400 m2), and the GDM Aster satellite of 30 meters. The SRTM elevation spot derives directly by radar interferometry technique. On the contrary, the IGM-DEM map is obtained not by remote sensing technology, but by cartographic data included in maps at 1:50,000 and 1:250,000 scale.\nThe remote sensing data from the Aster satellite (GDEM) presents a higher geometric resolution to the ground – 30 instead of 90 meters – but they show several false records due to the intense land cover of the MTF. Therefore, these satellite derived maps present a repetitive error more extensive in the representation, but homogeneous and, consequently, tolerable.\nMoreover, Landsat satellite imagery has been used to validate some uncertain segments of the perimeter represented by river course sections. Three satellite time-series of Landsat Thematic Mapper (TM) false colour composite (4 blue, 5 green, 7 red) [118] of the territory encompassed between points No. 7 and No. 8 in the southern sector of the Oil Block 31 (PetroAmazonas) have been acquired. The Landsat satellite scenes acquired are from the years 1991 and 2005 (2nd of July and 28th of August respectively). For a complete overview of the raster cartography used in the spatial analysis see Table S1.\nOther geodata has been used to perform the spatial analysis of the ZITT perimeter together with other key components shaping the Yasuní territory in order to have a systemic understanding of the complex territorial processes taking place in the area [20], [96].\nGeospatial information on protected areas (Yasuní National Park, Waorani Ethnic Reserve), hydrographic network, and urban centres are from institutional web sources and were publicly available at the time of the submission. These include the World Database on Protected Areas (UNEP-WCMC, IUCN World Commissions on Protected Areas, 2012) [119] and the EcuadorIan IGM Geoportal. The official ZITT perimeter and geographical data on Tagaeri Taromenane clans distribution have been acquired from the Ministerio de Justicia, Derechos Humanos y Cultos of Ecuador (2012), who from October 2010 holds the Plan of Precautionary Measures for the Protection of Isolated Indigenous Peoples of Ecuador (Plan de Medidas Cautelares, PMC) [88].\nRoad network of Ecuadorian Amazon was taken by the Instituto Nacional para el Eco-Desarrollo de la Region Amazonica Ecuatoriana (ECORAE) in 2008 [120].\nOil fields and oil wells have been acquired by the ECORAE institution (2008). Oil blocks with the latest changes (2011) of the 10th hydrocarbon licensing round have been derived by digitizing oil maps released in 2012 by the Secreteria de Hidrocarburos Ecuatorianos (SHE) [87]. An overview of characteristics of all vector geodata used in the spatial analysis are shown in Table S3.\nThe main tools to conduct cartographic and spatial analysis of the ZITT have been Geographical Information Systems (GIS) and remote sensing technologies. GIScience approach has been used to collect and elaborate different types of data, as long as spatially explicit, referring both to the biophysical components and to the anthropic one, so that cartographic outputs and synthesis maps make visible spatial relationships between relevant components of environmental systems and that of the anthropic one [121]–[124].\nAll the anthropic, physical, and geographical data collected have been integrated in a GIS database in order to manage spatially explicit, temporally referenced, and thematically differentiated information.\nSpatial analyses are based both on a critical examination and integration of the physical and thematic cartography previously acquired and on GIS elaboration in order to produce analytical and synthesis maps showing the following data: hydrographic network, river basins, protected areas (Yasuní National Park, Waorani Ethnic Reserve, ZITT) and buffer zones, urban centres and local communities, road network, and the spatial dimension of the hydrocarbon exploitation (blocks, oil fields and wells). Moreover, we have also acquired spatial anthropological data concerning the Tagaeri Taromenane as the family clans spatial distribution [88].\nArcGISTM 10.1 software has been used for GIS analysis mainly conducted by spatial functions such as overlay, geometrical interception, linear, and area measurements. Raster data analysis has been carried out using IDRISI-GIS software.\nTo validate important data such as spatial information of key rivers, local communities and roads the GPS ground-truth methodology has been adopted during fieldwork activities (2005, 2006, 2010, 2011) [38] [125].\nResults of GIS and remote sensing analysis, fieldwork data, and examination of all the available sources show different critical issues in the Decree 2187 when it comes to defining the perimeter of the ZITT. They belong both at an interpretation level and, therefore, in the same cartographic representations. On the other side, the policies implemented in the ZITT and in the Buffer Zone show conflicting territorial objectives.During the revision of this article, in the night between the 4th and 5th of March 2013, the old Waorani man Ompore and his wife Buganey were speared to death in proximity of Iro oil field, in Block 16, which is operated by Repsol-YPF (Figure 6, 7). The attack should be confirmed, and at the moment investigation into the incident has been initiated by police and Ecuadorian authorities to ascertain the responsibility of Tagaeri Taromenane groups. Information collected by Waorani relatives of Ompore Omeway and Buganey Cayga refer to repetitive requests by Tagaeri Taromenane directed to Ompore to act with the oil company to stop oil extraction, pollution of water and air, and noise [92], [93], [126]. Meanwhile, a group of Waorani from the Yarentaro community organized an expedition to avenge their dead relatives, entered the ZITT, and killed approximately dozens of Tagaeri Taromenane as well as abducted two uncontacted youths. This fact, confirmed by an official twitter account of Ministerio de Justicia on the 3rd of April 2013, is also under investigation.These two dramatic events should be deeply analyzed in order to understand the complex relationship among uncontacted Waorani groups and the wide group of Waorani, the discontinued presence of the State, the role of Waorani organizations, and the deterministic approach of some NGO advisors.The incident of 5th of March also demonstrates the long and complex process to enact human right policies in a controversial and complex territory. Moreover, the attack has major significance in terms of its location since it occurred near the geographic inconsistencies highlighted by this paper: about 100 meters from the Wipsi-1 oil platform, 10 km from point 7, almost 20 km from point 7, along the border of the Buffer Zone (Figure 7).So it is interesting to analyze how the limit line of the ZITT runs when it moves – close to point No. 6, from west to east direction – within this zone inward the oil block 16 (Repsol-YPF): after a sudden curve upper slope it drastically runs to south leaving the oil blocks by the Nashiño river, setting up a kind of cartographic enclave by a “bird beak” shape (Figure 7). After considering previous anthropological studies on Tagaeri and Taromenane in this area, one can logically ask the question: is the ZITT boundary line between the Dicaro and Nashiño rivers shaping an ethnical enclave as well?Using a systemic approach to better define this issue, it is relevant to also take into account the geographical position and the same history of the ZITT (Figure 6).During three years of the delimitation process (2004–2007), the Technical Commissions established by the Environment Ministry of Ecuador had to deal with blocks for oil exploitation which extensively overlap the northwest sector of the ZITT (Figure 6). This is the case of oil block No. 14 and No. 17 (Andes-Petroleum Company, China) which presented an overlap of 2,719 and 24,534 ha, respectively, in the same area of the ZITT, namely 1.4% and the 49.6% of each oil concession (Table 1) [64].Due to a possible and relevant reduction of oil block areas during the ZITT delimitation process, the Andes-Petroleum company suggested to the Ministry of Energy (Ministerio de Energia y Minas) and its Environmental Department (DINAPA) to modify the perimeter in order to facilitate oil exploitation in the area maintaining direct access to the proved oil field called Awant-1. Previous GIS analysis [20] showed the “cartographic suggestion” of Andes Petroleum to create – according to its productive requirements – a special corridor to exploit the Awant-1 oil field (Fig. S6) which is localized 12,8 km to south point No. 6, inside the same ZITT area (Figure 7).All of these geographical elements display the complexity of the Yasuní Biosphere Reserve in which different projects of territory – and, therefore, different modalities to use natural resources – are overlapped and often in conflict each other. A geography of energetic resources marked in maps by productive zoning (oil blocks) is superposed to that of protected areas (Yasuní National Park), to that of farmers settlements (Agrarian Reform), to that of indigenous territory (Waorani Ethnic Reserve), and finally to that of the ZITT [38], [51], [56].The same delimitation of the ZITT induced a national and international debate on uncontacted people and their territorial rights. Aguirre (2007) stresses that since first steps of delimitation (2004) the process has been sinuous and thorny, made by consultation and negotiations, in primis with oil companies operating in the area of the ZITT. Later with officers of United Nation and IUCN, with the Waorani indigenous Organization (NAWE) and the National Indigenous Confederation of Ecuador (CONAIE), and finally national and international NGOs, without avoiding the weak but existing pressures from the local colonos (settled farmers of agrarian reform).In general, beyond the technical issue to map out a perimeter for the ZITT, anthropological studies argue that a lack of understanding of territory and the same territoriality of the Tagaeri Taromenane is one of the main factors of the weakness in delimitating an Zona Intangible [2], [3], [6], [25], [41].It is not just a technical issue considering the geographical and the biophysical characteristics of an area placed in the core of the Amazon region. It is a remote zone in which the Moist Tropical Forest as well as the dense and homogeneous hydrographic network, both set up a territory that is difficult to study, understand, and control. Therefore, mapping out the boundaries by “remote” analyses, using unverified geospatial data and not considering the complex arena in which territorial dynamics and the same territoriality of the Tagaeri Taromenane take place, is very difficult without holistic approaches and knowledge based on fieldwork.Moreover, if we consider that the Decree 2187 uses “natural boundaries”, such as section of rivers, to delimit the ZITT perimeter, cartographic operations become more complicated, especially if the geospatial data is not precise, they present data gaps, or relevant geographic errors.On the other side, a ZITT perimeter only based on straight lines could be easily traced out. However, in tropical forest territory, whatever border marked out that does not have clear natural spatial references, such as rivers or hills, results in a concealed boundary. A ZITT cartographically perfect will be produced, but at the same time it is invisible on the ground to both residents and to the people “from outside”.A scientific contribution in the ZITT debate is necessary to make visible this territory (physically and graphically) to different components of Ecuadorian society because the ultimate objective of this policy is the right to liveable place of Tagaeri Taromenane uncontacted people.In a structural context where media debate about the Amazon as “space of exception” [100], and ZITT territoriality is disputed, it is important to remember what “The protection guidelines for indigenous peoples in isolation” issued in May 2012 by the Office of the High Commissioner for Human Rights (OHCHR, 2012) states: “While there is no consensus on the term to be used to refer to these people, internationally the most widely used is the concept of \"isolated peoples”, in some countries they are known as, inter alia, free people, uncontacted, hidden, invisible, in voluntary isolation. Despite the different formulations, all refer to the same concept “isolation was not a voluntary option but a survival strategy (par. 9)”."
        },
        "10.1371/journal.pone.0045985": {
            "author_display": [
                "Maxime Lenormand",
                "Sylvie Huet",
                "Floriana Gargiulo",
                "Guillaume Deffuant"
            ],
            "title_display": "A Universal Model of Commuting Networks",
            "abstract": [
                "\n        We show that a recently proposed model generates accurate commuting networks on 80 case studies from different regions of the world (Europe and United-States) at different scales (e.g. municipalities, counties, regions). The model takes as input the number of commuters coming in and out of each geographic unit and generates the matrix of commuting flows between the units. The single parameter of the model follows a universal law that depends only on the scale of the geographic units. We show that our model significantly outperforms two other approaches proposing a universal commuting model [1], [2], particularly when the geographic units are small (e.g. municipalities).\n      "
            ],
            "publication_date": "2012-10-01T00:00:00Z",
            "article_type": "Research Article",
            "citations": 2,
            "views": 1801,
            "shares": 4,
            "bookmarks": 17,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0045985",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0045985&representation=PDF",
            "fulltext": "IntroductionBillions of people move everyday from home to workplace and generate networks of socio-economic relationships that are the vector of social and economic dynamics such as epidemic outbreaks, information flows, city development and traffic [1], [3]. Understanding the essential properties of these networks and reproducing them accurately is therefore a crucial issue for public health institutions, policy makers, urban development, infrastructure planners, etc. [4], [5]. This challenge is the subject of an intensive scientific activity (see [6], [7] for reviews), in which the analogy of the gravitational attraction inspires a majority of approaches [8], [9]: the number of commuters between two geographic units (cities, counties, regions…) is supposed proportional to the product of the \"masses\" of each geographic unit (the population for example) and inversely proportional to a function of the distance between them. Unfortunately, numerous experiments showed that the optimum function and parameter values vary a lot with the case studies [4], [5], [10], [11]. This situation is not satisfactory because when one wants to generate a particular commuting network without having the total origin destination matrix of commuting, no practical heuristic is available for choosing the adequate type of function and parameter values. This paper addresses this problem.\nWe consider a recently proposed model [12], [13], differentiating itself from the usual gravity law models in two main features:\n\n\n\n\nIt takes as input the total number of commuters in and out from each geographic unit. With this starting point, the model focuses directly on the influence of the distance between geographic units on the commuting probability. The model is data demanding, but these data are widely available.\n\nIt builds the network progressively, allocating commuters one by one in the different flows, according to probabilities that increase with the number of commuters coming in the destination and decrease with the distance between the origin and destination. These probabilities are updated after each allocation.\n\nOur model is close to the traditional doubly-constrained gravity model [8], [9], but it is more flexible and less data demanding. Indeed, the doubly constrained model and the methods used to solve it require a closed network of commuters: they cannot take into account commuting links outside the considered geographical units. Our individual based stochastic approach overcomes this problem and can deal with the usually available data of total number of commuters in and out of geographic units.\nWe test this model on 80 case-studies with geographic units of different scales. For example in the same case-study the geographic unit can be either the municipality, the canton or the department, (see an example on Figure 1). More precisely, the case studies include: Czech Republic (municipality scale, 1 case-study), France (municipality scale, 34 case-studies), France (canton scale, 15 case-studies including whole France), France (département scale one case-study (whole France), Italy (municipality scale, 10 case-studies), Italy (province scale, 4 case-studies), USA (county level, 15 case-studies including whole USA). For a detailed description of the datasets see the Supporting Information .\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Three scales of geographic units (Auvergne region, France).doi:10.1371/journal.pone.0045985.g001We show that the single parameter of our model follows a simple universal law that depends only on the average surface of the considered geographic units. This implies that, given the number of commuters in and out of each geographic unit and their average surface, we can derive the whole matrix of flows with a very good confidence.\nTwo other approaches [1], [2] claim to catch universal properties of commuting networks. We show that our model yields significantly more accurate results, especially for case-studies with small geographic units (e.g. municipalities).\nThe ModelWe consider the basic double-constrained model setup, without adding any ingredient about the job market characteristics (professions, salary range, etc.). Instead of solving analytically the optimisation problem, we use an individual based procedure that allocates virtual individuals one by one in the different flows between geographic units, according to a probability that is updated after each allocation.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Plot of the average CPC (blue circle) and the average NMAE (red triangle) in term of  for 10 replications of the model for the Auvergne case study (FR1).The error bars represent the minimum value and maximum value obtain over the 10 replications.\ndoi:10.1371/journal.pone.0045985.g002This individual based approach can deal with less constrained data than the doubly-constrained gravity model that requires the total number of commuters in to be equal to the total number of commuters out. In other words the doubly contrained model can only deal with the flows between the considered geographic units; it cannot take into account the commuting links with destinations outside the case study area. This is a problem when only the numbers of commuters in and out the geographic units are available (and not the complete matrix of the commuting flows), because the data do not distinguish between the flows inside and outside the case study area. It is therefore difficult to estimate the correct data to take as input to the doubly-constrained model in this case. Our approach is more flexible and overcomes this difficulty. It does not require that the total number of commuters in and out to be equal (for more details see [13]), hence it can easily use directly the usually available data on the number of commuters in and out of each geographic unit.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Common part of commuters (CPC) for the 80 case-studies.The red squares represent the CPC obtained with the value of  optimised from data on the case-study network. Black plain triangles represent the average CPC obtained with  values estimated with the rule linking  and the average surface of the units obtain with the cross-validation; Dark bars represent the minimum and the maximum CPC obtained with the estimated  but in most cases they are too close to the average to be seen. The green circles represent the CPC obtained with the random model. The blue triangles represent the CPC obtained with the radiation model. The purple crosses represent the CPC obtained with the modified version of the radiation model.\ndoi:10.1371/journal.pone.0045985.g003Let  and  be respectively the global number of commuters starting from unit  and the global number of commuters arriving in unit . These numbers are initialised from data and then they are progressively modified by the procedure. More precisely, at each step we select unit  such that  at random, and we consider a virtual commuter starting from . We draw at random the working place  of this individual among all possible destinations  according to probabilities :(1)where  is the Euclidian distance in meter between units  and  (computable from the Lambert or GIS coordinates). Having drawn , we decrement of one  and . Note that decrementing  and  at each step complicates significantly the derivation of an analytical expression of the model. We chose a probability decreasing exponentially with the distance, in accordance with the investigations carried out in [13] and with the literature on commuting network models. The importance of the distance in the commuting choices is embedded in parameter : for  the probability tends to be independent from the distance, while for high values of , the probability tends to zero very rapidly when the distance increases, independently from the number of commuters arriving in the units.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Log-log scatter plot of the calibrated  values in terms of average surface of the geographic units for 80 case-studies; the line represents the regression line predicting .The surface is made non–dimensional by the unitary surface 1 .\ndoi:10.1371/journal.pone.0045985.g004To reduce the border effect (see [13]), we consider the job-search basin in an extended (EXT) area, composed by the  residential units and  units surrounding the area. Thus, we have  units which are commuting origins and  units that are commuting destinations. The generated network is saved in matrix  where each entry  represents the number of commuters between units  and . The algorithm is summarized in Algorithm 1.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Comparing the predictions of the radiation model with ours for two case studies, the first row ((a)–(c)) for  (USA at county scale) and the second row ((d)–(f)) for  (Auvergne region, France at municipality scale).Plots (a), (b), (d) and (e): Comparison between the observed (Census) and the simulated (model) non-zero flows. Grey points are the scatter plot for each pair of units. The boxplots (D1, Q1, Q2, Q3 and D9) represent the distribution of the number of simulated travelers in different bins of number of observed travelers. The blue circles represent the average number of simulated travelers in the different bins. Plots (c) and (f): Commuting distance distributions (km) (i.e. Probability for a commuters of the region to commut at a distance ). The blue line represents the observed data, the red one the results of our model and the green one the results of the radiation model.\ndoi:10.1371/journal.pone.0045985.g005Algorithm 1 Commuting generation model\nInput : , , , \nOutput : \nwhile  do\nPick at random , such that \nPick at random  from \nwith a probability \n\n        end while\n      \nreturn \nResults\n3.1 A Universal Law Ruling Parameter \nThe model depends on a single parameter ruling the importance of the distance in commuting choice. We show that this parameter can be derived as a function of the scale of the problem, independently from the socio-geographical location of the case study area. This opens the possibility to reconstruct the commuting flows (origin-destination matrix) when they are not provided.\nWe calibrated parameter  by maximising the common part of commuters (CPC), based on the Sørensen index [14].(2)with:(3)where  is the observed origin-destination matrix and  is the simulated one. This is a similarity measure based on the Sørensen index in ecology computing which part of the commuting flows is correctly reproduced, on average, by the simulated network. It varies between 0, when no agreement is found, and 1, when the two networks are identical. We priviledged this indicator because of its direct interpretation. Indeed, when  (it is the case for our model), the CPC represents the percentage of commuting connection correctly located (i.e. with the right pair origin - destination). Moreover, we tested on all case studies that the results obtained with the MAE, the RMSE or CPC are equivalent (see the Supporting Information   for more details). We have also shown in [12], [13] that the value of  yielding the maximum CPC also yields the maximum similarity between observed and simulated commuting distance distributions. As an example on the  case study, Figure 2 shows that the same  value maximizes the CPC and minimizes the MAE. In this Figure we can also note that the CPC is very sensitive to  and that its value does not vary much with the different replicas of the stochastic solving process.\nMoreover, in order to have an idea of the improvement of the model compared with complete randomness, we have computed the CPC of a random model where the probabilities presented in Equation (1) are uniform (, where  is the number of units). As shown on the Figure 3 we obtained an average CPC around 0.1. For our model, the CPC is always higher than 0.7 with an average around 0.8, which can be interpreted as 70 to 80% of correctly predicted commuting connections.\nOur goal is to derive the value of  from some easily available global characteristics of the case-study, giving the possibility to reconstruct the commuting flows when they are not available. Figure 4 gives strong evidence of such a universal relation.\nThe x-axis represents the average surface of the geographic units of the case-study ( in logarithm scale) and the y-axis the optimal  value (in logarithm scale). The linear regression in the log-log plane shows a simple relation:(4)with  and .  corresponds to the  value for the unitary surface 1 . The high value of the adjusted  confirms the quality of the linear model. We observe that  decreases with the average surface of the units , meaning that, when  is small (e.g. for municipalities in France) the distance is more important in the commuting choice than when  is large (e.g. for regions or counties).\nWe now evaluate the robustness of our estimation of  and  using a common statistical procedure: the cross-validation. The cross-validation aims at evaluating the potential error of using the  value derived from the regression model intsead of deriving this value by optimisation for a new case study. This procedure repeats a large number of times the following steps: define a sub-sample of the total sample of case studies, derive a regression model of  from this sub-sample, for each case study that do not belong to the sub-sample, derive  from this regression model and compare the corresponding CPC with the value of  directly calibrated on the complete origin - destination data. The dataset (including 80 case-studies) is randomly cut into two sets, called the training set (comprising 53 case-studies) and the test set (composed of 27 case-studies). We build a regression model on the training set, providing  and , from which we derive estimates of  for each of the 27 case-studies of the testing set. We have 27 estimations of  using the relation 4 where  and  are obtained from the random sub-sample of 53 case-studies. We repeat this process  times obtaining 270,000 estimations of  (uniformly distributed over the 80 case-studies) corresponding to about  estimations of  for each case study. Then we calculate the average, minimum and maximum CPC for each of these values of , and we compare them with the CPC obtained with value of  directly calibrated on the data.\nFigure 3 shows, for each case-study, the CPC associated with the calibrated , the average CPC obtained with the  values estimated from the cross-validation and the confidence interval defined by the minimum and the maximum values (but it is too small to be seen in most cases). The CPC obtained with the calibrated  value (black triangle) is almost the same as the average CPC obtained with the estimated  in most cases (red square). Globally, we can conclude that the  estimated with the log-linear model and the calibrated  lead to very similar CPCs and also very similar MAE and the RMSE as shown in the appendix  . The method appears therefore fairly robust and this gives confidence for using it with the value of  derived from our loglog regression in new cases studies.\n\n\n3.2 Comparaison with Other Universal Derivations of Commuting Networks\nTwo other different approaches, [1] and [2], claim also to provide a universal derivation of commuting networks. The objective of [1] is to generate a worldwide commuting network, and the model must deal with the wide variety of populations and surfaces of geographic units for which the data are available. To solve this difficulty, the authors project these data on ad-hoc units defined with a Voronoi diagram. They define their basic unit as a cell approximately equivalent to a rectangle of 25×25 kilometers along the Equator. This allows them to calibrate their model because a unit is the same object whatever the country. This is an interesting solution for generating a world-wide commuting network but it leads to an average commuting distance of 250 km which is much larger than the average distance of daily commuting. For example for the USA case study the average distance of daily commuting is about 68 km for the observed network and about 64 km for the simulated network obtained with our algorithm. For the Auvergne (France) case study at municipality scale the average distance of daily commuting is about 12 km for the observed network and about 11 km for the simulated one.\nIn the radiation model, proposed in [2], the commuting flow between two geographic units is a function of the cumulated population in a circle at the distance between the two units. The model has an elegant analytical solution and the average flow  from unit  to unit  can be approximated by(5)where  and  are respectively the population of units  and ,  is the total number of commuters and  is the total population in the case-study region, and  the total population in the circle of radius  centred at  (excluding the source and destination population).\nWe implemented their analytical approximation and reproduced the graphs presented in their paper. Figure 5 shows the comparison between the radiation model and ours in the US for inter-county commuting and in the French Auvergne region for inter-municipality commuting (see the Supporting Information for more examples). We observe that in both cases our approach yields significantly better results. Moreover, as shown on Figure 3, the average CPC for the radiation model on all the case studies is around 0.4, and lower for all case studies than the one obtained with our approach.\nHowever, it should be reminded that our model uses more specific data (total number of commuters in and out of each geographic unit) than the radiation model, hence one could expect our results to be more accurate. Therefore, to be fair with the radiation model we implemented a modified version of this model using the number of out and in commuters of each units. This new approximation is presented in Equation 6 where  the total number of in-commuters in the circle of radius  centred at  (excluding the source and destination).(6)\nAs shown on Figure 3, this new model reaches an average CPC around 0.5 which is higher than the original radiation model but still significantly lower than the results obtained with our model. Using the MAE and the RMSE leads to the same conclusions (see the appendix   for more details).\n\nThe power law of our model’s single parameter  with the average area of the case study geographic units, is surprising to us because of the high variety in our case studies in terms of scale, number of units, number of commuters and surface areas. For instance the Auvergne region in France is rural with a population density of about 50 hab./km whereas the New York City region is very urban with a population density of about 6500 hab./km. As far as we know, this is the first time that a single model is shown to fit such diverse group of datasets.We show that our approach outperforms the radiation model and that the difference of input data plays a minor role in this superiority. This superiority is not due to our particular treatment of the border effects either. Indeed, we could check our approach outperforms the radiation model also on particular case studies (e.g. on islands such as Corsica) where this border effect does not play. We can conclude that the accuracy of our model comes from a proper use of the number of commuters in and out of each geographic unit and an adequate choice of the function of the distance.The results of the cross validation procedure give a good confidence in the robustness of this law. However, we have to admit that, despite their diversity, our 80 case studies come all from western industrialised countries. Therefore it will be important to check the validity of our law on case studies coming from other continents and less industrialised countries. Moreover, we use a very rough approximation of the distance between the geographic units with the Euclidian distance between the unit centroids. More accurate approximations of this distance would certainly improve the results. Finally, we also intend to apply our approach to commuting networks inside urban areas because many cities of the world show an impressive growth and an increasing part of commuting takes place within them [15]. An important issue in our perspective is to check if our law holds at this scale."
        },
        "10.1371/journal.pbio.0040208": {
            "author_display": [
                "C. David L Orme",
                "Richard G Davies",
                "Valerie A Olson",
                "Gavin H Thomas",
                "Tzung-Su Ding",
                "Pamela C Rasmussen",
                "Robert S Ridgely",
                "Ali J Stattersfield",
                "Peter M Bennett",
                "Ian P. F Owens",
                "Tim M Blackburn",
                "Kevin J Gaston"
            ],
            "title_display": "Global Patterns of Geographic Range Size in Birds",
            "abstract": [
                "\n        Large-scale patterns of spatial variation in species geographic range size are central to many fundamental questions in macroecology and conservation biology. However, the global nature of these patterns has remained contentious, since previous studies have been geographically restricted and/or based on small taxonomic groups. Here, using a database on the breeding distributions of birds, we report the first (to our knowledge) global maps of variation in species range sizes for an entire taxonomic class. We show that range area does not follow a simple latitudinal pattern. Instead, the smallest range areas are attained on islands, in mountainous areas, and largely in the southern hemisphere. In contrast, bird species richness peaks around the equator, and towards higher latitudes. Despite these profoundly different latitudinal patterns, spatially explicit models reveal a weak tendency for areas with high species richness to house species with significantly smaller median range area. Taken together, these results show that for birds many spatial patterns in range size described in geographically restricted analyses do not reflect global rules. It remains to be discovered whether global patterns in geographic range size are best interpreted in terms of geographical variation in species assemblage packing, or in the rates of speciation, extinction, and dispersal that ultimately underlie biodiversity.\n      \n        The first global maps of variation in species range sizes for an entire taxonomic class--birds--reveals that range area does not follow a simple latitudinal pattern.\n      "
            ],
            "publication_date": "2006-06-20T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS Biology",
            "citations": 38,
            "views": 14864,
            "shares": 1,
            "bookmarks": 148,
            "url": "http://biology.plosjournals.org/perlserv/?request=get-document&doi=10.1371%2Fjournal.pbio.0040208",
            "pdf": "http://fetchObject.action?uri=http://biology.plosjournals.org/perlserv/?request=get-document&doi=10.1371%2Fjournal.pbio.0040208&representation=PDF",
            "fulltext": "IntroductionLarge-scale patterns of spatial variation in species geographic range size are central to many fundamental questions in macroecology and conservation biology. These include such issues as the origin and maintenance of diversity, the potential impacts of environmental change, and the prioritisation of areas for conservation [1–6]. However, the form taken by these patterns in geographic range size has remained surprisingly contentious.    \nInterest has focused foremost on the relationship between geographic range size and latitude, whose existence appears first to have been suggested by Lutz [7]. Rapoport [1] drew further attention to a tendency for range sizes to decline from high to low latitudes, and this was subsequently formalised as “Rapoport's rule” [2]. The generality of the rule has been much debated. A number of studies have argued that there is empirical evidence for the pattern [2,\n\t\t\t\t8–15], others that empirical evidence is lacking or is very weak [16–25]. Whilst some have argued that the pattern lacks sufficient generality to be termed a “rule” [5,\n\t\t\t\t17,\n\t\t\t\t20,\n\t\t\t\t24–26], others have treated the pattern as if it were a general one or have regarded the issue as unresolved [3,\n\t\t\t\t11,\n\t\t\t\t21,\n\t\t\t\t27–31].\n\t\t\t\nLatitudinal gradients in geographic range size have received such attention largely because of their possible implications for the mechanisms underlying spatial variation in species richness, and particularly the tendency for richness to be much greater in the tropics. Indeed, Stevens [2] argued that there may be a connection between the two. Temperate species may have larger geographic range sizes than tropical species, due to their tolerance of a broader range of environmental conditions, necessitated by greater variance in such conditions at individual sites. This could then promote higher levels of species coexistence in the tropics through a “mass effect” [32], whereby some of the occurrence of species is maintained by immigration of individuals into communities outside their restricted microhabitats.    \nMost tests for a relationship between spatial variation in species richness and geographic range size have been indirect, and based on the existence or otherwise of a latitudinal gradient in the latter. Yet, species numbers do not just vary with distance from the equator [33], as the recently published first map of global avian species richness clearly demonstrates [34]. Hotspots of bird species richness coincide with tropical mountain ranges, and richness declines away from these areas in all compass directions. If species richness and geographic range size are causally linked, then we would expect spatial variation in geographic range size to be equally richly textured. Yet, direct tests of such a relationship remain scarce, with some studies supporting a link [15,\n\t\t\t\t35,\n\t\t\t\t36] and others refuting it [6,\n\t\t\t\t14,\n\t\t\t\t18,\n\t\t\t\t37].\n\t\t\t\nSignificantly, the majority of analyses to date of relationships both between geographic range size and latitude, and between range size and species richness, have been conducted within individual biogeographic realms. It has repeatedly been observed that the outcomes may depend on which realm or smaller biogeographic unit is being considered [17,\n\t\t\t\t20,\n\t\t\t\t22,\n\t\t\t\t23,\n\t\t\t\t25,\n\t\t\t\t38]. In particular, studies that find evidence for the existence of Rapoport's rule are largely restricted to the Nearctic [5,\n\t\t\t\t20], leading to the suggestion that it may be a local phenomenon that does not generalise [17]. Moreover, limiting analyses to individual biogeographic realms almost invariably means that species whose geographic ranges extend beyond those realms are ignored, or their ranges truncated to the limits of the realms [5]. The consequences for results are unknown, but may be marked where a sizeable proportion of species are distributed across multiple biogeographic realms.    \nWhat has been almost entirely missing from discussion of spatial variation in geographic range sizes has been a global perspective for major taxa. All previous studies suffer from concerns about the generality of patterns from limited numbers of biogeographic realms, and hence fail to resolve uncertainty about the true nature of spatial variation in geographic range sizes. Here, we present the first global-scale analysis of spatial variation in the geographic range sizes of species for a major taxon, all extant species of birds. We use a global database of avian distributions, mapped on an equal area projection at a scale similar to a 1° grid, to derive estimates of range sizes as the geographic breeding range area of bird species (excluding primarily marine species; see      Materials and Methods). Global patterns of bird species richness were previously determined using this database [34], which allows us to explore in detail both spatial variation in geographic range size and its link to species richness, using data of unparalleled geographic extent and resolution.    \nResults\nSpecies-Range Area Distributions\n\n\t\t\t\t\tFigure 1 summarizes the global species-range area distribution for birds. The distribution is strongly right-skewed (Figure 1A), with the mean range area (2.82 × 106 km2) of the 9,505 extant species markedly larger than the median (0.87 × 106 km2). More than a quarter (27.6%) of bird species have geographic range areas smaller than 225,000 km2, less than the area of Great Britain or Minnesota. The species-range area distribution is formally neither lognormal (Figure 1B; D = 0.0656, \n\t\t\t\t\tp ≪ 0.001) nor logit-normal (D = 0.0636,       p ≪ 0.001), with strong left-skew in both cases. Normal probability plots reveal that the geographic range areas of the more restricted and the more widespread species are small relative to expectation (Figure 1C).\n\t\t\t\t\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Species-Range Area Distribution for the Global Avifauna(A) Untransformed range areas. (B) Log10-transformed range areas. (C) Normal probability plot for log10-transformed range areas, showing the expectation under a normal distribution (dashed line) and the observed distribution (open circles).       \ndoi:10.1371/journal.pbio.0040208.g001\n\nSpatial Variation in Range Area\nKey spatial patterns in species geographic range areas are shown in       Figure 2. The median geographic range area of the species coexisting in each grid cell shows strong spatial patterns (Figure 2A). The smallest median range areas occur on islands, in low-latitude mountainous areas, and to a large extent in the southern hemisphere (Figure 2A), as are the smallest range areas overall (the range area of the most narrowly distributed species in each grid cell;       Figure 2B). The variance in range area is typically higher in the northern hemisphere, especially in mid-latitudes (Figure 2C). Some previous analyses of spatial variation in geographic ranges have focused on the latitudinal extent of the distributions of species. The median latitudinal extent of the bird species coexisting in each grid cell shows a rather different, but equally complex, picture to that for range area (Figure 2D). The main differences lie in the higher Afrotropical and lower Palearctic latitudinal extents compared to the patterns of range area, however there are similarities in the distribution of the smallest median extents, notably in mountainous regions and some island groups. The spatial distribution of the smallest latitudinal extents (the range extent of the most narrowly distributed species in each grid cell;       Figure 2E) is very similar to that of the smallest range areas (Figure 2B).\n\t\t\t\t\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Geographic Distribution of Geographic Range Areas, Latitudinal Range Extent, and Species Richness for the Global Avifauna(A) Median geographic range area (km2). (B) Minimum geographic range area (km2). (C) Variance in geographic range area (km2). (D) Median latitudinal range extent (degrees). (E) Minimum latitudinal range extent (degrees). (F) Total species richness. The map scales are based on quartiles of the underlying distributions; the scale bars show the quartile values for each map. Parallels are shown at 45° S, the Equator, and 45° N.       \ndoi:10.1371/journal.pbio.0040208.g002The latitudinal trends in species geographic range sizes are summarized in       Figure 3. Contrary to the predictions of Rapoport's Rule, geographic range area does not decline towards the tropics in both hemispheres (Figure 2A, \n\t\t\t\t\tTable 1). Rather, median range area is greatest at high, albeit not the highest, northern latitudes and decreases toward high southern latitudes. This relationship shows a subtle change in the magnitude of the slope at the equator, but does not show the change in sign that Rapoport's rule would predict (Figure 3A; scoring southern latitudes as negative, range size versus latitude across all cells:       r = 0.60, \n\t\t\t\t\tn = 17,867, \n\t\t\t\t\tp ≪ 0.001; for cell averages per latitudinal band:       r = 0.89, \n\t\t\t\t\tn = 151, \n\t\t\t\t\tp ≪ 0.001). There is no relationship between median range area and absolute latitude (i.e. regardless of hemisphere), once the sample size dominance of the northern hemisphere has been removed by averaging within latitudinal bands (across all cells:       r = 0.45, \n\t\t\t\t\tn = 17,867, \n\t\t\t\t\tp ≪ 0.001; cell averages per latitudinal band:       r = 0.07, \n\t\t\t\t\tn = 151, \n\t\t\t\t\tp = 0.40).\n\t\t\t\t\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Global Relationships between Geographic Range Area, Latitudinal Range Extent, Species Richness, Land Area, Island Area, and Latitude(A) Median geographic range area and latitude. (B) Species richness and latitude. (C) Median range area and latitude for species with midpoints falling in each respective latitudinal band. (D) Total land area (km2) within latitudinal bands. (E) Median latitudinal range extent and latitude. (F) Median geographic range area and species richness. (G) Median latitudinal range extent and species richness. (H) Proportion island area and latitude. For (A–C) and (E and F), open circles represent latitudinal means, and grey points show the spread of individual grid cell values. Southern latitudes are indicated as negative, northern ones as positive.       \ndoi:10.1371/journal.pbio.0040208.g003\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1. Global and Within-Realm Patterns in Range Size\ndoi:10.1371/journal.pbio.0040208.t001Within individual biogeographic realms and hemispheres, median range areas increase with latitude in only seven out of 13 cases, and six out of these seven are in the northern hemisphere (Table 1; see \n\t\t\t\t\tProtocol S2 for tests using all cells). In contrast, species richness shows strong latitudinal correlations with highest richness in tropical regions, and notable peaks in the Andes, Himalayas and African Rift Valley (Figure 2F; richness [square root transformed] versus latitude across all cells: southern hemisphere       r = −0.56, \n\t\t\t\t\tn = 4,903, \n\t\t\t\t\tp ≪ 0.001, northern hemisphere \n\t\t\t\t\tr = −0.39, \n\t\t\t\t\tn = 12,964, \n\t\t\t\t\tp ≪ 0.001; richness versus latitude for cell averages per latitudinal band: southern hemisphere       r = −0.98, \n\t\t\t\t\tn = 75, \n\t\t\t\t\tp ≪ 0.001, northern hemisphere \n\t\t\t\t\tr = −0.77, \n\t\t\t\t\tn = 76, \n\t\t\t\t\tp ≪ 0.001). In addition, the southern hemisphere shows both higher diversity at mid-low latitudes and a steeper decline with increasing latitude than the northern hemisphere (Figure 3B). These analyses are not sensitive either to the omission of island cells or to more conservative tests using differences in latitudinal values that account for latitudinal autocorrelation (see       Protocol S2).\n\t\t\t\t\nThe relationship between geographic range area and latitude is not an artefact of the way that geographic range size variation was quantified. Because analyses based on the range size characteristics of species in cells can be biased by shared species composition, we also checked the relationship using the “midpoint method” [16]. In this method, species are allocated to latitudinal bins based on the location of the latitudinal mid-points of their geographic ranges, so each species only contributes to one data point, and the median range area of the species in each bin is plotted against the latitude of that bin (Figure 3C). Globally, these data show an increase in median geographic range area from southern to northern latitudes (      r = 0.58, \n\t\t\t\t\tn = 126, \n\t\t\t\t\tp ≪ 0.001) and the decline in range areas at high northern latitudes is more marked using the midpoint method. Within hemispheres, there is no global trend in southern latitudes and a strong increase in northern latitudes; within hemispheres and realms, median geographic range areas increase significantly with latitude in only three out of 13 cases (see       Protocol S2). Although phylogenetic autocorrelation could influence these findings, evidence suggests that species range area shows low phylogenetic dependence (see       Protocol S2; [5,\n\t\t\t\t\t39]) and thus such autocorrelation will have a weak effect.     \nThe latitudinal gradient in land area across all latitudes (Figure 3D) is strongly correlated with median range area (Figure 3A, \n\t\t\t\t\tr = 0.72, \n\t\t\t\t\tn = 152, \n\t\t\t\t\tp ≪ 0.0001), particularly if the sharp decline in both variables above 67° N is omitted (      r = 0.95, \n\t\t\t\t\tn = 146, \n\t\t\t\t\tp ≪ 0.0001). One possible explanation of the patterns in range area is thus that the latitudinal extents of ranges (Figure 2D) increase from low to high latitudes but that the longitudinal constraints on range area mask this relationship. However, the latitudinal gradient in species latitudinal range extents (Figure 3E) shows weaker support for Rapoport's rule than does that in range area. Globally, latitudinal extent decreases, rather than increases, from low to high latitudes in both hemispheres (Table 1). Within individual biogeographic realms and hemispheres, latitudinal extent increases with latitude in only six out of 13 cases (Table 1). Indeed, in the northern hemisphere, increases in land area seem to mask decreases in latitudinal extent at higher latitudes.     \n\n\nRange Area and Species Richness\nSimple correlations between median geographic range area and species richness (Figure 3F) yield significant negative relationships globally and positive relationships in three out of eight biogeographic realms (Table 1). A more consistent picture is revealed once the similarity of proximal cells arising from spatial autocorrelation is explicitly modelled. This method shows that at a global scale median range area and species richness are negatively related (Table 1). Also, when individual biogeographic realms are considered separately, there is a negative relationship between range area and species richness in seven of the eight realms, with the only positive relationship being statistically non-significant (Table 1).\n\t\t\t\t\n\nDiscussionThe global species-range area distribution is strongly right-skewed, with the majority of species having small, but not the smallest, geographic ranges (Figure 1). This is consistent with previous studies, both of birds and other taxa, within biogeographic realms [5,\n\t\t\t\t6,\n\t\t\t\t40–43]. The departure from a log-normal and a logit-normal distribution, the two null models that have previously been suggested, is also consistent with the findings of such studies [5,\n\t\t\t\t43]. Such departures are typically interpreted as resulting from a lack or an excess of rare species relative to expectation [6,\n\t\t\t\t44,\n\t\t\t\t45]; normal probability plots (Figure 1C) show that here both for the log-normal and a logit-normal the range areas of the more widespread species are also small relative to expectation. This is likely to be a consequence of dispersal limitation of species to a subset of the major land masses as even the most widespread species are not cosmopolitan.    \nOverall, there is no global tendency for avian geographic range sizes to decline in area, or in latitudinal extent, towards the tropics (Figures 2A, \n\t\t\t\t2D). Rather, there is a general trend of declining median range area from high northern latitudes to high southern ones (Figure 3A). This leads to entirely different relationships between range area and latitude in different biogeographic realms, with those in the northern hemisphere typically conforming to Rapoport's rule, and those in the southern hemisphere failing to do so (Table 1). This both confirms that Rapoport's rule does not generalise [17,\n\t\t\t\t20,\n\t\t\t\t46], and cautions against assuming that biological patterns from the relatively well-studied northern temperate regions will apply to the rest of the world [47]. Current evidence suggests that Rapoport's rule in other taxa is limited principally to the same regions as for birds [20]. Nevertheless, we have shown here that a global perspective is needed truly to understand spatial variation in geographic range sizes, as conclusions based on single/limited biogeographical data may not be informative of overall pattern. It will be interesting to test the wider generality of the relationships shown for birds, as data for other taxa become available.    \nDescription of variation in avian geographic range areas in terms of latitude alone masks substantial spatial patterning, especially at low latitudes and in southern regions (Figure 2A). Small range areas are associated, perhaps unsurprisingly, with islands, but also with mountain ranges in the tropics and sub-tropics. This suggests that range areas may be constrained by the availability of land area within the climatic zones to which species are best adapted [48]. Thus, in South America, for example, the relatively climatically uniform expanse of the tropical lowland Amazon basin allows broad geographic ranges in species evolving within this region, in contrast to the restricted ranges available to species evolving to exploit any of the restricted climatic zones that pertain at different altitudes in the tropical Andes. This idea is also consistent with the global decline in median geographic range areas from high northern to high southern latitudes being generated by the availability of land area in different climatic zones. The overall decline corresponds well with the latitudinal pattern of land area (Figure 3D). Additionally, regions with particularly small median range area correspond well with regions where the proportion of land area contributed by islands is high (Figure 3H), although this latter pattern is not sufficient of itself to generate the latitudinal gradient in range area (see      Protocol S2).\n\t\t\t\nThe evident contrast between the global decline in median range area from high northern to high southern latitudes and the equatorial peak in species richness argues against any simple relationship between geographic range area and species richness (Figure 3F; or between latitudinal range extent and species richness:      Figure 3G). However, whilst the complexities of the patterns of spatial variation in both variables mean that using non-spatial models may be problematic, there is nonetheless a significant (albeit weak) negative global relationship between the two (Table 1). This is echoed by weak negative relationships in five of the biogeographic realms separately, with a significant positive relationship between range size and species richness, contrary to Rapoport's rule, in the other three (Table 1). Nevertheless, relationships between richness and range area are likely to be heavily influenced by spatial autocorrelation, since neighbouring areas tend to contain largely the same numbers and average range sizes of species, and so do not contribute independent information to tests of association.    \nUsing spatial models, therefore, a rather different picture emerges. Globally, median range area and species richness remain significantly negatively related, as they do in four of the biogeographic realms (Table 1). However, within one region, Indomalaysia, a previously significant negative relationship becomes non-significant using a spatial model, and within two regions, the Neotropics and Australasia, significant positive correlations become significant negative relationships after controlling for spatial autocorrelation. In these last two regions, numerous grid cells falling within the Amazon basin and central Australian deserts dominate simple models. However, the similarity in species composition between these cells, as a consequence of the large range areas of the species occupying them, means that spatial autocorrelation is high across these areas. Their influence is dampened in spatial models in favour of less well-represented and less pseudo-replicated areas of high topographic variability and species richness but low geographic range area in the mountains of the Andes and Great Dividing Range. Thus, the fact that the richest areas house species with the smallest mean range areas was obscured by the numerical dominance of pseudo-replicated areas of lower richness housing species with broad geographic distributions. This emphasizes the potential importance of a spatially-explicit perspective in understanding large-scale biodiversity patterns, an approach that has almost been entirely lacking in previous considerations of spatial variations in geographic range size. It also suggests that whilst at a global scale for birds there is little support for Stevens' [2] notion of a general latitudinal gradient in range size, there is nonetheless some support for the link he suggested between geographic range size and species richness.    \nWe end with two notes of caution regarding the ecological and evolutionary mechanisms that may underlie the relationship between geographic range size and species richness. First, although we have detected statistically significant associations between range area and species richness, the strength of these relationships is typically not strong. For those analyses based on non-spatial models the maximum proportion of variance explained was only 18% (Table 1), and for spatial models of the form employed here explained variances cannot be derived (the shallow slopes reflect the relative magnitudes of the two variables). It would thus be wrong to conclude that smaller range areas are strongly associated with higher levels of species richness, although there is robust evidence that the two variables are not entirely independent, both globally and within some biogeographic realms. Second, so far we have largely discussed the link between range area and species in the context of Stevens' [2] mass effect mechanism, but it is equally plausible that other mechanisms could underlie the same pattern. It has been suggested, for instance, that spatial patterns in geographic range size may be due to geographical variation in the processes of speciation and extinction that ultimately generate biodiversity [5,\n\t\t\t\t49]. Although recent studies have confirmed that there is geographic variation in the net rate of cladogenesis [5,\n\t\t\t\t50,\n\t\t\t\t51], the relative role of such phenomena in determining large-scale patterns of diversity remains to be discovered.    \nThe analyses presented here are based on a previously reported database [34] of distribution maps for 9,505 extant, recognized bird species following a standard avian taxonomy [52]. This excludes primarily marine species, the vast majority of the geographic ranges of which are oceanic and thus of quite different character, and for which comparable data are not available. Primarily marine species were defined as those regularly foraging more than 50 km from land during the breeding season (and thus for which land did not constitute a high proportion of their ranges), were identified using a variety of sources [53–58], and were excluded from the analyses presented here. To obtain information on the geographical breeding range data on a global scale we used a wide range of data sources [34], which are described in detail in       Protocol S1 along with the associated notes on methods and availability. Briefly, breeding ranges from the published sources were mapped as vectors or “polygons” and converted to an equal area grid for analysis. The grid used a Behrmann projection and a cell size (96.3 km) that gives a scale identical to 1° grids at the 30° latitude of true scale. The vertical cell boundaries coincide with 1° lines of longitude but the horizontal boundaries vary systematically in their latitudinal separation, giving a grid with 360 columns and 152 rows. We have used these 152 equal area longitudinal bands for calculating latitudinal averages and for binning species by their latitudinal range midpoint. Species were scored as present in a grid cell if any of the available vector sources suggested that the breeding range fell within the cell boundaries. Overall species richness was derived by summing all species present within each cell. Cell land areas were calculated using a coastline vector dataset [59] divided into continental and island land masses. The geographic range areas of individual species were estimated as the sum of the areas of the cells in which they were scored as occurring. This will tend disproportionately to overestimate the range areas of particularly narrowly distributed species, and those whose distributions are associated with linear features (e.g. mountain chains, rivers), but this is unlikely to influence the broad patterns reported here. Latitudinal extent was defined as the difference between the northern and southern limits of the vector maps showing each species' breeding range. Biogeographic realms were delimited using the World Wildlife Fund ecoregions map [60].\n\t\t\t\tTo calculate the logit transformation of geographic range area we used the global breeding area of all species as the upper limit to the occurrence of any individual species [43]. The shapes of logarithmically- or logit-transformed range area distributions were assessed using Kolmogorov-Smirnov tests of the transformed data against normal distributions with mean and standard deviation taken from the transformed data. Species richness was untransformed or square root transformed for analyses, as appropriate, better to normalise distributions for particular tests. Relationships between median geographic range area/latitudinal extent or species richness and latitude were determined using correlation coefficients. Relationships between median geographic range area/latitudinal extent and species richness were determined using correlation coefficients, and using normal errors generalised least squares (GLS) models (SAS; 62), fitting spherical spatial covariance structures with longitudinal and latitudinal cell centroid values as spatial variables, in SAS version 9.1.3. GLS models took account of the differences among major biogeographical realms, in the maximum geographic distance or range parameter (ρ), measured in degrees, over which spatial autocorrelation in equivalent independent errors model residuals was observed to occur. This involved estimating ρ from the semi-variogram of residuals of non-spatial normal errors models that included the relevant combination of predictors, separately for each realm. All eight estimates of ρ were then entered as spatial covariance parameters in the model, with spatial autocorrelation assumed for observations within the same realm. Global models were run on a regular 50% grid of cells (chequerboard) in the full data set due to computer memory constraints (even when run on a mainframe); tests on regional subsets of the data confirmed that use of the reduced data set would not alter the conclusions.     "
        },
        "10.1371/journal.pbio.0030270": {
            "author_display": [
                "Mark Stoneking",
                "Brigitte Pakendorf",
                "Hiroki Oota"
            ],
            "title_display": "Authors' Reply",
            "abstract": [
                ""
            ],
            "publication_date": "2005-08-16T00:00:00Z",
            "article_type": "Correspondence and Other Communications",
            "journal": "PLoS Biology",
            "citations": 0,
            "views": 1871,
            "shares": 0,
            "bookmarks": 0,
            "url": "http://biology.plosjournals.org/perlserv/?request=get-document&doi=10.1371%2Fjournal.pbio.0030270",
            "pdf": "http://fetchObject.action?uri=http://biology.plosjournals.org/perlserv/?request=get-document&doi=10.1371%2Fjournal.pbio.0030270&representation=PDF",
            "fulltext": "Waters [1] makes a number of points concerning our article [2], which, in our view, require clarification. First, Waters states that classifying Southeast Asian highland groups as either strictly horticultural or strictly foraging is overly simplistic, as most groups practice horticulture supplemented with some degree of foraging. While we are sympathetic with the view that subsistence strategy is more complicated than a simple dichotomy (indeed, one of the main messages of our paper is that a strictly foraging group such as the Mlabri may have practiced horticulture in the past), we wish to emphasize that the Mlabri are, indeed, quite different from the other Southeast Asian highland groups in that they have never, in either their recorded or oral history, practiced horticulture. It is this distinction, coupled with their extreme paucity of genetic diversity, that sets them apart from other groups in the area.\nSecond, Waters suggests that our comparison of the Mlabri with hill tribes from a different geographic region (Chiang Rai and Mae Hong Son provinces of Thailand) leads to our conclusion that “the Mlabri were isolated from these groups,” and that had we examined neighboring groups of the Mlabri, we might have reached a different conclusion. These statements misrepresent our work; in particular, we found that the Mlabri were not genetically distinct from other hill tribes for which we had data, as the mtDNA sequence, Y-STR alleles, and autosomal STR alleles of the Mlabri are all found in other groups. Moreover, this sharing pattern is in stark contrast to African foraging groups, such as the !Kung and Pygmies, who are genetically distinct from their horticultural neighbors. It is precisely this sharing of alleles between the Mlabri and other groups that is the basis for our suggestion that the Mlabri may have reverted to their current exclusively foraging lifestyle from a previous horticultural lifestyle, rather than having always been foragers.\nFinally, Waters states that we claimed that our data “solidly” support the scenario of an extreme founder event from a horticultural group, followed by reversion to a foraging lifestyle, for the origin of the Mlabri. This is not true; we were careful to state that our data only suggest such a scenario. We agree with Waters that genetic analysis of neighboring groups of the Mlabri (in particular, the Tin Prai) would be useful to further evaluate the scenario we proposed for the origin of the Mlabri. And we clearly agree with Water's concluding statement concerning the importance of interactions between horticultural and foraging groups, as we make exactly that point in the penultimate sentence of our paper.\n"
        },
        "10.1371/journal.pone.0027027": {
            "author_display": [
                "Wolf L. Eiserhardt",
                "Stine Bjorholm",
                "Jens-Christian Svenning",
                "Thiago F. Rangel",
                "Henrik Balslev"
            ],
            "title_display": "Testing the Water–Energy Theory on American Palms (Arecaceae) Using Geographically Weighted Regression",
            "abstract": [
                "\n        Water and energy have emerged as the best contemporary environmental correlates of broad-scale species richness patterns. A corollary hypothesis of water–energy dynamics theory is that the influence of water decreases and the influence of energy increases with absolute latitude. We report the first use of geographically weighted regression for testing this hypothesis on a continuous species richness gradient that is entirely located within the tropics and subtropics. The dataset was divided into northern and southern hemispheric portions to test whether predictor shifts are more pronounced in the less oceanic northern hemisphere. American palms (Arecaceae, n = 547 spp.), whose species richness and distributions are known to respond strongly to water and energy, were used as a model group. The ability of water and energy to explain palm species richness was quantified locally at different spatial scales and regressed on latitude. Clear latitudinal trends in agreement with water–energy dynamics theory were found, but the results did not differ qualitatively between hemispheres. Strong inherent spatial autocorrelation in local modeling results and collinearity of water and energy variables were identified as important methodological challenges. We overcame these problems by using simultaneous autoregressive models and variation partitioning. Our results show that the ability of water and energy to explain species richness changes not only across large climatic gradients spanning tropical to temperate or arctic zones but also within megathermal climates, at least for strictly tropical taxa such as palms. This finding suggests that the predictor shifts are related to gradual latitudinal changes in ambient energy (related to solar flux input) rather than to abrupt transitions at specific latitudes, such as the occurrence of frost.\n      "
            ],
            "publication_date": "2011-11-03T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 7,
            "views": 2525,
            "shares": 0,
            "bookmarks": 45,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0027027",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0027027&representation=PDF",
            "fulltext": "IntroductionAmong the contemporary environmental factors that are correlated with species richness on broad scales, water and energy have emerged as key influences [1]–[4]. However, these two factors do not appear to be equally important worldwide. Based on a review of 85 studies of broad-scale richness gradients, Hawkins et al. [2] hypothesized that ‘the relative importance of the two components of water–energy dynamics shifts latitudinally’ (p. 3111). This conjecture has not received much attention (but see [5]), although appropriate tools exist for exploring spatial non-stationarity in environment–richness relationships [6]–[10].\nThe question of how patterns of species richness are controlled has been debated for decades [11], [12] and remains a central issue in macroecological and biogeographical research [13]–[15]. Various explanations have been proposed that emphasize the importance of area [16], geometric constraints [13], history [17], synergism between climate and history [18], and, most commonly, contemporary environment [2], [3], [19]. Many studies have focused on the role of contemporary climate as the main predictor of species richness, concluding in favor of a central role for water- and energy-related variables [2], [4], [19], [20]. Different mechanisms have been proposed to explain the suggested primacy of water and energy, including trophic, physiological, and metabolic effects [2], [21], [22]. Fundamentally, a dynamic relationship between energy and water may result from life's dependence on both liquid water and ambient energy [2], [4], [20].\nOf interest, richness gradients at low latitudes appear to correlate most strongly with water availability, while energy (for animals) or water–energy variables (for plants) are the best correlates of most richness gradients at high latitudes [2]. This pattern finds a convincing a posteriori explanation in the latitudinal gradient of solar flux. Accordingly, energy is expected to be the most limiting factor at high latitudes where energy levels are low, while water gradients should be more important at low latitudes where ambient energy is high and thus not limiting [2]. For plants, the mechanism causing this predictor shift is thought to be related to physiological processes, while for animals it is more difficult to determine whether the shift results from direct physiological effects or from plant productivity [2], [4], [23]. Worth noting, the pattern appears to be asymmetric, with predictor shifts being largely restricted to the northern hemisphere, whereas water is more important than energy in most southern hemisphere regions [2]. A potential explanation may lie in the more oceanic climates of the southern hemisphere [2]. Increasing evidence also suggests that climate–richness relationships depend on evolutionary processes (e.g., [24]), which may introduce spatial non-stationarity. In particular, phylogenetic niche conservatism can cause groups to be most species rich in their ancestral climates [25]. To the extent that different groups originate from different climates, this mechanism may lead to climate–richness relationships that vary among groups and regions [24].\nSince the meta-analysis by Hawkins et al. [2], only one study has formally tested the hypothesized predictor shifts and found support for it for plants and animals in Europe [5]. The questions of whether predictor shifts are abrupt or gradual, at which latitudes they occur, whether they can be related to specific climatic transitions (such as the subtropical-temperate boundary), and to what degree those parameters are taxon-specific await further investigation.\nGeographical shifts in the explanatory power of environmental predictors are broadly relevant to macroecological research because they pose a challenge to ‘global’ models of biodiversity. Regression techniques that are typically used to analyze environment–richness relationships, such as ordinary least squares regression [1], generalized additive models (e.g. [5]), or spatial autoregressive models [26], assume that the relationship is described by one set of parameters that applies equally throughout the study area. It has been argued that such models are misleading if the analyzed relationship is indeed variable in space (spatial non-stationarity) [6] (but see [27]). If the purpose is simply to identify correlates of richness, the problem can be referred to the scale-dependency of environment–richness relationships [28], with different correlations on global and smaller scales correctly describing a given pattern. If, however, the purpose is to understand the actual drivers of richness, the ‘average’ parameters obtained from a global model [6], [29] might not be informative if the driving dynamics occur at a scale smaller than the model. It is therefore relevant to explore both scale-dependency and spatial non-stationarity of such relationships [6], [7]. Specifically, spatial non-stationarity is a promising source of information because the relationship as such can be related to (second-order) predictor variables.\nGeographically weighted regression (GWR) is a geographically local modeling technique specifically designed to deal with spatial non-stationarity in the modeled relationships [8]. GWR performs one weighted ordinary least squares regression per observation in the analyzed dataset. Weights are applied as a (typically inverse) function of the distance from the location of the ‘focal’ data point. Of importance, modeling is carried out at a scale smaller than the study extent, defined by the distance decay (‘bandwidth’) of the weighting function. Thus, GWR should not be used as an alternative to global regression models but as a complementary technique for quantifying spatial variability (non-stationarity) in relationships between the predictor and response variables [27]. By allowing regression model parameters to vary in space and then mapping these coefficients, GWR makes it possible to quantify and test the spatial variability in the species–environmental relationships. GWR is increasingly used for analyzing species richness patterns [6], [10], [22], [29], [30], [31], and some of these studies [22], [29], [30] have produced results that are in agreement with the predictor shifts conjectured by Hawkins et al. [2]. However, no study has to date applied GWR in a formal test of this hypothesis.\nUsing GWR, we tested for predictor shifts not by comparing disparate studies from different regions, but by quantifying geographic variation in the ability of water and energy to explain the species richness of a single group of organisms (palms) across a continuous region, the American tropics and subtropics. Palms are a diverse, pan-tropical family of ca. 2,400 species worldwide [32]. They are important constituents of many vegetation types in tropical, subtropical, and, more rarely, warm-temperate parts of the New World [33]. Several previous studies have investigated the controls of the large-scale diversity patterns in American palms and have found water-related variables to be of primary importance among contemporary environmental factors [34], [35]. Thus, as a low-latitude group when regarded on a global scale, the palms conform well to the conjecture of Hawkins et al. [2]. However, here we focus on assessing whether the relative importance of water and energy also changes with latitude within the range of palms. Moving away from the tropics, the American palms are likely to become more controlled by available energy than many other plant groups because of key aspects of palm architecture and anatomy that have been previously described [32], [36], [37]. Thus, they are good candidates for displaying latitudinal predictor shifts sensu Hawkins et al. [2] at relatively low latitudes. Here, we used GWR to formally test the following two predictions for a species-rich organism group within the tropics/subtropics: (1) Temperature is a stronger correlate of palm species richness at high latitudes than at low latitudes, while water shows the opposite trend, as hypothesized by Hawkins et al. [2]; (2) this latitudinal shift is strongest in the northern hemisphere, reflecting the more oceanic southern hemispheric climates.\nMethods\nStudy species and area\nWe used distribution data for the complete palm family (Arecaceae) across the Americas (n = 547 spp.) extracted using ArcView 9.2 (ESRI Inc., Redlands, California, USA) from the range maps in the Field Guide to the Palms of the Americas [38]. The number of palm species present was registered for all cells of a continuous grid covering the whole range of palms in the Americas ranging from 34° North to 33° South. Based on the quality of the maps and our knowledge of the distribution of the palm family, we decided to work at a resolution of 1°×1° grid cells. Cells with less than 25% land surface were excluded from the analysis because their species richness might be more strongly determined by area than by climate. Moreover, we excluded 59 also mostly coastal grid cells for which climate variables (see below) were not available. These criteria resulted in 1510 grid cells across the Americas (Figure S1).\n\n\nEnvironmental variables\nAs predictors, we used the environmental variables of mean annual precipitation (AP), minimum precipitation of the driest month (MPDM), mean annual temperature (MAT), and minimum temperature of the coldest month (MTCM) from the WorldClim global climate database [39] at a resolution of 30 arc seconds (http://www.worldclim.org/current). Moreover, we used potential evapotranspiration (PET) and actual evapotranspiration (AET) from the 30 arc minutes resolution UNEP GNV183 data set (www.grid.unep.ch/GRID_search_details.php​?dataid=GNV183/) [40]. To match the resolution of the palm grid cells, the average of each variable was taken for the terrestrial part of each 1°×1° grid cell using ArcInfo 10 (ESRI Inc., Redlands, California, USA). AET was only used to calculate water deficit (WD  =  PET – AET), representing drought [10]. We did not use AET directly because it represents both water and energy [41], running counter to the study purpose of separating these two aspects of climate. Thus, we worked with two sets of variables, one set denoting water (AP, MPDM, and WD), and one denoting energy (MAT, MTCM, and PET). Non-climatic variables were not included because the explicit aim was to infer the roles of water and energy, not to explain as much variation in species richness as possible (cf. [5]).\n\n\nStatistical analyses\nThe software SAM [42] was used to fit GWR models using palm species richness as the response variable and different combinations of climatic variables as predictors. In a first step, we performed information-theoretic model selection using the corrected Akaike information criterion (AICC; [43]) to determine which combination of predictor variables had the highest explanatory power within each set of climatic variables (water and energy, respectively). In each set, AICC was calculated for all possible combinations of one to three predictor variables.\nWe then computed local R2 values for the best water model, the best energy model, and a model including the predictor variables of both the best water model and the best energy model (‘combined model’ in the following). These values represent the fraction of local variation in palm richness explained by water (Rw) and energy (Re). For each grid cell, variation partitioning [44], [45] was performed to determine the amount of variation that is uniquely explained by water and energy. Those fractions were calculated as Rpe  =  Rt – Rw for pure energy and Rpw  =  Rt – Re for pure water, where Rt is the local R2 value of the combined model.\nThe results of GWR depend on the choice of the spatial kernel function that determines how observations are weighted as a function of spatial distance from the focal cell [6], [8], [46], [47]. To ensure that our conclusions did not depend on a specific choice of this function, we repeated all GWR analyses with four different kernels. First, we used the bi-square function, which applies a continuous, near-Gaussian weighting function up to a distance b (the ‘bandwidth’) from the regression point and then zero weights to any observation beyond b. Two values of b were used, 1200 km and 1800 km. Second, we used the moving window approach, which assigns equal weights to observations within the bandwidth and zero to observations beyond [8], with the same two b values. The bandwidths were chosen based on the resolution of the palm diversity data (1° × 1°, i.e., 111 km × 110 km at the equator) and the total extent of the study area (approx. 10,000 km between the most northerly and most southern data points) to ensure a reasonable local sample size and a scale that was clearly local relative to the whole study area. Because we were specifically interested in responses from the marginal areas (alone and not lumped together to obtain a certain number of grid cells), we did not use adaptive spatial kernels, which adapt the bandwidth according to the variation in observations so that it is large in areas with low density of data and smaller in areas with plenty of observations [8].\nTo determine how the amount of local variation in palm richness explained by water and energy changes with latitude, we regressed Re, Rpe, Rw, and Rpw on absolute latitude separately for the northern and southern hemispheres. This was done using both ordinary least squares (OLS) regression and simultaneous autoregressive (SAR) models [26]. The local results of GWR are inherently spatially autocorrelated because the local results for geographically close cells are based on overlapping datasets. Spatial autocorrelation may cause false significance of parameter estimates or bias the parameter estimates themselves, and it has even been reported to sometimes invert the relationship between predictor and response when standard OLS regression is used [26], [48]. Here, the spatial scale of inherent autocorrelation was related to the GWR kernel function (Fig. 1). The neighborhood and distance weighting for the SAR models was thus implemented using the same function as for the GWR kernel; for the local results of the 1200-km bi-square GWR, for example, we used a neighborhood of 1200 km and weighted the cells according to the bi-square function. Lagged SAR models were used because these are designed to model inherent (as opposed to induced) spatial autocorrelation [26]. All regression analyses were carried out in R 2.10.1 [49]; the package spdep 0.5-31 (http://cran.r-project.org/web/packages/s​pdep/index.html) was used for the SAR analyses.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Inherent spatial autocorrelation of local GWR results.Moran's I correlogram of the amount of variation in American palm species richness locally explained by water and energy in geographically weighted regression. The black line shows the kernel function of the GWR analysis for comparison, a bi-square function with a bandwidth of 1200 km. Distance in km.\ndoi:10.1371/journal.pone.0027027.g001\nResultsThere was strong spatial heterogeneity in palm richness–climate relationships, as evidenced by a minimum AICC difference between GWR and OLS models of ΔAICC  =  663 (median 1515, maximum 3043) (Tables S1, S2, S3, S4). The model selection procedure clearly favored the models containing all water variables and all energy variables, respectively, with AICC differences of 108–179 and 103–146, depending on the GWR kernel, to the next best model (Tables S1, S2, S3, S4). Local R2 values also provided evidence for strong spatial heterogeneity in the importance of water and energy (Fig. 2). Of note, evidence was consistent for a decrease in the unique explanatory power of water (Rpw) with absolute latitude and a simultaneous increase in the unique explanatory power of energy (Rpe; Tables 1 and 2, Fig. 3). Latitudinal trends in the amount of variation of palm richness that is locally explained by water or energy (Fig. 3) were largely robust to the choice of models, i.e., the spatial kernel used in GWR and the use of OLS vs. SAR models for evaluating the GWR results against latitude (Tables 1 and 2). Different model combinations produced no significant latitudinal trends of opposite sign, but relationships were non-significant in some cases (Tables 1 and 2).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Variation in American palm species richness locally explained by water and energy.Local R2 values obtained from geographically weighted regression (GWR) of palm species richness on annual precipitation, precipitation of the driest month, and water deficit (A) and mean annual temperature, minimum temperature of the coldest month, and potential evapotranspiration (B). Fraction of variation uniquely explained by the water variables (C) and energy variables (D) obtained from variation partitioning. The green circle in (A) shows the GWR bandwidth for a cell situated at the equator.\ndoi:10.1371/journal.pone.0027027.g002\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Latitudinal trends in the ability of water and energy to explain American palm species richness.The amount of variation in palm species richness locally explained by energy variables (A–D) and water variables (E–G) plotted against latitude. A, B: total energy (Re). C, D: pure energy (Rpe). E, F: total water (Rw). G, H: pure water (Rpw). Regression lines obtained from OLS regression (black) and SAR regression (red).\ndoi:10.1371/journal.pone.0027027.g003\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Latitudinal trends in the amount of variation in American palm species richness locally explained by water and energy (OLS).doi:10.1371/journal.pone.0027027.t001\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Latitudinal trends in the amount of variation in American palm species richness locally explained by water and energy (SAR).doi:10.1371/journal.pone.0027027.t002As was apparent from the mapping of the local R2 values and fractions of variation uniquely explained by water and energy (Fig. 2), the GWR results exhibited strong spatial autocorrelation (Fig. 1), as expected based on the functional principles of GWR. However, comparison of the OLS and SAR results showed that this autocorrelation did not qualitatively affect the estimated latitudinal relationships (Tables 1 and 2, Fig. 3).\nWater–energy dynamics theory predicts that species richness is primarily controlled by the availability of water and ambient energy, with water being most influential at low latitudes and energy being most influential at high latitudes [2]. Results from our tests using American palms as a model group strongly support this latitudinal predictor shift (Tables 1 and 2). Similar water–energy predictor shifts have also been found for European mammals, birds, amphibians, and plants [5]. Whittaker et al. [5] used global modeling techniques on separate northern and southern datasets and found that energy had a relatively larger contribution to explained variance in northern data. Evidence also indicates that the relationship between Australian pteridophyte richness and water becomes weaker towards higher latitudes, while the relationship with temperature becomes stronger [30]. Similar conclusions have been drawn for American amphibians [29] and European dragonflies [22]. These three studies used GWR but did not involve statistical analysis of the resulting spatial patterns.Here, we moved beyond previous tests of water–energy dynamics theory in several ways. Water–energy predictor shifts were originally observed between tropical/subtropical areas, where water is the dominant environmental predictor of plant species richness, and temperate areas, where energy or water–energy variables are most influential [2]. For detection of such a shift, the analysis must include a part of the climatic gradient in which energy is clearly limiting for the studied taxon (either in terms of physiology, productivity, or food availability). In the previously studied taxa, this shift seems to occur at relatively high latitudes, usually north of the transition between subtropics and tropics (see [2], [5] for examples). Our results show that latitudinal predictor shifts can also occur within the tropical/subtropical zone, at least for megathermal taxa such as palms. Palms are thought to be maladapted to meso- or microthermal climates because of their soft and water-rich tissue, their inability to undergo dormancy, and their lack of physiological adaptations to frost [37]. These characteristics are obviously highly phylogenetically conserved, making the palms a prime example of a group that fits the tropical conservatism hypothesis [25]. Palms exhibit a strong latitudinal gradient of diversity in the Americas, with highest diversity close to the equator and no species beyond 34°Ν and 34°S [34], [35], [50]. In line with the expectations of water–energy dynamics theory for low latitudes, the broad-scale pattern of palm diversity in the Americas is best explained by water [34], [35], but energy plays an additional role [50]–[52]. Given that the latitudinal limits of palm occurrence are almost certainly set by low temperatures [53], an influence of energy especially on the high-latitude tails of the diversity gradient is plausible.The prediction that the latitudinal predictor shift is strongest in the northern hemisphere [2] is not supported by our results. Neither the magnitude nor the significance of the latitudinal trends in variable importance differ systematically between the hemispheres (Tables 1 and 2). This finding is likely due to our study being restricted to tropical and subtropical latitudes, where climatic gradients apparently do not differ strongly enough between the hemispheres to entail significant differences in latitudinal predictor shifts. Such differences may still be found in taxa that extend into temperate or arctic zones.Previous groups have tested for latitudinal differences in the predictive power of variables by dividing datasets into latitudinal bands [2], [5]. Central to this approach is finding the appropriate latitudinal threshold at which to split the dataset, and interpretation relies on the assumption that relationships are stationary within each latitudinal band. If this is not the case, ‘global’ models such as OLS or SAR models provide average estimates that can be difficult to interpret because they may not apply to any of the location within the study region [6], [29]. GWR is specifically designed to deal with geographic non-stationarity of model coefficients [6]–[8]. It is therefore a suitable approach to studying predictor shifts if no good argument can be made for dividing the dataset at a particular point, or spatial stationarity within the partial dataset is not guaranteed.A downside of GWR compared to the latitudinal-bands approach is that the datasets used for local models overlap excessively, resulting in strong spatial autocorrelation of local coefficient estimates (Fig. 1). Our results illustrate how difficult it is to visually interpret GWR results because of the high degree of inherent spatial autocorrelation; latitudinal trends are not that obvious when R2 values are mapped (Fig. 2). Moreover, OLS regressions that use local GWR results as the dependent variable are prone to bias because of the inherently strong spatial autocorrelation. The use of lagged SAR models is a way to overcome this bias [26], so that GWR results can be used to quantify the strength and shape of predictor shifts along latitudinal (or other) gradients.GWR is also an efficient tool for exploring the scale-dependency of relationships [6], [7], [27]. Predictors of species richness are thought to vary systematically with spatial scale [28]; this effect is also well documented for palms [36]. Predictor shifts sensu Hawkins et al. [2] might therefore depend on the scale at which climate–richness relationships are quantified. In the present study, the GWR bandwidth defined this scale. Latitudinal trends in energy and water effects on palm richness emerged irrespective of the scale of the GWR analysis (1200 km vs. 1800 km bandwidth). Smaller bandwidths were not used because GWR tends to over-fit at very small scales, leading to unrealistic R2 values [27]; and larger bandwidths were not used because they would approach the extent of the total dataset and therefore not allow for sufficient geographic variability. However, our results indicate that the observed latitudinal trends are not restricted to a certain spatial scale.To our knowledge, no previous study, whether it used global or local modeling techniques, has quantified the independent effects of water and energy on species richness. Our results show that taking into account parallel or synergistic effects of water and energy can strongly influence conclusions when testing for predictor shifts. When the water was analyzed irrespective of energy (“total water,” Tables 1 and 2), the influence of water increased with latitude in the northern hemisphere in contrast to the predictions [2]. However, this finding for the northern hemisphere seems to be the result of an interaction with energy. When variation partitioning [44], [45] was used to identify the amount of local variation in palm species richness that is uniquely explained by water-related variables (“pure water,” Tables 1 and 2), the expected negative relationship emerged also for the northern hemisphere. This finding suggests that studies that compare the explanatory power of variables (or sets of variables) without taking the interactions of these variables into account (e.g., [30], [54]) must be interpreted with caution.Scale dependency and spatial non-stationarity are prevalent features of environment–richness relationships and require consideration in the effort to explain spatial patterns of species diversity. Parallel to the current progress in finding global determinants of diversity (e.g., [55]) and understanding their scaling (e.g., [56]), evidence is accumulating for predictable patterns of spatial non-stationarity. Increases in the predictive power of water and decreases in the predictive power of energy variables with absolute latitude have been documented across continents, climatic zones, and taxa [2], [5], [22], [29], [30]; the current work now extends that to the tropics/subtropics. However, more exploration is needed into the universality of this relationship, its shape, and its variation across taxa. GWR or similar local modeling techniques are more powerful tools for this task than traditional ‘global’ models but are not without issues, and further statistical development is desirable, especially concerning spatial autocorrelation both at the level of single (local) models and the overall GWR fit."
        },
        "10.1371/journal.pmed.1001626": {
            "author_display": [
                "Rachel L. Pullan",
                "Matthew C. Freeman",
                "Peter W. Gething",
                "Simon J. Brooker"
            ],
            "title_display": "Geographical Inequalities in Use of Improved Drinking Water Supply and Sanitation across Sub-Saharan Africa: Mapping and Spatial Analysis of Cross-sectional Survey Data",
            "abstract": [
                ": Using cross-sectional survey data, Rachel Pullan and colleagues map geographical inequalities in use of improved drinking water supply and sanitation across sub-Saharan Africa. Background: Understanding geographic inequalities in coverage of drinking-water supply and sanitation (WSS) will help track progress towards universal coverage of water and sanitation by identifying marginalized populations, thus helping to control a large number of infectious diseases. This paper uses household survey data to develop comprehensive maps of WSS coverage at high spatial resolution for sub-Saharan Africa (SSA). Analysis is extended to investigate geographic heterogeneity and relative geographic inequality within countries. Methods and Findings: Cluster-level data on household reported use of improved drinking-water supply, sanitation, and open defecation were abstracted from 138 national surveys undertaken from 1991–2012 in 41 countries. Spatially explicit logistic regression models were developed and fitted within a Bayesian framework, and used to predict coverage at the second administrative level (admin2, e.g., district) across SSA for 2012. Results reveal substantial geographical inequalities in predicted use of water and sanitation that exceed urban-rural disparities. The average range in coverage seen between admin2 within countries was 55% for improved drinking water, 54% for use of improved sanitation, and 59% for dependence upon open defecation. There was also some evidence that countries with higher levels of inequality relative to coverage in use of an improved drinking-water source also experienced higher levels of inequality in use of improved sanitation (rural populations r = 0.47, p = 0.002; urban populations r = 0.39, p = 0.01). Results are limited by the quantity of WSS data available, which varies considerably by country, and by the reliability and utility of available indicators. Conclusions: This study identifies important geographic inequalities in use of WSS previously hidden within national statistics, confirming the necessity for targeted policies and metrics that reach the most marginalized populations. The presented maps and analysis approach can provide a mechanism for monitoring future reductions in inequality within countries, reflecting priorities of the post-2015 development agenda. Background: Access to a safe drinking-water supply (a water source that is protected from contamination) and to adequate sanitation facilities (toilets, improved latrines, and other facilities that prevent people coming into contact with human urine and feces) is essential for good health. Unimproved drinking-water sources and sanitation are responsible for 85% of deaths from diarrhea and 1% of the global burden of disease. They also increase the transmission of parasitic worms and other neglected tropical diseases. In 2000, world leaders set a target of reducing the proportion of the global population without access to safe drinking water and basic sanitation to half of the 1990 level by 2015 as part of Millennium Development Goal (MDG) 7 (“Ensure environmental sustainability”; the MDGs are designed to improve the social, economic, and health conditions in the world's poorest countries). Between 1990 and 2010, more than 2 billion people gained access to improved drinking-water sources and 1.8 billion gained access to improved sanitation. In 2011, 89% of the world's population had access to an improved drinking-water supply, 1% above the MDG target, and 64% had access to improved sanitation (the MDG target is 75%). Why Was This Study Done?: Despite these encouraging figures, the WHO/UNICEF Joint Monitoring Programme for Water Supply and Sanitation (JMP) estimates that, globally, 768 million people relied on unimproved drinking-water sources, 2.5 billion people did not use an improved sanitation facility, and more than 1 billion people (15% of the global population) were defecating in the open in 2011. The JMP estimates for 2011 also reveal national and sub-national inequalities in drinking-water supply and sanitation coverage but a better understanding of geographic inequalities is needed to track progress towards universal coverage of access to improved water and sanitation and to identify the populations that need the most help to achieve this goal. Here, the researchers use cross-sectional household survey data and modern statistical approaches to produce a comprehensive map of the coverage of improved drinking-water supply and improved sanitation at high spatial resolution for sub-Saharan Africa and to investigate geographic inequalities in coverage. Cross-sectional household surveys collect health and other information from households at a single time-point, including data on use of safe water and improved sanitation. What Did the Researchers Do and Find?: The researchers extracted data on reported household use of an improved drinking-water supply (for example, a piped water supply), improved sanitation facilities (for example, a flushing toilet), and open defecation from 138 national household surveys undertaken between 1991 and 2012 in 41 countries in sub-Saharan Africa. They developed statistical models to fit these data and used the models to estimate coverage at the district (second administrative) level across sub-Saharan Africa for 2012. For ten countries, the estimated coverage of access to improved drinking water at the district level within individual countries ranged from less than 25% to more than 75%. Within-country ranges of a similar magnitude were estimated for coverage of access to improved sanitation  (21 countries) and for open defecation (16 countries). Notably, rural households in the districts with the lowest coverage of access to improved water supply and sanitation within a country were 1.5–8 times less likely to access improved drinking water, 2–18 times less likely to access improved sanitation, and 2–80 times more likely to defecate in the open than rural households in districts with the best coverage. Finally, countries with high levels of inequality in improved drinking-water source coverage also experienced high levels of inequality in improved sanitation coverage. What Do These Findings Mean?: These findings identify important geographic inequalities in the coverage of access to improved water sources and sanitation that were previously hidden within national statistics. The accuracy of these findings depends on the accuracy of the data on water supplies and sanitation provided by household surveys, on the researchers' definitions for improved water supplies and sanitation, and on their statistical methods. Nevertheless, these findings confirm that, to achieve universal coverage of access to improved drinking-water sources and sanitation, strategies that target the areas with the lowest coverage are essential. Moreover, the maps and the analytical approach presented here provide the means for monitoring future reductions in inequalities in the coverage of access to improved water sources and sanitation and thus reflect a major priority of the post-2015 development agenda. Additional Information: Please access these websites via the online version of this summary at http://dx.doi.org/10.1371/journal.pmed.1001626. "
            ],
            "publication_date": "2014-04-08T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS Medicine",
            "citations": 2,
            "views": 8192,
            "shares": 58,
            "bookmarks": 16,
            "url": "http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.1001626",
            "pdf": "http://www.plosmedicine.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pmed.1001626&representation=PDF",
            "fulltext": "IntroductionRemarkable gains have been made in the provision of drinking-water supply and sanitation (WSS) globally, with over 2 billion people reportedly gaining access to improved drinking-water sources and 1.8 billion to improved sanitation between 1990 and 2010 [1]. The WHO and UNICEF's Joint Monitoring Programme for Water Supply and Sanitation (JMP) has reported that the world is “on track” to reach the Millennium Development Goal (MDG) target for water supply of reducing by half the proportion of the population without access to sustainable and safe water [1]. In spite of such global gains, important national and sub-national inequalities in WSS coverage remain, including significant rural-urban and regional disparities [1],[2]. National studies also highlight systematic inequalities in WSS access, typically focusing on rural-urban or socio-economic disparities and coverage in hard to reach groups [3]–[9].\nAccess to safe water and sanitation facilities is a fundamental human right. Unimproved drinking water and sanitation are responsible for an estimated 1% of global disability-adjusted life years (DALYs) [10], and 85% of diarrhoea mortality can be attributed to inadequate water, sanitation, and hygiene practices (WaSH) [11]. Populations with inadequate WSS are also disproportionately affected by the neglected tropical diseases (NTDs) [12]–[17], and as such WSS are increasingly recognised as critical for sustaining the impact of control and elimination strategies that rely on geographically targeted mass drug administration [18],[19]. There have however been few published analyses of sub-national geographical distribution of WSS at policy relevant scales. To better understand inequality within and between countries, and provide a bench mark for tracking progress and help prioritize resource allocation, there is a clear need to develop policy-relevant data on sub-national inequality and a standardised approach to mapping sub-national coverage in WSS.\nPopulation-based national household cluster-sample surveys, such as demographic and health surveys (DHS) and multiple indicator cluster surveys (MICS), provide a wealth of information on the coverage of health and development indicators, including WSS, within countries and have been used previously to map geographical variation in bed net coverage [20], anaemia [21], and under-nutrition [22]. There is an important trade-off however between accuracy and spatial resolution that must be resolved before these data can be truly useful for stakeholders. Surveys are usually powered to provide accurate data for provinces or regions, although access to WSS is likely to vary markedly at this spatial scale. Modern statistical approaches, including small area estimation (SAE) and Bayesian spatial conditional autoregressive (CAR) models, can help tackle the problem of providing feasible estimates for smaller geographical areas, whilst explicitly acknowledging uncertainty associated with data powered to be representative at larger spatial scales [23]–[27].\nIn this paper we combine multiple national surveys using spatial statistical methods to investigate differences in use of improved drinking water, improved sanitation, and open defecation at small spatial scales; identify specific geographical areas where coverage is substantially worse than national averages; and explore relative geographic inequalities within countries. Our analysis focuses on sub-Saharan Africa (SSA), as this is the region where progression towards the MDGs for water and sanitation is often least successful [1],[28]. Our intent is not to replace existing national JMP estimates, but instead to develop robust maps of contemporary WSS coverage at a finer spatial resolution useful for service delivery providers, policy makers and those planning investment within governments, implementers, and donors. The work is conducted within the context of the Global Atlas of Helminth Infections project (www.thiswormyworld.org), which aims to develop a suite of geographical resources and tools for NTD control.\nMethods\nData Sources\nWSS data were sourced from national household cluster-sample surveys undertaken as part of multiple indicator cluster surveys (MICS) (http://www.childinfo.org/mics4_surveys.h​tml; implementation supported by UNICEF), Demographic and Health Surveys (DHS) and national malaria and AIDS indicator surveys (MIS/AIS) (http://www.measuredhs.com/data/available​-datasets.cfm; USAID) and living standard measurement studies (LSMS) (http://iresearch.worldbank.org/lsms/lsms​surveyFinder.htm; World Bank). These surveys are based on probability sampling using existing sampling frames (primarily population censuses) and are conducted by trained enumerators conducting household visits, typically achieving household response rates >97%. Informed consent is obtained from all respondents before participating. All available DHS, MICs, and LSMS surveys conducted in SSA since 1990 that contained the necessary modules to calculate household WSS use were included (n = 138 surveys) [29]. Access to improved drinking water and sanitation were defined using the criteria outlined by the JMP, and are measured by reported use. For MDG monitoring, improved sanitation facilities are defined as those that “hygienically separate human excreta from human contact,” whilst improved (“safe”) drinking-water sources are defined as those that are “protected from outside contamination (especially faecal contamination).” The proportion of households reporting open defecation was also recorded. In the subset of data where distance to water source was available (n = 121 surveys), we also recorded the proportion of households using an accessible, improved drinking water source. This was defined as one within 1 km (or 15 minutes) of the household, which has been suggested as an appropriate distance for meeting the MDG targets [30]. Further details of indicator definition, and comparison with those used by the JMP, are provided in Box 1.\n\nBox 1. Comparison with the Joint Monitoring Programme Methodology\n\nData availability\n\n\n\n\nAvailable data sources included in this analysis (DHS, MICs, and LSMS) make up 42% percent of all data in the JMP database for the same countries. Data available to the JMP that could not be used in this analysis include national surveys implemented by governments and census data.\n\n\n\nCommon definitions\n\n\n\n\nImproved sources of drinking water: piped water into the dwelling; piped water to yard/plot/compound; public tap or standpipe; tubewell or borewell; protected dug well; protected spring; rainwater.\n\nImproved sanitation: flush toilet; piper sewer system; septic tank; ventilated improved pit latrine (VIP); pit latrine with slab; composting toilet.\n\nOpen defecation: No facilities or bush or field.\n\nNot used by JMP. Accessible, improved drinking water source: an improved drinking water source (see above) within either 1 km or 15 minutes from the household. Data available in 85% of surveys.\n\n\n\nShared sanitation facilities\n\n\n\n\nContrasting variable definition: JMP classify all shared facilities as unimproved, and make assumptions on the proportion shared when this data are not available. In our main analysis, we do not distinguish between shared and private sanitation facilities as this information is only available for 70% of included surveys, although a sub-analysis is performed to compare differences\n\nImplications: Coverage of improved sanitation in countries with large numbers of households reporting shared facilities will be considerably higher than current JMP estimates\n\n\n\nUnimproved pit latrines\n\n\n\n\nContrasting variable definition: older DHS surveys (phase 4 and earlier, pre-2003) do not distinguish the type of pit latrine (improved or unimproved), and for these surveys JMP make assumptions on the proportion that can be classified as improved. In our analysis all such latrines were included as unimproved to avoid misclassification.\n\nImplications: In our analysis past coverage may have been underestimated for countries with older DHS surveys, and temporal gains in coverage overestimated.\n\n\n\nMethodology\n\n\n\n\nJMP methods: for each country, separate linear time trends are fit through national (rural and urban) coverage estimates from survey and census data. Regression slopes are extrapolated two years outside available data; beyond this slope is assumed to be zero for a maximum of six years after which estimates are not made. Population averaging (to determine rural, urban, and overall coverage rates) relies on reported survey weights.\n\nSpatial multilevel methods: data from all countries are treated as a continuous time series and fit within a single multilevel model. This considers the hierarchical structure of the whole dataset: surveys sites are nested within administrative areas, within countries. The model estimates an average intercept and an average slope with residual variances across countries, whilst accounting for sub-national spatial correlation and non-spatial variation. In practice, this means that when there is reliable information for a specific country, predictions will closely follow the country survey points, whereas when there is little temporal (or sub-nationally disaggregated) information trends will tend to follow the regional (or national) mean. Population averaging uses high resolution population data generated by the WorldPop project [33],[34].\n\n\n\nFor each of the available surveys, the proportion of households reporting use of an improved drinking water source, improved sanitation, and open defecation were calculated at the cluster (survey site) level. Each survey site was then coded as either urban or rural as defined by the survey and was allocated, where possible, to sub-national administrative areas. For reference, in most SSA countries first administrative areas (admin1, often referred to as provinces, regions, or states) represent on average approximately 1.35 million people and are typically further subdivided into three to 15 second administrative areas (admin2, often referred to as districts, representing approximately 200,000 people). Admin1 and admin2 digital boundaries were derived for each country using the United Nations Second Administrative Level Boundaries dataset project (SALB) ([31]): for 74 surveys, survey sites were linked to admin2 using the provided survey site co-ordinates, whilst admin1 and admin2 names were used to match survey site locations for the remaining 64 surveys. For 45 of these, survey sites could be matched to admin1 only as admin2 names were not available. In these instances, survey site data were attributed to all admin2 contained within the admin1 boundaries, and weighted to reflect the number of admin2 this represented. No survey data were available for Botswana and Eritrea, and these countries (representing 0.85% of the population of SSA) are therefore not included in the analysis.\nDigital gridded population surfaces and urban/peri-urban extent maps for 2012 at 100 m2 resolution were provided by the WorldPop project [32],[33]. These population surfaces were used in combination with the admin2 boundaries to extract total admin2 populations and the proportion of each admin2 population living in urban/peri-urban and rural areas. As a confirmation step, the overall proportion urban generated for each country for 2012 was compared with estimates produced by the UN Urbanization Prospects [34] suggesting good agreement (r = 0.72, p<0.001).\n\n\nData Analysis\nFor each country, available data for urban and rural populations were plotted on a timescale from 1990 to 2012, and the temporal trend examined visually. Access to improved water and sanitation has risen approximately linearly over time, although there are large differences in this trend between countries, and between urban and rural populations within countries. It was therefore decided that a random coefficient model incorporating a linear time trend would be appropriate, allowing different intercepts and temporal slopes for urban and rural populations by country, but assuming an overall linear change over time.\nFor each water and sanitation indicator, spatially explicit logistic regression models were therefore developed and fitted using a Bayesian framework. These consider the hierarchical structure of the whole dataset: survey sites are nested within admin2, which are themselves nested within admin1 and countries. Instead of calculating an intercept and slope separately for each country, as is currently done by the JMP, the multilevel model estimates an average intercept and an average slope for rural and urban populations, with residual variances across countries. Neighbouring admin2 within countries are also expected to have similar coverage rates, and so an admin2-level random effect is included that explicitly models the correlation between neighbouring admin2 within countries using a conditional autoregressive (CAR) covariance structure. Models also include a regional random effect with an unstructured covariance structure, to account for non-spatial dependence between survey sites within admin1 areas. A detailed description of the model is given in Text S1.\nEssentially, the model structure implies that countries with scarce data over time will follow the mean trend, whilst countries with reliable information (i.e., several surveys across multiple time points) will closely follow the survey points with less associated uncertainty. Similarly, when there is little spatially disaggregated data available, sub-national predictions will follow national means, whilst if data coverage at the admin2 level is good (i.e., survey sites located in admin2 throughout the country) admin2-level coverage will be smoothed towards local, neighbouring values thus counteracting the problem of outliers resulting from under-sampling. Models were fit within a Bayesian framework in the software package WinBUGS [35] using a Markov chain Monte Carlo (MCMC) algorithm, a robust and flexible inference platform that is well suited to complex problems and that explicitly incorporates uncertainty in input data and model parameters. MCMC is an iterative, stochastic simulation technique, and as such this uncertainty is represented in the form of predictive posterior distributions.\n\n\nPredicting Coverage in 2012 and Analysis of Geographical Inequality\nThe developed models were used to predict coverage of water and sanitation for the urban and rural populations of each admin2 for 2012; overall population estimates are population-weighted averages of the urban and rural numbers. Following the approach of Gomez-Rubio and colleagues [24] and Banerjee and colleagues [36], we used information provided by the spatially correlated admin2-level random effect to make predictions for admin2 without data, as detailed in Text S1. To prevent unstable predictions for areas with few or no neighbours with data, this was only performed for countries with available data at admin2 level. For seven countries with no data located at admin2 level, predictions were made for admin1 only. At each MCMC iteration, multiplying the urban and rural population surfaces with the admin2 predicted coverage (or where applicable, coverage by admin1) and then aggregating allowed estimation for the overall population locally and nationally. Resulting uncertainty in predicted coverage will vary geographically, depending on observed sampling variation and the density of survey sites within countries and across time.\nSub-national inequality was evaluated using absolute and relative measures. As absolute measures, the range of coverage (by admin2) for each country was computed, and admin2 with coverage significantly lower (and higher) than the national mean identified based on 95% Bayesian credible intervals (BCIs). As a country-level measure of sub-national relative inequality, we developed a relative geographic inequality (RGI) index, derived from a geographical GINI coefficient. The GINI coefficient is a measure commonly used to describe the distribution of income within a country [37], and has been widely adapted for other applications, including measuring inequalities in health care provision [38],[39] and disease incidence across population groups [40]. It is based on the Lorenz curve, an accumulated frequency curve that compares the distribution of a specific variable with a uniform distribution that represents equality. To calculate Lorenz curves for each country, administrative areas were ranked smallest to largest by their share of national use; the cumulative proportion of use was then calculated and plotted against the cumulative percentage of population. The greater the deviation of the Lorenz curve from the diagonal line of equal distribution, the greater the inequality. The GINI coefficient was then calculated as twice the area between the diagonal and the Lorenz curve. Values can range from 0 to 1, with 0 representing perfect equality and 1 total inequality [37]. Importantly however, in contrast to income, WSS coverage is a bounded variable (i.e., it cannot fall outside the range 0%–100%) and as such the GINI coefficient would be expected to decline as coverage increases. Outlier countries (with higher or lower levels of inequality given their level of coverage) were thus identified using linear regression of GINI coefficient against national coverage for overall, rural, and urban populations and the RGI score generated as the difference between the observed and expected GINI coefficient given national coverage based upon this modelled relationship. This approach draws upon the methods of Jamison and colleagues in their investigation of the relationship between survival curves and life expectancy, which follow similar rules [41]. In the case of open defecation, the GINI and RGI scores were calculated on the basis of households not reporting open defecation (i.e., with access to any type of sanitation) to enable better comparison with the other variables.\n\n\nEthics Statement\nThis is a secondary analysis of previously collected and published household survey data, and as such ethical approval was not required for this work. Ethical clearance for each included survey was provided by review boards in each country, with informed consent provided by all participants. Details of this process vary by survey.\n\nResults\nNational WSS Coverage\nIn total, we obtained 138 DHS, MICs, and LSMS surveys conducted between 1991 and 2012, representing over 1.15 million households from over 50,000 survey sites located in 2,751 admin2 across SSA (outlined in Table 1). Figure 1 emphasises the high data coverage in West and East Africa, and low coverage in much of Southern and Central Africa. Data coverage was very limited for South Sudan and much of Angola.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Availability of nationally representative, cluster survey data on improved drinking water and sanitation across sub-Saharan Africa for the period 1990–2012.Data are linked to second administrative areas where possible and if not to the first administrative level; administrative boundaries are provided by the United Nations Second Administrative Level Boundaries (SALB) project.\ndoi:10.1371/journal.pmed.1001626.g001\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Regional summary of water and sanitation coverage data sources and quantity for 41 sub-Saharan African countries.doi:10.1371/journal.pmed.1001626.t001Table 2 provides predicted national estimates and admin2 summaries; the predicted distributions of coverage in urban and rural populations are shown in Figure 2. Model results suggest that in 2010, 62.0% (95% BCI: 61.5%–62.4%) of the population of SSA (excluding Eritrea and Botswana) reported using an improved drinking-water source, although at national levels this was seen to vary between as low as 11.5% (95% BCI: 6.2%–19.0%) in rural Somali populations to 99.0% (95% BCI: 98.7%–99.3%) in urban populations in Namibia. Similar ranges are seen for reported use of improved sanitation: whilst we predict that 42.5% (95% BCI: 42.1%–43.1%) of the total population in SSA used improved sanitation, this ranges from less than 30% for 11 countries to greater than 70% in the top four performing countries (Ghana, Guinea-Bissau, Equatorial Guinea, and Rwanda). For six countries, it is estimated that at least 50% of households habitually defecate in the open (Niger, Namibia, Benin, Burkina Faso, South Sudan, and Chad).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Predicted population coverage in 2012 for rural and urban populations, by second administrative area.Access to improved drinking-water supply in (A) rural and (B) urban populations; access to improved sanitation in (C) rural and (D) urban populations; and open defecation in (E) rural and (F) urban populations. Model results showing posterior median predicted coverage (i.e. most likely value) for each second administrative area. No data was available for Botswana and Eritrea (hatched). Each indicator was modelled independently.\ndoi:10.1371/journal.pmed.1001626.g002\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  National estimates and admin2 summaries resulting from the modelling procedures for access to improved drinking-water source, improved sanitation, and open defecation.doi:10.1371/journal.pmed.1001626.t002For 30 countries there was sufficient survey data to investigate reported use of accessible, improved drinking water supply (defined as those <1 km away or within a 30-minute round trip) and for 27 countries sufficient survey data to investigate reported use of private, improved sanitation (defined as one that is not shared with another household). Overall, in 2012 only 55.0% of rural populations using an improved drinking water source used an “accessible” source (range: 23.9% in Uganda to 98.8% in Namibia) rising to 74.1% of urban populations (range: 39.9% in Cote d'Ivoire to 95.8% in Mali). As can be seen in Figure 3, this urban-rural disparity is less apparent for private and shared improved sanitation: overall, 54.1% of rural populations using improved sanitation had access to a private latrine (range: 12.6% in Liberia to 88.7% in Mozambique) compared with 48.9% of urban populations (range: 14.4% in Togo to 82.7% in Mozambique). Given the reduced data availability for these additional indicators, further analysis at a sub-national level is restricted to use of any improved drinking-water source and any improved sanitation facility.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Comparison of (A) any improved drinking water source against accessible, improved drinking water source and (B) any improved sanitation against private improved sanitation.Dots show national comparisons for urban (red) and rural (blue) populations. An accessible drinking water supply is defined as one within 15 minutes of the household; private sanitation is defined as a facility used by only one household.\ndoi:10.1371/journal.pmed.1001626.g003\n\nSub-national Variation in WSS Coverage\nSubstantial within-country variation in use is predicted for all three main outcomes, with some of the greatest differences in coverage between admin2 seen for the highly populous countries of Nigeria, Ethiopia, and DR Congo. For ten countries, admin2-level coverage in use of an improved drinking-water source ranged within country from less than 25% to over 75%; within-country ranges of the same magnitude were seen for improved sanitation for 21 countries and open defecation for 16 countries. Figure 4 highlights those admin2 where coverage of both improved drinking water and improved sanitation were significantly lower than national averages based on 95% BCIs (shown in red), highlighting the considerable overlap in admin2 with low access to improved drinking water and improved sanitation. Similarly, when considering all countries in SSA, correlation between admin2-level coverage of improved drinking water and improved sanitation is reasonably high (r = 0.44, p<0.001) although as can be seen in Figure 5 this varies considerably by country.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Predicted second administrative areas that differ significantly from national mean coverage.Second administrative areas shaded red have significantly lower coverage than the national average, based on 95% BCI, for either both (dark red) or one (light red) of improved drinking water and improved sanitation; administrative areas shaded blue have significantly higher coverage rates than the national average. Administrative areas shaded grey are not significantly different from the national mean.\ndoi:10.1371/journal.pmed.1001626.g004\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Comparison of coverage in improved drinking water against improved sanitation in overall population, by second administrative area.Comparisons are made for (A) Nigeria, (B) Mozambique and (C) Uganda. r is the Pearson pairwise correlation coefficient. Each dot represents one administrative area.\ndoi:10.1371/journal.pmed.1001626.g005The distribution of coverage across all admin2 within each country is shown in Figure 6. For use of an improved drinking-water source, the majority of countries display a linear pattern of coverage, where the distance between quintiles is similar from lowest to highest levels of access. For access to improved sanitation (where coverage is generally lower) a larger number of countries display a top inequality pattern, meaning that coverage in the top quintile is substantially higher than the rest (e.g., Ethiopia, Gabon, Mali, Mozambique, Madagascar, South Africa). In contrast, several countries (including Malawi, Chad, and Sudan) display very little variation between quintiles, especially for access to improved sanitation.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Distribution of modelled WSS coverage across administrative areas by country.Second administrative areas were stratified into quintiles based on coverage of each indicator. Dots show median proportion of households with access for each quintile; lines show the full range in coverage.\ndoi:10.1371/journal.pmed.1001626.g006\n\nInequality Analysis\nThe degree of geographic inequality in access to WSS varies substantially across SSA. As would be expected, plotting GINI against national coverage reveals strong negative relationships for all three indicators (range in r −0.78 to −0.88, p<0.001); the linear regression of GINI scores against national coverage thus provides a straightforward mechanism for identifying outlier countries with lower, or higher, levels of inequality than would be expected given their level of coverage. As shown in Figure 7, there are a number of outliers for all three WSS indicators, most noticeably for use of improved sanitation. For use of improved drinking water and any type of sanitation, higher than expected levels of inequality are seen across the full range of national coverage. In contrast, for use of improved sanitation countries with low national coverage are more likely to have higher than expected levels of geographical inequality. There are also some notable outliers with lower than expected geographical inequality given their national coverage rates, especially in access to improved sanitation. The first of these is South Sudan, although this is most likely due to a paucity of data to adequately describe sub-national variation. However Congo, Malawi, and Sierra Leone all display low geographical inequality in access to both improved drinking water and sanitation.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 7.  Empirical relationship between inequality (GINI score) as a function of national coverage.Plots are shown for (A) use of improved drinking water, (B) use of improved sanitation facilities, and (C) use of any type of sanitation. All plots show the linear regression prediction (solid line) with 95% confidence interval (shaded area). Labelled countries (by 3-letter ISO codes) are those with GINI scores significantly higher or lower than would be expected, given national coverage.\ndoi:10.1371/journal.pmed.1001626.g007For reference, the difference between the observed and expected GINI scores for the overall population (RGI) are provided in Table 2. There was no evidence for correlation between RGI scores for improved drinking water and sanitation for the overall population at national levels (r = 0.18, p>0.2). When considering rural and urban populations separately however, there was some evidence that countries with higher levels of relative inequality for use of an improved drinking-water source (i.e., positive RGI scores) also experienced higher levels of inequality for use of improved sanitation (shown in Figure 8; rural populations r = 0.47, p = 0.002; urban populations r = 0.39, p = 0.01).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 8.  Relationship between relative geographical inequality for use of improved drinking water and RGI for use of improved sanitation for(A) rural populations (correlation (r) = 0.47, p = 0.002) and (B) urban populations (r = 0.39, p = 0.01).\ndoi:10.1371/journal.pmed.1001626.g008\nIt is already known that coverage of WSS varies considerably between countries in SSA [1]. To our knowledge, this is the first attempt to systematically map and analyse geographical inequalities in WSS within countries. We selected three widely reported WSS coverage indicators (access to improved drinking water, access to improved sanitation, and open defecation) and have generated a new resource for researchers and country stakeholders. Our robust analytical approaches, which explore geographical heterogeneity and geographic inequalities, showed consistent findings for all the three indicators that have important implications not only for the WSS sector, but also the international public health and development communities, policy makers, and donors.In October 2013, key sector stakeholders issued a joint statement calling on the international community to ensure that the post-2015 development agenda be framed around the principle of equality, emphasising that future goals, targets, and indicators must rely on disaggregated data to allow inequalities to be effectively measured [42]. Our results suggest that there are substantial geographical inequalities in use of WSS across SSA that surpass simple urban-rural disparities and are of similar magnitude to the large socio-economic inequalities highlighted in a number of national studies [2]–[8],[12],[43]–[45]. In almost all countries, rural households in lowest coverage admin2 were 1.5 to 8 times less likely to access improved drinking water, 2 to 18 times less likely to access improved sanitation, and 2 to 80 times more likely to defecate in the open than rural households with the best access. Even across urban populations, coverage in use of improved drinking water varies on average by 30% nationally, use of improved sanitation by 5%, and dependence upon open defecation by 26% nationally, emphasising how population growth and rapid urbanisation may frequently outstrip service provision in poorer urban areas [46]. Considerable overlap in those admin2 with significantly worse coverage than the national average for both improved drinking water and improved sanitation suggests that vulnerable and marginalized populations often suffer the compounded effects of unsafe water and poor sanitation.Perhaps our most striking finding, however, is that high geographical inequality (i.e., higher than would be expected given absolute national coverage) was seen across all levels of national coverage, although for improved sanitation this was typically greater than would be expected for those countries with lowest national coverage. In addition, those countries with high levels of relative inequality in improved drinking water also experienced higher levels of relative inequality in use of improved sanitation. This finding suggests that countries struggling to increase coverage often also struggle with issues of poor targeting of resources, or patchy implementation of government or NGO delivered interventions, and need to develop strategies and investment plans with reduction of inequalities in mind. Notably, evidence from across the public health and development arena suggests that, unless governments and stakeholders deliberately adopt strategies aimed at reaching lowest coverage areas and population groups, it is unlikely that countries will achieve universal coverage [43],[47]–[49]. It is likely that areas without access to improved water supply and basic sanitation are not only the poorest, but most challenging in terms of environmental conditions and demand for resilient infrastructure, such as low or inconsistent rainfall, poor soils and deep water tables, and a paucity of available markets and materials. The publication in 2012 of the first quantitative aquifer storage maps for the African continent reveals that most countries have the potential for sufficient well-placed hand-pumps to support rural populations; however, the potential for higher yielding boreholes (required to support rapid urbanisation) is limited and will require careful planning [50]. We hope that the information presented here, when combined with quantitative mapping of groundwater resources at sub-national levels, can provide a useful tool to accurately target WSS investments geographically to those admin2 where they are most needed. These data also demonstrate the promise of using geographically disaggregated data as a complementary tool to stratifying on the basis of socio-economic status [51] when systematically measuring progress in reducing inequalities within low coverage countries.It is commonly recognised that there is no ideal measure for expressing the magnitude of inequalities [37],[52],[53], and as such we have presented both absolute (within country ranges and quintiles) and relative measures. Despite an increasing focus on spatial analysis within health research, there are relatively few publications concerning measurement methods for geographic inequality in health outcomes [54]–[57], and very few papers have explored the use of absolute or relative metrics for sub-national coverage data such as this [58],[59]. We have developed an RGI score, an adaptation of the GINI coefficient. This widely applied coefficient is a ratio analysis measure, and is independent of scale and population size, making it helpful for comparing diverse countries and groups within countries, such as urban and rural populations. It also takes full account of the entire distribution, rather than simply comparing those at the extremes. However, it is a relative measure, and as such countries with vastly differing national coverage can share the same score if the relative distribution is similar. Its use is also limited with bounded variables, such as coverage. Generating an RGI however, which compares the observed GINI coefficient with that expected given existing levels of coverage, helps circumvent the negative correlation that is to be expected when generating GINI coefficients using bounded indicators and allows identification of countries with large relative inequality in provision of WSS. Metrics such as these that accurately align with development principles, including equality of access, are urgently needed. However we would like to add a note of caution: measures of inequality should not be used in isolation, but rather in combination with indicators of absolute coverage. Importantly, equality of use does not reflect how much overall coverage has been achieved, which means that a country could achieve significantly low levels of relative inequality whilst still having very low overall coverage, as clearly demonstrated by Sierra Leone and Ethiopia.Crucially, our results are entirely dependent upon the quality of the water and sanitation data available. DHS, MICs, and LSMS surveys contain a standardised water and sanitation module in which survey questions and response categories are fully harmonised [29], helping to ensure internal consistency and comparability between countries. However, the only available proxy for sustainable access to improved drinking water source and sanitation is self-reported use and ownership. These reports are rarely if ever supplemented by visual inspection to confirm the functionality and whereabouts of the water source, or correct use of the latrine. Whilst many epidemiological studies rely on self-report to measure type and use of such facilities, to our knowledge, very few studies have examined the reliability of self-reported water and sanitation practices and few have used objective measures to assess actual use [60].Our analysis is based on those indicators currently recommended for monitoring the MDG targets, but these indicators are subject to considerable debate [61]. For example, not all water sources that are classified as improved will provide water that is safe to drink [62]. Correction with data on drinking water quality would likely have a large impact on estimates, calling into question whether the MDG water target would actually be met by 2015 [61]. For example, the only nationally representative water quality data that exists for SSA, to our knowledge, suggests that only 72% of improved drinking-water sources in Ethiopia were in compliance with WHO and national guidelines on drinking water quality [63]. There is currently massive lack of capacity in many countries to generate reliable and representative information on drinking water quality [64], and so we are unable to make appropriate corrections. Neither have we been able to account for the reliability of water supplies—studies across SSA have suggested that up to around one-third of hand-pumps may be non-functioning—nor of household treatment of water [65]. Our preliminary analysis of distance to water source however has suggested that only a little over half of all rural households reported as using an improved water source were using one within 30-minutes round trip of their household. This is especially striking given that a recent systematic review and meta-analysis has suggested significant increase in illness risk in people living further away from their water source [66]. Distance is an important determinant of the quantity of water brought to the household and used for drinking, cooking, and hygiene behaviours [65], and given our results we would suggest that future monitoring activities should always attempt to quantify the time taken for households to obtain their water.Similarly controversial is the definition of improved sanitation. For example, a latrine that is correctly used by all members of the household and well maintained can provide an effective barrier to the transmission of faecal-oral disease, whereas a latrine that is not used correctly or that is poorly maintained can actually become a focus for the transmission of disease. Our reported use measures that rely upon the potentially subjective definition of an “improved” pit latrine. Again to avoid approximation, we only include latrines classified as having a slab as improved, which is supported by evidence from Tanzania suggesting the presence of helminth eggs in a large majority (71%) of soil samples collected from more simple pit latrines [67]. Nevertheless, it is not possible to correct for the appropriate use and cleanliness of all latrines classified as improved. Finally, as detailed in Box 1, we have classified all sanitation facilities that use improved technology as improved, regardless of whether they are shared, primarily to avoid approximating when data were unavailable (e.g., all pre-2004 surveys). Although this will include some public sanitation facilities (which may often be used by too many people, be poorly maintained, expensive or distant, and even post a risk for interpersonal violence [68]–[70]), in most instances these represent private facilities shared between five households or fewer. There is little evidence surrounding the health impacts of such facilities [71],[72], but it has been argued that a high proportion are probably safe for health [73]. This will however mean that our estimates of coverage for improved sanitation will be substantially higher than those produced by the JMP, who define all shared facilities as unimproved, preventing them from being directly comparable.The multilevel modelling approach used here is transparent and flexible, offering advantages over the traditional country-specific linear regression models currently employed by the JMP. As demonstrated in a recent similar exploration of multilevel modelling for estimating national access to drinking water and sanitation [46], using a single model for all countries provides a continuous time series, providing additional information for those countries with scarce data. Our spatial Bayesian approach also enables use of information from neighbouring areas to improve predictions in unsampled areas [24], and quantifies the uncertainty resulting from modelling sub-national estimates at small spatial scales. Some limitations should be noted: we still relied on fitting linear time trends whereas a more flexible curve fitting approach might be more appropriate [46]; we did not include further covariates in the model; we did not consider uncertainties associated with the survey estimates arising from non-sampling errors; and we modelled household, not population, use (i.e., for each survey site we modelled the proportion of households reporting use). The latter may systematically bias our estimates upwards, as it is likely that individuals living in smaller (and potentially richer) households with greater access to WSS will be overrepresented. It will also be important to validate this work as new data become available.Although not intended to reproduce the JMP, and not directly comparable for improved sanitation, in most instances our estimates are in keeping with official estimates (improved drinking water: r = 0.77, root mean squared error [rmse] 11.3%; open defecation: r = 0.92, rmse = 7.7%) although they do differ substantially for individual countries, up to 33% for access to an improved drinking-water source and 19% for access to improved sanitation. These differences may be attributed in part to our hierarchical modelling approach—which combines both a more robust handling of temporal trends, and potentially more accurate population weighting as a result of disaggregation. However, the reduced amount of data available for this analysis for many countries (we did not have access to national census data or government surveys) will have played a very important role. For example, the largest difference we see for access to improved drinking water is for Guinea (32%), for which we relied on only two DHS surveys in contrast to the nine surveys and censuses available to JMP.The implications of geographical inequalities in WSS coverage go beyond logistic considerations of effective service provision. Being able to target where inequalities exist will bring us closer to the goal of universal water and sanitation, which will fulfil human rights obligations. Understanding geographic inequities in WSS coverage can also provide insight into the epidemiology and control of many infectious diseases. Access to safe water and improved sanitation significantly reduce not only diarrhoeal disease [74],[75], but also water-borne diseases such as cholera, typhoid, and cryptosporidiosis [12], and NTDs such as soil-transmitted helminthiasis (STH), schistosomiasis, and trachoma [13]–[16],[76]. In recent years, there has been a concerted effort to develop geographical resources on a range of infectious and tropical diseases [77]–[79] and combining these data with knowledge of local WSS provision and other externalities [18],[80] can provide an empirical basis on which to interpret and predict the impact of public health programmes. For example, there is a large body of evidence that the risk of cholera transmission is greatly reduced as water and sanitation coverage improves [81]–[83], with recent mathematical models suggesting that WSS interventions may be as effective as the oral cholera vaccine in averting cholera cases, with the greatest impacts seen when the two are implemented in combination [84]. Similarly, it is unlikely that drug-based interventions alone will eliminate or control STH, schistosomiasis, and trachoma where WSS coverage remains low [18],[85]–[88]. Whilst growing resolve for collaboration and coordination between the WSS and health sectors is encouraging, there is still a pressing need to build a strong evidence base for collaborative programming [89]. For instance, it has been argued that an historic dissociation of WSS and health sectors has led to major problems in developing and maintaining essential infrastructure [65], and as such the public health community must play a role in setting health-based targets and indicators [51]. An essential component of this is the development of more effective and cross-sectoral coverage and impact indicators [89]. These should be able to effectively measure improvements in both WSS and infection control, and should consider in detail all potential factors that contribute to infection risk.Although reducing inequalities was not a key element of the original health-related MDGs, there is a growing consensus that monitoring indicators solely at national levels fails to incentivise the targeting of areas of greatest need and potential greatest impact [7],[42],[90]. Here, we have revealed substantial levels of inequality in contemporary access to both improved drinking-water supplies and sanitation and open defecation within countries, and have shown how mapping the geographical distribution of WSS at policy relevant scales can help to make visible those deprived subgroups that were previously hidden within national statistics. As a consequence, we would urge the JMP to consider providing sub-national estimates of coverage within its country profiles, coupled with summary statistics of relative inequality. Mapping a phenomenon, however, does not explain it, and reasons for inequalities are likely to vary substantially between countries. Detailed investigation of the influence of contextual and programmatic factors on contemporary coverage, as well as changing patterns of inequality over time, are outside the scope of the present analysis, but are issues we intend to address in future work at both national and regional scales. More generally, this work has highlighted those countries that are struggling to target resources to areas of greatest need, and it is the responsibility of the international community to provide these countries with assistance in the development of strategies and investment plans to reduce inequality and marginalization. Finally, our intention was to create a vital resource for both researchers and planners, and to this end sub-national maps can be viewed on www.ntdmap.org and the data are available on www.thiswormyworld.org."
        },
        "10.1371/journal.pone.0098361": {
            "author_display": [
                "Frank A. La Sorte",
                "Stuart H. M. Butchart",
                "Walter Jetz",
                "Katrin Böhning-Gaese"
            ],
            "title_display": "Range-Wide Latitudinal and Elevational Temperature Gradients for the World's Terrestrial Birds: Implications under Global Climate Change",
            "abstract": [
                "\nSpecies' geographical distributions are tracking latitudinal and elevational surface temperature gradients under global climate change. To evaluate the opportunities to track these gradients across space, we provide a first baseline assessment of the steepness of these gradients for the world's terrestrial birds. Within the breeding ranges of 9,014 bird species, we characterized the spatial gradients in temperature along latitude and elevation for all and a subset of bird species, respectively. We summarized these temperature gradients globally for threatened and non-threatened species and determined how their steepness varied based on species' geography (range size, shape, and orientation) and projected changes in temperature under climate change. Elevational temperature gradients were steepest for species in Africa, western North and South America, and central Asia and shallowest in Australasia, insular IndoMalaya, and the Neotropical lowlands. Latitudinal temperature gradients were steepest for extratropical species, especially in the Northern Hemisphere. Threatened species had shallower elevational gradients whereas latitudinal gradients differed little between threatened and non-threatened species. The strength of elevational gradients was positively correlated with projected changes in temperature. For latitudinal gradients, this relationship only held for extratropical species. The strength of latitudinal gradients was better predicted by species' geography, but primarily for extratropical species. Our findings suggest threatened species are associated with shallower elevational temperature gradients, whereas steep latitudinal gradients are most prevalent outside the tropics where fewer bird species occur year-round. Future modeling and mitigation efforts would benefit from the development of finer grain distributional data to ascertain how these gradients are structured within species' ranges, how and why these gradients vary among species, and the capacity of species to utilize these gradients under climate change.\n"
            ],
            "publication_date": "2014-05-22T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 1271,
            "shares": 31,
            "bookmarks": 14,
            "url": "http://www.plosone.org/article/info:doi/10.1371/journal.pone.0098361",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0098361&representation=PDF",
            "fulltext": "IntroductionA variety of responses are available for species as global climate change progresses [1], [2] and changing climatic conditions increasingly impact species' performance and fitness. The two primary options, which are not necessarily independent, are for species' populations to shift their geographic ranges to regions containing suitable climatic conditions, or to remain and adapt phenotypically or genetically [3], [4]. One of the most important immediate responses is for species to track their geographic climatic associations across space. This may be possible provided that opportunities are available for geographic range-shifts, species have the physiological and behavioral capacity to take advantage of them, and they can keep pace with the velocity of climate change [5]. Geographic niche tracking has been observed under climate change over geological time scales [6] and more recently under modern climate change [7], [8].\nTwo environmental gradients appear particularly relevant when considering geographic niche tracking under past and current climate change: surface latitudinal and elevational temperature gradients. Both gradients represent natural environmental features whose location and general form remain consistent across ecological time-scales, and there is empirical evidence that many taxa including birds are currently tracking these gradients under climate change [7], [9]. Latitudinal temperature gradients are strongest outside the tropics, especially in the Northern Hemisphere where the most extensive land masses are located [10]. Elevational temperature gradients are stronger and less variable than latitudinal temperature gradients but are geographically more restricted [11]. Latitudinal and elevational temperature gradients are not equally available for all species nor are they uniformly distributed within species' geographic ranges. Under current climate change projections, there is no evidence to suggest the strength of elevational temperature gradients are expected to change; whereas there is evidence that the strength of latitudinal temperatures gradients may change over broad geographic regions [2]. Due to latitudinal asymmetry in surface warming, latitudinal temperature gradients are projected to weaken in the Northern Hemisphere [2], [12], which is expected to result in diminished seasonality and increased vegetation growth at the high northern latitudes [13], [14].\nLife history, morphological, and physiological traits are known to vary among species' populations at geographic scales [15] and, in some cases, this variation is correlated with latitudinal or elevational gradients [16], [17] and associated temperature gradients [18]. This variation in ecological traits across latitudinal or elevational gradients can in turn affect reproductive potential and rates of population growth, especially at range limits [19], [20]. Under climate change, the structure of latitudinal and elevational temperature gradients can be modified, affecting population growth rates, especially at the leading and trailing edge of the gradient within species' ranges [21], [22]. Specifically, climate change can result in population growth increasing at the leading edge and declining at the trailing edge of the gradient, with the increased likelihood of range shifts at the leading edge and extirpation of populations at the trailing edge.\nThe strength of latitudinal and elevational temperature gradients within species current geographic ranges relative to projected changes in temperature under climate change can determine the potential for range-shifts to occur under climate change. For species whose geographic ranges have similar spatial dimensions in relationship to latitudinal or elevational temperature gradients, the steeper the gradient, the broader the species' geographic climatic niche (i.e., species' association with climatic conditions across space) and the greater the likelihood that a larger component of current climatic associations will be retained within the geographic range as temperatures increase (Figure 1A,B,C). In contrast, the shallower the gradient, the narrower the species' geographic climatic niche and the smaller the likelihood that a significant geographic component of current climatic conditions will be retained within the geographic range as temperatures increase (Figure 1D,E,F). One potential consequence of shallow latitudinal or elevational temperature gradients is the formation of range-shift gaps [10]. Here, increasing temperatures can result in the complete loss of current climatic associations within species' current geographic ranges, an outcome that may substantially hinder successful range-shifts. Range-shift gap width is also likely to be negatively correlated with the strength of the temperature gradients (Figure 1), suggesting species' distributions containing very weak gradients, as found with many tropical species, are particularly susceptible to the consequences of climate change [10]. In total, climate change can modify latitudinal or elevational temperature gradients within species' geographic ranges, which in turn can alter population dynamics at range boundaries and range-shift potential based on the strength of those gradients relative to the magnitude of climate change.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Conceptual figures of six geographic range-shift scenarios for species containing steep (A,B,C) or shallow (D,E,F) latitudinal or elevational temperature gradients within their geographic ranges.The spatial breadth of the latitudinal or elevational gradient within the geographic range is identical in each case. The current temperature gradient (solid line) and the projected temperature gradient under climate change (dashed line) are shown for each scenario. The location of the geographic range of the species along the temperature gradient is depicted with a bold line between the solid blue and red arrows. The species' leading (blue arrow) and trailing (red arrow) range limits are shown based on the current gradient (solid arrows) and the projected gradient under climate change (dashed arrows). Three gradations in projected climate change are shown: weak (A, D), moderate (B, E), and strong climate change (C, F). The horizontal dotted lines identify the boundaries of each species' geographic climatic niche as defined along the temperature gradient. Everything else being equal, species with steeper environmental temperature gradients have broader geographic climatic niches and need to shift their geographic range less in order to track climate change.\ndoi:10.1371/journal.pone.0098361.g001Here, we estimate latitudinal and elevational temperature gradients within the geographic ranges of ca. 90% of the world's avifauna, numbering 9,014 species. To evaluate whether bird species that are currently threatened with extinction also face particularly high risks under climate change, we structure our analysis to compare species classified as threatened (n = 878) vs. non-threatened with extinction (n = 8,136) on the IUCN Red List [23]. We additionally examine the ability of species' geography (range size, shape, and orientation relative to the equator) to explain variation in elevational and latitudinal temperature gradients among species. These characteristics of species' ranges tend to be geographically similar across taxa [24] and are determined in part by spatial variation in climate and topography [25]. Lastly, to assess the degree of correspondence between projected warming and the availability of elevational and latitudinal temperature gradients within species' ranges, we consider how gradient strength is related to variation in projected changes in temperature under climate change.\nMaterials and Methods\nData sources and preparation\nWe acquired range maps for the world's birds from BirdLife International and NatureServe [26] whose data sources are described in Buchanan, Donald and Butchart [27]. We considered breeding/resident ranges only and excluded species that were not well suited for our analysis, specifically those associated with marine environments. This resulted in a total of 9,014 extant species for analysis (see Table S1). These species were classified as either threatened (n = 878) or non-threatened with extinction (n = 8,136) using the IUCN Red List [23]. Here threatened refers to species identified as vulnerable, endangered, or critically endangered under the IUCN Red List.\nSpecies range data was analyzed using a gridded system having a cylindrical equal-area projection and a cell area of 3,091 km2. We defined species' geographic breeding ranges as the spatial combination of equal area cells with ≥50% terrestrial surface area (based on the proportion of the cell that contained non-marine terrain) that intersected each species' geographic range polygon. We placed each species into one of six biogeographical realms [28] based on the realm that contained the greatest proportion of each species' gridded range (Nearctic, Palaearctic, IndoMalaya, Neotropics, Afrotropics, and Australasia). Species whose ranges occurred primarily in Antarctica or Oceania were excluded from analysis. From the 9,014 species, we acquired minimum and maximum elevation associations for 4,978 species from BirdLife International [23]. The distribution of these species across the six realms did not show geographic biases, with the proportion of species in each realm not diverging significantly from expectation (χ2 = 0.029, df = 5, P = 1.0).\nTemperatures within species' geographic ranges were estimated using the annual average of 1950–2000 average-monthly temperatures from WorldClim [29] gridded and analyzed at a 30-arcsec resolution (ca. 1 km at the equator). Elevations within species' ranges were estimated using the USGS global digital elevation model (GTOPO30) gridded and analyzed at a 30-arcsec resolution. Tropospheric lapse rates, the association between temperature and elevation within the troposphere, were estimated globally using a gridded model at a resolution of 2.5° (ca. 278 km at the equator) for the period 1948–2001 [11].\nBroad-scale, expert-based range maps are only coarsely defined estimates of species occurrence and reliably characterize presences only above 100 km grain [30], [31]. Here, we are interested in the form of latitudinal and elevational temperature gradients (i.e., shallow or steep) as defined across the geographic breadth of species' distributions independent of patterns of occurrence. The use of relatively high resolution global estimates of temperature and elevation provided the detail of data necessary to estimate the full structure of these gradients for individual species, especially in situations where geographic ranges were small or temperature gradients were non-linear. For the purpose of this baseline study we assume that low probability of occurrence naturally incurred at the finer spatial grain are not biasing the geographic and species comparisons. We acknowledge that this assumption is not necessarily valid and will require careful follow-up assessments at local scales or at broader scales once more detailed distribution data becomes available.\nProjected changes in temperature (temperature anomalies) were estimated using projections averaged over four Atmosphere Ocean General Circulation Models (AOGCMs). The four AOGCMs were CNRMCM3, CSIRO-Mk3.0, ECHam5, and MIROC 3.2. The gridded projections from these models were downscaled to 30 arc-minute resolution using pattern scaling with the MarkSim weather generator [32]. The projections were based on the A2 scenario, which is currently considered to be the most relevant under current greenhouse gas emission rates [33], [34]. The anomalies represent the change in temperature between the twentieth century control 30-year normal (1961–1990) and the 30-year period 2081–2100. The temperature anomalies were bilinearly interpolated to match the resolution of the 3,091 km2 equal-area grid.\n\n\nElevational temperature gradients\nWe estimated latitudinal and elevational temperature gradients separately in order to better represent each gradient's unique characteristics. The presence of elevational temperature gradients is dictated by the presence of mountain systems or orographic features, which show substantial geographic variation (see Figure S1). Mountain systems often contain strong topographic heterogeneity resulting in a mosaic of climatic conditions, which are poorly estimated using interpolated weather station data [35], [36]. Here, we use elevation data, which does not rely on similar interpolation methods, to estimate elevational temperature gradients within species' ranges.\nUnlike latitudinal temperature gradients, the strength of elevational temperature gradients are broadly consistent from the equator to the poles. Tropospheric lapse-rate averages 6.2°C km−1 over the continents, with a range of 4.5 to 6.5°C km−1 from the polar latitudes to the equator, respectively [11]. To account for this geographic variation, we calculated the average tropospheric lapse-rate within each species' geographic range. To account for the high variability in the extent and spatial distribution of orographic features and the decline in terrestrial area with increasing elevation resulting in strongly skewed distributions of elevation within species' ranges, we estimated two components of the elevational temperature gradient for each species: (1) the overall magnitude and (2) the evenness of the gradient. We then combined tropospheric lapse-rate with magnitude and evenness to generate one metric designed to represent the overall strength of elevational temperature gradients within each species' range.\nMore specifically, for species whose minimum and maximum elevation associations had both been estimated (n = 4,978), we excluded elevations within the geographic range below and above these associations. We then identified the actual minimum and maximum elevations within each species' range based on our gridded elevation data, which in combination with species' documented elevation associations were used to calculate each species' realized vertical range extent. Vertical range extent was used to estimate the magnitude of each species' elevational temperature gradient. To assess how evenly the vertical range extent was distributed across each species' range, we first summed the number of 30 arc-second elevation cells in all 100-m elevational bands found within each species range. We then used the Lorenz curve and associated Gini coefficient [37]–[39], explained below, to estimate how evenly the cells were distributed across elevation bands. Here, the 100-m bands were first ranked from lowest to highest elevation. The frequency of cells within these bands was then used to plot the Lorenz curve, i.e. the cumulative proportion of the number of cells in each band (x-axis) against the corresponding cumulative proportion of their elevations (y-axis). Gini coefficients (or Gini ratio) were then calculated, which is defined as twice the area contained between the Lorenz curve and the line of perfect equality (or perfect evenness). The Gini coefficient has a range from 0 to 1, high to low evenness, respectively. To ease interpretation, for each species we took the product of the vertical range extent, average tropospheric lapse rate, and the inverse of evenness (1 – Gini coefficient) to generate a single index for the elevational temperature gradient. We interpret this index as the overall strength of the elevational temperature gradient across each species' range: larger values indicate the presence of a strong and more evenly distributed gradient; lower values indicate the presence of a weak gradient due to either a small vertical range extent or an unevenly distributed vertical range extent.\n\n\nLatitudinal temperature gradients\nOutside the tropics, average annual temperature declines on average 0.7°C for each degree of latitude in the Northern Hemisphere and on average 0.5°C for each degree of latitude in the Southern Hemisphere (Figure S2). With one degree of latitude equal to approximately 111 km, this translates to a decline of 1°C for every 150 km in the Northern Hemisphere and a decline of 1°C for every 197 km in the Southern Hemisphere.\nWe estimated the latitudinal temperature gradient within the breeding ranges of 9,014 species while controlling for variation in elevation. For our approach, we first found the average tropospheric lapse rate within each species' range. We then compiled gridded 30 arc-second average annual temperatures and elevations within each species' range. We chose annual average temperature because matching each species' breeding season distribution with their unique breeding season temperatures, especially for migratory species that only spend a few weeks to months on the breeding grounds, was not feasible. A likely consequence of using annual average temperature is that estimates of latitudinal temperature gradients for species that breed outside the tropics are likely to be steeper than they would be if estimates were based on the breeding season alone. Beyond these qualitative differences, using annual average temperature is unlikely to alter our qualitative conclusions. To control for the effect of elevation on temperature, we applied quantile regression to temperature as a function of elevation for 11 quantiles (0.0, 0.1,…0.9, 1.0). The residuals for the quantile whose slope most closely matched the tropospheric lapse rate estimated for each species was retained for further analysis. This approach allowed us to interpret latitudinal temperature gradients as the gradient along a fixed elevation within each species' range.\nThe relationship between these residuals and latitude was then assessed for each species using segmented linear regression [40]. If species' ranges occurred in both hemispheres, the range was first split in half at the equator and each portion was analyzed separately. We tested if the relationship between temperature and latitude contained one or two segments using Davie's test [41], which tests for a non-zero difference in the slope parameters of the segmented relationship. Relationships with two segments are more likely to occur if species' distributions are intersected by the Tropics of Cancer or Capricorn (Figure S2). When there was statistical evidence for a segmented relationship, we estimated a break point between segments and slope coefficients for each segment. When there was no statistical evidence for multiple segments, a single slope coefficient was estimated. Coefficients estimated south of the equator were multiplied by −1 to allow for comparison between the two hemispheres (i.e., latitudinal temperature gradients north and south of the equator are both negative; Figure S2). For species whose ranges were estimated by one slope coefficient, this coefficient was used to represent the overall gradient. For species whose ranges contained multiple slope coefficients, we used a weighted average to summarize the gradient. Weights were based on the latitudinal extent of the region where each slope coefficient was estimated. Based on this procedure, larger negative values indicate steeper latitudinal temperature gradients and values approaching zero indicate shallower latitudinal temperature gradients.\n\n\nAnalysis\nTo summarize how steep elevational and latitudinal temperature gradients are on average within species' ranges globally, we first calculated the average elevational temperature gradient and median latitudinal temperature gradient within each equal-area cell for all species whose geographic ranges intersected that cell. To summarize how gradients differed for threatened and non-threatened species, we quantified how the gradients varied by species' median latitude. This analysis was implemented separately for species in each of the six biogeographical realms. For elevational temperature gradients, we used generalized additive models (GAM; [42]) with a Gaussian error distribution. For latitudinal temperature gradients, we used robust MM-type polynomial regression [43], [44]. GAM was selected because it adjusts automatically to the nonlinear associations observed between elevational temperature gradients and species' median latitude. This feature also supported the use of GAM to examine how species richness, range size, elevational extent, and Gini coefficient varied by species' median latitude for threatened and non-threatened species. Robust polynomial regression was selected for latitudinal temperature gradients because of the presence of many extreme outliers that were poorly modeled with GAM. In addition, the general curvilinear form of the relationship was well represented with a polynomial function. The order of the polynomial function was determined using robust Wald-type or deviance-type tests of nested model pairs. Median latitude was defined as the median of the full latitudinal extent of each species' range.\nWe used robust multiple regression to examine the ability of geographic and climatic predictors to explain the variability observed in species latitudinal and elevational temperature gradients. The four predictors were estimated within each species' geographic range and included average projected temperature anomaly, and geographic range size, shape, and orientation. Range size was log10 transformed before analysis. Range shape and orientation were estimated based on a principle component analysis (PCA) of the latitude and longitude of the grid cells contained within each species geographic range. PCA was conducted using a singular value decomposition of the centered data matrix. The square roots of the two eigenvalues were used to estimate range shape, where the minimum eigenvalue was divided by the maximum eigenvalue. Here, values approaching zero indicating elongated ranges and values approaching one indicating circular ranges. Range orientation was estimated based on the angle of the major axis of the PCA eigenvector from the equator. Values for range orientation were defined from 0° (parallel with the equator) to 90° (perpendicular to the equator). We evaluated our four predictors for multi-collinearity and singularity using variance inflation factors (VIFs) where predictors with VIF>5 indicate cause for concern and VIF>10 indicate the presence of significant collinearity [45]. We retained all four predictors after they were deemed to be statistically independent (VIF≤1.6).\nTo evaluate differences between threatened and non-threatened species, we included in addition to the four continuous predictors, a species' IUCN Red List status as threatened or non-threatened with extinction. For the analysis of the latitudinal temperature gradient, we also considered if the median latitude of a species' range was located within or outside the tropics. Median latitudes between 23.5°S and 23.5°N latitude were considered tropical. The tropical/extratropical classification was included because it defined a major feature of the gradient (Figure S2). In total, we examined eight robust multiple regression models that consisted of the two gradients, the four continuous predictors, associated categorical predictors, and all pairwise interactions.\nAll analyses were conducted in R version 3.0.1 [46]. Segmented regression was conducted using the ‘segmented’ library, robust regression using the ‘robust’ and ‘robustbase’ libraries, and GAM using the ‘mgcv’ library [42]. We used the default optimization procedure to estimate the degree of smoothing in the GAMs [42]. The Lorenz curves were estimated using the ‘ineq’ library and the Gini coefficients were estimated using the ‘reldist’ library.\n\nResultsThreatened and non-threatened bird species presented similar latitudinal trends in species richness with peaks for both groups occurring in the tropics (Figure 2A and Figure S3). Range sizes for threatened species were significantly smaller on average except for species located at the most extreme northern and southern latitudes (Figure 2B and Figure S3). For the 4,978 species with documented elevational associations, threatened species had smaller vertical range extents within their geographic ranges (Figure 2C and Figure S3) and threatened species had higher elevational evenness within their geographic ranges in the central tropics and more equivalent elevational evenness elsewhere (Figure 2D and Figure S3). Projected increases in temperature within species' ranges were highest for species located at higher latitudes in the Northern Hemisphere and lowest for species located at lower latitudes in the Southern Hemisphere (Figure S4).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Plots summarizing geographic patterns for species' currently identified as threatened (red points; n = 878) and non-threatened (green points; n = 8,136) with extinction by the median latitude of each species' range.(A) The number of species' whose median latitudes occur within one degree latitudinal bands; (B) species' range size; (C) the elevational extent within species' ranges for 4,978 species with minimum and maximum elevation associations; and (D) the evenness of the distribution of 100 m elevations bands within species' ranges based on the Gini coefficient (0 =  high evenness; 1 =  low evenness; see Materials and Methods for details) for 4,978 species with minimum and maximum elevation associations. The trend lines are the fits of generalized additive models for species threatened (red) and non-threatened (black) with extinction.\ndoi:10.1371/journal.pone.0098361.g002There was strong geographic variation in the steepness of elevational temperature gradients (Figure 3). When considering the combined effects of elevational extent and elevational evenness (Figure S3) with tropospheric lapse rate, the strongest gradients were found for species located in Africa, the western part of North and South America, and the Tibetan Plateau region. In contrast, species in Australasia, the islands of IndoMalaya and the lowlands of the Neotropics had the weakest elevational temperature gradients (Figure 3). Threatened species were associated with weaker elevational temperature gradients across the six biogeographical realms, with the most significant differences occurring in the Neotropics and Afrotropics (Figure 3).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  The elevation temperature gradient index summarized across species' geographic ranges (map) and summarized within six biogeographical realms (plots) as a function of the median latitude of each species' range.The temperature gradient was estimated spatially within species' current ranges (see Materials and Methods for details). In the plots, red points are threatened species (n = 766) and green points are non-threated species (n = 4,212). Trend lines are the fits of generalized additive models for threatened (red) and non-threatened species (black). The solid line is the equator and the dashed lines are the Tropics of Cancer and Capricorn (23.5°N and 23.5°S latitude, respectively).\ndoi:10.1371/journal.pone.0098361.g003After controlling for elevation, latitudinal temperature gradients were on average shallowest for species in the tropics and steepest for species outside the tropics (Figure 4). The steepest latitudinal temperature gradients occurred in the Northern Hemisphere. Australasia had the strongest gradients in the Southern Hemisphere, which did not match the strength observed in the Northern Hemisphere. There was no evidence that the latitudinal temperature gradients differed between threatened and non-threatened species across the six biogeographical realms (Figure 4).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  The latitudinal temperature gradient index summarized across species' geographic ranges (map) and summarized within six biogeographical realms (plots) as a function of the median latitude of each species' range.The temperature gradient was estimated spatially within species' current ranges (see Materials and Methods for details). In the plots, red points are threatened species (n = 878) and green points species non-threated species (n = 8,136). Trend lines are the fits of generalized additive models for threatened (red) and non-threatened species (black). The solid line is the equator and the dashed lines are the Tropics of Cancer and Capricorn (23.5°N and 23.5°S latitude, respectively).\ndoi:10.1371/journal.pone.0098361.g004Variation in elevational temperature gradients among species was poorly explained by our four predictors (Table 1 and Figure S5). Species with higher projected temperature increases and larger geographic ranges had steeper elevational temperature gradients, and this was the case for both threatened and non-threatened species. Species with geographic ranges that were elongated in shape and oriented perpendicular to the equator had stronger gradients, and this was more pronounced for non-threatened species.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Coefficients and test statistics from robust linear models examining predictors of elevational temperature gradients as estimated within the geographic ranges of 4,978 bird species.doi:10.1371/journal.pone.0098361.t001Variation in latitudinal temperature gradients was better explained by our four predictors, with evidence for significant and strong differences between tropical and non-tropical species (Table 2 and Figure S6). Tropical species had weaker relationships for all four predictors with little evidence for differences between threatened and non-threatened species. For threatened and non-threatened extratropical species, species with higher projected temperature increases, larger ranges and ranges that were elongated in shape and oriented parallel with the equator had steeper latitudinal temperature gradients. For threatened extratropical species, gradients were even steeper for species with larger ranges.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Coefficients and test statistics from robust linear models examining predictors of latitudinal temperature gradients as estimated within the geographic ranges of 9,014 bird species.doi:10.1371/journal.pone.0098361.t002Species face unique challenges under climate change, with the availability and strength of range-shift opportunities along latitudinal or elevational temperature gradients playing a significant role in determining species' likelihood of persistence. In this study we provide the first estimation of these gradients for the world's avifauna relative to species' IUCN Red List category, geography, and projected exposure to climate change. Our approach expands upon current methods for estimating species' geographic climatic associations by measuring these two gradients individually using fine-grained environmental data, which more accurately captures their unique spatial structures. The application of this approach has the potential to increase biological realism and predictive quality in current large-scale modeling efforts by removing problematic assumptions and minimizing potential biases. Our findings suggest threatened species are at a distinct disadvantage globally based on their association with shallower elevational temperature gradients. Latitudinal temperature gradients are primarily relevant for species located outside the tropics, a region where few bird species occur year-round, including threatened species whose associations with latitudinal temperature gradients did not differ from non-threatened species. Our results also indicate a degree of correspondence between range-shift opportunities and climate change, in that species in regions that are projected to experience greater warming are more likely to be associated with steeper temperature gradients.The strength of latitudinal temperature gradients is determined in part by the location, size, shape and orientation of species' geographic ranges. Not surprisingly, the strongest latitudinal temperature gradients were associated with species whose ranges occurred outside the tropics, especially at the higher latitudes in the North Hemisphere where the largest land masses are located. Species in these regions tend to have larger geographic ranges (see Figure 2B) and ranges that tend to be elongated parallel to the equator. As our results indicate, the strength of the latitudinal temperature gradient is of sufficient size within this region that geographic ranges with narrow latitudinal extents are still able to capture a significant gradient. In contrast, the spatial distribution of elevation temperature gradients reflects the form and extent of orographic features and how species' distributions intersect these features. We found that the strength of these gradients varied substantially across the globe and, unlike latitudinal temperature gradients, differed significantly between threatened and non-threatened species. In agreement with other studies [47], [48], species in Australia and the Islands of IndoMalaya stood out as particularly vulnerable, with some of the shallowest elevational gradients, followed by species located in the lowlands of the Neotropics. In contrast to latitudinal gradients, our ability to predict species' elevational temperature gradients was limited, reflecting the greater geographic heterogeneity in the location and structure of these gradients.The strength of both temperature gradients was positively correlated with projected changes in temperature, but only for species outside the tropics with latitudinal temperature gradients. The greatest increases in temperature under climate change are projected to occur at the high northern latitudes (see Figure S4). Our findings therefore suggest a geographic correspondence with the greatest projected warming occurring in regions with the strongest latitudinal temperature gradients. However, few bird species occur at the high northern latitudes year round. The proportion of migratory species (ca. 19% of extant bird species [49]) within breeding communities increasing linearly as you travel north from the equator, approaching 100% at the highest latitudes [50]. Migratory species spend the bulk of their annual cycle in migration or on the non-breeding ranges at lower latitudes. Thus, very few bird species are likely to benefit substantially from this correspondence.IUCN Red List categories of extinction risk are assigned using the Red List criteria, which have quantitative thresholds relating to population and range size, structure and trends [23]. Only 3.5% of threatened or near threatened bird species (77/2193) are assessed as threatened by climate change so severely that the global population may be declining rapidly or very rapidly over three generations, and hence potentially contributing to their Red List category through the A criterion. Of these, only 26 (1.2%) actually qualify under the A criterion. Our findings suggest this proportion is likely to increase, especially for threatened species associated with shallow elevational temperature gradients. As confirmed in our analysis, threatened species occur primarily in the tropics and their distributions are defined by smaller geographic ranges and elevational extents. Hence, threatened species may be at a geographic disadvantage under climate change with limited associations with elevational or latitudinal temperature gradients, although we found no evidence for the latter. Our findings suggest threatened species that occur in the central tropics have higher elevational evenness across their range, which is a likely consequence of their distributions occurring on steep montane slopes outside of lowland areas, which should result in stronger elevational temperature gradients for these species. However, our analysis suggests that the differences in elevational evenness between threatened and non-threatened species was not substantial enough to overcome the smaller elevational extents contained within threatened species' geographic ranges. Thus, our findings suggest the weaker elevational temperature gradients identified for threatened species is due in large part to a consistent association with smaller elevational extents within their ranges.When considering both gradients in combination, two regions stand out as having both strong elevational and latitudinal temperature gradients: western North America and Central Asia (see Figures 3–4). These results suggest species in these regions will have greater opportunities to track their climatic niches geographically. Depending on how these temperature gradients intersect spatially within species' ranges, some combination of these gradients could be used. Similarly, two regions stand out as having both shallow elevational and latitudinal temperature gradients: the islands of IndoMalaya and the Amazon Basin in South America (see Figures 3–4). Species in these regions are likely to develop substantial range-shift gaps under climate change [10], resulting in limited opportunities to track their climatic niche geographically. Bird species occurring in the Amazon Basin are also considered to have low adaptive capacity, which may increase their vulnerability to climate change [51].Species in tropical montane regions are more likely to have restricted lateral and vertical distributions [48], [52] and are also more likely to be threatened with extinction (see Figure S3). Even though in many cases these species occur in some of the most extensive montane systems in the world, our findings suggest that the overall steepness of elevational gradients is constrained by their limited representation within species' geographic ranges. Local climatic heterogeneity, however, may allow for microclimatic adjustments that could compensate for broader climatic trends [53]–[56]. The level of climatic heterogeneity is determined by local topographic variation and also by species' habitat associations [57]. For example, species in forested habitats may have greater opportunities for microclimatic adjustments relative to species in open habitats [58]. In general, however, behavior resulting in microclimatic adjustments might not be sufficient to address the full consequences of climate change [59].Even if latitudinal and elevational temperature gradients are readily available, dispersal along these gradients is not guaranteed. Dispersal abilities are determined in large part by species' morphological, physiological and behavioral characteristics and adaptations [60], [61], all of which are likely to be altered under rapid climate change [62]. Dispersal under climate change may be constrained by unsuitable habitat, population or habitat fragmentation [58], [63], or interspecific competition [64]. In addition, gene flow that occurs in association with dispersal can both support and hinder range-shift potential [65]. Gene flow among populations at the leading edge of the range has been found to benefit fitness, whereas gene flow from the interior to the leading edge has been identified as maladaptive [66], [67]. Dispersal abilities also vary tremendously among species and between taxa [60]. Even highly vagile taxa such as birds do not necessarily have the full complement of behavioral or physiological characteristics needed to support successful dispersal, which is considered particularly relevant for tropical species [68], [69]. Although there is evidence tropical montane birds have moved upslope under climate change [70], the low annual variation in temperatures within the tropics relative to temperate regions may result in stronger physiological barriers to dispersal for tropical species [52], [71], [72]. When tropical habitats are fragmented through human activities – a process that is occurring at an increasing rate – dispersal can become even more constrained [73], [74]. If species' responses are hindered by dispersal limitations or interspecific competition, as is expected for many tropical montane bird communities, substantial ecological disruptions are likely to occur [75].In contrast, species that breed outside the tropics that are also migratory [76] or have larger geographic ranges tend to have stronger natal dispersal abilities [77]–[79]. Thus migratory species that are broadly distributed and breed outside the tropics might be in a better position to track elevational or latitudinal temperature gradients. However, exceptions exist [80] and factors that are strongly correlated with dispersal abilities might not be similarly correlated with broad-scale distributional responses under climate change [81]. Nevertheless, current evidence indicates distributional responses for birds along latitudinal [7] and elevational temperature gradients [82] are lagging behind warming trends, suggesting stronger responses could develop as climate change progresses and time lags are overcome [7].We estimated temperature gradients in this study using relatively high resolution environmental data that was finer than the resolution of the species' range data [30]. This grain mismatch resulted in temperature gradients likely being estimated over fine-grain sections of species' ranges where probability of occurrence is low. Our main results focus on the geographic form of the gradient across the spatial breadth of species' distributions, but future work will benefit from further examination of potential biases this grain mismatch may have on comparisons. How patterns of occurrence are structured within species' ranges, based on proportion of occupancy and level of aggregation [83], may affect the chances of successful dispersal along these gradients. For example, sparse or fragmented patterns of occurrence may diminish the chances of successful dispersal, while more continuous patterns of occurrence may improve them. Alternatively, large contiguous regions with low probabilities of occurrence might contain the steepest gradients with very little dispersal value, as when lowland species' distributions border mountain systems. These remaining uncertainties highlight the need for advancing the spatial grain of our global biodiversity knowledge and for the necessary collaborative data mobilization, integration and quality control to realize this [31].In addition to temperature, other climate factors and conditions are likely to be important in determining the strength and importance of latitudinal and elevation temperature gradients in defining range-shift potential. Two that have received particular attention are changes in precipitation regimes and changes in climatic variability. In contrast to temperature, precipitation has more complex spatial and temporal variability and more complex associations with latitudinal and elevational gradients. Projected changes in precipitation are also more ambiguous [2], which adds uncertainty to current estimates on how species are likely to respond to climate change [84]. A second factor is climatic variability, which is projected to increase through an increased frequency of extreme climate events [2]. Temporal climatic variability is considered an important factor defining species' distributions limits [85] and increasing climatic variability can have varied effects on species' populations [86] including the potential to hinder range shifts [87] and, especially under rapid climate change, increase extinction risk [88].Additional research is needed to improve our understanding of how latitudinal and especially elevational temperature gradients are structured within species' ranges and the ability of species, especially tropical, to take advantage of these gradients. In particular, studies using higher resolution climate data (e.g., [22]) for species with varying spatial patterns of occurrence and forms of elevational or latitudinal gradients would be informative. Birds are often used as a model biological system due to high data availability, but less vagile taxa also need to be considered such as plants or vertebrate ectotherms that have alternative modes of dispersal and different physiological and behavior capacities. With birds, examining how these gradients are structured within non-breeding ranges, where migratory birds tend to occur during the majority of the annual cycle and are typically less well studied, would be beneficial [9]. Efforts to better understand the role of precipitation and climate variability in defining range-shift potential along latitudinal and elevational temperature gradients would support more comprehensive inferences.Current efforts to project species' distributions or to estimate movement corridors under climate change often assume perfect and complete dispersal along temperature gradients (e.g., [89]). Temperature gradients are typically estimated using coarse-grained interpolated weather-station data based on current climatic conditions where the contrasting spatial forms of latitudinal and especially elevational temperature gradients are often poorly summarized [35], [36]. These methods may exaggerate estimated dispersal distances [90], especially at high northern latitudes where latitudinal temperature gradients are projected to weaken under climate change [12]. These considerations in addition to the correlative nature of these models have promoted the development of more mechanistic or process-based methods intended to improve biological realism and minimize bias in current projections [91]. Our findings suggest the quality of model predictions may be improved by considering the spatial form and strength of species' demographic responses [92] to projected latitudinal and elevational temperature gradients within species' ranges and along possible movement corridorsLastly, in order to maximize range-shift potential under climate change, conservation efforts are needed to maximize the size of areas of contiguous habitat, minimize the extent of habitat fragmentation and promote the permeability of the matrix of habitats around key sites. In particular, efforts to assess and promote connectivity at broad geographic extents are likely to be the most beneficial (e.g., [93], [94]) where existing gradients are represented as continuously as possible across space [95]. The more thoroughly this can be achieved, the greater the potential for species to track changing climatic conditions based on their unique dispersal abilities and ecological and environmental requirements."
        }
    },
    "subject-physics": {
        "10.1371/journal.pone.0092566": {
            "author_display": [
                "Rab Nawaz",
                "Muhammad Ayub",
                "Akmal Javaid"
            ],
            "title_display": "Plane Wave Diffraction by a Finite Plate with Impedance Boundary Conditions",
            "abstract": [
                "\nIn this study we have examined a plane wave diffraction problem by a finite plate having different impedance boundaries. The Fourier transforms were used to reduce the governing problem into simultaneous Wiener-Hopf equations which are then solved using the standard Wiener-Hopf procedure. Afterwards the separated and interacted fields were developed asymptotically by using inverse Fourier transform and the modified stationary phase method. Detailed graphical analysis was also made for various physical parameters we were interested in.\n"
            ],
            "publication_date": "2014-04-22T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 1,
            "views": 346,
            "shares": 0,
            "bookmarks": 0,
            "url": "http://www.plosone.org/article/info:doi/10.1371/journal.pone.0092566",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0092566&representation=PDF",
            "fulltext": "IntroductionDiffraction theory can be applied successfully to reduce the noise due to heavy traffic, environmental pollution and industrial growth by means of barriers in heavily built up areas. A barrier should be good attenuator of sound and inexpensive at the same time. Such barriers may have absorbing lining on the surfaces and satisfy impedance boundary conditions as well. The scattering of sound and electromagnetic waves has been studied extensively since the half plane problems were investigated by Poincare [1] and Sommerfeld [2]. Many classical problems related to electromagnetic waves diffraction due to line source and point source have been studied so far. These problems constitute a canonical problem for the GTD (geometrical theory of diffraction). Scattering analysis by metallic tapes on paneled compact range reflectors [3] and the line source diffraction of electromagnetic waves by a perfectly conducting half plane was investigated by Jones [4]. Rawlins [5] then considered line source diffraction by an acoustically penetrable or an electromagnetically dielectric half plane having smaller width as compared to the incident wave length. In continuation to this, diffraction by an absorbent semi-infinite plane having different impedance faces [6] is also examined. Rawlins [7] used Ingard's conditions [8] on the boundaries to discuss scattering of sound waves by a half plane. Later on Rawlins' idea is extended to calculate the diffraction by finite strip [9] and diffraction of spherical acoustic wave from an absorbing plane [10]. A related study of diffraction by a finite airfoil in uniform flow is presented by Jeon et al. [11]. Myers' presented an improved form of impedance boundary conditions [12] which were used in [13], [14] for sound wave diffraction problems.\nDiffraction by strips is a significant and classical subject both in electromagnetic and acoustic wave theory. In specific, scattering from resistive, conductive and impedance strips have been considered by Herman et al. [15], while, Senior has also made an attempt to solve a problem related to resistive strip configuration [16]. Many analytical, numerical or approximate analytical methods have been used to study a single or multiple diffraction patterns from a strip. To name a few for example, geometrical theory of diffraction [17], Kobayashi's potential method [18], [19] spectral iteration technique (SIT) [20], method of successive approximations [21] and the W-H technique [22] have positively been utilized. Some recent advances in the literature are also found on Bessel's potential spaces [23] and Maliuzhinetz-Sommerfeld integral representation [24].\nKeeping in view the aforementioned studies, the major aim of this article is to discuss a wave diffraction problem relating field and its normal derivative as first order impedance conditions which are referred to as standard impedance conditions. These conditions are used as they introduce simplification in calculations to make the problem tractable and to achieve a solution simple enough to use. The impedance conditions can be used effectively for the problems dealing with material surfaces whose solution would be impractical without them. Such conditions are widely used to give analytical solutions to canonical problems. This article provides the comprehensive treatment of impedance boundary conditions applied to electromagnetic. The analytical solutions are amenable to develop high frequency electromagnetic scattering codes and should therefore be of interest to practicing engineers as well as researchers concerned with high frequency diffraction by impedance structures. As mentioned earlier, the finite strip problems have been solved by many researchers who considered different impedance boundary conditions. In the present analysis, the solution to plane wave diffraction by a finite conducting plate with impedance type boundary conditions is produced by the two edges of the finite plate.\nThe structure of the paper is organized as follows. In Section 0, governing problem which is composed of Helmholtz's equation, impedance boundary conditions and continuity conditions, is stated along with its geometrical configuration. The integral transforms are introduced to convert the problem in complex plane so that two unknown functions are defined. The three part boundary value problem is simplified in terms of two Wiener-Hopf functional equations in Section  from which we derive integral equations in Section  using the standard Wiener-Hopf procedure. The procedure is inspired by the book of Noble [22] which is concerned with the application of the Wiener-Hopf technique to the problems involving semi-infinite and finite geometries and discusses a wide range of extensions. In Section , analytic approximation for the two unknown functions are derived using asymptotic analysis of the integral equation for large complex argument. The analytical expressions for separated and interacted fields at both edges are computed. In Section  the amplitude of the separated field (which contributes in a physical situation) versus observation angle is tested graphically while problem is concluded in Section  It is mentioned that the time factor is supposed to be  and neglected throughout the analysis.\nStatement of the Boundary Value ProblemThis section is dedicated to yield the geometric configuration, governing mathematical equation, corresponding boundary conditions and the transformation used to obtain standard Wiener-Hopf functional equations.\nConsider the scattering of a time harmonic, plane wave incidence by an impedance finite plate having specific impedance say . A plate of length    is encountering a small gust with uniform flow  parallel to the finite plate as shown in Figure 1.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Geometrical configuration of the problem.doi:10.1371/journal.pone.0092566.g001The finite plate is assumed to be infinitely thin and straight. For analysis purpose, it is convenient to express the total electric field as follows(1)where  is the plane wave incident field and is given by(2)while  denotes the field reflected from the finite plate at  and is given by(3)here for an absorbent surface it is required that  The diffracted electric field  satisfying the following Helmholtz's equation in the range  is given by(4)where  is the free space wave number and for analytic convenience we shall assume that   It is supposed that the medium is slightly lossy and the solution for real  is obtained by letting  The under considered boundary value problem is expressed in terms of the reduced potential in dimensionalized form, and it is appropriate to denote separated field in the different regions.\nAs we are interested in determining the diffracted field due to plane wave incidence on the impedance finite plate, Neumann and Dirichlet conditions are imposed along the plate line in mixed type. Different impedance conditions are imposed on upper and lower faces of the conducting plate. Therefore the total diffracted field  (which may be named as diffracted electric field due to a conducting plate) is to be determined with the following boundary and continuity conditions(5)and\n(6)(7)\nThe boundary conditions (5) are the first order impedance conditions relating field and its normal derivative as outlined by Senior et al. [25]. These impedance boundary conditions were subsequently used to model radio waves propagation along the surface of earth and near conducting obstacles. The detail discussion and practical importance of these impedance conditions can be found in [26]. The obstacle (finite conducting plate) occupies   with the velocity of the moving fluid parallel to the axis having magnitude . The fluid flow is considered as uniform flow moving along the plate. The governing equations are linearized and the special effects of viscosity, thermal conductivity and gravity are ignored while the fluid is assumed to have a constant density (incompressible fluid) and sound speed .\nProblem in Transform DomainFor the solution of boundary value problem (4–7), let us introduce Fourier transform with respect to variable  as(8)for  While taking into account the asymptotic behaviors of  for  as\n(9) is a regular function of  in  and  to be regular in  and  to be analytic in the common region  which will provide the analytic region for the use of Wiener-Hopf technique, hence.\n\n(10)\nNow for a plane wave incident on a finite plate,  and the incident field in the transformed domain in the region ,  gives.(11)\nAlso the reflected field  in the transformed domain  is given by.(12)\nThe Fourier transform of Eqs. (4–7) yields.(13)where  with , also equation (13) is valid for any  in the strip  The Fourier transform of boundary conditions (5–7) gives\n(14)(15)and(16)\nThe solution of Eq. (13) satisfying radiation condition as  is given by.(17)Now with the help of Eqs. (14–17), the following Wiener-Hopf functional equations are computed(18)(19)where(20)(21)(22)(23)(24)Since Eqs. (18–19) are the Wiener-Hopf equations therefore we proceed to find the solution for these equations in next section."
        },
        "10.1371/journal.pone.0058770": {
            "author_display": [
                "Agustín Pérez-Madrid",
                "Luciano C. Lapas",
                "J. Miguel Rubí"
            ],
            "title_display": "A Thermokinetic Approach to Radiative Heat Transfer at the Nanoscale",
            "abstract": [
                "\n        Radiative heat exchange at the nanoscale presents a challenge for several areas due to its scope and nature. Here, we provide a thermokinetic description of microscale radiative energy transfer including phonon-photon coupling manifested through a non-Debye relaxation behavior. We show that a lognormal-like distribution of modes of relaxation accounts for this non-Debye relaxation behavior leading to the thermal conductance. We also discuss the validity of the fluctuation-dissipation theorem. The general expression for the thermal conductance we obtain fits existing experimental results with remarkable accuracy. Accordingly, our approach offers an overall explanation of radiative energy transfer through micrometric gaps regardless of geometrical configurations and distances.\n      "
            ],
            "publication_date": "2013-03-18T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 1,
            "views": 796,
            "shares": 6,
            "bookmarks": 9,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0058770",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0058770&representation=PDF",
            "fulltext": "IntroductionThere is a general consensus that the search for clean sources of energy with no climatic and environmental impact constitutes a major strategic objective at present. In this sense, nanoscale thermal radiation conversion offers a source for intensive clean energy generation. Thermal radiation has always been an active field of study. Throughout the 19th century, great scientists (Boltzmann, Stefan, Rayleigh, etc.) dedicated a considerable effort to this problem which was completely solved at the turn of the 20th century due to Planck's contribution to the founding of quantum mechanics. After this, it seemed to be a well-established fact that the maximum power extracted from a hot body depended on the temperature as .\nHowever, recently, thanks to modern technological advances, it has been shown that energy exchange through thermal radiation at nanometric distances breaks by several orders of magnitude the limits posed by Stefan-Boltzmann law for black body radiation. Moreover, near-field thermal radiation is approximately monochromatic and reveals itself coherent in space and time, which may lead to stationary interference phenomena in a micro-cavity. Therefore, this monochromaticity and coherence along with the overcoming of Stefan-Boltzmann limits, all of these distinguishing features together confer near-field radiation a great potential for future applications in nanotechnology and, as we have said at the beginning, energy conversion as well. Several reviews on this issue have been written recently, see Ref. [1]–[3] by way of example.\nAn ever-increasing number of investigators has marked the recent history of the research on near-field radiation. Polder and van Hove [4] first studied heat transfer between two objects at nanometric scales maintained at different temperatures by following a stochastic or fluctuational electrodynamics formalism established by Rytov et al. [5]. Recently, it has been emphasized [6] that surfaces modes as included in the solution of Maxwell's equations in Ref [4] can greatly enhance the heat flow. Experiments showing that the heat flow at the nanoscale is indeed greater than the blackbody radiation limit among materials supporting surface modes were reported in Ref [7], [8] and between two gold surfaces in Ref [9]. Likewise, Pendry [10] gave a simple derivation of the expression found by Polder and van Hove and some interpretation in terms of heat transfer channels in addition to a discussion of the maximum heat flux. The problem was reformulated by using a Landauer-like approach by Biehs et al. [11]. Finally, Sasihithlu and Narayanaswamy [12] performed a discussion of the proximity approximation. All these approaches have something in common: the linear response regime and, consequently, the fluctuation-dissipation theorem (FDT), whose validity is not guaranteed at this level. Thus, an approach based on the contributions of fluctuating dipole effects seems to be the heart of a considerably simplified treatment of energy transfer at the nanoscale [7], [8], [13]. These approaches include a wide range of phenomena in which the energy between molecules is dominated by dipole-dipole interactions, also known as the Förster energy transfer [14].\nNonetheless, as two nanostructures thermalized at different temperatures come closer to each other, the distribution of charges and currents becomes asymmetric and therefore, defies description in terms of dipolar interactions. Hence, it becomes clear that one must bear in mind higher order effects beyond the dipole [14] and also include other contributions to thermal conductance quite common in disordered amorphous materials, leading to a generalization of the FDT mentioned above [16].\nIn this article, we will go deeper into these aspects by showing how phonon-photon coupling effects account for microscale radiative energy transfer by considering non-Debye relaxation due to the excess of modes in the low and high frequencies in the bulk material [15]–[17]. In the current literature on disordered systems, the non-Debye refers to an excess of modes of vibration over the Debye level observed in inelastic light (Raman) and neutron scattering. From here, we obtain a general expression for the heat transfer coefficient including both Debye and non-Debye contributions, providing an overall explanation of the energy transfer through micrometric gaps. The findings of our theory fit existing experimental results with a high degree of accuracy.\nMethodsOur theoretical framework has been described in previous publications [16], [17] and is briefly summarized here. We consider a gas of quanta distributed in phase space according to the probability density , where , , and , with  and  being the momentum and position of a quanta, respectively. Here, we must pay attention to the fact that  possesses dimensions of ,  being Planck's constant. The thermodynamic description that we propose entails the formulation of the second law of thermodynamics, which can be carried out by means of the Gibbs entropy postulate [18],(1)This equation gives us the nonequilibrium entropy of the gas of quanta plus the bath, with  being the equilibrium entropy and  the equilibrium probability density.\nIn general, entropy is produced due to irreversible processes, in such a manner that irreversible processes in nonequilibrium systems are described by means of currents, thermodynamic forces (affinities), and the entropy production rate, which is always positive. It is precisely this positive character of the entropy production that enables us to derive relaxation equations. Since the probability density is conserved the existence of a density gradient  yields a current  which unleashes a relaxation process . In addition, this current satisfies the relation , derived from the entropy production, which can be obtained from Eq. (1) [16], [18]. Here,  is a material-dependent quantity, the matrix of diffusion coefficients, satisfying Onsager's symmetry principle. In general, due to the tensorial character of the diffusion matrix, both currents  and  are coupled(2)where  are the diffusion matrix components.\nThe main contribution of this article is to apply this formalism to the description of the radiative heat transfer between a nanosphere and a plate at different temperatures,  (hot) and  (cold), separated by a distance  (see Fig. 1). We assume that heat transfer results from two different mechanisms. (I) On one hand, we consider a conventional radiative heat exchange involving the dynamics of quasiparticles as the result of two simultaneous processes: elastic emission and absorption of hot photons from the medium at  and elastic emission and absorption of cold photons from the medium at ; these processes also involve the presence of surface modes [6], [10]. (II) The second mechanism behind heat transfer has to do with the excitation of coupled resonant modes from the collection of acoustic states related to defective soft structures in a disordered regime [19]–[21]. This process encompasses inelastic scattering of the impinging radiation, which is linked to nonequilibrium contributions. In both scenarios, there is no diffusion in configuration space since quanta are massless particles [16], and as a consequence , which brings about . Hence, from Eq. (2) one obtains the appropriate diffusion current in momentum space(3)where  is the relaxation time, which depends on the diffusion matrix components.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Schematic diagram of the radiation exchanged via conventional radiative transfer.Elastic collision of photons with atoms or molecules of materials and phonon-photon coupling contributions between a sphere and a plate maintained at different temperatures,  and , separated by a distance .\ndoi:10.1371/journal.pone.0058770.g001\nNear-field analysis\nIn the near-field regime, confinement of the electromagnetic waves in a micrometric gap separating neighboring nanostructures introduces peculiar effects in the spectrum of the thermal radiation. Here is where the collective modes (phonons) excited in the material by the impinging radiation come into play. To discern whether confinement effects are important or absent we shall assume a cut-off wavelength, the thermal wavelength of a photon , which is proportional to the Wien's displacement law through a proportionality constant, i.e. , where here . Actually, when  we have a blackbody spectrum of radiation. One may wonder what happens when . Since according to Heisenberg's principle , assuming that the maximum value of  is , one obtains , and thus the minimum value of  is , leading to , with . Here,  is, precisely, the inverse of the refractive index. Hence, not all frequencies are possible and the frequency of resonance  appears.\nIn these circumstances, integrating by parts the resultant continuity equation gives us(4)with , where we have assumed that  at . Performing a second integration of Eq. (3) through from 1 to 2, we find the net current(5)where  is related to the population of quasiparticles at  and . Moreover, , with  corresponding to a hierarchy of relaxation times ubiquitous in complex systems.\nIn the stationary state, once the system has thermalized at temperatures  and , ; the factor  comes from the polarization of a photon and  being the averaged number of quasiparticles in a elementary cell of volume  of the phase-space given by Planck's distribution [22], . Besides, ; therefore leading to a stationary value, i.e. . Thus, the heat flow  can be obtained from the sum of all the contributions as(6)where , with  being the unit vector in the direction of , and the distribution of frequencies is given by(7)\nAt this point, it is worthwhile to make a short digression about the physical meaning of the time scale . It has been known for a long time [23] that for most condensed systems in time-dependent fields, the orientation polarization behavior can, as a good approximation, be characterized by a relaxation time distribution (); this behavior is generally meant as dielectric relaxation. In harmonic fields, this implies that the complex dielectric permittivity in the frequency range corresponding to the characteristic times for the molecular reorientation can be written as(8)where  is the static dielectric constant and  is the permittivity at the infinite frequency. Here, the relaxation time distribution  satisfies the normalization condition,  and Eq. (8) constitutes a generalization of the Debye treatment based on Clausius-Mossotti equation [24](9)This Debye's equation, Eq. (9), follows after Fourier-transforming the relaxation function  which coincides with the normalized dielectric function . Note that if we assume a time-independent time scale  in Eq. (5), after integration we shall obtain the Debye relaxation function in terms of . Hence, we can understand  as defined through the relation(10)which shows that the Callen-Welton FDT [25] is not valid at this level. In fact, unlike here, the FDT is related to decaying equilibrium fluctuations characterized, precisely, by a single relaxation time.\nNow, let's return to the main topic after that brief digression. Note that according to Eq. (6), in the limit ,(11)giving us the blackbody radiation limit provided , . On the other hand, in the limit ,  which in contrast to the descriptions based on evanescent surface waves avoids divergences in the heat flux in a self-consistent way. Nonetheless, for finite  we can rephrase the expression of the heat current given by Eq. (6) introducing a new varible (12)where the mean value theorem has been used to approximate the integral, with . Hence, since , Eq. (12) reduces to(13)In terms of Planck's distribution, Eq. (13) can be rewritten(14)while  must be determined in a more general way, by scrutinizing the interaction processes between light and bulk material.\nFor first order in the temperature difference ,(15)with , and . Thus, it follows that the heat transfer coefficient, the quantity usually measured in experiments and defined by , is given through(16)\nWhen the mechanism of heat exchange is through elastic collisions, which is similar to Rayleigh scattering, it is known that the intensity of radiation is proportional to  [26]. Therefore,(17)where the time scale  is a material-dependent parameter. On the other hand, regarding the inelastic contribution to the near-field heat exchange, this is the analogue to the Raman scattering of light. In this case, the distribution of modes presents anomalies which result from states located at a lower energy region [21]. The Raman spectra is fitted using a lognormal function first proposed by Denisov and Rylev [19]. This lognormal distribution is a statistical model, which can describe collective motions causing extremely slow structural relaxation, thereby fitting the non-Debye anomalies [27]. Therefore, this accounts for the high nonlinear behavior of the thermal conductance between both materials. For the proposed case, we assume that the density of vibrational states is achieved through the use of a lognormal distribution, which corresponds to(18)Here,  (characteristic frequency) and  (standard deviation) are two fitting parameters characterizing the lognormal distribution. The lognormal in Eq. (18) stems from the existence of a hierarchy of relaxation mechanisms in the material, related to the presence of collective effects. This distribution, results from the fact that the energy of the system consists of a large number of contributions and the application of the central limit theorem of probability theory (see Appendix S1). In view of the properties of the lognormal distribution, it must be noticed that(19)being a kind of closure relation for the relaxation times.\nHence, the excitation of a mode constitutes a photoinduced cooperative phenomenon. It is plausible to assume that the cumulative effect of incident photons ends perturbing the material, thus triggering collective oscillations. In addition, the lognormal accounts for an excess of ways of adsorbing energy by the system with respect to the ways obtained when merely the Debye squared-frequency law describes the relaxation. Therefore, this provides a reasonable description of the non-Debye law [20], [21], which as in Raman scattering also becomes manifests in radiation problems.\nAccordingly, the heat transfer coefficient, Eq. (16), results from the addition of the elastic and inelastic contributions mentioned above(20)where  and . In Eq. (20), the first term inside the square brackets corresponds to the usual contributions found up to now in the current literature [6], taking into account surface phonon-polaritons owing to the presence of evanescent waves close to the interface. As we have mentioned above, the second term takes into account cooperative phenomena, becoming manifest through the existence of collective modes of vibration in the system, which appear in the density of states. Consequently, a more general formulation for the radiative heat transfer problems must come from the superposition of Debye and non-Debye relaxation mechanisms, combining in this way the contributions from the material surfaces as well as the bulk.\n\nResults and DiscussionThe thermal conductance is obtained by integrating the heat transfer coefficient over the surface of a sphere of radius  divided by its area (), i.e. . Since the distance  between both surfaces depends on geometrical characteristics, the local distance between the sphere and the plane surface must be measured through the local radius , assuming the effective distance as , with  being a surface roughness parameter [7].\nWe have calculated numerically the surface average by adjusting the parameters mentioned above to the experimental results obtained in the Ref [9], which takes into account only the near-field contribution when decreasing the sphere-plate distance. In Fig. 2, we show the near-field conductance fitting the values of the integral of Eq. (20) to the data for glass-glass [8] and gold-gold materials [9]. For both the material we have used  Hz,  Hz, and . For the glass-glass material we have obtained  and for gold-gold . The near-field heat transfer described using evanescent waves as solutions of classical electrodynamics equations leads to heat flux divergences as the gap vanishes [28]. However, in our approach this divergence does not occur, reaching a constant-conductance value as the distance between the nanoparticles decreases (shown in the inset). In essence, this is due to the fact that the time relaxation distribution herein described by lognormal distribution incorporates two effects: (I) Phonon branches in a real structure affecting the density of states in different frequency regions, similar to actual behavior observed in metals [29]; and (II) the vibrational modes in the bulk material absorbing the energy excess. Hence, since our assumption of a distribution of relaxation times accurately describes the dynamics of radiative systems at the microscale, we conclude that the FDT in the Callen-Welton formulation [25] is not applicable at the nanoscale and must be modified. By going beyond the Debye theory, a way for this generalization is offered here.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Sphere-plate near-field heat transfer coefficients between a gold (or glass) sphere and a gold (or glass) substrate versus gap distances.The data are from Ref [9] for the 50 µm diameter spheres. The dotted lines are comparisons with the theoretical predictions from the proximity theorem. The inset shows a non-divergent regime as the gap vanishes.\ndoi:10.1371/journal.pone.0058770.g002\nConclusions\nIn summary, we have evaluated thermal conductance in the near-field, giving a thermokinetic description of some experiments involving heat radiation through a very narrow gap. Although near-field radiative transfer is a highly complex phenomenon, we have been able to provide a unified and highly accurate explanation of heat exchange processes at the nanoscale. Our theory covers all distances from the far-field up to contact. Since the experiments examined may involve a great variety of nanostructures, our theory possesses a wide scope of applications. The general methodology presented here may also be used in the study of other heat exchange processes such as those occurring in phonon systems and in the analysis of thermal contributions to Casimir forces, even in charge conduction problems in nanosystems.\n\nAppendix S1. Supporting Information.doi:10.1371/journal.pone.0058770.s001(PDF)"
        },
        "10.1371/journal.pone.0056086": {
            "author_display": [
                "Laszlo B. Kish",
                "Robert D. Nevels"
            ],
            "title_display": "Twisted Radio Waves and Twisted Thermodynamics",
            "abstract": [
                "\n        We present and analyze a gedanken experiment and show that the assumption that an antenna operating at a single frequency can transmit more than two independent information channels to the far field violates the Second Law of Thermodynamics. Transmission of a large number of channels, each associated with an angular momenta ‘twisted wave’ mode, to the far field in free space is therefore not possible.\n      "
            ],
            "publication_date": "2013-02-12T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 2,
            "views": 1743,
            "shares": 0,
            "bookmarks": 10,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0056086",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0056086&representation=PDF",
            "fulltext": "IntroductionRecently it has been claimed in scientific literature that it is possible to generate radio waves, at a single frequency, with more spatially orthogonal modes, “orbital modes”, than the usual two polarization modes [1]–[4]. An experimental demonstration with N = 2 modes purporting to confirm the twisted wave concept has been carried out and published [2]. Such radio waves would have angular momenta, also referred to as orbital angular momenta, in a way similar to the orthogonal (l) wave modes of electrons that exist at the same frequency and belong to the same main quantum number (n). Communication utilizing such independent/orthogonal modes would expand the available frequency band by a factor given by the number of additional spatially orthogonal modes. Because the information channel capacity of radio waves scales linearly with the number of spatially orthogonal modes N, in the case of fixed bandwidth and signal-to-noise ratio if N can be more than two or even infinite as claimed [1], wireless communication would be revolutionized.\nIt is important to note that recently two independent groups published papers [5], [6] concluding that the proposed twisted wave schemes are a special case of the traditional multiple-input-multiple-output (MIMO) technique and are thus not conceptually new. Furthermore [5] points out that in the far field the twisted wave scheme does not provide any increase in information channel capacity. Paper [6] shows that the experiments [2] have not been performed in “far-enough-field” conditions. A true far field wireless experiment would show further losses and other deficiencies in individual twisted wave modes [5].\nResults and DiscussionIn this paper we address a fundamental physics question: Can modes with non-zero angular momenta representing extra, beyond N = 2, independent communication channels be radiated to the far field and selectively picked up by a proper antenna, which is insensitive to standard plane wave modes? If the polarization is circular–a common situation in wireless technology–one has N = 2 with plane waves in the two polarization modes phase-shifted by 90°. Thus it is clear that up to N = 2 orthogonal plane wave polarization modes can exist in the far field and the circularly polarized mode carries angular momentum. Yet to date whether a greater number of angular momentum modes can exist at the same frequency and carrying independent signals in the far field has not been shown to violate fundamental physical principles.\nIt should first be noted that based on physical principles the assumption that there can be more than the two far-field polarization modes is counter-intuitive. In the atom, the existence of waves with different angular momenta at the same energy originates from the potential and the ensuing localized nature of the waves. A charge revolving in a Coulomb potential field will have an infinite number of different classical physical paths with the same energy, and Bohr-Sommerfeld quantization will select a finite number of states that are allowed within quantum theory. But, in stark contrast no such state components exist for free electron waves. In light of this intuitive argument, the existence of spatially orthogonal modes for electromagnetic waves is fine for photons propagating under spatially confined conditions such as in wave guides and optical fibers [7], [8], or in the immediate surroundings of a black hole [9]. We reiterate that the existing experimental radio wave demonstrations [2] hold only for .\nRather than analyzing the theoretical treatments for errors, we use another approach to prove that the hypothesis that independent communication channels based on orbital modes can be selectively picked up by a proper antenna that is insensitive to standard plane wave modes violates the Second Law of Thermodynamics, which states that it is impossible to construct a perpetual motion machine of the second kind. First let us specify the necessary conditions that are essential for the utilization of the M-th orbital mode as a parallel independent information channel:\n\n\n\n\nA selective antenna must exist that is able to radiate in the M-th orbital mode.\n\nThe same antenna should selectively pick up a signal from an electromagnetic wave only at the M-th orbital mode while discarding all the other orbital and non-orbital mode components in that signal.\n\nAccording to Planck’s Law [10], the a black-body (with unity emissivity) radiates in each polarization with a power spectral intensity(1)where f is frequency,  Js is Planck’s constant,  JK–1 is Boltzmann’s constant and T is absolute temperature. This means that a unit surface area of the black-body emits, in each polarization, the power(2)within an infinitesimally small frequency band around . Thus the total radiated power from a unit area is(3)where N = 2 is the number of orthogonal polarization modes. Thus the Planck formula [10], is:\n(4)Inspired by Nyquist’s treatment of Johnson noise [11], we now devise the following gedanken experiment, see Figure 1: A large box (much larger than the wavelengths considered) is located in a thermal reservoir of temperature T. We assume that its internal walls are ideally black. Furthermore an isolated resistor (with radiation screening and thermal isolation) and a “twisted-wave” antenna tuned to the M-th orbital mode in a bandwidth of  around frequency f are in the box and the resistor is connected to the electrodes of the antenna. We start from thermal equilibrium, i.e., a uniform temperature within the box, including the walls, the inherent thermal radiation, the antenna, the resistor, and the thermal isolation/screening.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Outline of the gedanken experiment.doi:10.1371/journal.pone.0056086.g001Conditions i) and ii) will result in the following situation:\n\n\n\n\nIn accordance with condition i), the energy supplied by the resistor will be radiated by the antenna in the M-th orbital wave mode. This energy will be absorbed in the walls. The wall will emit thermal radiation in the form of plane waves [10] with a power given by Eq. 4.\n\nIn accordance with condition ii), the antenna can pick up a signal only at the M-th orbital mode while it will discard all plane wave components radiated by the walls of the box. This means that the antenna will not pick up any signal because the walls emit only plane waves [10].\n\nThus the energy will flow out from the resistor and cannot return. Therefore Boltzmann’s Principle of Detailed Balance [12] cannot be satisfied. The resistor cools down, which implies that a temperature inhomogeneity is induced in the system in thermal equilibrium and hence the Second Law of Thermodynamics is violated. The only way to avoid violation of the Second Law of Thermodynamics with the above set-up is to suppose that the antenna also picks up plane wave modes. However, in that case the antenna cannot offer a separate information channel for the orbital mode.\nIt should also be mentioned that it is well-known that corresponding antenna types that can emit circularly polarized waves (which also have non-zero angular momentum) are sensitive to plane waves because a plane wave will excite its relevant polarization mode. Thus a circularly polarized antenna will not violate the Second Law when it is used in the same gedanken experiment as described above.\nMethods and ConclusionsWe have presented and analyzed a gedanken experiment with a black body and a twisted-wave antenna in thermal equilibrium. We have shown that the assumption that at a single frequency more than two independent information channels can be provided by an antenna violates the Second Law of Thermodynamics. In conclusion, twisted waves cannot carry information that is independent from the information contained in plane wave modes at the same frequency.\n"
        },
        "10.1371/journal.pone.0081659": {
            "author_display": [
                "Shafiq R. Qureshi",
                "Waqar A. Khan",
                "Robert Prosser"
            ],
            "title_display": "Behaviour of a Premixed Flame Subjected to Acoustic Oscillations",
            "abstract": [
                "\nIn this paper, a one dimensional premixed laminar methane flame is subjected to acoustic oscillations and studied. The purpose of this analysis is to investigate the effects of acoustic perturbations on the reaction rates of different species, with a view to their respective contribution to thermoacoustic instabilities. Acoustically transparent non reflecting boundary conditions are employed. The flame response has been studied with acoustic waves of different frequencies and amplitudes. The integral values of the reaction rates, the burning velocities and the heat release of the acoustically perturbed flame are compared with the unperturbed case. We found that the flame's sensitivity to acoustic perturbations is greatest when the wavelength is comparable to the flame thickness. Even in this case, the perturbations are stable with time. We conclude that acoustic fields acting on the chemistry do not contribute significantly to the emergence of large amplitude pressure oscillations.\n"
            ],
            "publication_date": "2013-12-20T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 604,
            "shares": 0,
            "bookmarks": 1,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0081659",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0081659&representation=PDF",
            "fulltext": "IntroductionThermoacoustic instabilities result from the uncontrolled amplification of acoustic waves during combustion. These instabilities are more apparent in combustion systems operating on a lean premixed air fuel ratio, and several mechanisms for the instability have been identified (i.e. [1], [2], [3], [4], [5], [6], [7], [8]). Although combustion systems are normally designed for steady state conditions, some regions of the operating envelope may be prone to the growth of instabilities arising from small initial disturbances. Although these disturbances consume only a very small part of the available energy in the chamber, large pressure oscillations may follow, leading to structural vibrations and—in extreme cases—- “equipment failure” [9].\nThe noise arising from unsteady combustion is commonly expressed in terms of a thermoacoustic Efficiency (TAE), defined asTypical values of the TAE for turbulent flames are  and for laminar flame are  [3], [10], [11]. Instabilities occur for thermoacoustic efficiencies of  [10]; for every order of magnitude change in the TAE, the sound pressure level (SPL) changes by about 10 dB [11].\nA small acoustic wave propagating through the flame may be altered either in amplitude or frequency and this may effect the combustion dynamics. The direct influence of acoustic wave propagation on reaction rates to our knowledge has not been discussed in the literature separately. However, the effect of a wave propagating through a non-equilibrium background has been discussed by numerous authors. Einstein (cited in [12]) and Clarke & McChesney [13] suggest that wave attenuation may occur in dissociating mixtures when the wave itself drives the non-equilibrium component of the flow. Elaine et al. [12] describe how frequency dispersion emerges when a sound wave alters its shape while propagating through a non-equilibrium background. Furthermore, they suggest that acoustic wave amplification is expected only if the non-equilibrium flow already exists in the background, or is caused by an external source and not by the propagating wave itself. Clarke [14] has shown that the non-equilibrium background flow can indeed amplify the acoustic wave . Experimental work by Toong et al. [15] has shown evidence of both the amplification and the suppression of sound waves when they interact with a flame, although these observations are based upon a diffusion flame. Similar conclusions have also been drawn by Melvin [16], Srinivasan & Vincenti [17], and Bauer & Bass [18].\nThe focus of this paper is, therefore, to study the response of a premixed laminar methane flame to small acoustic disturbances and to identify which—if any—acoustic modes induce positive feedback in the pressure oscillations. The novelty of the work comes from the relative complexity of the reaction mechanism employed (18 species and 68 individual reaction steps), and the configuration studied (Low Mach number flow, with fully non-reflective inlet and outlet boundary conditions).\nSection provides a review of flame-acoustic interaction and reaction rate chemistry. The governing equations, discretization schemes and boundary condition treatment for reacting flows are given in section 0.3, along with a brief description of the code used. Results of the simulations are presented in section 2, and conclusions are presented in section 0.6.3.\nAcoustic Waves and Reaction RatesA generalized inhomogeneous wave equation can be derived to describe the relationship between the pressure and heat release fluctuations in an acoustically active field such as a combustion chamber. In the combustion chamber, the source of heat release is solely due to the chemical reactions between oxidizer and fuel. Any acoustic perturbation in the combustion chamber will interact with the flame and may modify the flame structure substantially [19]. Sound generation due to heat release has been reviewed by Higging, Sondhauss and Rijke; an account of their work is given in [20]. Numerous authors (i.e. Putnam and Dennis [21], Shimmer and Vortmeijer [22]) have undertaken experimental studies to investigate flame-acoustic interactions. Putnam et al. [21] have also provided a mathematical formulation for the development of these acoustic instabilities .\nThe generation of acoustic waves in a flame may be due to a natural mode of system, the addition of energy by an external source or by chemical reactions within the system [12]. An order of magnitude analysis of a turbulent reacting mixture shows that heat release fluctuations driven by the species reaction rates  provide the dominant sources [23]. The inhomogeneous acoustic wave equation governing reacting flows involving N chemical species can be expressed in the following form [23], [12]:(1)where  is the pressure fluctuation,   and  is the species enthalpy, defined aswith  taking the value of the reference state enthalpy.  here is an integration variable. The reaction rate for species  is derived by considering I elementary reactions between N species;(2) and  are the stoichiometric coefficients for species  during reaction step , and  represents the chemical species.  is then given bywith(3)The term  represents the collision frequency and is often known as the frequency factor or pre-exponential factor, E is activation energy [24]. The values of ,  and  are empirical parameters and are based on the nature of the elementary reactions. The activation energy is the energy required to move the reactants over the energy barrier to begin the reaction [25].  is universal gas constant.  representing the molar concentration of species  For reversible reactions,  is modified with the addition of an analogous term describing the backwards rate of reaction. This may be specified explicitly as part of the reaction mechanism, or derived via equilibrium considerations.\nSimulationTo study the effect of acoustic waves on flame chemistry, a number of simulations have been carried out using an in-house code. The code is based around a fully compressible solver and was initially developed to study multidimensional reacting flows with arbitrarily complex reaction mechanisms. For the purposes of this work, the problem is specified as one dimensional. Explicit 4th order spatial differencing was employed to calculate the derivatives appearing in the transport equations, while time integration was handled via the low storage 3rd order Runge Kutta scheme proposed by Wray [26]. Prior to this study, the code has been validated against a number of test problems, as recommended by Roache [27], and has been used in a number of other test cases.\n\n0.1 The governing equations\nThe governing equations for a compressible viscous reacting flow can be written in the following form:Where tensor indices i,k = 1,2,3. The transport equations are closed via the thermal equation of state, and the stagnation energy relation [28]The viscous stress tensor is defined asand , , , ,  are the density, momentum, total energy, pressure and characteristic gas constant, respectively. The effects of gravity and radiative heat transfer are assumed to be negligible [29], [30]. The heat flux  is given byLewis and Prandtl numbers are considered constant in this study [29], [30]. Therefore the mass diffuivities  of each species and viscosity are derived via assumption of constant Lewis and Prandtl numbers using following expressions:The value for  is obtained using the CHEMKIN thermodynamic database for the constituent specific heat capacities  [31], and the thermal conductivity is assumed to be given by(4)\n\n\n0.2 Boundary conditions\nBoundary conditions for flows within a finite domain (i.e. closed ducts) are relatively straightforward to treat. In the case where the flow domain is infinite and unbounded, a truncation of the physical domain is desirable for a numerical solution, but such a truncation requires an artificial boundary. Since the focus of our study is to investigate the behaviour of acoustic waves passing through a flame, and since any reflection from the inlet or outlet boundaries may produce spurious effects, we use non reflecting boundary conditions based upon the method of characteristics.\nThe method of characteristics describes how systems of hyperbolic equations can be decomposed into sets of wave modes, each with a definite velocity [32]. At each boundary of the computational domain, some waves enter the domain and some waves leave the domain. The outgoing waves are entirely defined by the interior solution. The incoming waves depend on the exterior solution and require a boundary condition. Thompson [32] gives a complete mathematical analysis and describes the incoming and outgoing waves in a primitive variable form for the Euler equations. This approach has been extended by Poinsot and Lele [33] for the application of non-reflecting boundary conditions to the Navier-Stokes Equations. This approach is commonly referred to as the Navier–Stoke Characteristics Boundary Conditions (NSCBC) approach. An application of this method to reacting flows was initially proposed by Baum et al. [28] and later extended by Sutherland and Kennedy [34].\nFurther refinements to the NSCBC approach have been proposed by Prosser [35], who used a two-scale low Mach number expansion [36] to identify a linearization based around a divergence free state for cold flows. These have been extended to include conducting and reacting flows [37]. For the boundary conditions used in this study, we effectively set(5)where the sign depends on the boundary under consideration,  is the sound speed based on the far field base-state and  is the acoustic divergence, defined as [37]Equation 5 thus specifies an inflow boundary condition which is fixed, modulo the passage of acoustic transients. Details regarding development and implmentation of NSCBC for reacting flows can be seen elsewhere [33], [35], [37], [32]\n\n\n0.3 Discretization schemes, chemistry, and boundary conditions\nA one dimensional domain of length  is discretized using 1024 nodes, resulting in a grid spacing of  The reaction zone (flame thickness) is approximately 4 mm long. An explicit 4th order finite difference method is used for the spatial discretization of the continuity, momentum, energy and species transport equations [38]\nA methane mechanism comprising 68 reaction steps and 18 species is used for the source terms in the species transport equations. The specific heat capacities, enthalpy and entropy are calculated using the polynomial coefficients of the CHEMKIN thermo chemical tables [31]. The simulation is initiated using assumed profiles for key species, and then allowing the calculation to proceed until all of the dependent variables have approached a steady state. By setting the inlet mass flow rate equal to the consumption rate, a stationary flame solution is achieved; this is used as the initial condition for the acoustically active simulation. All simulations are performed assuming an equivalence ration  The pressure and temperature profiles of the steady state solution are shown in Figure 1 and Figure 2. The equilibrium flame temperature is approximately 2200 K, and the flame speed is calculated to be 0.32 m/s\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Steady state pressure profile in the domain.doi:10.1371/journal.pone.0081659.g001\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Steady state temperature profile in the domain.doi:10.1371/journal.pone.0081659.g002The acoustic wave trains directed toward the flame are generated by manipulating the incoming characteristics. For a quiescent field with no significant viscous effects or chemical reactions, it is straightforward to show that the left () and right  going acoustic amplitudes may be related viaAt the left hand boundary, we set(6)Tthe boundary condition produced by equation 6 produces a wave train of amplitude A and frequency  on the inlet plane.\n\nWe are interested in the interaction between the acoustic field and the reaction zone. The coupling between the chemistry and the acoustics can manifest itself in one of two waysFrom the flame's perspective, low frequency waves induce negligibly small pressure gradients on the length scales associated with the reaction zone. In such cases, it is extremely unlikely that lighter species could be preferentially displaced with reference to the heavier species. Hence, we have selected relatively high frequency ranges, up to the point where the acoustic wavelength is of the same order as the flame itself; typically, this is around 90 kHz. These latter frequencies are beyond those typically encountered in industrial applications; our interest in them here stems from the fundamental physics.The presence of the flame in the domain acts almost as a discontinuity in the flow due to the sudden changes in density, temperature and subsequently the sound speed. According to acoustic theory [39], when a wave crosses an interface between two different media, some acoustic energy is reflected. In reacting flows, the density of the flow before and after the flame varies significantly. Therefore the acoustic wave passing through a flame resembles a wave crossing an interface between two different media. Figure 3 shows the piecewise continuous acoustic perturbationwhere A is again the maximum perturbation amplitude,  is the Heaviside function, and  is the angular frequency (set to give  in this example).We observe that the acoustic wave is partially reflected when it hits the flame as shown in Figure 4. The reflection of the wave depends upon the product of density and sound speed in the media via the acoustic impedance [39]. The relation between the reflected and incident waves is established by the reflection coefficient, given by [39]:(7)where the 0 and  subscripts refer to the hot and cold sides of the flame, respectively. If the amplitudes of incident, transmitted and reflected waves are I, T and R, respectively, we can write(8)(9)The above relationships are derived for two media with different speeds of sound and density. The results of our simulation have shown that the amplitude of the reflected and transmitted waves are in agreement with analytical calculations obtained from Equations 8 and 9. The single wave simulation was performed for a simulation time of  sec, corresponding to 1.304 acoustic transit times (based on the cold flow variables) to observe attenuation or amplification in the transmitted and reflected waves. Figures 5(a)–(d) show the waves at different time intervals, and we observe that both waves travel smoothly out of the domain without any further change to amplitude or frequency. The nonreflecting character of the inlet and outlet boundaries is evident in figures 5(a)–(d); Separate tests have demonstrated that the reflection coefficients for this boundary condition is  for physical waves, and  for numerical waves [35].The amplitudes of the reflected wave and the transmitted wave are approximately  and , respectively, as shown in the figure 4. We define relative errors in the incident and reflected waves asThe subscripts  and  refer to the numerical and analytic result, respectively. We find that for our simulations,  with a similar figure for  Rather than be a product of a non-linear phenomenon, this figure is more likely a result of the manner in which the amplitudes are measured—the wave peak almost never exactly collocates on a grid point, and so there is a small phase error induced in estimating the peak amplitude. Notwithstanding the foregoing argument, the error is small and the essential constancy of  leads us to conclude that the acoustic wave has been neither amplified nor attenuated during its transit of the nonequilibrium region of the flow. This test has been repeated a number of times with different amplitudes and frequencies. The results were the same as those reported here.To study the effect of pressure waves on combustion chemistry, we have examined the response of the heat release, the reaction rate and the burning velocity to a number of imposed frequencies. Instantaneous integral values of reaction rate are obtained by integrating  for a particular species over the domain length at each time step. Similarly the integral values of burning velocity and heat release are calculated. Figures 6, 7 and 8 show the time history of the relative change of the integral values of reaction rate of , heat release and burning velocities respectively. The relative change is calculated using the following expressions:(10)(11)(12)where the suffix  is used to refer to an acoustically quiescent benchmark solution i.e. no acoustic wave passing through the flame, and additionally  is a constant,  depends on which species you choose)c and h refer to the cold and hot sides of the flame, respectively.The relative changes in reaction rate, heat release and burning velocities are very small. A small perturbation in the integral values of heat release and burning velocities is also visible in figures 7 and 8 during initial stages ( sec), which shows the effect on integral values when the wave is crossing the inlet boundaries. As the density and pressure are related through the equation of state, any fluctuation in pressure will also produce a fluctuation in the density. Consequently a fluctuation in the conservative form of species mass fraction  at the inlet will effect the integral values. This initial perturbation disappears once the wave has crossed the inlet (i.e. after  sec).The perturbation in reaction rate and burning velocities are essentially instantaneous when the wave passes through the flame. However, a time delay can be seen in the heat release, which is due to the time scales related to the chemical reaction. Although the study of a single wave did not provide any direct effect of combustion on the amplification or attenuation of the acoustic wave, the perturbations in these three parameters may feed some energy to the subsequent acoustic waves.In this section, we extend our study to that of a high frequency wave train propagating through the flame structure. The purpose of this test is to identify additional effects arising from the coupling of the incoming waves to the flame, such as (say) standing waves local to the reaction zone. The simulation is run for a sufficient time  to ensure that at least 3 waves have crossed the flame thickness. Low frequency acoustic waves produce only negligibly small differential pressure gradients across the flame; such waves are felt by the flame essentially as a uniform background pressure oscillation. It is difficult to see how such a bulk effect could give rise to significant changes in the flame structure. Consequently, we restrict our attention to comparatively high frequencies: 3 kHz, 5 kHz, 8 kHz and 10 kHz are chosen. In order to study the sensitivity of the flame to both amplitude and frequency, each frequency is simulated for three different pressure perturbations of amplitudes ,  and  corresponding to sound pressure levels of 140 dB, 168 dB and 180 dB, respectively.Figures 9 and 10 show the dependences on pressure of the burning velocity and heat release on the pressure.The reaction rate integrals of  and  are shown in figures 11 and 12. It can be seen that the relative change in the reaction rate of  (and hence its integral) is larger than that associated with . The relative change in the reaction rates of a number of other species is also shown in figure 13. Although the relative change in the  and  is moderate, the net effect of these species in terms of the heat release is very small.The relative changes in burning velocity and heat release for  perturbations imposed at different frequencies are shown in figures 14 and 15, respectively. Interestingly, both quantities exhibit a frequency dependence, with their peak values increasing with increasing frequency. This effect appears to result from a change in the flame structure. Evidence for this observation comes from figures 16 and 17 which, between them show different sensitivities on the  and  production rates. Additionally, figure 18 depicts the maximum change in production of a number of other species, with respect to the incident wave frequency. This figure shows that there exists no simple relation between the molecular weight of a species and its relative change. The  and  curves, for example share very similar molecular weights, but exhibit very different behaviours with respect to imposed frequency. We conclude from this that the change in flame speed cannot result simply from the pressure gradient acting preferentially on the light species.In the foregoing parts of the paper, the flame thickness is small compared to the incident acoustic wavelength (i.e. a  wave has a wavelength  times greater than the simulated flame thickness of approximately ). In such cases the effect of the pressure wave will produce very small pressure gradients across the flame. To obtain a more realistic measure of the pressure fluctuation on the flame, we have extended the range of high frequencies to ensure a more comparable relation between flame thickness and wavelength.Following McIntosh [40], we define the ratios of time and lengthscale for flame-acoustic interaction as:(13)(14)Using the Mach number  Based on the flame speed, both time and length scales can be related as [40]:(15)For a harmonic wave, the disturbance length is taken as half of the wavelength: for  (say) the disturbance length is  based upon the initial sound speed in the fuel/air mixture of . The parameter  is critical in establishing the flame-acoustic interaction. Strong pressure effects on flame/acoustic configurations with small  arise as a result of sharp pressure gradients across the flame [41]. McIntosh [42] has also observed that the effect of pressure gradients will be more important when  and .We have adopted an alternate form to define the acoustic time scale ratio  in terms of frequency:The above expression shows a direct relation to frequency of the incident wave. In our analysis of high frequencies, we have found that the effect of pressure perturbations increases when N is decreased. The relative change is a maximum when N reaches unity. Figure 19 and 20 depict the maximum values of  with pressure perturbations of  and . Pressure perturbations of  do not appear to have a significant effect on the flame speed perturbation. This is in marked contrast to the  case, for which there exists a marked peak for  (corresponding approximately to ). This lends further strength to the notion that acoustic influences are not restricted just to preferential acceleration of the light species; the pressure gradients seen by a flame are the same for a wave of amplitude p and frequency f as they are for a wave of amplitude  and frequency —yet the figures show no such correspondence in their profiles. Hence, it appears that the pointwise value of pressure (as well as its gradient) is important to the flame. This is ostensibly a surprising result, since a  perturbation only corresponds to  os the total pressure the flame sees. Nevertheless, this figure is approximately consistent with the flame speed changes observed. For oscillations of  we see that a peak change is near , and a downward trend is observed for . This shows that for a value of , the effect of the pressure amplitude becomes less significant.We have not studied further frequencies beyond 120 kHz because these frequencies are not often found (i.e. ) in practical applications. Although large fluctuations may result in extinction and re-ignition of the flame, the relative change in the burning velocities in our simulations is not substantial for the range of pressure fluctuations studied."
        },
        "10.1371/journal.pone.0000133": {
            "author_display": [
                "Timothy J. Crone",
                "William S.D. Wilcock",
                "Andrew H. Barclay",
                "Jeffrey D. Parsons"
            ],
            "title_display": "The Sound Generated by Mid-Ocean Ridge Black Smoker Hydrothermal Vents",
            "abstract": [
                "\n            Hydrothermal flow through seafloor black smoker vents is typically turbulent and vigorous, with speeds often exceeding 1 m/s. Although theory predicts that these flows will generate sound, the prevailing view has been that black smokers are essentially silent. Here we present the first unambiguous field recordings showing that these vents radiate significant acoustic energy. The sounds contain a broadband component and narrowband tones which are indicative of resonance. The amplitude of the broadband component shows tidal modulation which is indicative of discharge rate variations related to the mechanics of tidal loading. Vent sounds will provide researchers with new ways to study flow through sulfide structures, and may provide some local organisms with behavioral or navigational cues.\n         "
            ],
            "publication_date": "2006-12-27T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 7,
            "views": 13156,
            "shares": 2,
            "bookmarks": 21,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0000133",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0000133&representation=PDF",
            "fulltext": "IntroductionMid-Ocean ridge hydrothermal systems support rich communities of chemosynthetic organisms and are conduits for large heat and chemical exchanges between young oceanic lithosphere and the ocean. On a global scale the time-averaged hydrothermal heat flux and many chemical fluxes are well constrained [1]. On local scales these fluxes are temporally and spatially variable [2]–[4], but the variations are poorly quantified because there are few time-series measurements of fluid flow with which to integrate temperature and chemical observations. While time-series measurements of flow have been obtained in low-temperature vents [4], [5], and point measurements have been obtained in black smokers [6], [7], no time-series measurements of black smoker flow exist. High temperatures, low pH, and mineral precipitation limit the long-term effectiveness of invasive flow measurement techniques commonly employed in these environments.\nThe development of a non-invasive flow measurement technique could solve this problem and enable the collection of extended time-series flow data. One proposed method [8] would use passive acoustic measurements and capitalize on the potential for fluid flow to produce sound [9]. Passive acoustic measurements near black smokers could provide flow rate information if flow-related sounds can be detected, and if a relationship between flow rate and acoustics can be established. Point measurements of flow using an invasive measurement technique [6] could be used to convert time-series measurements of acoustically-determined relative flow rates into absolute measurements.\nWhile previous studies have noted an apparent increase in ambient noise within several hundred meters of two hydrothermal vent sites [10], [11], another study found no conclusive evidence that hydrothermal vents generate sound [8]. In this report we present the first detailed description of the localized sound generation by two mid-ocean ridge black smoker hydrothermal vents. We discuss the likely sound source mechanisms that operate to produce both broadband and narrowband signals. We then discuss the tidal variations observed in one record, which we argue is related to tidal forces affecting fluid circulation within the hydrothermal system. We conclude with speculation on the biological implications of black smoker sound production.\nResultsUsing a stand-alone deep-sea digital acoustic recording system, we recorded 45 hours of continuous sound sampled at 1000 Hz in 2004, and 136 hours of continuous sound sampled at 1920 Hz in 2005, from the “Sully” and “Puffer” vents respectively (Figure 1). These two vents are situated approximately 2200 m below the sea surface, within the Main Endeavour vent field on the Juan de Fuca Ridge. We also recorded the local ambient noise field at a distance of ~150 m from the nearest black smoker in 2004, and ~25 m from the nearest black smoker in 2005. Movie S1 and Audio S1 contain samples of the recorded audio, and Figure 2 shows the recording system deployed at Sully and Puffer.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1. Geological map of the southern part of the Main Endeavour vent field, adapted from [43]. The acoustic recording system was deployed at the Sully and Puffer vents in September 2004 and 2005 respectively. On this map, dark gray objects represent active sulfide structures; light gray objects represent inactive sulfides or diffuse vents; triangles depict talus at the base of the west axial valley wall; and thin lines indicate faults and fissures. Inset shows the location of the Juan de Fuca Ridge in the northeast Pacific Ocean, approximately 500 km west of Seattle, WA, with the location of the Main Endeavour Field (MEF) marked with a star. Surrounding plate boundaries are also depicted.\ndoi:10.1371/journal.pone.0000133.g001\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2. Photograph showing a) the first-generation digital acoustic recording system deployed at the Sully vent in September 2004, and b) the second-generation system deployed at the Puffer vent in September 2005. The cylindrical titanium pressure case housed the battery and recording electronics. The “case” hydrophone was attached to the stand assembly just above the bulkhead connectors at one end of the case. The bracket which held the “remote” hydrophone can be seen behind the black smoker jet in the center of a).\ndoi:10.1371/journal.pone.0000133.g002The power spectra of the recorded signals show that both vents radiate significant acoustic energy at all frequencies up to the anti-aliasing filter which has a corner frequency of 500 Hz (Figure 3). Both vents generate a broadband acoustic signal with power levels ~10–30 dB above the ambient noise level. Both vents also produce numerous narrowband tones at frequencies ranging from ~10–250 Hz, with power levels ~10–20 dB above the broadband signal level and bandwidths of ~5–15 Hz. Root-mean-square pressure fluctuations associated with these spectra, computed from the integral of these curves over the frequency range 5–500 Hz, are shown in Table 1.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3. Typical hour-average power spectra derived from the two vent recordings, and the ambient noise recordings. Sharp peaks on the ambient spectra are associated with a nearby research vessel.\ndoi:10.1371/journal.pone.0000133.g003\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Root-Mean-Square Pressuresa.doi:10.1371/journal.pone.0000133.t001A spectrogram generated from the Puffer recording (Figure 4) and an animation of Puffer's acoustic power spectrum (Movie S2) illustrate the temporal evolution of this vent's acoustic signature. The general shape of the spectrum remains relatively constant, but some fine-scale features vary. For example, the spectral peaks centered at ~27 Hz and ~67 Hz each split into two separate peaks, and the power spectrum within the 150–250 Hz band changes significantly over the measurement period.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4. Spectrogram showing the temporal evolution of Puffer's power spectrum, based on 10-minute average spectra. Upper and lower panels have different color scales and vertical scales. Overlain on an arbitrary scale in the lower panel is the predicted tidal height for the study area [44]. The peak-to-peak tidal amplitude is ~2 m. Tidal variability is discernible between ~200–250 Hz for this choice of color scales. The very narrow peak at 360 Hz is associated with a nearby research vessel.\ndoi:10.1371/journal.pone.0000133.g004Tidal variability is also evident in the acoustic record of Puffer. A comparison of the predicted tidal heights with the spectrogram (Figure 4) hints at this variability. For the choice of color scales in this figure, the power levels within the ~200–250 Hz band appear elevated after high tide. A spectral analysis of the power in different frequency bands reveals that the acoustic signals contain semi-diurnal periodicity within the ~150–325 Hz and ~450–500 Hz bands (Figure 5). The ~1.95 cycles per day signal corresponds closely to the 1.93 cycles per day frequency of the 12.42-h M2 tidal component [12]. Cross correlation of the tidal signal with the time-series of power level in these frequency bands shows that maximal acoustic output lags the M2 tidal component by ~115–127° (Figure 6). This is equivalent to a lag of ~238–262 minutes with respect to high tide.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5. Contours of the spectra computed from the average spectral power of Puffer's acoustic signal in 5-Hz bands. The power level time-series have been normalized by their root-mean-square, thus the units of the contoured values are inverse cycles per day (cpd), and the relative magnitudes of the contours are only meaningful in the horizontal direction. The length of the time-series (136 hours) renders spectral information below ~1 cycles per day unresolved. Semi-diurnal (~1.95 cycles per day) variability of Puffer's acoustic power occurs within the ~150–325 Hz and ~450–500 Hz frequency bands.\ndoi:10.1371/journal.pone.0000133.g005\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6. Phase lags of the maximal acoustic power output, relative to the M2 tidal component period of 12.42 h, for parts of the acoustic spectrum showing strong semi-diurnal periodicity (Figure 5). Phase lags cluster around 120°, or just over 4 h. Similar phase lags are predicted by numerical models of tidal loading on poroelastic hydrothermal convection systems [29].\ndoi:10.1371/journal.pone.0000133.g006Discussion\nSource Mechanisms\nThere are a variety of sound source mechanisms that could potentially operate within black smoker systems. The presence of both broadband and narrowband components in the acoustic signals indicate that multiple mechanisms are operating within Sully and Puffer. Potential broadband sources include boiling, cavitation, turbulent shear, advected fluid heterogeneity, pulsating exit flow, fluid–structure interactions, and volume changes associated with the cooling of hydrothermal fluid. Many of these source mechanisms were discussed in the context of black smoker systems in [8]. Here we review some of this work in light of our results.\nSound source mechanisms are typically divided into three separate classes of acoustic radiators: monopoles, dipoles, and quadrupoles. Each of these source types has a different near-field radiation pattern, where the near-field is defined as the region within one acoustic wavelength of the sound source [8]. Monopoles radiate sound through volume or mass fluctuations, and have one-dimensional pressure fields. This type of acoustic radiation would be generated by a sphere vibrating in its first mode, expansion and contraction [13]. In the near-field, pressure perturbations generated by a monopole fall off in proportion to 1/r, where r is the distance from the source. Dipole radiation usually arises from force fluctuations at a fluid–fluid or fluid–solid interface, and has a two-dimensional pressure field. This type of acoustic radiation would be generated by a rigid sphere oscillating side to side, or by two monopoles spaced closely together and vibrating out of phase [13]. In the near-field, pressure perturbations generated by a dipole fall off in proportion to 1/r\n               2. Quadrupole radiation arises from the fluctuating shear stresses within a turbulent fluid [14] and has a three-dimensional pressure field. This type of acoustic radiation would be generated by a sphere vibrating in its third mode, ellipsoidal distortion, or by two dipoles of opposite polarity situated side-by-side [13]. In the near-field, pressure perturbations generated by a quadrupole fall off in proportion to 1/r\n               3.\nBoiling produces monopole acoustic radiation as the expansion, and sometimes the subsequent collapse, of vapor bubbles within a heated fluid produce pressure fluctuations that propagate away from the forming bubble [15]. While this mechanism could be a significant source of sound in vents that are boiling [8], the temperatures of Sully and Puffer were about 15–20°C below the ~375°C boiling point of hydrothermal fluid at 2200 m depth [16] when we recorded their sound. Thus it is unlikely that boiling is a source of sound in these recordings.\nSimilar to boiling, cavitation also produces monopole radiation during the formation and collapse of vapor pockets. These pockets form in response to hydrodynamic pressure drops that bring local fluid pressures below the vapor pressure [17]. Cavitation can be a significant source of sound when it occurs [17], however the fluids issuing from Sully and Puffer are far from the two-phase curve [16], and are unlikely to cavitate as a result of hydrodynamic forces within these two chimneys [8].\nFree turbulence in a fluid generates quadrupole acoustic radiation which is associated with the fluctuating shear stresses in the flow [14]. The power output from this mechanism is typically quite small in low Mach number flows [18], [19]. Near-field pressure perturbations associated with this mechanism can be approximated by [8]:1where P is the root-mean-square pressure fluctuation, r is the fluid density, U is the mean fluid velocity, D is the vent orifice diameter, and r is the distance from the source. Letting r = 625 kg/m3 \n               [16], U = 1 m/s, D = 0.05 m, and r = 0.5 m, we find that pressure fluctuations associated with free turbulence may equal ~6×10−3 Pa. Such pressures are far below the ambient sound pressures recorded at the Main Endeavour field (Table 1), thus this source mechanism is not likely responsible for any of the acoustic radiation recorded in this study.\nTurbulent fluid flows containing heterogeneous density or compressibility fields can generate significantly more sound than uniform fluid flows [20]. Pockets of differing density or compressibility can interact with hydrodynamic pressure variations to produce dipole acoustic radiation with wavelengths that are much longer than the heterogeneity length scale [20]. In black smoker systems, this can result in pressure fluctuations that are a factor of ~4×102 greater than those generated by turbulent shear in a uniform flow [8]. Thus pressure perturbations generated by this source mechanism may equal ~2.4 Pa, which is similar in magnitude to the pressures measured near Sully and Puffer (Table 1), and suggests that fluid heterogeneity may play a role in sound production in these vents.\nMass flux variations at the vent orifice caused by pulsating flow can produce monopole acoustic radiation [18], [19]. This source mechanism functions much like a baffled piston [17], and the sound from this mechanism would originate from the plane of the vent orifice [18], [19]. Pressure perturbations associated with this mechanism can be approximated by [8]:2where Pp\n                is the root-mean-square pressure fluctuation of the pulsating flow. Letting Pp\n                equal 10 percent of the total mean pressure, which can be approximated by ρU\n               2 \n               [8], and letting r = 625 kg/m3 \n               [16], U = 1 m/s, and r = 0.5 m, we find that pressures from pulsating flow may equal ~3.7 Pa. Pressures of this magnitude would be easily detected over the background noise, and are indeed similar to those measured at both vents (Table 1). Thus pulsating flow may contribute to the sound signals recorded in this study.\nThe interaction of unsteady flow with the internal walls of the chimney can create force fluctuations at the fluid–solid interface, which can, in turn, cause the structure to vibrate and emit dipole acoustic radiation [9]. Sound from this mechanism would originate from within the chimney structure. It is difficult to predict the magnitude of acoustic pressure perturbations associated with source, as it will depend on many factors including the stiffness of the chimney and the geometry of the fluid conduits. However, this mechanism is strongly dependent on fluid flow, with acoustic intensities being proportional to roughly the fifth power of the mean flow rate [17], and coupling between the fluid and solid increasing with increased conduit roughness and tortuosity. Considering the rapid flow rates and the rough and often tortuous fluid pathways found in black smoker structures [21], we consider it possible that this sound source is significant in these systems.\nAnother potential sound source mechanism is related to fluid volume changes driven by the mixing of hydrothermal fluid with seawater. The equation of state for hydrothermal fluid at high temperatures and pressures predicts a significant volume decrease when this fluid mixes and exchanges heat with ambient seawater [16]. This process will produce monopole acoustic radiation with the source located in the jet mixing region some distance above the vent orifice. To the best of our knowledge, this sound source mechanism has not been investigated, thus there is no theory to quantify the magnitude of the pressure perturbations it may generate. However, considering the large density differences between hot and cold hydrothermal fluid [16], we consider it possible that this source is significant in these systems. Small-scale laboratory experiments could be used to investigate this mechanism and determine its sound production potential.\nThus we have identified four potential source mechanisms that might generate the broadband acoustic signals measured in this study, each of which being of a different mechanism class, and or having a different locus. Pulsating flow would generate monopole acoustic radiation with its locus situated at the vent orifice. Volume changes associated with the cooling of hydrothermal fluid would also generate monopole radiation, but with its locus situated in the mixing region of the jet. Fluid heterogeneity would generate dipole acoustic radiation with its locus situated in the mixing region of the jet. And fluid–structure interactions would generate dipole acoustic radiation with its locus situated within the chimney structure. The deployment of a hydrophone array in the near-field may therefore help discriminate between these possible sources by measuring the fall-off rate of the pressure fluctuations and by locating the sound sources.\nPotential narrowband sources within black smokers are related to the excitation of the acoustic modes of resonators by unsteady flow. Such resonance typically involves flow past cavities, or flow impinging upon solid bodies. Among the many possible resonators are Helmholtz resonators, half-wave or quarter-wave resonators, and solid structures such as tubes, plates, or cavities within the chimneys [22]–[24]. Considering the typical acoustic properties of black smoker fluids, and the geometry of these structures, the frequencies of the observed resonant signals are reasonable. For example, the fundamental frequency f of a Helmholtz resonator is given by [9]:3where c is the speed of sound in the fluid filling the cavity, A is the area of the cavity opening, V is the volume of the cavity, and L is the length of the cavity opening. For a relatively small 2-liter cavity connected to the chimney conduit by an opening of diameter 0.02 m and a length of 0.04 m, filled with hot hydrothermal fluid for which c = 450 m/s [16], f would equal ~120 Hz. For a quarter-wave oscillator, the first resonant mode is given by [23]:4where l is the length of the cavity. Thus for a 1-m tube closed at one end and filled with hot hydrothermal fluid, the fundamental frequency is ~113 Hz.\nBoth Sully and Puffer produce acoustic tones at several different frequencies. The different tones might be produced by different types of resonators, or by several resonators of a single type but with different geometries and different relationships to the fluid flow. In either case, the tones that are generated will depend strongly on the vent's morphological structure, and each vent within the vent field is likely to have its own unique acoustic signature.\n\n\nTemporal Variability\nThe relatively long timescale changes observed in the broadband and narrowband signals could be related to a variety of factors. Changes in fluid temperature or chemical composition could affect the density or compressibility of the fluid which are both critical parameters for all of the above-mentioned acoustic source mechanisms, and could affect both the amplitude and frequency content of the acoustic signals. Such changes in fluid temperature and composition are common in these systems [25]–[28]. Changes in fluid flow rate can also affect acoustic amplitudes and frequencies for several of the above-mentioned source mechanisms, such as fluid–structure interactions [17]. Changes in the geometry of the vent structure could also affect the acoustic signals. Either through episodic cracking events, or through gradual mineral deposition/dissolution, channels and voids within the structure may change shape, thereby affecting relative flow rates through the chimney, and the fundamental resonant frequencies of any resonant bodies. Such changes in vent geometry have been observed on these timescales [21].\nThe tidal variations in the Puffer record are likely related to changes in fluid discharge rate. The strongest evidence for this is the observed lag in acoustic output relative to high tide (Figure 6). Recent two-dimensional numerical models of tidally forced poroelastic convection within mid-ocean ridge hydrothermal systems predict that maximal discharge rates will lag high tide by ~125° [29]. Similar one-dimensional analytical models predict a lag of 135° [30]. Thus our observations are close to the expected values if it is assumed that faster discharge results in higher acoustic output—a good assumption for many of the potential acoustic source mechanisms. Other evidence shows that the tidal signal is not related to ocean currents, which are another conceivable source of tidal variability. We do not observe a ~4 cycles per day signal which would be associated with local current speeds that peak ~4 times per day [31]. We also do not observe a ~1.5 cycles per day signal which would be associated with the motions of regional inertial currents [32]. Thus it is likely that changes in discharge rate are reflected in the acoustic record, and further investigation into passive acoustic flow measurement techniques is warranted.\nIt is not entirely clear why the observed tidal variations are confined to the ~150–325 Hz and ~450–500 Hz frequency bands. We suspect that the acoustic power in these bands is dominated by source mechanisms that are more sensitive to flow rate, such as fluid–structure interactions, which can have intensities that depend on the fifth power of the fluid velocity [17]. The sound in the other parts of the spectrum could be generated by a source mechanism that is not as strongly dependent on mean discharge rate, such as pulsating exit flow [17]. This hypothesis could be tested with an array of hydrophones capable of determining which sound source mechanisms dominate in different frequency bands, by locating the sound sources in different bands, and measuring the near-field pressure distribution in different bands.\n\n\nBiological Implications\nSound production by black smokers has possible implications for local organisms. Considering the near-field sound pressure fall-off predicted for the likely sound source mechanisms, we estimate that vent sound levels would be above ambient levels at a distance of ~5–15 m from the vent orifice at the time our measurements were made. Thus fish, crustaceans, and cephalopods, which are common in these environments [33], [34] and can typically detect and process sound [35]–[38], might utilize this source of environmental information to their advantage. The acoustic detection of vent locations could help an organism avoid damage from hot hydrothermal fluid, and could provide foraging or reproductive benefits by helping an organism find food or a mating partner. An analogous adaptation is suspected in the Mid-Atlantic Ridge shrimp species Rimicaris exoculata which may use infrared light to locate hydrothermal vents [39]. In reef settings, certain fish larvae are known to use environmental sounds in their search for settlement habitat [40]. Novel field or laboratory studies could be used to investigate the effects of vent sounds on the local animal community.\n\n\nConclusion\nOur study shows that high-temperature seafloor vents produce high levels of acoustic radiation which can provide valuable information about geological and physical processes occurring within these systems, and may provide animals with information about the environment they inhabit. Several new lines of inquiry regarding the acoustical, geophysical and biological implications of hydrothermal vent sounds should soon be explored.\n\nWe developed two versions of the deep-sea digital acoustic recording system used in this study. Both versions were based on the Persistor CF2 microcontroller and both had 4 GB of flash memory capable of storing ~6–10 days of continuous sound data. Both versions could be equipped with two hydrophones, based on the Benthos AQ-2000 piezoceramic sensor element. One hydrophone was affixed to the instrument's titanium pressure case, and the other had a 3 m cable and was attached to a bracket allowing it to be positioned just a few centimeters from a black smoker vent orifice. We refer to these two hydrophones as the “case” and “remote” hydrophones, respectively. The first-generation system, used in 2004, sampled on three channels continuously at 1000 Hz with a 12-bit A/D converter and a fixed gain for each channel. The second-generation system, used in 2005, sampled on two channels continuously at 1920 Hz with a 16-bit A/D converter and a programmable gain. Additional information on the instrument specifications is shown in Table 2.In September 2004 we lowered the acoustic recording system to the seafloor using a free-falling platform (called an elevator) which landed about 150 m east of the Sully vent. The instrument recorded several hours of ambient noise at this location. We then used a remotely operated vehicle to move the instrument to the Sully vent, with the case hydrophone positioned approximately 0.3 m from the vent orifice, and the remote hydrophone positioned approximately 0.1 m from the orifice. Figure 2a shows the instrument deployed at Sully. The acoustic output of this vent was higher than expected, and the remote hydrophone signal was heavily clipped on both channels. The case hydrophone was sufficiently sensitive to record the vent sounds, thus all the 2004 data shown in this paper were collected with the case hydrophone. After about 45 hours of data collection, the remote hydrophone cable was destroyed by hot hydrothermal fluid, at which time the instrument stopped recording.In September 2005 we deployed the system at the Puffer vent with the case hydrophone positioned approximately 0.3 m from the vent orifice. The remote hydrophone had been destroyed by venting fluid in a previous deployment attempt during the same year, thus all of the 2005 data shown in this paper were collected with the case hydrophone. Figure 2b shows the instrument deployed at Puffer. After about 136 hours of data collection, the instrument was recovered, then placed on an elevator and returned to the seafloor to collect an ambient noise recording. The elevator landed about 25 m east of the Hulk vent in the northern part of the vent field, and at this location the instrument recorded several hours of data before being recovered.We converted the raw hydrophone values into units of zero-mean pressure by first applying a 1-Hz high-pass 4-pole Butterworth filter. We divided the signal by the overall system gain used by the recording instrument for that channel, then applied the nominal sensitivity of the hydrophone element as published by the manufacturer.We obtained 10-minute average spectra using Welch's method [41] with 215-point fast Fourier transforms applied to 9600-point sections of the record. Each section of the record overlapped adjacent sections by 50 percent, and was multiplied by a Hamming window [42]. These 10-minute spectra were then time averaged to obtain the representative hour-average spectra shown in Figure 3. The 10-minute spectra are contoured in Figure 4.We conducted the spectral analysis of the power time-series for different acoustic frequencies by first averaging the power into 5-Hz bands. For each of these records, we applied a 1-cycles-per-day high-pass 4-pole Butterworth filter, then normalized the record by its root-mean-square. Finally we multiplied each record by a 50-percent Tukey (cosine-taper) window [42], and computed periodograms using a 214-point fast Fourier transform. Contours of these periodograms are shown in Figure 5."
        },
        "10.1371/journal.pone.0107287": {
            "author_display": [
                "Junaid Ahmad Khan",
                "Meraj Mustafa",
                "Tasawar Hayat",
                "Ahmed Alsaedi"
            ],
            "title_display": "On Three-Dimensional Flow and Heat Transfer over a Non-Linearly Stretching Sheet: Analytical and Numerical Solutions",
            "abstract": [
                "\nThis article studies the viscous flow and heat transfer over a plane horizontal surface stretched non-linearly in two lateral directions. Appropriate wall conditions characterizing the non-linear variation in the velocity and temperature of the sheet are employed for the first time. A new set of similarity variables is introduced to reduce the boundary layer equations into self-similar forms. The velocity and temperature distributions are determined by two methods, namely (i) optimal homotopy analysis method (OHAM) and (ii) fourth-fifth-order Runge-Kutta integration based shooting technique. The analytic and numerical solutions are compared and these are found in excellent agreement. Influences of embedded parameters on momentum and thermal boundary layers are sketched and discussed.\n"
            ],
            "publication_date": "2014-09-08T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 45,
            "shares": 0,
            "bookmarks": 0,
            "url": "http://www.plosone.org/article/info:doi/10.1371/journal.pone.0107287",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0107287&representation=PDF",
            "fulltext": "IntroductionThe fundamental problem of two-dimensional flow due to stretching plane surface, initially discussed by Crane [1], is involved in various industrial processes such as metal and polymer extrusion, drawing of plastic films, paper production etc. Owing to such applications, the researchers have discussed this problem under various aspects including suction or injection, variable surface temperature, convective boundary condition, mass transfer, mixed convection etc. The three-dimensional flow due to plane bi-directional linearly stretching sheet was first discussed by Wang [2]. He found an exact similarity solution of the classical Navier-Stokes equations. Later, Lakshmisha et al. [3] numerically examined the unsteady three-dimensional flow with heat and mass transfer over an unsteady stretching sheet. In contrast to this problem, Takhar et al. [4] investigated the three-dimensional flow of an electrically conducting fluid due to an impulsive motion of the stretching sheet. Ariel [4] derived approximate analytic and numeric solutions for steady three-dimensional flow over a stretching sheet. Xu et al. [5] provided uniformly valid series solutions for three-dimensional unsteady flow caused by the impulsively stretching sheet. Liu and Andersson [6] considered the heat transfer in three-dimensional flow due to non-isothermal stretching sheet. The unsteady three-dimensional flow of elastico-viscous fluid and mass transfer due to unsteady stretching sheet with constant wall concentration was studied by Hayat et al. [7]. In another paper, Hayat et al. [8] described the three-dimensional flow of Jeffrey fluid due to stretching sheet. Liu et al. [9] firstly discussed the three-dimensional flow due to exponentially stretching sheet numerically. Steady flow of nanofluid past a linearly bi-directional stretching sheet through Buongiorno's model was examined by Junaid et al. [10]. Sheikholeslami and Ganji [11] discussed the flow and heat transfer of nanofluid between parallel sheets in the presence of Brownian motion and thermophoresis effects.\nThe literature cited above deals only with the case of either linearly or exponentially driven velocity of the sheet. Vajravelu [12] numerically discussed the viscous flow due to stretching sheet when the velocity of the sheet was assumed to obey the power-law distribution, i.e.. He computed numerical solutions for various values of power-law index  Cortell [13] extended this problem by considering viscous dissipation effects and variable surface temperature. The steady boundary layer flow of micropolar fluid over non-linearly stretching sheet was discussed by Bhargava et al. [14]. Radiation and viscous dissipation effects on the boundary layer flow above a non-linearly stretching sheet were explored by Cortell [15]. Homotopy analytic solutions for mixed convection flow of micropolar fluid past a non-linearly stretching vertical sheet were obtained by Hayat et al. [16]. Kechil and Hashim [17] derived analytic solutions for MHD flow over a non-linearly stretching sheet by Adomian decomposition method. Hayat et al. [18] used modified decomposition method for the series solutions of MHD flow of an electrically conducting fluid over a non-linearly stretching surface. The impact of chemical reaction on the flow over a non-linearly stretching sheet embedded in a porous medium was investigated by Ziabakhsh et al. [19]. Rana and Bhargava [20] computed numerical solutions for two-dimensional flow of nanofluid due to non-linearly stretching sheet by finite element method. Shahzad et al. [21] obtained closed form exact solutions for axisymmetric flow and heat transfer when the velocity of the stretching sheet was proportional to . Partial slip effects on the boundary layer flow past a non-linearly permeable stretching surface have been addressed by Mukhopadhyay [22]. In another paper, Mukhopadhyay [23] analyzed the flow and heat transfer of Casson fluid due to non-linearly stretching sheet. Rashidi et al. [24] derived homotopy based analytic solutions for flow over a non-isothermal stretching plate with transpiration.\nTo our knowledge, the three-dimensional flow due to non-linearly stretching sheet has not been yet reported. It is obvious that three-dimensional flows can be appropriate in giving more clear physical insights of the real world problem when compared with the two-dimensional flows. The present work is therefore undertaken to fill such a void. The study also assumes that the temperature across the sheet is non-linearly distributed. Introducing a new set of similarity variables the boundary layer equations are first reduced into self-similar forms and then solved both analytically and numerically. It is pertinent to mention that computation of either approximate analytic or numerical solutions of the boundary layer equations governing the flow and heat transfer is often challenging (see [25]–[33] for details). Attention is focused on the physical interpretation of parameters including the power-law index \nMathematical ModelingLet us consider the three-dimensionalincompressible flow over a plane elastic sheet located at  as shown in the Fig. 1. The flow is induced due to stretching of the sheet in two lateral directions. Let  and  be the velocities of the sheet along the  and  directions respectively with  are constants (see Table 1).  is the variable surface temperature where  is constant and  is the ambient fluid temperature. Under the usual boundary layer assumptions, the equations governing the three-dimensional flow and heat transfer in the absence of viscous dissipation and internal heat generation/absorption can be expressed as (see Liu et al. [9])(1)\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Physical configuration and coordinate system.doi:10.1371/journal.pone.0107287.g001\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  List of symbols.doi:10.1371/journal.pone.0107287.t001(2)\n(3)\n(4)where  and  are the velocity components along the  and  directions respectively,  is the kinematic viscosity,  is the fluid temperature and  is the thermal diffusivity (see Table 1). The boundary conditions are imposed as under:\n(5)\nWe introduce the new similarity transformations as follows:(6)\nWe have modified the similarity transformations used by Liu et al. [9] for the current problem. Using (6), Eq.(1) is identically satisfied and Eqs. (2)–(5) become(7)\n(8)\n(9)\n(10)\nwhere  is the Prandtl number and  is the ratio of stretching rate along the  direction to the stretching rate along the  direction (see Table 1). The above equations reduce to the case of two-dimensional flow when . Moreover, when , the equations governing the axisymmetric flow due to non-linearly stretching sheet are recovered. When  the solution of  is also a solution of . The quantities of practical interest are the skin friction coefficients and the local Nusselt number which are defined as below:(11)where  and  are the wall shear stresses and  is the wall heat flux. These are given by\n(12)\nusing Eqs. (6) and (12) in Eq. (11), one obtains(13)where  and  are the local Reynolds numbers along the  and  directions respectively (see Table 1). The vertical component of velocity at the far field boundary can be expressed as\n(14)Optimal homotopy analytic solutionsThe non-linear differential equations (7)–(9) with the boundary conditions (10) have been solved by optimal homotopy analysis method (OHAM) [34], [35]. For this purpose, we first select the initial guesses   and  of   and  as under:(15)\nand the auxiliary linear operators are selected as below(16)\nIf  is the embedding parameter and  the non-zero auxiliary parameter, then the generalized homotopic equations corresponding to (7)–(10) can be written as follows(17)\n(18)\n(19)\n(20)\nwhere the non-linear operators ,  and  are\n(21)(22)\n(23)\nBy Taylor's series expansion one obtains(24)\n(25)\n(26)\nSubstituting  in the above equations yields the final result. The functions  and  can be determined from the deformation of Eqs. (7)–(10). Explicitly the mth-order deformation equations corresponding to Eqs. (7)–(10) are as below(27)\n(28)\n(29)\n(30)\n\n(31)\n(32)\n(33)\n(34)\nIn order to determine the optimal values of  we define the squared residuals of the governing Eqs. (7)–(10),  and  as(35)\n(36)\n(37)\nSuch kind of error has been considered in other works [36]–[41]. The smaller  the more accurate the mth order approximation of the solution. The optimal values of  can be obtained by minimizing the  through the command Minimize of the software MATHEMATICA (see Liao [36] for details). Alternatively MATHEMATICA package bvph 2.0 can also be used to calculate such values (see [41] for details).\nEqs. (7)–(9) subject to the boundary conditions (10) have been solved numerically by shooting method with fifth order Runge-Kutta integration procedure. First, we reduce the original ODEs into a system of 1st order ODEs by substituting  and  which gives(38)and the corresponding initial conditions are(39)Suitable values of the unknown initial conditions  and  are guessed and then integration is carried out. The values of  and  are then iteratively computed through Newton's method such that the solutions satisfy the boundary conditions at infinity (given in Eq. (10)) with error less than ."
        },
        "10.1371/journal.pone.0039999": {
            "author_display": [
                "Wade A. Walker"
            ],
            "title_display": "The Repeated Replacement Method: A Pure Lagrangian Meshfree Method for Computational Fluid Dynamics",
            "abstract": [
                "\n        In this paper we describe the repeated replacement method (RRM), a new meshfree method for computational fluid dynamics (CFD). RRM simulates fluid flow by modeling compressible fluids’ tendency to evolve towards a state of constant density, velocity, and pressure. To evolve a fluid flow simulation forward in time, RRM repeatedly “chops out” fluid from active areas and replaces it with new “flattened” fluid cells with the same mass, momentum, and energy. We call the new cells “flattened” because we give them constant density, velocity, and pressure, even though the chopped-out fluid may have had gradients in these primitive variables. RRM adaptively chooses the sizes and locations of the areas it chops out and replaces. It creates more and smaller new cells in areas of high gradient, and fewer and larger new cells in areas of lower gradient. This naturally leads to an adaptive level of accuracy, where more computational effort is spent on active areas of the fluid, and less effort is spent on inactive areas. We show that for common test problems, RRM produces results similar to other high-resolution CFD methods, while using a very different mathematical framework. RRM does not use Riemann solvers, flux or slope limiters, a mesh, or a stencil, and it operates in a purely Lagrangian mode. RRM also does not evaluate numerical derivatives, does not integrate equations of motion, and does not solve systems of equations.\n      "
            ],
            "publication_date": "2012-07-06T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 1631,
            "shares": 2,
            "bookmarks": 1,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0039999",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0039999&representation=PDF",
            "fulltext": "IntroductionIn this paper, we first present background material on CFD and discuss previous CFD methods which have informed this work. Then we motivate RRM and explain its workings in depth. Next, we show that RRM gives correct results for many standard test problems. We also demonstrate that RRM shows steadily decreasing error in its solutions as we increase the desired accuracy, and that RRM handles many common types of boundary conditions. Finally, we discuss the similarities and differences between RRM and other CFD methods.\n\nBackground\nCFD is the use of numerical methods to model liquid and gas flow. CFD has many practical uses, from the analysis of the airflow over vehicles to the design of water turbines.\nCFD covers a vast range of fluid compositions and flow types. For simplicity, we only consider a fluid that is:\n\n\n\n\nContinuous: Infinitely subdividable, unlike real fluids which are made of discrete atoms and molecules.\n\nSimple: Completely described by density, velocity, and pressure at each point, which we call the “primitive variables”, and write as ρ, u, and p. We do not consider other possible fluid properties like chemical reactivity. We also do not consider the action of non-pressure forces like gravity or electromagnetism on the fluid.\n\nIdeal: Described by the ideal gas law, in which the internal energy of a fluid is purely a function of ρ, p, and γ. The constant γ is called the ratio of specific heats, and has a value of about 1.4 for air.\n\nSingle-phase: Consisting entirely of either liquid or gas, but not a mixture of the two. This means we need not model liquid-gas interfaces. We also do not consider the interaction of solid objects with the fluid.\n\nInviscid: Having no resistance to deformation. This simplifies the equations of fluid motion.\n\nAdiabatic across contacts: Allowing no heat to flow from one side of a contact discontinuity to the other. This means that contact-adjacent regions will not tend towards the same temperature. We compare RRM’s results to fluid flows that are adiabatic across contacts because of the availability of analytic solutions, but we show later that RRM is not adiabatic across contacts.\n\nOne-dimensional: Having only one spatial dimension. This makes illustration and programming simpler.\n\nEven though our fluid is infinitely subdividable, for illustration and analysis we divide it into finite-sized cells. Figure 1 shows a cell c1 with its left edge at x1 and its right edge at x2. The density, velocity, and pressure components are shown on separate graphs.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Fluid cell with three separate components.Fluid cell c1 has density, velocity, and pressure components ρ, u, and p. The left and right coordinates of the cell are x1 and x2.\ndoi:10.1371/journal.pone.0039999.g001When we do not need to show all three components separately, we combine them onto one axis for simplicity as shown in figure 2, with the understanding that ρ, u, and p may have different values even though they are drawn with the same line.\nWe can describe fluid flow with cells in two main ways. The Eulerian description considers the cells to be stationary, and the fluid to flow across their edges and through them. The Lagrangian description considers the cells to move along with the fluid, so any given bit of fluid is always found in the same cell. We will initially use the Eulerian description since it is the most common. We will later switch to the Lagrangian description when we describe RRM in more detail.\nGiven the restrictions and cell definition above, we can model fluid flow with a set of equations called the Euler equations, which can be derived from the local conservation of mass, momentum, and energy. The Euler equations take on different forms depending on whether we write them for the Eulerian or Lagrangian description of fluid flow. For the Eulerian description, we write the Euler equations in English like this:\n\n\n\n\nConservation of mass: The mass in a cell changes by the amount that flows across its edges.\n\nConservation of momentum: The momentum in a cell changes by the amount that flows across its edges, and by the amount due to the pressure acting on its edges.\n\nConservation of energy: The energy in a cell changes by the amount that flows across its edges, and by the amount due to work done by the pressure acting on its edges.\n\nThe Euler equations are typically written as partial differential equations, but we write them here as integral equations because it is more natural for our derivative-free numerical method. Here are the Euler equations for a cell, written for the Eulerian description of fluid flow, in conservation form:(1)\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Fluid cell with three superimposed components.Fluid cell c1 has density, velocity, and pressure components all superimposed on the same axis. The left and right coordinates of the cell are x1 and x2.\ndoi:10.1371/journal.pone.0039999.g002The coordinates x1 and x2 are the left and right edges of the cell. The times t1 and t2 are the starting and ending times of a period where fluid is flowing into and out of the cell, and pressure is acting on the cell edges.\nThis form is called the conservation form because it is written in terms of the conserved quantities per unit length. These conserved quantities are mass per unit length ρ, momentum per unit length ρu, and energy per unit length ρeT.\nThe specific total energy eT is the energy per unit mass due to both macroscopic and microscopic motion. The ideal gas law gives us equations for eT and for the speed of sound a, which we will use later.(2)(3)\nTo write the Euler equations in a more compact form we define a vector of the conserved quantities(4)and a vector of the fluxes (plus the pressure and pressure work) at the edges\n(5)Then the Euler equations can be written as a single vector equation(6)\nFor the general initial conditions ρ(x,t1), u(x,t1), and p(x,t1), the Euler equations have no known analytical solution. This is inconvenient when we wish to check the results of a numerical method. So in this paper we restrict ourselves to simple initial conditions known as the Riemann problem, where ρ, u, and p take on the constant values (ρl, ul, pl) and (ρr, ur, pr) on the left and right sides of an initial discontinuity, as shown in figure 3.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  The Riemann problem.The Riemann problem specifies initial density, velocity, and pressure values of ρl, ul, pl on the left side of the origin and ρr, ur, pr on the right side of the origin.\ndoi:10.1371/journal.pone.0039999.g003Unlike the general initial conditions, the Riemann problem has an analytical solution, though this solution contains a nonlinear implicit equation and a number of special cases that we must treat carefully. In this paper, we use a Riemann solver due to Toro [1] as a standard to test RRM’s results against. Many CFD methods, beginning with Godunov’s method in 1959 [2], use an embedded Riemann solver as a part of their algorithms, though RRM does not.\nEven for the Riemann problem, accurate numerical solutions to the Euler equations are challenging, mainly because the solutions can include discontinuities. At these discontinuities, the spatial derivatives in the differential form of the Euler equations are undefined, which spoils many simple numerical methods and requires special-case code in more advanced methods.\nIn the solutions to many other partial differential equations such as the heat equation, initial discontinuities will smear out and become increasingly smooth over time. But in the solutions to the Euler equations, initial discontinuities do not always smear out, and indeed new discontinuities may arise over time.\nFor example, consider Sod’s shock tube problem [3], a special case of the Riemann problem. A shock tube is a gas-filled tube with a diaphragm in the center. The diaphragm is initially airtight, so the left and right sides of the tube can be separately charged to specific pressures and densities as shown in figure 4, which sets (ρl, ul, pl) = (1.0, 0.0, 1.0) on the left side, and (ρr, ur, pr) = (0.125, 0.0, 0.1) on the right side.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Sod’s shock tube problem at t = 0.0 seconds.Sod’s shock tube problem showing initial density, velocity, and pressure values (ρl, ul, pl) = (1.0, 0.0, 1.0) and (ρr, ur, pr) = (0.125, 0.0, 0.1).\ndoi:10.1371/journal.pone.0039999.g004At time t = 0.0, we instantly remove the diaphragm and let the fluid start flowing from left to right. Figure 5 shows the fluid at t = 1.5 seconds. We can see both types of discontinuity that are possible in solutions to the Euler equations, as well as the “expansion fan” that joins the high-pressure left state to the flat area in the center.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Sod’s shock tube problem at t = 1.5 seconds.Sod’s shock tube problem showing density ρ, velocity u, and pressure p after 1.5 seconds of time evolution. We can see three flow features: an expansion fan, a contact, and a shock.\ndoi:10.1371/journal.pone.0039999.g005The first type of discontinuity, a contact, separates two areas that differ only in density. Contacts travel along with the fluid, and since velocity is constant across a contact, no fluid flows across them. Contacts cannot form spontaneously; they must either be present in the initial conditions as in Sod’s problem, or they must be formed by the intersection of two shocks. As a real-world example, if stationary hot and cold water masses are carefully placed side by side, they will be separated by a contact discontinuity, at least until heat energy flows across the discontinuity and smears it out.\nThe second kind of discontinuity, a shock, can be formed by a pressure gradient steep enough to force the fluid to move faster than the local speed of sound a. Shocks can develop over time, and need not be present in the initial conditions. Density, velocity, and pressure can all change across a shock. As a real-world example, if you pilot a boat through the water faster than waves can travel through the water, the boat creates a shock at its bow.\nThe 2D graphs of Sod’s shock tube problem above show flow features at specific times, but do not show how the fluid flow evolves over time. Figure 6 uses 3D to add a time dimension.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Sod’s shock tube problem time evolution from t = 0 to t = 1.5 seconds.Sod’s shock tube problem showing density ρ, velocity u, and pressure p from time t = 0 to time t = 1.5 seconds. We can see three flow features: an expansion fan, a contact, and a shock. The contact and the shock both start at the origin and move to the right, with the shock running ahead due to its higher speed. The expansion fan gradually slopes left as more and more fluid flows to the right to feed the shock.\ndoi:10.1371/journal.pone.0039999.g006These graphs show how the contact and the shock both start at the origin and move to the right, with the shock running ahead due to its higher speed. They also show how the expansion fan gradually slopes left as more and more fluid flows to the right to feed the travelling shock.\nFor subsequent figures we will mainly use 2D graphs, since they allow easier comparison of our results with those of a Riemann solver. We will use 3D only when the time evolution of the flow is of special interest, such as when we illustrate boundary conditions.\n\n\nPrevious Work\nIf you simply use the definition of the derivative to convert the Euler equations from differential equations to algebraic equations, you get the finite difference method (FDM). In conservation form, FDM models a fluid as a set of cells, each of which contains the values of the conserved quantities at a point within the cell. The explicit version of FDM calculates those values at the next time step from the values in nearby cells at the current time step. The set of nearby cells is called the stencil.\nThe finite volume method (FVM) also models a fluid as a set of cells, but it stores cell average values instead of point samples in the cells. In its explicit conservation form, FVM calculates the values at the next time step by adding and subtracting fluxes of the conserved quantities across each neighboring cell’s edges during the time step.\nThe finite element method (FEM) was historically used for structural mechanics [4], but began to find use in fluid dynamics [5] as the method was generalized and applied to time-varying problems. FEM starts by creating a mesh of elements (cells in our terminology) which are shaped to fit the problem geometry. FEM then solves a system of equations at each time step to determine the unknown fluid values in each element. Fluid values in FEM are typically stored in piecewise polynomial form, as opposed to the point samples of FDM or the cell averages of FVM.\nIn FDM and FVM, the fluid is usually considered to move through stationary cells in a single, global coordinate system. This is the Eulerian description of fluid flow mentioned above.\nFigure 7 shows an example with three stationary cells c1, c2, and c3. The measuring points xm1, xm2, and xm3 are at the cell centers. The entire fluid has a rightward velocity u. In panel A at time t1, we measure cell c1’s ρ and p values at xm1, and c2’s values at xm2. Cell c3 is empty.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 7.  Eulerian description of fluid flow.In the Eulerian description of fluid flow, the fluid moves through stationary cells. Consider a global coordinate system divided into three cells c1, c2, and c3. The fluid is traveling rightwards with velocity u. In panel A at time t1, a measurement at point xm1 will show the density ρ and pressure p of cell c1. In panel B at a later time t2 = (xm2– xm1)/u, we measure the same density and pressure at point xm2 because the fluid has moved to the right by one cell width.\ndoi:10.1371/journal.pone.0039999.g007In panel B at a later time t2 = (xm2– xm1)/u, all the fluid from c1 has moved into c2, and all the fluid from c2 has moved into c3. Now we measure the same ρ and p values at xm2 that we previously measured at xm1, and the same values at xm3 that we previously measured at xm2. The fluid has moved one cell width to the right, but the cells themselves have stayed in place.\nEulerian methods are relatively simple to implement, but shocks, contacts and other steep gradients may smear out or gain unphysical oscillations as they cross cell edges, depending on the algorithm used. Researchers have proposed many refinements over the years to increase accuracy, such as Total Variation Diminishing (TVD) methods [6], Essentially Non-Oscillatory (ENO) methods [7], Monotone Upwind Schemes for Scalar Conservation Laws (MUSCL) [8], the Piecewise-Parabolic Method (PPM) [9], and many more.\nAnother approach to Eulerian fluid flow is the lattice Boltzmann method (LBM) [10]. Instead of a mesh of cells, LBM uses a lattice of connected sites, each of which can “stream” fluid to a fixed number of neighboring sites. Each site contains a distribution function that represents how much fluid is streaming in each direction. After each streaming step, LBM executes a “collision” step at each site to alter the distribution functions to maintain conservation. LBM has many attractive features, including ease of programming and simple handling of boundary conditions.\nIn contrast to FDM, FVM, and LBM, FEM often uses the alternative Lagrangian description of fluid flow, in which the cells travel along with the fluid.\nFigure 8 shows an example, with two cells c1 and c2 moving to the right with a velocity u, similar to the Eulerian example above. However, in the Lagrangian description the fluid does not move across cell edges. Instead, the cells themselves move, carrying local coordinate systems along with them.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 8.  Lagrangian description of fluid flow.In the Lagrangian description of fluid flow, the cells are part of the fluid and move along with it. Consider fluid cells c1 and c2 traveling with their own local coordinate systems. The cells and their coordinate systems are both traveling rightwards at velocity u in the global coordinate system. In panel A at time t1, and in panel B at any later time t2, points xm1 and xm2 in the cells’ coordinate systems remain at the same places in those cells. No fluid crosses cell edges.\ndoi:10.1371/journal.pone.0039999.g008Panel A shows us measuring the values at time t1 of ρ and p at point xm1 in the local coordinate system of cell c1. Panel B shows that we will measure the same values at any later time t2, since cell c1 and its coordinate system move together. The same holds true for cell c2 in its local coordinate system at its own point xm2.\nLagrangian methods handle shocks and contacts naturally, because those flow features travel with the fluid instead of smearing out as they cross cell edges. But pure Lagrangian methods are rare, because as the fluid flows, the cells can become excessively bunched up, stretched out, or deformed, which can reduce simulation accuracy and efficiency.\nThe cells of FDM, FVM, and FEM, and the lattice sites of LBM, are usually connected in a mesh. Each cell has a well-defined shape, and each cell or site has a fixed set of neighbors. In simple methods, these shapes and sets of neighbors are constant over the whole course of the simulation. But in Eulerian methods, a fluid may have complex flow features that move around over time, so we may want to create smaller cells in those complex areas and larger cells in other areas. Or in Lagrangian methods, some cells may become degenerate or singular in a complex flow, so that the method’s equation solver no longer works correctly.\nThe process of changing the mesh to alleviate these problems is called remeshing. Figure 9 panel A shows eight small fluid cells, and panel B shows those eight cells remeshed into two cells that cover the same area.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 9.  Remeshing.The eight cells in panel A can be remeshed into two cells in panel B that cover the same area and contain the same mass, momentum, and energy. Some CFD methods require remeshing to maintain accuracy or to prevent numerical difficulties.\ndoi:10.1371/journal.pone.0039999.g009To avoid this complication, the so-called meshfree methods do away with mesh connectivity entirely. One of the first meshfree methods was smoothed-particle hydrodynamics (SPH) [11], [12]. SPH is a purely Lagrangian method which models a fluid with a set of moving particles, and computes the fluid’s properties at any point by summing the contributions of nearby particles using a kernel function which smooths out the particles’ properties over some “smoothing length”. SPH was originally motivated by the study of astrophysical problems such as galaxy formation, where the constituents were already discrete particles. SPH was later applied to other problems where the fluid was presumed to be continuous before being discretized.\nThe moving-particle semi-implicit method (MPS) [13] is a meshfree method similar to SPH, which was originally intended for simulation of incompressible fluids with interacting free surfaces. It also uses a kernel function (called a weight function in the MPS literature), but one which is specially designed to repel particles at short distance, thereby maintaining approximately constant density in the fluid. MPS has been applied to many situations, including simulations of coastal waves and dam breaks.\nMuch research in meshfree methods has been done in recent years, and there is now a great variety of such methods with different kernel functions, particle properties, and integration techniques. Li and Liu [14] and Huerta et al. [15] have both written excellent surveys of the field.\n\nMethods\nMotivation\nRRM was motivated by Chaikin’s corner-cutting algorithm for curve generation, which evolved into the subdivision curves of computational geometry [16], [17]. A curve of this type starts as a set of lines joined end to end to form a roughly faceted curve, shown in figure 10 panel A. First we cut off each of the corners one-quarter of the way along each side, shown in panel B. Then we cut the corners off the new corners, shown in panel C, iteratively refining the curve into smaller and smaller line segments, until a desired level of smoothness is reached in panel D.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 10.  Chaikin’s corner-cutting algorithm.Starting with the triangle in panel A, cutting the corners off one-quarter of the way along each side gives us panel B. Panels C and D show the process carried out two more times. We can repeat this process until the curve has any desired smoothness.\ndoi:10.1371/journal.pone.0039999.g010RRM does the same sort of iterative refinement, but on a moving fluid instead of a stationary curve, and with constraints on conservation of mass, momentum and energy rather than constraints on surface continuity and smoothness.\n\n\nOverview\nTo begin, we divide a fluid into finite-sized cells. In one dimension, each cell is a line segment with an associated density, velocity, and pressure, all of which are constant across the cell. Figure 11 shows a fluid divided into three cells c1, c2, and c3. For now we use periodic boundary conditions, so the left side of c1 is adjacent to the right side of c3. We indicate this with the dotted line on the right of c3.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 11.  Fluid cells.A fluid divided into three cells c1, c2, and c3. The dotted line at the right shows that there are periodic boundary conditions, so the right side of c3 is adjacent to the left side of c1.\ndoi:10.1371/journal.pone.0039999.g011At each cell edge, we send tracer particles left and right through the fluid at the local speed of sound a, as shown in figure 12. Each pair of tracer particles defines an expanding wavefront of change that originates at the cell edge. For example, in figure 12 we show w23, the wavefront originating between c2 and c3, along with its left tracer particle pl and its right tracer particle pr.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 12.  Fluid cells showing wavefronts and tracer particles.A fluid divided into three cells c1, c2, and c3. The dotted line at the right shows that there are periodic boundary conditions. Wavefront w23 originates between c2 and c3, and contains tracer particles pl and pr that travel through the fluid at the local speed of sound a = sqrt(γp/ρ). The constant γ depends on the fluid (it has a value of 1.4 for air). The ρ and p values are those of the cell the particle is traveling through. Note that w31 (not labeled) extends into both c3 and c1 due to the periodic boundary conditions.\ndoi:10.1371/journal.pone.0039999.g012As each tracer particle travels, it accumulates an error metric that tracks how much each of the primitive variable values has changed, and over what distance. Figure 13 shows a tracer particle p, the right-hand particle of wavefront w, moving through the fluid. The particle’s error metric Δ grows as the particle moves, with the slope of Δ changing when the particle crosses each cell edge.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 13.  Error metric growing as a tracer particle travels.A particle p traveling right as part of wavefront w. Its error metric Δ increases as the particle travels, with the slope of Δ changing at cell edges.\ndoi:10.1371/journal.pone.0039999.g013When this error metric for either of the two tracer particles in any wavefront exceeds a preset maximum, we chop the wavefront area out of the fluid, flatten the chopped-out cell parts into a single new cell, and insert that new cell into the hole left by the chopping.\nIn areas of the fluid where primitive variable values differ greatly from cell to cell, tracer particles’ error metrics will accumulate quickly, so new cells will be chopped out soon after wavefront creation. This leads to more, smaller cells in areas of the fluid with steep slopes. Conversely, in areas where values are very similar from one cell to the next, error metrics will accumulate slowly, so we will chop out fewer, larger cells in areas of the fluid with shallow slopes.\nWe illustrate this whole process in figure 14. In panel A, we chop wavefront w23 out of the fluid, removing the wavefront’s tracer particles from the fluid at the same time. This leaves us with chopped cell parts c2c and c3c, shown in panel B. Panel C shows us flattening c2c and c3c into a new cell c4 of the same mass, momentum and energy. Then in panel D we insert c4 into the fluid and create new wavefronts w24 and w43 at the cell edges.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 14.  Chopping, flattening, and new cell creation.Panel A shows the chopping of wavefront w23 out of the fluid, which chops off the right side of c2 and the left side of c3. Panel B shows the resulting chopped parts c2c and c3c. Panel C shows the flattening of the two chopped parts into a new cell c4 with the same mass, momentum, and energy. Panel D shows the insertion of the new cell c4 into the fluid and the creation of the new wavefronts w24 and w43 on its edges.\ndoi:10.1371/journal.pone.0039999.g014The chop-flatten-create process always results in exactly one new cell, and always shrinks two other cells by chopping parts off of them. But this process can also remove any number of whole cells if the maximum error metric allows the wavefront to grow wide enough. For example, if wavefront w23 in figure 14 had grown wider, it could have chopped off the right side of c1, entirely removed c2, and chopped off the left side of c3, resulting in no net change in the number of cells. An even wider wavefront which removes two whole cells would reduce the total number of cells in the fluid by one, and so on. This is how RRM increases and decreases the total number of cells over time to adapt to changing fluid conditions.\nThe last step in the RRM algorithm is to choose the next wavefront whose tracer particles have reached the maximum error metric and repeat the chop-flatten-create process detailed above. This repetition evolves the fluid simulation forward in time.\n\n\nStored Quantities\nIn each cell, we store three main types of data:\n\n\n\n\nThe size, shape, and position of the cell. In one dimension, cells have only width, so we need only store the time-varying x coordinate x1(t) of the cell’s left edge, and the cell’s width w.\n\nThe cell’s three primitive variable values ρ, u, and p.\n\nFour extra vector quantities which help us ensure conservation.\n\nBelow we explain the relationships between these quantities and show how to derive other necessary values from them.\nRRM is purely Lagrangian and represents the fluid as finite-sized cells, so we use the integral Lagrangian form of the Euler equations, written in terms of the primitive variables:(7)\nThe values m1 and m2 are the mass coordinates of the left and right side of the cell. The mass coordinates move with the fluid, unlike the fixed spatial coordinates x1 and x2 that we used in the Eulerian form of these equations in equation set 1. This means that the fluid between m1 and m2 stays between m1 and m2, with no fluid flow across the cell edges. We can get the mass coordinate m at a point in the fluid from the Eulerian coordinate x at that point by integrating all the mass up to that point:(8)\nSo the value of the mass coordinate at any point is the sum of all the mass to the left of that point in the fluid.\nNote that the conservation of mass equation does not appear in equation set 7. That is because the mass between m1 and m2 is constant, so that equation would be trivial. Instead we show the conservation of volume equation, which merely says that a cell’s volume v changes as its edges move towards or away from each other. In the Eulerian form of these equations, it was the conservation of volume equation that was trivially equal to a constant, so we omitted it from equation set 1.\nNote also that the equations for the conservation of momentum and energy are simpler in the Lagrangian form than in the Eulerian form. This is because we do not need the flux terms that describe momentum and energy flowing across the cell edges, now that the cell edges move with the fluid.\nAs we saw with the Eulerian form, if we define a vector of the conserved quantities(9)and a vector of the velocity, pressure and pressure work at the cell edges(10)then the Lagrangian form of the Euler equations can be written as a single vector equation\n(11)We do not store the cells’ conserved quantities directly, but we can calculate them by integrating over the primitive variables. Since our primitive variables are piecewise constant, the integrals are simply multiplications by w, the width of the cell.(12)\nTo allow our flattening process to exchange energy between kinetic and potential forms while remaining conservative, we store two extra vector quantities on each edge of each cell: pressure momentum and pressure energy.\nFigure 15 shows these quantities for a single cell c1. We define them as follows:\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 15.  Pressure momentum and pressure energy vectors.A single cell c1 showing left and right pressure momentum Ppl and Ppr, and left and right pressure energy Epl and Epr. Portions of these vectors are transferred to new cells during the chopping and flattening process, and this transfer is what causes the conversion between potential and kinetic energy and vice versa.\ndoi:10.1371/journal.pone.0039999.g015(13)The first quantity Δt is the time it takes a tracer particle to cross a cell, and also the time it takes for a cell to completely expend its store of pressure momentum and pressure momentum upon its neighbors. The next four quantities are the terms on the right-hand sides of the momentum and energy Euler equations from equation set 7. They represent the changes in momentum and energy due to pressure work that a cell has the potential to cause to its neighbors. We store equal quantities of Pp and Ep in each direction, so for each cell they sum to zero, leaving the overall momentum and energy of the fluid unchanged.\nAs the fluid evolves, the total fluid mass, momentum, and energy remain strictly constant when these extra Pp and Ep vectors are summed along with the cells’ mass, momentum, and energy.(14)\nThis insures strict conservation of mass, momentum, and energy over the course of the simulation.\n\n\nCell Chopping and Flattening\nWhen we chop off one side of a cell, we are removing five quantities: mass, momentum and energy, plus part of the pressure momentum and pressure energy from the vectors on the chopped-off edge of the cell. Figure 16 shows this for a single cell c1. Panel A shows the quantities remaining in c1 after chopping, and panel B shows the quantities that are chopped off to form cell part c1c.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 16.  Chopping pressure momentum and pressure energy vectors.Panel A shows a single cell c1 with the right side chopped off. Panel B shows the chopped mass, momentum, energy, pressure momentum Pp, and pressure energy Ep that are now contained in the chopped cell part c1c, which will be flattened into a new cell along with any other cells chopped out by the same wavefront. The amounts of mass, momentum, and energy transferred to c1c are proportional to the width of c1c, but the amounts of pressure momentum and pressure energy transferred to c1c are proportional to the time since the creation of c1.\ndoi:10.1371/journal.pone.0039999.g016Note that we chop off mass, momentum and energy in amounts proportional to the width of the chopped part, but we chop off pressure momentum and pressure energy in an amount proportional to the time since the chopped cell was created. This is because mass, momentum and energy are inherent properties of the fluid that must be integrated over space, whereas pressure momentum and pressure energy act over time to convert energy from potential to kinetic form when there is a gradient in the fluid pressure. We can see this in equation set 7, where the left-side integrals are spatial, and the right-side integrals are temporal. In RRM, we treat Pp and Ep as acting steadily over time, starting at time tc when a cell is created, and ending at time tc+Δt, the time at which both of the cell’s tracer particles (the left wavefront’s right particle, and the right wavefront’s left particle) leave the cell.\nWhen chopping out a new cell, we first determine its intersections with existing cells. Then we chop off those intersections and add up all the mass M, momentum P, and energy E the intersections contained. Then, using the width of the new cell w and the requirement that density, velocity, and pressure are constant across it, we can calculate the flattened values of the primitive variables for the new cell.(15)\n\n\nNegative Pressure Fix\nVery occasionally, the flattening process will produce a cell with negative pressure, either because of rounding or truncation error, or because a very small wavefront chops a large amount of pressure momentum and pressure energy, which would accelerate the newly created small cell more than its store of potential energy can support.\nWhen this happens, we flatten the cell without adding in the chopped pressure momentum and pressure energy, which turns off pressure-to-momentum conversion for that cell and gives us a positive pressure after flattening. The unused pressure momentum and pressure energy are added to that of the newly created cell, which spreads the pressure-to-momentum conversion out over a slightly longer time.\nWhitehurst’s signal method [18] uses a similar fix for negative pressures, but averages over space instead of time. When negative pressure occurs in a cell, the signal method averages that cell’s mass, momentum and energy with its three neighbors, in proportion to their volumes.\n\n\nTracer Particles and Their Error Metric\nThe movement of the tracer particles through the cells of the fluid models the movement of characteristics or acoustic wavefronts through the fluid. The tracer particles do not represent real physical particles, they are merely a computational device. They do not carry mass, momentum, or energy, they do not interact with each other, and they do not affect cells’ properties. They always travel at the local speed of sound a in the cell that contains them.\nAs the tracer particles move through the fluid, we accumulate an error metric that tells us when to stop and chop out a new cell. The error metric Δ1,n is the error accumulated by a tracer particle as it travels from cell 1 to cell n.(16)where di is the distance the tracer particle travels in cell i, and the metric vector Mi for cell i is(17)When Δ1,n for either tracer particle exceeds a user-supplied Δmax, we chop out a new cell.\nThis error metric needs a bit of explaining. First, the metric is a vector of all the primitive variables (instead of choosing just one or two) so that variation in any of them across the fluid can trigger the chopping of a new cell. So our maximum error metric Δmax is a vector Δmax = [Δmax ρ, Δmax u, Δmax p]T, with each value set separately by the user. When we say that Δ1,n exceeds Δmax, we mean that some element of Δ1,n exceeds the corresponding element of Δmax.\nWe take the absolute value of Mi – Mi–1 so the error metric will increase monotonically as the tracer particle travels. If we did not, the error metric might go up and down many times without exceeding Δmax, which could lead to chopping out new cells that contain more total variation in the primitive variable values than we meant to allow.\nWe multiply the error metric by distance so that the error metric grows even as the tracer particles move across cells, not just as the particles cross cell edges. This prevents us from chopping out unduly large new cells in areas of shallow density, velocity, or pressure gradients.\nThere is also a special case in this formula. When i is 1, M0 is the metric vector of the cell on the other side of the edge from where the tracer particle was created. So the tracer particle does not travel through cell 0, but its metric vector contributes to the overall error metric.\nFigure 17 panel A shows two tracer particles pl and pr traveling through a fluid as part of wavefront w23. Panel B shows how the error metrics Δpl and Δpr of the two particles change as the particles travel.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 17.  Error metric growing as particles cross cell edges.Panel A shows two particles pl and pr traveling through the fluid as part of wavefront w23. Panel B shows the particles’ error metrics Δpl and Δpr growing as the particles travel, and demonstrates how the error metric of each particle in a wavefront is tracked separately. Note how the slope of the error metric across each cell is proportional to the difference in the cells’ density ρ, velocity u, and pressure p at the edge the particle crossed to get into the cell.\ndoi:10.1371/journal.pone.0039999.g017Note that the slopes of Δpl and Δpr are shallow in the center of the graph, because the density, velocity, and pressure of c2 and c3 are similar. As the particles cross into c1 and c4, the slopes of Δpl and Δpr increase substantially, which means that w23 will reach Δmax sooner than it would have with a shallower gradient in the fluid.\n\n\nWavefront Unioning\nWhen we choose a wavefront that we wish to chop the fluid with, we first must check for overlap with other wavefronts. The final area we chop out will be the union of the first wavefront with all the wavefronts that overlap it, and all the wavefronts that overlap them, and so on. Figure 18 shows an example: if we choose w12, we see that it overlaps w23, which overlaps w34, so the final area we would have to chop is wunion.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 18.  Wavefront unioning.A fluid divided into four cells c1, c2, c3, and c4. Wavefront w12 overlaps wavefront w23, which overlaps wavefront w34, so we must chop out the union wavefront wunion to properly account for the effects of each wavefront on the others.\ndoi:10.1371/journal.pone.0039999.g018Wavefront unioning was motivated by the observation that once two expanding wavefronts overlap, the fluid in each one has affected the fluid in the other, so they can no longer be treated separately.\nWavefront unioning turns out to be essential for the stability of the simulation. Without wavefront unioning, it is possible to chop out an area that contains unbalanced pressure momentum and pressure energy, even in a perfectly “flat” fluid that has no density or pressure gradient. This imbalance can cause a newly-created cell’s velocity to be abnormally high, which causes a glitch in the simulation where fluid cells pile up or spread out in an unphysical way.\nConsider figure 19 panel A, which shows three cells c1, c2, and c3 with ρ = 1, u = 0, and p = 1. We call this the “213 problem” because the widths of the cells are 2, 1, and 3 from left to right.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 19.  The “213 problem”.Panel A shows a fluid divided into three cells c1, c2, c3 of widths 2, 1, and 3 from left to right (hence the name “213 problem”). All three cells were created at time t = 0, and all three have density ρ = 1, pressure p = 1, and velocity u = 0. In a simulation without wavefront unioning, if wavefront w23 chopped out a new cell at time t = 2.5, that new cell would have a net momentum of −0.5. Panel B shows that this is because the rightward momentum Ppr from c1 levels off at t = 2.0, while the leftward momentum from c3 continues to increase until t = 3.0. This demonstrates that wavefront unioning is required to avoid unphysical changes in cell velocity during simulation.\ndoi:10.1371/journal.pone.0039999.g019Assume that all three cells were created at time t = 0, and that the speed of sound a = 1. Since there is no density or pressure gradient, chopping out a new cell anywhere in this fluid should result in a new cell with ρ = 1, u = 0, and p = 1.\nLet us consider wavefront w23 expanding from the right side of c2 and see if this is true. At time t = 2, w23 will contain equal and opposite amounts of pressure momentum from c1 and c3, since Ppr1 = 2 and Ppl3 = −2. The pressure momenta Ppl2 and Ppr2 from c2 will cancel since the whole cell is covered, so the overall pressure momentum Pp = Ppr1+Ppl2+Ppr2+Ppl3 contained in w23 is zero, as shown by the dotted line at t = 2. So far, so good.\nAt time t = 2.5, the pressure momentum Ppr1 from c1 is still 2, since it ran out of pressure momentum to contribute at t = 2. But the pressure momentum Ppl3 from c3 is −2.5, since it will not run out until time t = 3.0. Figure 19 panel B shows how the wavefront’s left and right pressure momenta Ppl = Ppl2+Ppl3 and Ppr = Ppr1+Ppr2 increase as the wavefront expands, with Ppr leveling off at t = 2 when Ppr1 stops increasing.\nSo if we chop out a new cell at time t = 2.5, it will have an overall pressure momentum Pp of −0.5, as shown by the dotted line at t = 2.5. This will make the new cell move to the left, even though there is no pressure gradient in the fluid! Unioning w23 with w12 (not shown) fixes this problem.\nThe analysis of the 213 problem shows that if we run a simulation without wavefront unioning, it will show occasional unphysical glitches. Since mass, momentum, and energy are all strictly conserved, the glitches sometimes smooth out over time, but if a glitch is big enough, it may create a large local gradient and significantly slow down simulation. Wavefront unioning avoids this problem.\n\n\nDiscrete Event Simulation\nRRM uses a discrete event simulation flow. We keep a priority queue of events, sorted in order of increasing event time. There are two kinds of events: particle events, where particles intersect cells, and wavefront events, where one of the particles in the wavefront exceeds the maximum error metric.\nParticle events merely transfer particles from their current cell to the intersected cell, which changes their speed and the rate at which they accumulate error. Wavefront events chop new cells out of the fluid.\nFigure 20 shows what the event queue might look like for the previous example of the 213 problem at time t = 0, if we assume that the wavefront w23 would chop out a new cell at time t = 1.5. For simplicity, we show only the events associated with wavefront w23. In a real simulation there would be a wavefront between each pair of cells, so the event queue would be much more cluttered.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 20.  Simulation event queue.Queue of events associated with wavefront w23 in the previous example of the 213 problem. Events are ordered by increasing time, and the simulator always executes the event at the head of the queue.\ndoi:10.1371/journal.pone.0039999.g020The first event transfers particle pl from c2 to c1. The second event uses wavefront w23 to chop a new cell out of the fluid. The third event is removed at the same time the second event is processed, since a wavefront’s tracer particles are removed in the chopping process.\nHere is how we determine the event times. When we create a wavefront with its pair of tracer particles, we find the intersection time tintersection of each particle with the nearest cell edge in its direction of travel, and the time tmax error when the error metric of each particle will exceed the maximum error metric. Figure 21 shows all four of these times for particles pl and pr in wavefront w23.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 21.  Particle intersection time and maximum error time.This figure shows all four possible event times for a single wavefront. Consider a fluid divided into four cells c1, c2, c3, and c4. Wavefront w23 contains particles pl and pr. Particle pl intersects cell c1 at time tintersection l, and reaches the maximum error metric at tmax error l. Particle pr intersects cell c4 at time tintersection r, and reaches the maximum error metric at tmax error r.\ndoi:10.1371/journal.pone.0039999.g021We enqueue the two tracer particles as events, using the intersection times as the event times. We also enqueue the wavefront as an event, using the soonest of the max error times as the event time.(18)\nEach time we pull an event off the event queue, we check whether the event is a particle intersecting a cell, or a wavefront whose particle is reaching the maximum error metric. If a particle has intersected a cell, we transfer it into the intersected cell, recalculate the event time, and requeue the particle.\nIf either particle in a wavefront has reached the maximum error metric, we union the wavefront with any overlapping wavefronts, chop and flatten the area of the union into a new cell, and insert the new cell into the fluid. Then we create a new wavefront for each edge of the new cell and insert the corresponding events into the event queue. Finally, we transfer any particles caught in the chopped-out area into the new cell, which changes their speeds to the local speed of sound in the new cell, recalculates their event times, and requeues them.\n\n\nRRM Algorithm Flowchart\nFor reference, figure 22 is a flowchart that outlines the entire RRM algorithm.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 22.  RRM flowchart.Flowchart showing the outline of the complete RRM algorithm.\ndoi:10.1371/journal.pone.0039999.g022\nResultsWe tested RRM on nine standard test problems, and plotted RRM’s results (solid lines) against the output of Toro’s Riemann solver (dashed lines). The two match closely in most cases, with some exceptions discussed below.\nIn the following tests, RRM typically uses a maximum of 200 to 400 cells during the simulation, depending on the maximum error metric we set. Most of those cells are concentrated in areas of high gradient, with only a few wide cells in flat areas. We set the maximum error metrics to obtain good results in a relatively short time. In the error analysis section that follows these test results, we will justify our choices of these maximum error metrics and show how the quality of the results varies as the maximum error metrics are varied.\n\nTest 1\nFigure 23 shows test 1, which is Sod’s problem [3] with the initial conditions (ρl, ul, pl) = (1.0, 0.0, 1.0) and (ρr, ur, pr) = (0.125, 0.0, 0.1). The maximum error metrics for ρ, u, and p are (Δmax ρ, Δmax u, Δmax p) = (1.0e-5, 1.0e-3, 1.0e-3). The results are for time t = 1.5.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 23.  Test 1: Sod’s test problem at high accuracy.Sod’s problem with initial conditions (ρl, ul, pl) = (1.0, 0.0, 1.0) and (ρr, ur, pr) = (0.125, 0.0, 0.1), with maximum error metric (Δmax ρ, Δmax u, Δmax p) = (1.0e-5, 1.0e-3, 1.0e-3), at time t = 1.5. This test shows typical RRM results: an s-shaped contact because RRM is not adiabatic across contacts, and a slight peak at the shock due to finite shock thickness.\ndoi:10.1371/journal.pone.0039999.g023These results are typical of RRM, and match the Riemann solver’s results closely with two exceptions: the s-shaped contact, and the slight peak where the shock shows a finite thickness.\nThe s-shaped contact occurs because unlike a Riemann solver, RRM is not adiabatic across contacts, and models heat diffusion as a side effect of the algorithm. Wavefronts are created at contacts the same as at any other cell edges, so new cells are created across contacts, and gradual diffusion is the result.\nWe could easily make RRM adiabatic across contacts by adding a rule that when a tracer particle reaches a contact, its error metric is set to the maximum. This would insure that new cells are always created on one side of the contact or the other, keeping the contact sharp. We have not tried this yet, so RRM’s current behavior is more like a real fluid than a Riemann solver in this respect.\nShocks in RRM have a finite thickness that manifests as a thin peak at the shock front. The shock thickness decreases as the accuracy is increased. This is because RRM creates new cells at the shock front at a rate proportional to the accuracy, and the more frequently cells are created there, the more quickly the change in density, velocity, and pressure is propagated to the area behind the shock. In the limit of infinite accuracy, the shock would be infinitely thin as it is in the Riemann solver’s results.\nShocks in a real fluid also have a finite thickness of a few mean free paths, for a similar reason. It takes fluid atoms or molecules a few collisions to transition from their state in front of the shock to their state behind the shock. But because real fluids are not continuous, the shock thickness at a given set of conditions is essentially fixed by the fluid’s physical properties.\nUnlike contacts, shocks in RRM will always be sharply defined, because they are formed by the edge of a supersonic cell pushing into slower fluid. Our wavefronts always travel at the local speed of sound a, so shocks are naturally self-forming because nearby wavefronts cannot outrun them.\n\n\nTest 2\nFigure 24 shows test 2, which is a modified version of Sod’s problem due to Laney [19]. This problem has a 100-to-1 pressure differential instead of the 10-to-1 differential of Sod’s problem. The initial conditions are (ρl, ul, pl) = (1.0, 0.0, 1.0) and (ρr, ur, pr) = (0.01, 0.0, 0.01). The maximum error metric is (Δmax ρ, Δmax u, Δmax p) = (1.0e-4, 1.0e-4, 1.0e-3), and the results are for time t = 1.5.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 24.  Test 2: Modified Sod’s test problem with 100-to-1 pressure differential.A modified version of Sod’s problem with initial conditions are (ρl, ul, pl) = (1.0, 0.0, 1.0) and (ρr, ur, pr) = (0.01, 0.0, 0.01), with maximum error metric (Δmax ρ, Δmax u, Δmax p) = (1.0e-4, 1.0e-4, 1.0e-3), at time t = 1.5. This test shows that RRM still gives good results on a problem that has a 100-to-1 pressure differential instead of the 10-to-1 differential of Sod’s problem.\ndoi:10.1371/journal.pone.0039999.g024This test shows that RRM can handle strongly supersonic flows. We can see that the contact is s-shaped as usual, and there is just a hint of a u peak at the shock front, but otherwise the results are in agreement with the Riemann solver. The velocity at the shock front is higher than in the original Sod’s problem, as we expect due to the greater pressure differential.\n\n\nTest 3\nFigure 25 shows test 3, which is a modified version of Sod’s problem where the entire fluid moves right with u = 1.0. The initial conditions are (ρl, ul, pl) = (1.0, 1.0, 1.0) and (ρr, ur, pr) = (0.125, 1.0, 0.1). The maximum error metric is (Δmax ρ, Δmax u, Δmax p) = (5.0e-5, 1.0e-3, 1.0e-3), and the results are for time t = 1.5.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 25.  Test 3: Modified Sod’s test problem with initial u = 1.0.A modified version of Sod’s problem where the entire fluid moves right with u = 1.0, with initial conditions (ρl, ul, pl) = (1.0, 1.0, 1.0) and (ρr, ur, pr) = (0.125, 1.0, 0.1), with maximum error metric (Δmax ρ, Δmax u, Δmax p) = (5.0e-5, 1.0e-3, 1.0e-3), at time t = 1.5. This test shows one of the benefits of the fully Lagrangian nature of RRM. Since the cells all move to the right with u = 1.0, the shock front does not have to cross cell edges during the simulation, so the shock is just as sharp as in the u = 0 case.\ndoi:10.1371/journal.pone.0039999.g025This test shows one of the benefits of the fully Lagrangian nature of RRM. Since the cells all move to the right with u = 1.0, the shock front does not have to cross cell edges during the simulation, so the shock is just as sharp as in the u = 0 case. The u curve is identical to the u = 0 case, but shifted upwards by 1.0.\n\n\nTest 4\nFigure 26 shows test 4, which is test problem 1 from page 225 of Toro’s book on Riemann solvers and numerical methods [1]. The initial conditions are (ρl, ul, pl) = (1.0, 0.75, 1.0) and (ρr, ur, pr) = (0.125, 0.0, 0.1). The maximum error metric is (Δmax ρ, Δmax u, Δmax p) = (1.0e-5, 1.0e-4, 1.0e-4), and the results are for time t = 0.8.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 26.  Test 4: Toro test 1.Toro’s test problem 1, with initial conditions (ρl, ul, pl) = (1.0, 0.75, 1.0) and (ρr, ur, pr) = (0.125, 0.0, 0.1), with maximum error metric (Δmax ρ, Δmax u, Δmax p) = (1.0e-5, 1.0e-4, 1.0e-4), at time t = 0.8. This test is similar to Sod’s problem, but the left cell is initially ramming into the right cell, so the velocity at the shock front is somewhat higher.\ndoi:10.1371/journal.pone.0039999.g026This test is similar to Sod’s problem, but the left cell is initially ramming into the right cell, so the velocity at the shock front is somewhat higher.\n\n\nTest 5\nFigure 27 shows test 5, which is test problem 2 from Toro’s book [1]. The initial conditions are (ρl, ul, pl) = (1.0, −2.0, 0.4) and (ρr, ur, pr) = (1.0, 2.0, 0.4). The maximum error metric is (Δmax ρ, Δmax u, Δmax p) = (1.0e-5, 1.0e-5, 1.0e-5), and the results are for time t = 0.6.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 27.  Test 5: Toro test 2.Toro’s test problem 2, with initial conditions are (ρl, ul, pl) = (1.0, −2.0, 0.4) and (ρr, ur, pr) = (1.0, 2.0, 0.4), with maximum error metric (Δmax ρ, Δmax u, Δmax p) = (1.0e-5, 1.0e-5, 1.0e-5), at time t = 0.6. This test shows that RRM can correctly handle the near-vacuum state created in the center.\ndoi:10.1371/journal.pone.0039999.g027This test creates a near-vacuum in the center, which can cause problems in the iteration schemes that some Riemann solvers use to find p. RRM does not have any special difficulty with vacuum areas, either as part of the initial conditions, or evolved during the simulation as we see here.\nNote that in this test we set Δmax u relatively low. This is to resolve the velocity features near the origin that are far from the large density and pressure gradients on either side.\n\n\nTest 6\nFigure 28 shows test 6, which is a modified “converging” version of test problem 2 from Toro’s book [1]. The initial conditions are (ρl, ul, pl) = (1.0, 3.0, 0.4) and (ρr, ur, pr) = (1.0, −3.0, 0.4). The maximum error metric is (Δmax ρ, Δmax u, Δmax p) = (5.0e-4, 5.0e-4, 5.0e-4), and the results are for time t = 1.1.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 28.  Test 6: Toro test 2 converging.A modified “converging” version of Toro’s test problem 2, with initial conditions (ρl, ul, pl) = (1.0, 3.0, 0.4) and (ρr, ur, pr) = (1.0, −3.0, 0.4), with maximum error metric (Δmax ρ, Δmax u, Δmax p) = (5.0e-4, 5.0e-4, 5.0e-4), at time t = 1.1. This is a test of symmetry and momentum conservation, to make sure that two colliding cells will pile up into one stationary mass with sharp edges.\ndoi:10.1371/journal.pone.0039999.g028This is a test of symmetry and momentum conservation, to make sure that two colliding cells will pile up into one stationary mass with sharp edges.\n\n\nTest 7\nFigure 29 shows test 7, which is test problem 3 from Toro’s book [1]. The initial conditions are (ρl, ul, pl) = (1.0, 0.0, 1000.0) and (ρr, ur, pr) = (1.0, 0.0, 0.01). The maximum error metric is (Δmax ρ, Δmax u, Δmax p) = (1.0e-5, 5.0e-3, 1.0e-2), and the results are for time t = 0.04.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 29.  Test 7: Toro test 3.Toro’s test problem 3, with initial conditions (ρl, ul, pl) = (1.0, 0.0, 1000.0) and (ρr, ur, pr) = (1.0, 0.0, 0.01), with maximum error metric (Δmax ρ, Δmax u, Δmax p) = (1.0e-5, 5.0e-3, 1.0e-2), at time t = 0.04. This test’s solution contains a strong shock very close to a contact. Since RRM is spatially adaptive, it simply creates many new cells between the shock and the contact to get the required accuracy.\ndoi:10.1371/journal.pone.0039999.g029The solution to this test requires a strong shock to be placed very close to a contact. Since RRM is spatially adaptive, it simply creates many new cells between the shock and the contact to get the required accuracy.\n\n\nTest 8\nFigure 30 shows test 8, which is test problem 4 from Toro’s book [1]. The initial conditions are (ρl, ul, pl) = (5.99924, 19.5975, 460.894) and (ρr, ur, pr) = (5.99242, −6.19633, 46.0950). The maximum error metric is (Δmax ρ, Δmax u, Δmax p) = (5.0e-4, 1.0e-2, 1.0e-2), and the results are for time t = 0.15.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 30.  Test 8: Toro test 4.Toro’s test problem 4, with initial conditions (ρl, ul, pl) = (5.99924, 19.5975, 460.894) and (ρr, ur, pr) = (5.99242, −6.19633, 46.0950), with maximum error metric (Δmax ρ, Δmax u, Δmax p) = (5.0e-4, 1.0e-2, 1.0e-2), at time t = 0.15. The solution to this test has two rightward-traveling shocks with a contact between them. As usual, the shocks are sharply resolved and the contact is s-shaped due to RRM’s modeling of heat diffusion.\ndoi:10.1371/journal.pone.0039999.g030The solution to this test has two rightward-traveling shocks with a contact between them, which can be smeared out by some non-adaptive methods. As usual, the shocks are sharply resolved and the contact is s-shaped due to RRM’s modeling of heat diffusion.\n\n\nTest 9\nFigure 31 shows test 9, which is test problem 5 from Toro’s book [1]. The initial conditions are (ρl, ul, pl) = (1.0, −19.59745, 1000.0) and (ρr, ur, pr) = (1.0, −19.59745, 0.01). The maximum error metric is (Δmax ρ, Δmax u, Δmax p) = (1.0e-5, 1.0e-2, 1.0e-2), and the results are for time t = 0.03.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 31.  Test 9: Toro test 5.Toro’s test problem 5, with initial conditions (ρl, ul, pl) = (1.0, −19.59745, 1000.0) and (ρr, ur, pr) = (1.0, −19.59745, 0.01), with maximum error metric (Δmax ρ, Δmax u, Δmax p) = (1.0e-5, 1.0e-2, 1.0e-2), at time t = 0.03. The initial values of this test were designed to give an almost stationary contact at the origin, which causes difficulties for some numerical methods. RRM handles stationary contacts the same as it does moving contacts, due to the Lagrangian nature of the simulation.\ndoi:10.1371/journal.pone.0039999.g031The initial values of this test were designed to give an almost stationary contact at the origin, which causes difficulties for some numerical methods. RRM handles stationary contacts the same as it does moving contacts, due to the Lagrangian nature of the simulation.\n\n\nAbsolute Error Analysis\nTo analyze RRM’s error as compared to a Riemann solver, first we will show qualitatively how the accuracy of the simulation decreases as the maximum error metrics are increased. Then we will define a quantitative measure of the error between RRM’s solution and that of the Riemann solver, and show how it decreases as each of the maximum error metrics is decreased. We will also show how the number of cells in the simulation increases as the error is reduced.\nFigure 32 shows test 1 again, at the same accuracy as before, but this time only drawing one dot per cell (except for the edge cells, which have a dot on each side). The results are for time t = 1.5.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 32.  Sod’s test problem at high accuracy, showing cell density.Sod’s problem with initial conditions (ρl, ul, pl) = (1.0, 0.0, 1.0) and (ρr, ur, pr) = (0.125, 0.0, 0.1), with maximum error metric (Δmax ρ, Δmax u, Δmax p) = (1.0e-5, 1.0e-3, 1.0e-3), at time t = 1.5. Each of the approximately 800 cells is shown by a single dot, except the two edge cells which have two dots apiece. This figure shows that RRM is good at concentrating cells (and thereby computational effort) in areas of primitive variable gradient.\ndoi:10.1371/journal.pone.0039999.g032We can see that almost all of the approximately 800 cells are concentrated along the expansion fan and at the contact, with only one or two cells for each flat area. This illustrates how well RRM concentrates its computational effort on the active areas of the fluid.\nFigure 33 shows test 1 again, but with the accuracy reduced by increasing the maximum error metric to (Δmax ρ, Δmax u, Δmax p) = (1.0e-3, 1.0e-2, 1.0e-2) to show how the simulation begins to degrade. The results are for time t = 1.5.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 33.  Sod’s test problem at medium accuracy.Sod’s problem with initial conditions (ρl, ul, pl) = (1.0, 0.0, 1.0) and (ρr, ur, pr) = (0.125, 0.0, 0.1), with the accuracy reduced by increasing the maximum error metric to (Δmax ρ, Δmax u, Δmax p) = (1.0e-3, 1.0e-2, 1.0e-2) to show how the simulation begins to degrade, at time t = 1.5. The shock is of increased thickness due to the lower accuracy, and is “blown back” from the correct location.\ndoi:10.1371/journal.pone.0039999.g033At this accuracy, the widths of the fluid cells are directly visible in the jagged curve of the expansion fan, and the contact is mostly smeared out. As expected, the shock thickness is greater due to the decreased accuracy. The shock front is also “blown back” so that it trails a constant distance behind the correct location. This is because the shock in RRM is a dynamic phenomenon with no special-case code. If the accuracy is not set high enough, fluid will pile up at the shock front where it cannot be redistributed fast enough to maintain the correct wave shape.\nFigure 34 shows test 1 yet again, but with the accuracy further reduced by increasing the maximum error metric to (Δmax ρ, Δmax u, Δmax p) = (1.0e-2, 1.0e-2, 1.0e-2) to show a more extreme failure. The results are for time t = 1.5.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 34.  Sod’s test problem at low accuracy.Sod’s problem with initial conditions (ρl, ul, pl) = (1.0, 0.0, 1.0) and (ρr, ur, pr) = (0.125, 0.0, 0.1), with the accuracy further reduced by increasing the maximum error metric to (Δmax ρ, Δmax u, Δmax p) = (1.0e-2, 1.0e-2, 1.0e-2) to show a more extreme failure, at time t = 1.5. The shock is even thicker due to the lower accuracy, and is “blown back” even farther from the correct location.\ndoi:10.1371/journal.pone.0039999.g034The jagged expansion fan is even more pronounced here, since the cells are even larger. The contact is completely gone, the shock is blown back even further, and the spike at the shock front is even higher, since more fluid is piled up there.\nOf course, we would never run a real simulation at such low accuracy. These figures are merely to show how the weaknesses of RRM differ from those of other methods. In particular, though RRM is a conservative method, that alone is does not guarantee correct shock placement as it does in FDM and FVM. But shocks in RRM remain very sharp even at very low accuracy, and there are no Gibbs oscillations near the shocks. This is because our cells are a piecewise-linear representation of the primitive variable values of the fluid.\nNow we present a more quantitative analysis. We define.(19)to be the vector error between the primitive variable values in RRM’s solution and the primitive variable values in the Riemann solver’s solution. Then we define a maximum integral error norm(20)to represent the maximum value, from the start time to some chosen end time, of the spatial integral of the norm of the error e(x,t) over the whole fluid.\nNote that we choose the maximum integral error norm instead of the simpler maximum error norm.(21)because the maximum error norm for RRM is typically the thin peak right at the shock front, which is of almost constant height (though decreasing thickness) as simulation accuracy is increased.\nFigure 35 shows emaxinorm and the maximum number of cells nmax vs. the maximum density error metric Δmax ρ for Sod’s problem. Δmax ρ is swept from 1.0e-1 to 1.0e-5, while Δmax u and Δmax p are held constant at 1.0e-1. Simulation was from time t = 0.0 to t = 1.5.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 35.  Integral error norm vs. maximum density error metric.Maximum integral error norm emaxinorm and maximum number of cells nmax vs. the maximum density error metric Δmax ρ for Sod’s problem. Δmax ρ is swept from 1.0e-1 to 1.0e-5, while Δmax u and Δmax p are held constant at 1.0e-1. Simulation was from time t = 0.0 to t = 1.5. This figure shows that the error decreases logarithmically as the number of cells (and thus the computational effort) increases logarithmically.\ndoi:10.1371/journal.pone.0039999.g035We can see that once Δmax ρ gets smaller than about 1.5e-4, the maximum number of cells in the simulation (which is a good proxy for the computational effort required) increases rapidly to maintain the approximately logarithmic decrease in maximum integral error norm. This computational effort goes into squaring off the contact, which is inherently diffusive in RRM.\nSince emaxinorm is integrated over a width of 10, emaxinorm = 0.1 corresponds to an average absolute density error of 0.01, or about 1%. But the error is not evenly distributed. Most of the error is around the s-shaped contact, with a lesser amount near the shock front due to the transition region.\nFigure 36 shows emaxinorm and the maximum number of cells nmax vs. the maximum velocity error metric Δmax u for Sod’s problem. Δmax u is swept from 1.0e-1 to 1.0e-5, while Δmax ρ and Δmax p are held constant at 1.0e-1. Simulation was from time t = 0.0 to t = 1.5.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 36.  Integral error norm vs. maximum velocity error metric.Maximum integral error norm emaxinorm and the maximum number of cells nmax vs. the maximum velocity error metric Δmax u for Sod’s problem. Δmax u is swept from 1.0e-1 to 1.0e-5, while Δmax ρ and Δmax p are held constant at 1.0e-1. Simulation was from time t = 0.0 to t = 1.5. This figure shows that the error cannot be decreased past a certain point solely by adjusting Δmax u, since there is little velocity gradient across the contact (where most of the error is concentrated in this test).\ndoi:10.1371/journal.pone.0039999.g036Note that we get less than a decade of decrease in maximum integral error norm as we decrease Δmax u, and for values lower than about 4.0e-4 we get very little additional benefit, though we increase computation effort by a factor of 5. This is because the velocity gradient across the contact is small, so decreasing Δmax u will not cause more cells to be created there.\nFigure 37 shows emaxinorm and the maximum number of cells nmax vs. the maximum pressure error metric Δmax p for Sod’s problem. Δmax p is swept from 1.0e-1 to 1.0e-5, while Δmax ρ and Δmax u are held constant at 1.0e-1. Simulation was from time t = 0.0 to t = 1.5.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 37.  Integral error norm vs. maximum pressure error metric.Maximum integral error norm emaxinorm and the maximum number of cells nmax vs. the maximum pressure error metric Δmax p for Sod’s problem. Δmax p is swept from 1.0e-1 to 1.0e-5, while Δmax ρ and Δmax u are held constant at 1.0e-1. Simulation was from time t = 0.0 to t = 1.5. This figure shows that the error cannot be decreased past a certain point solely by adjusting Δmax p, since there is little pressure gradient across the contact (where most of the error is concentrated in this test).\ndoi:10.1371/journal.pone.0039999.g037We see similar behavior to the Δmax u sweep, where we get about one decade of decrease in the maximum integral error norm, with little further improvement as Δmax p is reduced further. Again, this is because there is little pressure gradient across the contact, so reducing Δmax p cannot improve the contact shape.\nFigure 38 shows emaxinorm and the maximum number of cells nmax vs. all three maximum error metrics Δmax ρ, Δmax u, and Δmax p for Sod’s problem. All three maximum error metrics are swept from 1.0e-1 to 1.0e-5 in tandem. Simulation was from time t = 0.0 to t = 1.5.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 38.  Integral error norm vs. all maximum error metrics.Maximum integral error norm emaxinorm and the maximum number of cells nmax vs. all three maximum error metrics Δmax ρ, Δmax u, and Δmax p for Sod’s problem. All three maximum error metrics are swept from 1.0e-1 to 1.0e-5 in tandem. Simulation was from time t = 0.0 to t = 1.5. This figure shows that there is a slight synergistic effect between the three maximum error metrics, since the minimum error achieved here is slightly lower than when each of the three is set to 1.0e-5 individually.\ndoi:10.1371/journal.pone.0039999.g038This last graph shows that if all three maximum error metrics are decreased together, we can get a bit less error than if they are decreased separately. This indicates that there is some interaction between their effects, though it is small for this test case.\nTaken together, the previous four figures demonstrate that for this particular test case, RRM’s error decreases almost logarithmically as Δmax ρ is decreased logarithmically, whether by itself or in combination with the other maximum error metrics. However, other tests are sensitive to different maximum error metrics. For example, test 5 is sensitive to Δmax u, because its solution has velocity features near the origin that are far from the largest density or pressure gradients.\nThe maximum number of cells used by the simulation goes up almost logarithmically as the maximum error metrics are decreased logarithmically. And since the current RRM implementation performs O(n) operations per new cell created, where n is the number of cells in the fluid, this implies that the computational effort goes up logarithmically as well. This restricted our ability to run simulations with more than about 1000 cells in a reasonable time, which we define to be less than 5 minutes on one core of a 2.4 GHz Intel Core2 Quad CPU.\nFuture RRM implementations could perform as few as O(log n) or even O(1) operations per new cell created, if they used a more sophisticated data structure for cell intersection. We have so far avoided improving this data structure, since it makes the code much more difficult to maintain and alter for research purposes.\nA note on the error-reducing efficiency of Δmax ρ, Δmax u, and Δmax p in RRM is appropriate here. In the sweeps of Δmax u and Δmax p above, we see that the integral error norm decreases by fewer orders of magnitude than the number of cells increases by. This may simply show that decreasing the maximum error metrics of velocity and pressure does not efficiently reduce an error which is mostly in the density near the contact. Indeed, it appears that reducing Δmax ρ reduces the integral error norm more efficiently, by approximately one order of magnitude as the number of cells increases from 100 to 1000. But for fewer than 100 cells, the error-reducing efficiency of Δmax ρ is not as great. Overall, the error-reducing efficiency of the maximum error metrics in RRM is not yet fully understood.\nThese results indicate that RRM requires some further refinement if it is to efficiently produce results at any desired precision. If we assume the RRM implementation can be improved so that it performs only O(1) operations per cell, we would still like to insure that the number of cells always increases at a rate slower than the integral error norm decreases. One possibility is to adapt RRM to produce results more like those of a Riemann solver. As mentioned earlier, it would be straightforward to split new cells across contacts to maintain their sharpness, which would reduce a major source of error. Another possibility is to attempt to develop a new analytical solution to the Riemann problem that incorporates heat diffusion across contacts, and measure RRM’s error against that instead.\n\n\nConservation Error Analysis\nConservation error is the difference between the conserved quantities currently present in all cells, and the original conserved quantities at the start of the simulation, assuming any boundary effects are properly accounted for. We define one conservation error for each conserved quantity:(22)where Mf(t) is the total fluid mass, Pf(t) is total fluid momentum, and Ef(t) is total fluid energy, all functions of time.\nFigure 39 shows the three conservation errors for Sod’s problem over the first five seconds of flow time, at a time resolution of 0.01 seconds.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 39.  Conservation error vs. time on Sod’s problem over 5 seconds.The three conservation errors for Sod’s problem over the first five seconds of flow time, at a time resolution of 0.01 seconds. This graph shows that mass, momentum and energy are all conserved to within about ±6.0e-14. Total mass in the simulation is 11.25 kg, total momentum is 0.0 m·s, and total energy is 27.5 J.\ndoi:10.1371/journal.pone.0039999.g039This graph shows that mass, momentum and energy are all conserved to within about ±6.0e-14. Total mass in the simulation is 11.25 kg, total momentum is 0.0 m·s, and total energy is 27.5 J.\nThe first detail to note about this graph is that the conservation error is roughly two orders of magnitude larger than the floating-point precision εfp, which on our test machine is about 2.22e-16 for 64-bit IEEE floating point.\nThis is due to our use of the primitive variable form of the Euler equations in the current RRM implementation. The conserved quantities are derived from the primitive variable values of cell width, density, velocity, and pressure in a series of floating-point operations, each of which may be incorrect by roughly εfp. It takes only a few multiplicative operations for the error to grow to the observed value. Fortunately, since the signs of the individual errors are essentially random, the overall error does not tend to grow over time.\nIf we instead used the conservation form of the Euler equations in the RRM implementation, with a careful treatment we could get the error down to a smaller multiple of εfp. But since the error is already small in an absolute sense and does not grow over time, we chose to stay with the primitive variable form because it is simpler to code.\nThe second detail to note about the conservation error graph is the overall trends of the lines. The mass and energy lines are as expected, with floating-point truncation error causing random fluctuation about the horizontal axis. However, the momentum line differs, showing instead a fluctuation around approximately emomentum = 0.5e-14.\nThis is due to the initial conditions and time evolution of Sod’s problem. At time t = 0, the density and pressure are between 0.1 and 1.0, and calculating with these numbers to get mass and energy results in some nonzero error. However, the initial velocities are exactly zero, so initially emomentum will also be exactly zero. As the simulation proceeds and cell velocities increase, the effective baseline of emomentum is raised, since the calculations leading to momentum are no longer involve exact values of zero. Other test problems show variations on this behavior, but no problems tested so far show any time trend in conservation error.\n\n\nOther Boundary Conditions\nSo far we have considered only periodic boundary conditions, which are simple to implement since they do not affect the conserved quantities of the fluid. But RRM can handle many other types of boundary conditions by adjusting the conserved quantities of each new cell just before it is flattened. We will illustrate three more types of boundary conditions: solid, Dirichlet, and free.\nSolid boundaries are immovable and impermeable. To make a solid boundary, we check if each new cell touches or crosses the boundary. If so, we set its momentum to zero, and adjust its width so its edge just touches the boundary. Figure 40 shows Sod’s problem with solid boundaries at x = −5 and x = 5, from time t = 0 to t = 20. The initial conditions are (ρl, ul, pl) = (1.0, 0.0, 1.0) and (ρr, ur, pr) = (0.125, 0.0, 0.1). The maximum error metric is (Δmax ρ, Δmax u, Δmax p) = (1.0e-4, 1.0e-2, 1.0e-2).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 40.  Sod’s problem with solid boundaries at x =  −5 and x = 5.Sod’s problem with solid boundaries at x = −5 and x = 5, from time t = 0 to t = 20. The initial conditions are (ρl, ul, pl) = (1.0, 0.0, 1.0) and (ρr, ur, pr) = (0.125, 0.0, 0.1). The maximum error metric is (Δmax ρ, Δmax u, Δmax p) = (1.0e-4, 1.0e-2, 1.0e-2). The shock wave hits the right boundary, reflects off it, and travels back across the fluid until it reflects off the left boundary.\ndoi:10.1371/journal.pone.0039999.g040We can see the shock wave hit the right boundary, reflect off it, and travel back across the fluid until it reflects off the left boundary. If we let the simulation run indefinitely, the shock will travel back and forth many times, until numerical dissipation finally smooths it out. Eventually, the density and pressure will be flat, and the velocity will be everywhere zero.\nDirichlet boundaries hold the primitive variable values of the fluid constant at the boundaries. To make a Dirichlet boundary, we check if each new cell touches or crosses a boundary. If so, we set its density, velocity, and pressure to some constant boundary values, and adjust its width so its edge just touches the boundary. Figure 41 shows an inrush problem with Dirichlet boundaries at x = −5 and x = 5, from time t = 0 to t = 10. The initial conditions are (ρl, ul, pl) = (0.1, 0.0, 0.2) and (ρr, ur, pr) = (0.1, 0.0, 0.2). The boundary values are (ρl, ul, pl) = (0.3, 0.6, 0.4) and (ρr, ur, pr) = (0.6, −0.6, 0.5). The maximum error metric is (Δmax ρ, Δmax u, Δmax p) = (1.0e-4, 1.0e-3, 1.0e-3).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 41.  Inrush problem with Dirichlet boundaries at x =  −5 and x = 5.An inrush problem with Dirichlet boundaries at x = −5 and x = 5, from time t = 0 to t = 10. The initial conditions are (ρl, ul, pl) = (0.1, 0.0, 0.2) and (ρr, ur, pr) = (0.1, 0.0, 0.2). The boundary values are (ρl, ul, pl) = (0.3, 0.6, 0.4) and (ρr, ur, pr) = (0.6, −0.6, 0.5). The maximum error metric is (Δmax ρ, Δmax u, Δmax p) = (1.0e-4, 1.0e-3, 1.0e-3). Two asymmetrical shocks propagate in from the edges, cross near the center, and continue to the opposite edges, where they are squelched by the boundary conditions.\ndoi:10.1371/journal.pone.0039999.g041We can see the two asymmetrical shocks propagate in from the edges, cross near the center, and continue to the opposite edges, where they are squelched by the boundary conditions. If we let the simulation run longer, the continuous fluid inflow fills the area higher and higher, with velocity everywhere zero, and density and pressure eventually becoming flat due to diffusion.\nFree boundaries let fluid flow in or out of the boundaries, without creating any disturbance that might propagate back into “interesting” parts of the fluid. To make a free boundary, we check if each new cell touches or crosses a boundary. If so, we set its density, velocity, and pressure to those of the intersected cell nearest the boundary (for inflow) or farthest from the boundary (for outflow), and adjust its width so its edge just touches the boundary. Figure 42 shows a rightward flow problem with free boundaries at x = −5 and x = 5, from time t = 0 to t = 15. The initial conditions are (ρl, ul, pl) = (0.8, 0.8, 0.1) and (ρr, ur, pr) = (0.1, 0.7, 0.05). The maximum error metric is (Δmax ρ, Δmax u, Δmax p) = (5.0e-5, 1.0e-4, 1.0e-4).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 42.  Rightward flow problem with far-field boundaries at x =  −5 and x = 5.A rightward flow problem with free boundaries at x = −5 and x = 5, from time t = 0 to t = 15. The initial conditions are (ρl, ul, pl) = (0.8, 0.8, 0.1) and (ρr, ur, pr) = (0.1, 0.7, 0.05). The maximum error metric is (Δmax ρ, Δmax u, Δmax p) = (5.0e-5, 1.0e-4, 1.0e-4). Fast-moving fluid flows in from the left boundary and pushes the slower fluid in front of it, forcing it out of the right boundary. After about t = 14 all of the fluid is in the left state, since all the right fluid as been pushed out.\ndoi:10.1371/journal.pone.0039999.g042We can see that fast-moving fluid flows in from the left boundary and pushes the slower fluid in front of it, forcing it out of the right boundary. After about t = 14 all of the fluid is in the left state, since all the right fluid as been pushed out. Note that when the shock hits the right boundary at about t = 4, we can see a glitch. This is because if a cell is near the edge of the fluid, and there is no other cell between it and the edge, we extend it to touch the edge. In this case, it just happens to catch a narrow cell in the transition region and widen it so it is visible.\nFinally, we mention two details that apply to all three boundary condition types discussed above. First, when producing 3D graphs of RRM simulations, we remove the very thin cells that can occur in the transition regions at shock fronts. This is simply to make the graphs more legible, since otherwise these very thin cells hide the details of the fluid behind them. You can see one of these cells at t = 4 on the right side of the very last graph above.\nSecond, we note that these adjustments to the conserved quantities of new cells require us to change the stored initial conserved quantities of the entire fluid by a commensurate amount, so the simulation will not fail its ongoing per-event conservation checks. This models the mass, momentum, and energy that are being added and removed at the boundaries.\n\n\nSource Terms\nOur analysis so far has only treated the Euler equations in the homogeneous case, where no mass, momentum, or energy are added to or removed from the fluid during the simulation, except for a special case at the boundaries.\nIn the more general case, we augment the Euler equations with source terms thus:(23)using a new vector of source terms\n(24)Presenting a scheme to simulate these equations is beyond the scope of this paper. However, we can make a few remarks about how it might be possible.\nA few cases would be simple. For example, mass or energy source terms that are constant in space and time could easily be implemented by adding mass or energy to new cells during flattening.\nA few more cases are somewhat difficult, but feasible using splitting schemes similar to those described in chapter 15 of Toro’s book [1]. For example, momentum source terms that are constant in space and time could be implemented by changing the cells’ equations of motion from constant-velocity to varying-velocity, at the cost of complicating the intersection calculation of particles with cells.\nThe general case becomes very difficult. If spatially- and temporally-varying source terms are allowed, cells’ masses and energies could change in ways that would require both space and time integration to resolve at flattening time. The intersection calculation of particles with cells would also require solving ordinary differential equations, rather than simple algebraic equations.\n\nSo far we have shown that RRM gives correct results for many standard test problems, that RRM’s error decreases steadily as we increase the desired accuracy, and that RRM handles many common types of boundary conditions. Now we explain the similarities and differences between RRM and other CFD methods in detail, list some of RRM’s limitations, and suggest directions for future research.Simple CFD methods advance time across the whole fluid in lockstep. But this wastes effort in smooth areas of the fluid, and gives suboptimal resolution in steep areas. To solve this problem, Osher and Sanders proposed locally varying time steps [20]. Such methods can make simulation much more efficient, but they require that special care be taken at the interfaces between areas of differing time resolution.Adaptive mesh refinement (AMR) methods such as the one proposed by Berger and Oliger [21] flag points in the fluid with high estimated error for possible refinement. Then every so often, the flagged points are clustered together to determine the size, shape, and orientation of a new, finer sub-mesh to cover them. Finer sub-meshes are integrated using proportionately shorter time steps, so AMR is adaptive in both space and time. AMR requires special care when integrating the sub-meshes, to insure that the boundary conditions with the rest of the fluid maintain conservation.RRM does not divide the fluid into areas of different mesh fineness or time step size. Instead, each new cell is created at its own individually chosen time, which need not bear any relation to the creation times of other cells. This gives us a very fine-grained spatio-temporal adaptivity, with the disadvantage that such a simulation must use an event queue instead of a simple time-stepping loop. RRM does not require a clustering algorithm, since new cells are preferentially created in high-gradient areas. But RRM does require the unioning of wavefronts before creating a new cell, which is a similar operation.RRM differs from FDM and FVM in that it does not use numerical derivatives in the cell chopping and flattening process, only integrals. This means that RRM does not need a flux limiter or slope limiter to smooth spurious oscillations that can be caused by the extremely large gradients near shocks.RRM differs from LBM firstly in that RRM is meshfree and Lagrangian, where LBM has a mesh (though it is called a lattice in LBM literature) and is Eulerian (since the fluid flows through fixed lattice sites). A more interesting difference between RRM and LBM is that in RRM, cells are free to move in any direction, where in LBM, fluid can only move in a fixed number of directions between adjacent sites. Fluid flows in LBM can therefore exhibit anisotropies, depending on the choice of lattice type and connectivity.Cells in 2D and 3D RRM will have angular velocity, which will complicate the flattening process somewhat, where in LBM the collision process is much the same for all dimensionalities. Since collision takes place only at zero-size lattice sites, angular quantities do not arise, which keeps the programming simpler than RRM.To adapt to the time-varying features of a fluid, a meshed Lagrangian method can move the mesh relative to the fluid, change the mesh connectivity, or both.The Arbitrary Lagrangian-Eulerian method (ALE) [22] combines the Eulerian and Lagrangian forms by creating a third “referential” coordinate system that is independent of both the fixed world coordinates and the moving material coordinates. This allows cells to move independently of both the fixed coordinate system and the material.As an ALE simulation progresses in Lagrangian mode, the cells can be “rezoned” by allowing the mesh to move relative to the fluid, while keeping the same mesh connectivity. This rezoning helps keep the cells from becoming tangled or degenerate, which would prevent further simulation. Rezoning is an Eulerian process, since it allows fluid to flow across cell edges, and it can smear out contacts and shocks unless one is careful when rezoning near them. For this reason, if the fluid motion is complex enough, it may be impossible to keep the original mesh connectivity without unacceptably degrading accuracy.The “free Lagrange” methods such as FLAG [23] and Whitehurst’s signal method [18] allow mesh points to be dynamically linked and unlinked over the course of the simulation, thereby changing the initial mesh connectivity as the fluid moves. These methods use a variety of heuristics to maintain a reasonable mesh, such as trying to keep nearest neighboring points connected, or trying to keep the angles of mesh triangles as equal as possible.In RRM, there is no mesh connectivity, so there is no need to track or alter it over the course of simulation. RRM constantly creates new cells, which has an effect similar to rezoning in that it allows fluid to flow across the edges of chopped cells.Moving finite element (MFE) methods [24]–[27] generalize the finite element method to better track moving fluid flow features using moving elements. MFE methods result in extremely stiff systems of ordinary differential equations (ODEs), and so require sophisticated implicit ODE solvers. They also require careful tuning with user-chosen parameters to keep the elements from becoming too small or bunching up at shocks. It is also possible to adaptively create and destroy nodes in an MFE method, as shown by Kuprat in 1992 [28].Since each new RRM cell chops out and replaces what was underneath it, cells can never bunch up, and there is only one user-chosen parameter to set, the maximum error metric (though this metric does have three components). And since RRM does not use systems of equations, it does not need numerical solvers.Of all current CFD methods, the meshfree methods are the most similar to RRM. Indeed, we categorize RRM itself as a meshfree method, since it shares the characteristics of being purely Lagrangian and not using a mesh. But as we will see, there are many differences between previous meshfree methods and RRM in how cells or particles are formed, how their motion is calculated, and how long they persist during a simulation.Previous meshfree methods and RRM both discretize a fluid into a set of particles or cells with no connectivity between them. In previous meshfree methods like SPH and MPS, the particles are acted upon by forces over time, and thus change their velocities. In contrast, once a cell is created in RRM, it moves at a constant velocity even as parts of it are chopped away by the creation of subsequent new cells. This means RRM does not require integration of the equations of motion of its cells.A major difference between RRM and SPH or MPS lies in how the primitive variable values like density and pressure are determined at each point in the fluid. SPH and MPS store the conserved quantities in moving material particles. These particles have zero extent, but a smoothing function allows us to find the primitive variable values in the spaces between the particles. In contrast, RRM does not store the conserved quantities of the fluid directly. Instead, they are the result of integrating the stored primitive variable values over the cell areas. Since the cells have non-zero sizes, and new cells are constantly being created to fill any gaps, we do not need a smoothing function. The tracer particles in RRM are non-material particles that do not carry any conserved quantities, they simply trace out the expanding acoustic waves in the fluid.A particle-based meshfree method like SPH can be made adaptive by allowing the smoothing length to vary inversely with density, as shown by Benz in 1990 [29]. This is refined by Owen et al. in 1998 [30] to give each particle a time-varying, anisotropic smoothing length that attempts to keep the number of neighboring particles the same in each direction.The motion of the tracer particles in RRM gives an effect similar to the use of anisotropic smoothing length in adaptive SPH, since the tracer particles sweep out an area that varies with the local speed of sound and the local fluid motion.SPH can also be made adaptive by splitting and merging particles during simulation [31]–[33]. Splitting is done in low-density areas, and merging in high-density areas, to insure that the number of particles is appropriate to accurately track the fluid motion.The previous meshfree method most similar to RRM is the Finite Mass Method (FMM) [34], [35]. FMM divides the fluid into finite-sized cells (called mass packets in FMM papers) with an internal distribution typically described by third-order B-splines. These cells can move, deform, and interact during simulation. If the cells become too deformed, the simulation is stopped, the fluid is remeshed into new, undeformed cells and the simulation is restarted.Cells in RRM do not change shape, except in that parts of them are chopped away by the creation of new cells. Therefore RRM does not need to remesh to fix excessive cell deformation. RRM constantly creates and destroys cells, so any excessive bunching or gapping due to cell movement is fixed incrementally rather than all at once in a remeshing operation.RRM should work for most systems of conservation equations that have a complete wave description, which are often described as hyperbolic. In one dimension, such a system typically has three types of waves: entropy waves, left-propagating acoustic waves, and right-propagating acoustic waves. In RRM, the motions of the cells model the entropy waves, and the moving tracer particles model the propagation of the acoustic waves.RRM’s tracer particle methodology is useful only for compressible fluids, since incompressible fluids have no acoustic waves. So for example, RRM should work for the compressible Navier-Stokes equations, but not the incompressible Navier-Stokes equations.RRM’s “chop out and flatten” methodology is most applicable to systems that are locally conservative. A system that is globally conservative but not locally conservative cannot easily be chopped out and flattened, since the flattening would require access to parts of the fluid other than those that were chopped out.One limitation of RRM is that its data structures are complex and difficult to code correctly. Allowing cells to move and be chopped up over time is straightforward, but involves extra bookkeeping that many other methods do not require. Extension to 2D and 3D is possible, but will be even more complex since cells must be allowed to rotate as well as translate.Another limitation of RRM is its use of event-driven simulation. This gives good spatial and temporal adaptivity, but it makes the algorithm more difficult to parallelize, especially on GPGPUs (General-Purpose Graphics Processing Units) such as NVIDIA’s Tesla where data-dependent branching is penalized.A final limitation applies to von Neumann boundary conditions. Since RRM does not use spatial derivatives, it is difficult to hold them constant at the fluid boundaries. RRM could implement von Neumann boundary conditions with some difficulty by creating “ghost cells” just outside the boundaries to give the correct behavior, but it would not fit neatly into the “adjusting the conserved quantities of new cells” paradigm discussed in the above section on boundary conditions.The obvious future research directions for RRM are extension to 2D and 3D, replacement of the Euler equations with the compressible Navier-Stokes equations, and parallelization. A not-so-obvious direction is using RRM to simulate the behavior of non-fluid fields containing inherent discontinuities or intractable nonlinearity. Since RRM does not evaluate numerical derivatives or solve systems of equations, it might be applicable to fields whose traditional discretizations are numerically ill-behaved or difficult to formulate.RRM could also benefit from further investigation into its rate of convergence. As noted at the end of the section on absolute error, increases in computational effort do not always result in proportional decreases in the integral error norm. More research into this area would be helpful to insure that RRM can efficiently achieve any desired level of error."
        },
        "10.1371/journal.pone.0061531": {
            "author_display": [
                "Ilyas Khan",
                "Farhad Ali",
                "Sharidan Shafie"
            ],
            "title_display": "Stokes' Second Problem for Magnetohydrodynamics Flow in a Burgers' Fluid: The Cases γ = λ<sup>2</sup>/4 and γ>λ<sup>2</sup>/4",
            "abstract": [
                "\nThe present work is concerned with exact solutions of Stokes second problem for magnetohydrodynamics (MHD) flow of a Burgers' fluid. The fluid over a flat plate is assumed to be electrically conducting in the presence of a uniform magnetic field applied in outward transverse direction to the flow. The equations governing the flow are modeled and then solved using the Laplace transform technique. The expressions of velocity field and tangential stress are developed when the relaxation time satisfies the condition γ = λ2/4 or γ>λ2/4. The obtained closed form solutions are presented in the form of simple or multiple integrals in terms of Bessel functions and terms with only Bessel functions. The numerical integration is performed and the graphical results are displayed for the involved flow parameters. It is found that the velocity decreases whereas the shear stress increases when the Hartmann number is increased. The solutions corresponding to the Stokes' first problem for hydrodynamic Burgers' fluids are obtained as limiting cases of the present solutions. Similar solutions for Stokes' second problem of hydrodynamic Burgers' fluids and those for Newtonian and Oldroyd-B fluids can also be obtained as limiting cases of these solutions.\n"
            ],
            "publication_date": "2013-05-08T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 1,
            "views": 1314,
            "shares": 0,
            "bookmarks": 1,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0061531",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0061531&representation=PDF",
            "fulltext": "IntroductionMagnetohydrodynamics is the study of flow of electrically conducting fluids in electric and magnetic fields. This phenomenon is essentially one of the mutual interaction between the fluid velocity and electromagnetic field i.e. the motion of the fluid affects the magnetic field and the magnetic field affects the fluid motion. Basically, magnetohydrodynamics is a research area that involves the study of motion of electrically conducting fluids such as plasma and salt water. MHD flows are found to have influential applications in many natural and man made flows. They are frequently used in industry to heat, pump, stir and levitate liquid metals. Another application for MHD is the magnetohydrodynamic generator in which electrically conducting fluid is used to generate electric power. The flows of an electrically conducting fluid in the presence of a magnetic field have important applications in various areas of technology such as, accelerators centrifugal separation of solid from fluid, purification of crude oils, astrophysical flows, petroleum industry, polymer technology, solar power technology, nuclear engineering applications and other industrial areas [1], [2].\nThe literature on the study of MHD viscous fluid is abundant (see for example [3]-[10] and the references therein). However, such studies for non-Newtonian fluids are limited. To the best of author's knowledge, MHD flow of non-Newtonian fluids was first studied by Sarpkaya [11]. Subsequently, several other investigations considering the MHD flow of non-Newtonian fluids were carried out and currently this field has become an active area of research. Ersoy [12] examined the MHD flow between eccentric rotating disks for an Oldroyd-B fluid. Hayat and Hutter [13] obtained exact solutions for flows of an electrically conducting Oldroyd-B fluid over an infinite oscillatory plate in the presence of a transverse magnetic field. Khan et al [14] developed exact solutions of Stokes second problem for MHD Oldroyd-B fluid. Liu et al [15] and Zheng et al [16] and [17] analyzed the MHD flow of generalized Oldroyd-B fluid for different fluid motions using frictional derivatives. On the other hand, studies on MHD flow of Burgers' fluid are very limited. Therefore, any MHD analysis of this model will be genuine contribution towards the enhancement of the theory of non-Newtonian fluid mechanics. Hayat et al. [18] studied the MHD flow of Burger's fluid whereas with heat transfer analysis was investigated by Siddiqui et al. [19], [20]. Very recently, Khan et al [21] studied MHD flow of Burger's fluid and obtained exact solutions of Stokes' first problem by using the Laplace and Fourier sine transforms. The MHD flows of these fluid models and some other well known non-Newtonian fluids models such as second grade fluid [22]–[27], third grade fluid [28], Maxwell fluid [29], [30], generalized Burgers' fluid [31], [32], Micropolar fluid [33], [34], Walters-B liquid fluid [35], Jeffery fluid [36] and Nanofluid [37] are used to describe stress relaxation, shear thinning or shear thickening, normal stress effects, earth's mantle, asphalt and asphalt mixes, food products and soil, dilute polymeric solutions, hydrocarbons, paints and several other industrial and geomechanical fluids.\nKhan et al [38] extended the work of Fetecau et al [39] to the MHD flow of an Oldroyd-B fluid induced by the impulsive motion of a plate between two side walls perpendicular to the plate. The analytical solutions are carried out by using the Fourier sine and Laplace transforms. Vieru et al [40] determined exact solutions corresponding to the flow of a Burgers' fluid over a suddenly moved flat plate when the relaxation times satisfy the condition  or  They used the Laplace transform technique to find the expressions for velocity and shear stress fields which were reduced to the similar solutions for Newtonian and Oldroyd-B fluids as limiting cases. Recently, Khan et al [41] extended the work of Vieru et al [40] to the flow of a Burgers' fluid over an oscillatory moved flat plate. They used a similar method of solution and obtained the exact solutions.\nFrom the literature survey, it is found that there are very few problems of Newtonian fluids for which the exact solutions are available. However, these solutions become even more rare if the constitutive equations of non-Newtonian fluids are considered. The importance of exact solutions is not only that they can explain the physics of some fundamental flows but also that such solutions can be used as checks against complicated numerical codes that have been developed for much more complex flows. Moreover, one of the most common mistakes that has been overlooked for the last coupled of decades has been identified by Christov [42]. Christov pointed out that in the case of Stokes first and second problems, the plate's velocity is given by , where  denotes the Heaviside step function, and  is some smooth function. This inclusion of Heaviside step function was ignored previously. There are several comments and errata published in the literature for the modification of such erroneous results. It is important to mention here that such type of mistakes reported by Christov [42] are avoided in the present communication.\nThe main purpose of the present investigation is to extend the work of Vieru et al. [40] and Khan et al. [41] for the MHD flow of an electrically conducting Burgers' fluid past an oscillating plate when the magnetic field is acting perpendicular to the flow direction. It is also interesting to study the flow of non- Newtonian fluids with externally imposed magnetic fields which control the boundary layer and increase the performance of many systems. For example, when we use the electrically conducting fluid in MHD power generators, their performance increase in comparison to conventional electric generators where solid conductors are used to generate electric power. The present work can also be helpful to study underground oil, where there is a natural magnetic field and the motion of blood through arteries [43], [44].\nThe rest of the paper is arranged as follows. The governing equations of the problem are given in section 2. The mathematical formulation of the problem is given in Section 3.The solution of the problem is given in section where the Laplace transform technique is used and the expressions for velocity and shear stress fields are obtained when the relaxation time satisfies the condition  or Limiting solutions are given in section 5. Graphical results are displayed in section and discussed for the embedded flow parameters. This paper ends with some conclusions given in section \nGoverning EquationsThe unsteady incompressible flow of an electrically conducting fluid is governed by the following equations(1)(2)(3)where is the velocity vector, is the density of the fluid, is the pressure,is the the extra stress tensor, is the current density, is the total magnetic field where denotes the applied magnetic field and is the induced magnetic field, is the magnetic permeability, is the electric field and is the electrical conductivity of the fluid.\nThe extra stress tensor for non-Newtonian Burgers' fluid constitutes the following equation [40], [41](4)in whichis the dynamic viscosity,is the first Rivlin Ericksen tensor, is the velocity gradient,is the transpose of the velocity gradient,andare the relaxation and retardation times respectively and is the material constant of Burgers' fluid multiplies the upper second order convected time derivative of defined as(5)where is the material time derivative.\nFor the problem under consideration, we are looking for velocity and stress fields of the form(6)where is the -component of velocity field and is the unit vector in the -direction.\nIn order to calculate Lorentz force, it is assumed that the polarization effects are zero (, the magnetic field is applied in outward perpendicular direction to the flow and the induced magnetic field is negligible compare to the applied magnetic field under the assumption of small magnetic Reynolds number, is the strength of applied magnetic field. Thus in view of these assumptions and using Eq. (3), the Lorentz force becomes [21](7)\nThus using Eq. (6), the continuity Eq. is identically satisfied and the momentum Eq. (2) in the absence of a pressure gradient in the flow direction and Eq. (4) after using Eqs. and (7) and having in mind the initial conditions give the following governing equations(8)(9)where is the non-trivial shear stress.\nMathematical Formulation of the ProblemWe consider the unsteady incompressible flow of an electrically conducting Burgers' fluid occupying the upper half space of plane over a rigid flat plate. The axis is taken parallel to the flow direction whereas axis is taken normal to the plate. The magnetic field is applied in outward transverse direction to the flow. Initially, we assume that both fluid and plate are at rest. After time the plate begins to oscillate in its own plane and the fluid is gradually moved as shown in Fig. 1.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Physical model and coordinates system.doi:10.1371/journal.pone.0061531.g001For such type of motions the governing equations are (8) and (9) with the following initial and boundary conditions(10)(11)where is the characteristic velocity, is the imposed frequency of the velocity of the plate and is the Heaviside step function.\nMoreover, the natural conditions (12)which are the consequences of the fact that the fluid is at rest at infinity and there is no shear in the free stream, have to be also satisfied.\nIntroducing the following non-dimensional variables(13)with the constant the governing Eqs.  and take the following forms(14)(15)whereThe corresponding initial and boundary conditions become(16)(17)(18)In order to solve the initial and boundary-value problem we consider two different cases and and use the Laplace transform.Case-I: Solution of the problem for In order to determine exact solutions for our problem, we substitute into Eq. apply the Laplace transform to Eqs. and and use the initial conditions We find that(19)(20)where is the transform parameter. In view of the boundary conditions, the Laplace transforms and of and have to satisfy the conditions(21)where(22)The solutions of Eqs. and satisfying the boundary conditions are(23)(24)(25)(26)where the subscripts and denote the solutions corresponding to the cosine and sine oscillations of the boundary, respectively.In order to find we follow a similar procedure to and write Eq. in the product form(27)where(28)(29)(30)Of course, in view of Eq. we have(31)where the denotes the convolution product and  andare the inverse Laplace transforms of  and respectively.Applying the inverse Laplace transform to Eqs. and we find that(32)(33)(34)(1)where  and are the modified Bessel functions of the first kind of order zero and order one respectively.Now using Eqs. into Eq. and by the definition of Heaviside step function, we get(35)Similarly for the sine part of velocity, we get the following expression(36)In order to find the dimensionless shear stress, we write given by Eq. in the form(37)where(38)(39) (40)For we employ(41)with(42)The Laplace inverse transforms of Eqs. yields(43)(44)(45)where(46)The convolution product of Eqs. gives(47)Similarly for sine oscillation we obtain(48)Furthermore, it is noted that the expressions and are valid only for Therefore, we are separately considering the case when Hence, Eq. (25) can successively be written in the form(49)where(50)(51) (52)The inverse Laplace transforms of Eqs. and are given as(53)(54)Now taking the convolution product of Eqs. and we finally obtain(55)In the same way we find that(56)Now, in order to find the associated expressions for velocity, we directly put into Eqs. and make the change of variable in the first integral, and finally we get(57)and(58)Equivalent expressions for the velocities andcan also be derived from Eqs. and For example, decomposing given by equation under the formwe can write(59)where(60)(61)Applying the inverse Laplace transforms to Eqs.we find that(62)(63)(64)Consequently, introducing Eqs. (63) and (64) into Eq. (62), we obtain(65)Following a similar way, we also obtain(66)Case-II: Solution of the problem for Let us now consider the expressions of velocity fields and tangential stresses when . From the system of equations we obtain(67)(68)(69)(70)Here the second grade equation  has complex roots.In order to find the Laplace inverse transform of , we write Eq. (67) as(71)where(72)(73) The Laplace inverse transform of Eq. is given by(74)where(75)Moreover, the Laplace inverse transform of Eq. yields(76)(77)where is the Dirac delta function, is the Bessel function of the first kind of order one and(78)In view of the relations and it clearly results(79)Adopting a similar procedure for the sine oscillation of the boundary, we get an expression similar to Eq.with(80)where(81)The corresponding expressions for the shear stresses are given by(82)(83)(84) (85) (86)where is the Bessel function of the first kind of order zero."
        },
        "10.1371/journal.pone.0053228": {
            "author_display": [
                "Andreas Hanke"
            ],
            "title_display": "Non-Equilibrium Casimir Force between Vibrating Plates",
            "abstract": [
                "\n        We study the fluctuation-induced, time-dependent force between two plates confining a correlated fluid which is driven out of equilibrium mechanically by harmonic vibrations of one of the plates. For a purely relaxational dynamics of the fluid we calculate the fluctuation-induced force generated by the vibrating plate on the plate at rest. The time-dependence of this force is characterized by a positive lag time with respect to the driving. We obtain two distinctive contributions to the force, one generated by diffusion of stress in the fluid and another related to resonant dissipation in the cavity. The relation to the dynamic Casimir effect of the electromagnetic field and possible experiments to measure the time-dependent Casimir force are discussed.\n      "
            ],
            "publication_date": "2013-01-10T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 1104,
            "shares": 0,
            "bookmarks": 4,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0053228",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0053228&representation=PDF",
            "fulltext": "IntroductionA fundamental advance in the understanding of nature was the insight that physical forces between bodies, instead of operating at a distance, are generated by fields; the latter obeying their own dynamics, implying a finite speed of propagation of signals and causality [1]. Moreover, time-varying fields can sustain themselves in otherwise empty space to produce disembodied waves; exemplified by electromagnetic fields and waves, and gravitational fields. Gravitational waves are believed to be detected in the near future [2].\nAnother force seemingly operating at a distance is the Casimir force. This force was first predicted by Casimir in 1948 for two parallel conducting plates in vacuum, separated by a distance , for which he found an attractive force per unit area  [3]. It can be understood as resulting from the modification of quantum-mechanical zero-point fluctuations of the electromagnetic field due to confining boundaries [4]–[7]. In the last decade, high-precision measurements of the Casimir force have become available which confirm Casimir's prediction within a few per cent [8]–[11]; recent experiments demonstrate the possibility of using the Casimir force as an actuation force for movable elements in nanomechanical systems [10]–[12]. The thermal Casimir force, generated by thermal rather than quantum fluctuations of the electromagnetic field, has recently been confirmed [13]. This development goes along with significant advances in calculating the Casimir force for complex geometries and materials [7], [14]–[21]. A force analogous to the electromagnetic Casimir force occurs if the fluctuations of the confined medium are of thermal instead of quantum origin [5], [22], [23]. The thermal analog of the Casimir effect, referred to as critical Casimir effect, was first predicted by Fisher and de Gennes for the concentration fluctuations of a binary liquid mixture close to its critical demixing point confined by boundaries [24]; recently, the critical Casimir effect was quantitatively confirmed for this very system [25]. (For computational methods concerning the calculation of critical Casimir forces, see, e.g., Refs. [26]–[28].)\nThe vast majority of work done on the Casimir effect, and fluctuation-induced forces in general, pertain to the equilibrium case. That is, the system is in its quantal ground state in case of the electromagnetic Casimir effect, or in thermodynamic equilibrium in case of the thermal analog. A number of recent experiments probe the Casimir force between moving components in nanomechanical systems [10]–[12], and effects generated by moving boundaries have been studied, e.g., for Casimir force driven ratchets [29]; however, the data are usually compared with predictions for the Casimir force obtained for systems at rest, corresponding to a quasi-static approximation.\nDistinct new effects occur if the fluctuating medium is driven out of equilibrium. In this case the observed effects become sensitive to the dynamics governing the fluctuating medium, which may lead to a better understanding of these systems and may provide new control parameters to manipulate them [30]–[40]. The generalization of the electromagnetic Casimir effect to systems with moving boundaries, referred to as dynamic Casimir effect, exhibits friction of moving mirrors in vacuum and the creation of photons [41]–[44]. Related effects due to oscillating media [45], and nonequilibrium Casimir-Polder forces on moving atoms [46], have also been considered. Interesting effects occur if each body immersed in the fluctuating electromagnetic field is at a different temperature [47], [48]. The associated nonequilibrium Casimir forces and heat transfer between the bodies lead to observable effects [49], [50]. For the thermal analog, fluctuation-induced forces in non-equilibrium systems have been studied in the context of the Soret effect, which occurs in the presence of an external temperature gradient [31]. Effects of temperature changes in classical free scalar field theories and thermal drag forces have been studied in [32]–[35]; however, recently it was argued that the method presented in [32]–[35] is invalid to obtain the fluctuation-induced force exerted on an inclusion or plate embedded in the medium, whereas the stress tensor method, as used in the present work, yields the correct force [40]. Fluctuation-induced forces have also been obtained for macroscopic bodies immersed in mechanically driven systems [30], granular fluids [36], and reaction-diffusion systems [37]. Recently it was shown that non-equilibrium fluctuations can induce self-forces on single, asymmetric objects, and may lead to a violation of the action-reaction principle between two objects [38].\nIn this work we consider a correlated fluid driven out of equilibrium mechanically by a vibrating plate, and study the resulting fluctuation-induced, time-dependent force  on a second plate at rest. We wish to study the time-dependence of this force in view of the finite speed of diffusion of perturbations in the fluctuating medium, and causality. We consider the simplest possible dynamics of the medium between the plates, namely the purely relaxational dynamics of model A. Specifically, we consider two infinitely extended plates parallel to the -plane, where plate 1 is at rest while plate 2 is vibrating parallel to the -direction by some external driving, resulting in a time-dependent separation (see Fig. 1)(1)with amplitude  and driving frequency . The fluctuating medium is described by a non-conserved scalar order parameter field , corresponding to the critical dynamics of model A [51], [52], subject to Dirichlet boundary conditions  at the plates. The field  may describe the order parameter of a fluid thermodynamically close to a critical point, or a massless Goldstone mode arising from the breaking of a continuous symmetry such as in nematic liquid crystals or superfluid 4He [5], [22]. Our results hold in Gaussian approximation right at the critical point  for which the bulk correlation length  diverges. For  the fluctuation-induced force also depends on the finite correlation length . Moreover, for near-critical fluids and binary liquid mixtures the boundary conditions at confining walls correspond to the so-called normal rather than Dirichlet surface universality class [22]. However, it should be noted that dynamic critical behavior is less universal than equilibrium critical behavior. For example, for the liquid-gas transition and the demixing transition in a binary liquid mixture a conserved order parameter convects with the conserved transverse momentum current of the fluid, resulting in critical dynamics referred to as model H [53]; conversely, the superfluid transition of 4He corresponds to critical dynamics of model F [54]. In addition, critical dynamics of real fluids may be modified by effects due to gravity and the coupling of the order parameter to secondary densities, which further complicates a quantitative comparison of theory and experiment (see [51], [52] for reviews). Strictly speaking, purely relaxational dynamics of a non-conserved scalar order parameter corresponding to model A only applies to uniaxial magnetic systems and simple lattice gases. However, the results and conclusions derived here for model A dynamics yield new insight in non-equilibrium behavior and may serve as a starting point for more realistic models.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Geometry of two parallel plates separated by a varying distance . Plate 1 is at rest while plate 2 is vibrating parallel to the -direction. The plates are immersed in a fluctuating medium with purely relaxational dynamics described by a non-conserved scalar order parameter . The fluctuation-induced, time-dependent net force  on plate 1 is the sum of forces  and  acting on opposite sides of the plate.\ndoi:10.1371/journal.pone.0053228.g001Our results for the time-dependent force  on plate 1 hold to first order in  (cf. Eq. (1)). As shown in Fig. 1,  is the sum of forces  and  acting on opposite sides of the plate;  being the force acting on plate 1 from the side of the cavity, and  the (time-independent) force on the boundary surface of a semi-infinite half-space filled with the fluctuating medium. The net force  is expected to be finite and overall attractive, i.e., directed towards plate 2.\nResults\nRelaxational Dynamics\nIn traditional studies of the fluctuation-induced force between two plates, both plates are assumed to be at rest at constant separation  (see Fig. 1). The system is in thermal equilibrium and the fluctuations of the order parameter  are described by the statistical Boltzmann weight  with Gaussian Hamiltonian(2)where  with the Boltzmann constant  and the temperature  (assumed to be constant). The fluctuation-induced force  on plate 1 per unit area  is found to be [5], [22], [23](3)where the minus sign indicates that the force is attractive. Equation (3) is a universal result, independent of the underlying dynamics of the fluctuating medium, as long as the equilibrium is described by Eq. (2).\nWe now turn to the case where plate 2 is vibrating parallel to the -direction, resulting in a time-dependent separation  between the plates (see Eq. (1) and Fig. 1). The time-dependent boundary conditions for the order parameter  in the medium between the plates now drive the system out of equilibrium. Locally, the order parameter will relax back to equilibrium according to the dynamics of the medium; in this work, we consider a purely relaxational dynamics described by the Langevin equation (see, e.g., Chapter 8 in Ref. [55], and references cited therein)(4)where  is the friction coefficient. The random force  is assumed to have zero mean and to obey the fluctuation-dissipation relation(5)where the brackets  denote a local, stochastic average and  is the delta function in 3 dimensions.\n\n\nNon-Equilibrium Casimir Force\nThe force per unit area acting on plate 1 from the side of the cavity can be expressed as  where  are the components of  parallel to the plate and  is the -component of the stress tensor [6], [56] (see text below Eq. (104) in Ref. [56] for a discussion of the stress tensor in connection with critical dynamics). Similarly, the force per unit area acting on the other side of plate 1 is given by  where  is again evaluated in the cavity between the plates but for the limit  (see Fig. 1). The net force per unit area on plate 1 yields as(6)Using the Dirichlet boundary condition  at the plates we obtain(7)To calculate the two-point correlation function of  on the right-hand side of Eq. (7) we note that the solution  of Eq. (4) can be expressed as(8)where  is the volume of the cavity at time  and the Green's function  is defined as the solution of(9)subject to the boundary condition  whenever  or  is located on the surface of one of the plates (note that  is symmetric in  and ). In addition,  for  by causality. Thus,  can be expressed as a linear superposition of contributions from the source  at times  and positions , carried forward in time by the propagator . Using Eqs. (8) and (5) one finds the two-point correlation function(10)In the present set-up, the system is translationally invariant in -direction at any time , whereas translation invariance in time is broken due to the varying separation  between the plates. Thus, introducing the partial Fourier transform  of  as(11)the function  depends explicitly on one of the time coordinates in , say, . Using Eqs. (10), (11) we find for the expression in Eq. (7) (the star symbol indicates the complex conjugate for real-valued argument )(12)where(13)For given propagator , hence function ,  is obtained using Eqs. (6) and (12) (cf. Methods).\n\n\nDiffusion of Stress and Resonant Dissipation\nThe ratio  of the fluctuation-induced, time-dependent force  on plate 1 due to the vibrating plate 2 and the corresponding static force  is a universal (cutoff-independent) function of  (geometry),  (time-dependence of the driving), and the dimensionless parameter(14)(see Eqs. (1), (3), (4), and Fig. 1). Our results for  correspond to an expansion to first order in  and can be cast in the form(15a)where the dimensionless function  is normalized such that  for . For  the function  can be represented as(15b)in terms of an amplitude  and a phase shift .\nThe amplitude  is shown in Fig. 2. For , i.e., , the normalization  in conjunction with  (see below) implies . For , i.e., , the length  is a measure of the distance over which a perturbation diffuses during an oscillation. For increasing  a variation of stress generated at the vibrating plate 2 is more and more attenuated, i.e., washed out, due to the diffusive nature of the medium before it reaches plate 1; thus  is monotonically decreasing for increasing  (Fig. 2, black line).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Amplitude of the non-equilibrium Casimir force.Amplitude  of  as a function of  (see Eq. (15)) (black line). Also shown are the amplitudes  of  (dif, red line) and  of  (res, green line) describing contributions to  due to diffusion of stress and resonant dissipation, respectively (see pone.0053228.e140Eqs. (17), pone.0053228.e142(18)).\ndoi:10.1371/journal.pone.0053228.g002The force  in pone.0053228.e104Eq. (15) has contributions of different physical origin related to diffusion of stress (dif) and resonant dissipation (res) in the medium between the plates, i.e.,(16)The contributions  and  are related to real and imaginary poles in the complex-frequency plane occurring in the calculation of , respectively (see Eqs. (35) and (37)). Both  and  may be expanded as in pone.0053228.e104Eq. (15):(17a)(17b)and(18a)(18b)The amplitude  is shown as the red line in Fig. 2. For , i.e., , there is no contribution from resonant dissipation, thus  and  (however,  for  because ). For  the amplitude  is attenuated due to the diffusive nature of the medium as discussed in relation to  above; thus  is monotonically decreasing for increasing  (Fig. 2, red line). Finally, the amplitude  due to resonant dissipation is shown as the green line in Fig. 2. Resonant dissipation is absent in the static case , i.e., , which implies . For large  the amplitude  is attenuated due to the diffusive nature of the medium as discussed in relation to  above; thus  starts at , is increasing for small  and monotonically decreasing for large  (Fig. 2, green line).\nFigure 3 shows the phase shift  of the function  in Eq. (15b) in terms of the variable . Thus, using Eq. (14), , where the lag time  is a measure of the time a variation of stress generated at the vibrating plate 2 takes to diffuse through the medium to reach plate 1. For illustration, for  and  we obtain .  is fairly constant over a wide range of  and approaches a finite value  for  (Fig. 3a, black line); thus  as expected for the present diffusive dynamics in the medium between the plates. In the limit  the relation  with finite  implies . A qualitatively similar behavior occurs for the phase shift  of the function  in Eq. (17b) (Fig. 3a, red line). Finally, Fig. 3b shows the phase shift  of the function  in Eq. (18b); in this case,  itself is approximately constant and approaches a finite value for  (in contrast to , for which  is approximately constant, see above). This implies that the corresponding lag time  formally diverges for , i.e., , which reflects the fact that resonant dissipation is absent in the static case . However, the divergence of  for  is suppressed in the net force  since the amplitude  in Eq. (18b) vanishes for , so that the lag time  of  stays finite for  (see pone.0053228.e104Eq. (15) and Fig. 3a).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Phase shift of the non-equilibrium Casimir force.(a) Phase shift  of  in terms of  (see pone.0053228.e104Eq. (15)) (black line). Also shown is the phase shift  of  describing the contribution to  due to diffusion of stress (dif, red line) (see pone.0053228.e140Eq. (17)). (b) Phase shift  of  describing the contribution to  due to resonant dissipation (res) (see pone.0053228.e142Eq. (18)).\ndoi:10.1371/journal.pone.0053228.g003\nDiscussionWe have studied the fluctuation-induced, time-dependent force  between two plates confining a fluid which is driven out of equilibrium mechanically by harmonic oscillations of one of the plates, assuming purely relaxational dynamics of the fluid (corresponding to the critical dynamics of model A [51], [52]) (see Fig. 1). Our main results for , valid to first order in the amplitude  of the oscillations, are summarized in Figs. 2 and 3. We find two distinct contributions to  related to diffusion of stress in the fluid and resonant dissipation, respectively. Resonant dissipation has been studied for the dynamic Casimir effect of the electromagnetic field, where it is a result of enhanced creation of photons if the driving frequency corresponds to a resonance frequency of the cavity [41]–[44]. In the present case, dissipation is generated by the viscosity of the fluid described by the friction parameter  in the Langevin equation (4).\nFluctuation-induced forces may be observed, e.g., by means of atomic force microscopy (AFM). To avoid the experimental difficulty of keeping two flat plates parallel one usually employs geometries in which one of the surfaces is curved; for example, by measuring the force between a sphere attached to the tip of an AFM cantilever and a flat plate. The force  on a sphere of radius  separated by a distance  surface-to-surface from a flat plate is related to the energy of interaction per surface area  between two flat plates by the proximity force rule . For example, the static force per unit area  in Eq. (3) yields  PT for  K, ,  nm, which is readily accessible by AFM.\nFluctuation-induced forces between moving objects are expected to occur for any medium exhibiting long-ranged correlations. However, as mentioned in the Introduction, for real fluids the model needs to be modified to take into account conservation of the order parameter and its convection with the transverse momentum current of the fluid, thus treating Casimir and hydrodynamic interactions on the same footing (corresponding to the critical dynamics of model H [53]); these effects are important and will modify the result for  in pone.0053228.e104Eq. (15) obtained for the purely relaxational dynamics of model A (see [51], [52] for reviews on dynamic critical behavior and its comparison with experiments). However, the time-dependent force  predicted in pone.0053228.e104Eq. (15) may be observable by means of computer simulations of the ferromagnetic Ising model (or corresponding lattice gas models) confined between two plates, one of which is vibrating at small amplitude.\nA much-studied subject related to the present study are hydrodynamic interactions of microscopic objects in viscous fluids since they are relevant, e.g., to the motility and locomotion of swimming microorganisms [57]. Recently, motivated by devices such as the AFM, the drag experienced by a cylindrical object (modeling an AFM cantilever) and a sphere oscillating at small amplitude near a flat surface were studied in detail [58], [59]; however, few results are available concerning the hydrodynamic force generated by an oscillating object on a different object nearby. Thus, it would be interesting to probe effective hydrodynamic interactions between different, moving objects immersed in a viscous fluid, and how these interactions are modified by the Casimir force when correlations in the fluid become long-ranged.\nThe fluctuation-induced, time-dependent force  on plate 1 due to the vibrating plate 2 is universal in the sense that it is largely independent of microscopic details of the system. The force ratio , where  is the force at fixed separation , only depends on the dimensionless variables  (geometry),  (time-dependence of the driving), and  characterizing the viscosity of the fluid. It would be interesting to generalize our approach to the electromagnetic field to study the analogous, time-dependent Casimir force  for the dynamic Casimir effect of the electromagnetic field.\nThe calculation of the non-equilibrium Casimir force  by Eqs. (6) and (12) requires the propagator  solving Eq. (9) subject to the time-dependent boundary conditions due to the vibrating plate 2. This problem can be solved, for general modulations of the plate(s) in space and time, by the method developed in Refs. [60], [61]. For the present set-up, we find for the partial Fourier transform  of  (i.e., transforming the spatial coordinates ,  parallel to the plates as in Eq. (11) but keeping the time coordinates , ; in what follows, we omit the argument  for ease of notation) [60], [61](19)where  is the propagator in the half-space  bounded by a Dirichlet surface at  (that is, the function  itself is independent of the vibrating plate 2; the dependence of  on plate 2 in Eq. (19) only enters through the arguments ,  in , and the kernel ). The kernel  is defined by(20)In this work, we consider small variations of the separation between the plates about a mean separation , i.e.,(21)Our results hold to first order in . To this end, we insert Eq. (21) in Eq. (19) and expand everything to first order in  (note that  also enters the upper boundary of the integration over  in Eq. (12)). This results in expansions  and  of the functions  and  from Eqs. (11), (13) in powers of . Equations (6), (12) then yield the corresponding contributions to .Let us first consider the leading order, i.e.,  and . Using Eq. (19) and transforming to -space as in Eq. (11) we find (omitting the arguments  and  for ease of notation)(22)where  with  from Eq. (26) below and . Thus,(23)and, using Eq. (13),(24)Using Eqs. (6), (12), (24) we thus obtain to leading order [31](25a)(25b)The integral in Eq. (25b) is finite and yields Eq. (3). In Eq. (25a) we use(26)so that  if  is real. Integrations over  as in Eq. (25a) are readily computed by contour integration in the complex -plane. In Eq. (25a) and throughout this work we use the convention that in -integrations we integrate above the pole in ; this can be accomplished by the replacement  in the denominator of the integrand in Eq. (25a). The limit  in final results is always understood. Note that this prescription introduces a positive time direction and ensures causality.  has a branch cut along the negative imaginary axis , , whereas  has a branch cut along the positive imaginary axis , . The integral over  in Eq. (25a) has two contributions. For the contribution involving , the contour integral can be closed in the upper complex -plane (thus avoiding the branch cut of ), where this term has no poles, so that the contribution from this term vanishes. Likewise, for the contribution involving , the contour integral can be closed in the lower complex -plane (avoiding the branch cut of ), where, in turn, this term has no poles. The single pole at  in the lower complex -plane then yields the expression in Eq. (25b); cp. Fig. 4a with .We now turn to the contribution to  to first order in . Using the expansion(27)in Eq. (12), with  from Eq. (13) and  from Eq. (24), we find for general (28)where(29)The symbol  denotes a convolution of two functions ,  involving an insertion of :(30)For functions , , the expression  is the representation in -space of ; i.e., . The functions ,  are the representations in -space of , , respectively. Equations (28), (29) are obtained by using Eq. (19) with , expanding everything to first order in , and using Eq. (30) for the resulting insertions of . The contribution of  to first order in  is determined by Eq. (20), resulting in  where the subscripts 0 and 1 indicate the order in ;  is given below Eq. (22).For the special case that plate 2 is vibrating with harmonic oscillations of amplitude  and frequency  (see Eqs. (1), (21)), i.e.,(31)we obtain . The integral  in Eq. (28) decays into two contributions corresponding to the terms in square brackets on the right-hand side of Eq. (29):(32)where the subscripts QQ and QP indicate the contributions from the first and second term in square brackets of Eq. (29), respectively. In what follows we show that these two terms yield distinct contributions to  corresponding to real-valued and imaginary poles in the complex -plane.For the first contribution in Eq. (32) we find (the  symbol indicates the complex conjugate of the preceding expression; regarding the replacement  in the denominator of the integrand, see the discussion below Eq. (26))(33)where(34)with ,  from Eq. (26). Computing the right-hand side of Eq. (33) by contour integration in the complex -plane, the contributions from the two terms in square brackets in the integrand are analyzed along similar lines as discussed below Eq. (26). Thus, for the first term in square brackets, the contour integral can be closed in the upper complex -plane, where  has no poles, so that the contribution from this term vanishes. For the second term in square brackets, the contour integral can be closed in the lower complex -plane, where  has no poles. The only contribution from this term is from the single pole at ; see Fig. 4a. Thus, including the contribution from the complex conjugate in Eq. (33), we obtain(35)The corresponding contribution to  is given by  (see Eqs. (28) and (32)). In the static case, where  and  in Eq. (1), this result can also be obtained directly from Eq. (25b) by replacing  with  and expanding to first order in . For finite , Eq. (35) emerges from the static case by a shift from  to a finite value of . This shift may be understood in terms of a transition from stationary modes in the cavity in the static case to modes with a time-dependence  in response to the vibrating plate 2. The resulting fluctuation-induced force  on plate 1 is characterized by a finite lag time  which is a measure of the time a variation of stress in the medium generated at the vibrating plate 2 takes to diffuse through the medium to reach plate 1 (see Fig. 1). For the present diffusive dynamics,  is related to the distance  between the plates by .For the second contribution in Eq. (32) we find(36)The contour integral over  can be closed either in the upper or the lower complex -plane, yielding identical results; the contributions from the poles at  and  in the lower complex -plane cancel. Closing the contour integral in the lower plane, the integral picks up contributions from the imaginary poles  of , where  and  is a positive integer. Note that  has a branch cut along the negative imaginary axis on which the poles  are located (see the related discussion below Eq. (26)); however, this branch cut may be cured using the identity , with , which holds close to the negative imaginary axis. The expression  is analytic in the lower complex -plane with isolated poles at ; see Fig. 4b. Summing over the residues of these poles yields(37)The corresponding contribution to  is given by . Note that  is proportional to , which implies that this term is absent in the static case  and solely generated by the fact that the system is driven out of equilibrium by the vibrating plate 2 (see Fig. 1). The imaginary-frequency poles  leading to Eq. (37) are related to resonant dissipation in the cavity, where the spectrum of imaginary resonance frequencies  is continuous due to the presence of the continuous in-plane wave number  (compare the related discussion of resonant dissipation in the context of the dynamic Casimir effect of the electromagnetic field in Ref. [43]). Resonant dissipation has been studied for the dynamic Casimir effect of the electromagnetic field, where it is a result of enhanced creation of photons if the driving frequency corresponds to a resonance frequency of the cavity [41]–[44]."
        },
        "10.1371/journal.pone.0017503": {
            "author_display": [
                "Shinji Deguchi"
            ],
            "title_display": "Mechanism of and Threshold Biomechanical Conditions for Falsetto Voice Onset",
            "abstract": [
                "\n        The sound source of a voice is produced by the self-excited oscillation of the vocal folds. In modal voice production, a drastic increase in transglottal pressure after vocal fold closure works as a driving force that develops self-excitation. Another type of vocal fold oscillation with less pronounced glottal closure observed in falsetto voice production has been accounted for by the mucosal wave theory. The classical theory assumes a quasi-steady flow, and the expected driving force onto the vocal folds under wavelike motion is derived from the Bernoulli effect. However, wavelike motion is not always observed during falsetto voice production. More importantly, the application of the quasi-steady assumption to a falsetto voice with a fundamental frequency of several hundred hertz is unsupported by experiments. These considerations suggested that the mechanism of falsetto voice onset may be essentially different from that explained by the mucosal wave theory. In this paper, an alternative mechanism is submitted that explains how self-excitation reminiscent of the falsetto voice could be produced independent of the glottal closure and wavelike motion. This new explanation is derived through analytical procedures by employing only general unsteady equations of motion for flow and solids. The analysis demonstrated that a convective acceleration of a flow induced by rapid wall movement functions as a negative damping force, leading to the self-excitation of the vocal folds. The critical subglottal pressure and volume flow are expressed as functions of vocal fold biomechanical properties, geometry, and voice fundamental frequency. The analytically derived conditions are qualitatively and quantitatively reasonable in view of reported measurement data of the thresholds required for falsetto voice onset. Understanding of the voice onset mechanism and the explicit mathematical descriptions of thresholds would be beneficial for the diagnosis and treatment of voice diseases and the development of artificial vocal folds.\n      "
            ],
            "publication_date": "2011-03-07T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 2,
            "views": 2701,
            "shares": 3,
            "bookmarks": 5,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0017503",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0017503&representation=PDF",
            "fulltext": "IntroductionThe self-excited oscillation of the vocal folds located at the larynx produces the major sound source of a voice [1]–[8]. This self-excitation is caused by the flow-structure interaction between respiratory airflow and vocal fold tissue. During self-excitation, airflow must provide the energy necessary for the development of vocal fold oscillation, otherwise, the vocal fold motion decays with time owing to frictional damping in the tissue [4], [5]. Van den Berg [9] provided the first mechanics-based explanation for the voice onset mechanism. He argued in his myoelastic-aerodynamic theory that a pressure drop across the constricted glottis (i.e., the flow path formed by a pair of vocal folds; see Fig. 1), which is created by the Bernoulli effect, sucks the vocal folds together and closes the glottis. Alternatively, the glottis may initially be closed only by laryngeal muscle contraction without the help of flow (see Fig. 2A, 1) [10]. A drastic increase in the subglottal pressure up to the lung pressure, accumulated beneath the closed glottis, pushes the vocal folds downstream and eventually opens the glottal width (Fig. 2A; 2, 3). The blown apart vocal folds are then able to return to their original position owing to an elastic restoring force because the surrounding air pressure at this stage must be relatively low due to the restart of the flow after the glottal opening (Fig. 2A; 3, 4), thereby creating repeated open-close movements. Thus, the vocal fold closure ensures a requirement for self-excitation, i.e., a continuous energy transfer from the flow to the vocal folds.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Schema of the front section of the entire glottis.doi:10.1371/journal.pone.0017503.g001\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Relationship between the glottal pressure and vocal fold deformation.(A) The positive energy transfer from airflow to vocal fold motion with glottal closure or collision, previously submitted by van den Berg [9] as the myoelastic-aerodynamic theory. The inferior half of the glottis (dashed rectangular region in Fig. 1) is modeled. The theory suggests that the vocal folds are initially sucked together due to the Bernoulli effect. Alternatively, the vocal folds may initially be closed due to laryngeal muscle contraction without the help of the fluid (1). In either case, the glottal closure increases the subglottal pressure, resulting in upward deformation of the vocal folds (2). Here, the size of the letter P indicates the magnitude of the glottal pressure, and the dashed lines indicate the original position of the vocal folds. The pressure is more or less diminished due to the restart of the flow after the reopening of the glottis (3), which then allows the vocal folds to return to the original position due to elastic recoil (4). (B and C) Mucosal wave-based explanation for self-excitation of vocal fold motion without closure and the resultant drastic increase in the subglottal pressure. Net positive energy transfer is not achieved when a single degree of freedom model is employed (B) while it may be possible for a model with a degree of freedom of more than two (C). However, the schema (C) assumes a quasi-steady flow assumption, which is not applicable to a high frequency range such as that for a falsetto voice. For details, see the text.\ndoi:10.1371/journal.pone.0017503.g002Then, the question arises as to how glottal pressures asymmetric in magnitude over one oscillatory cycle (i.e., larger in the opening phase than in the closing phase, resulting in a net positive energy transfer) are created with no such glottal closure during vocal fold oscillations [5]. This requirement may not be satisfied in the absence of glottal closure because the Bernoulli effect, whose magnitude is determined by the absolute value of the width or cross-sectional area (but not its time rate of change), has the same value whether the glottis is opening or closing; therefore, the same pressure in magnitude may be applied to the vocal folds, resulting in the failure of developing oscillations (see Fig. 2B) [5]. Note that the vocal folds exhibit wavelike motion in the coronal plane during speech [9], [11]. On the basis of an analytical two-mass vocal fold model, Ishizaka and Matsudaira [1], [2] demonstrated that such wavelike motion or phase lag in motion between the upper and lower masses of the vocal folds led to self-excitation through flow-structure interaction (see Fig. 2C). Titze [4], [5] more explicitly highlighted the role of a mucosal wave while explaining that the vocal folds could experience higher pressure during the opening phase owing to the wavelike motion with the glottis opening at the bottom (upstream) first and then at the top (downstream) and likewise closing at the bottom first and then at the top (see Fig. 2C).\nThen, how are these theories related to the actual phonation involving two main vocal registers, i.e., modal and falsetto? In the modal register, oscillating vocal folds (observed with a laryngo-stroboscopy or high-speed camera) entirely close the glottis in each oscillation cycle while deforming with the entire body [9]. Therefore, the mechanism of the modal or normal voice with such pronounced glottal closure is basically explained by the myoelastic-aerodynamic theory (Fig. 2A), which is analogous to the airflow-induced buzzing of the lips that also results in an intermittent outflow [12]. In contrast, the vocal folds in the falsetto register often keep the glottis open while oscillating, through this opening a certain volume of airflow continuously escapes. The myoelastic-aerodynamic theory is not applicable to such oscillations without complete glottal closure (Fig. 2B) [5]. The pioneering analytical models, provided by Ishizaka and Matsudaira [1], [2] and by Titze [5] in the mucosal wave theory (Fig. 2C), were intended to elucidate the onset mechanism for such small amplitude oscillations without complete glottal closure. Then, does the mucosal wave theory describe the falsetto voice mechanism precisely? It is noteworthy that during falsetto phonation, the vocal folds do not always exhibit mucosal wave motions [9], [13]–[19]. The main body of each fold, which consists of the thyroarytenoid muscle, is more or less relaxed in falsetto [5], [11]. The pull of the tissue by the cricothyroid muscle activation thins the vocal folds [9]. Therefore, only the ligamentous superficial layers of the vocal folds enter into vibration, possibly resulting in a negligible mucosal wave. Critical involvements of flow separation mobility in the oscillation mechanisms have been proposed [20]–[25]; however, it is not clear if the reported flow separation behavior is likely in the falsetto voice production with such a negligible vertical motion of the thin vocal folds.\nMore importantly, although the mucosal wave theory has been the cornerstone of nearly all subsequent theoretical developments of voice mechanics [e.g.], [ 26]–[30], it was developed under a quasi-steady flow assumption [8]. In particular, steady mass conservation and Bernoulli equations [1]–[5] were applied in those analytical studies without experimental verification of the decisive assumption under oscillating conditions. However, recent experimental and theoretical studies have demonstrated that the pressure distribution along the glottis essentially differs from that in a static condition in a range of high but realistic voice fundamental frequencies [23], [31]–[33]. In particular, our group analytically explained that the flow behavior in oscillating constriction (such as the pressure amplitude and phase difference between related variables) depends on the Strouhal number, i.e., a dimensionless number describing which effect is stronger, a flow induced by rapid wall motion (numerator) or a flow induced by the convective acceleration such as the speeding up of the airflow entering the converging glottis (denominator) [33]. The Strouhal number should not be negligible in the high vocal frequency range. In fact, Ishizaka and Matsudaira [1] described that their theory aimed at revealing self-excited oscillations in the chest register (i.e., a modal voice), possibly implying that the Bernoulli effect-based explanation might not be effective in the falsetto register.\nIt is also known that air column oscillation in the axially long or narrow vocal tract (including the pharynx and oral cavity) could play an additional role in producing higher pressure in the opening phase than in the closing phase [1]–[5], [34]. However, the threshold lung pressure due to the acoustic coupling between the vocal tract and glottis derived by Titze [4, Equation (50)] was not a function of the glottal width, suggesting that the acoustic coupling may only assist other primary mechanisms in reaching the oscillation threshold. Indeed, falsetto-like oscillations are produced even without a vocal tract in both self-excited physical vocal fold models and excised larynx models [35]–[39]. These observations suggest that the mucosal wave and vocal tract response may not perform a critical role in the onset mechanism of falsetto voice production [1].\nIn our previous theoretical work on fluid-structure interaction in the glottis oscillating at high speeds, we analytically derived the relationship between the time-varying glottal width and pressure perturbation from general unsteady flow equations [33]. The study demonstrated that a convective acceleration (i.e., a change in velocity over position) of a flow that was originally induced by rapid wall movement becomes comparable in magnitude to the Bernoulli effect within a physiological frequency range typical for a falsetto voice (e.g., >400 Hz). Because of this unsteady flow effect associated with the considerable Strouhal number, which was not taken into account in previous analytical studies on the phonation onset [e.g.], [ 1], [4,27], a phase difference (or time lag) can appear between the vocal fold motion and glottal pressure fluctuation. The time lag in the driving force may thus meet the requirements for self-excitation, i.e., continuous energy transfer from the airflow to vocal fold motion. Thus, we suggested that self-excitation could possibly occur from the inherent glottal flow property independent of the mucosal wave motion or inertial acoustic loading from the vocal tract or subglottis [40].\nIn our unsteady flow theory, we linearized the flow descriptions through perturbation analysis around the time-mean value; these small amplitude assumptions were also used in the previous analytical studies by Ishizaka and Matsudaira [1], [2] and Titze [4], [5]. While sustained modal voice in general involves large amplitude motion of the vocal folds with collision, the linearized small amplitude restriction indicates that the results are applied only to analyses of the phonation onset with no glottal closure. A falsetto voice often has a simple sinusoidal sound waveform with few higher harmonic waves, even in sustained conditions [9], indicating that the vocal fold oscillation in a falsetto voice occurs only near the surface tissue with small amplitudes. In addition, the fundamental frequency of a falsetto voice is sufficiently high to provoke the phase alteration caused by the unsteady flow effect [33]. Our flow theory thus seems to fit well with the analysis of the falsetto voice onset.\nIn the present study, we employed this flow theory to analytically derive the threshold conditions required for self-excitation of the vocal folds. The analysis demonstrated that an unsteady flow effect, or more specifically a convective acceleration of a flow induced by rapid wall movement, provides negative damping at the critical subglottal pressure or volume flow, inducing self-excited oscillation reminiscent of falsetto voice onset. Large amplitude behavior associated with established limit cycles, which involves nonlinear effects such as register transition and onset/offset hysteresis [27], [30], [41], [42], will not be discussed in this study. The tools of analysis for such large amplitude oscillations are primarily numerical [6], [7], [43], [44]. It is sometimes elusive to intuitively understand our ability to speak with a falsetto voice from such numerical (computational) results and the criteria for estimating the efficiency of voice production. Thus, it would be appropriate to devote a separate study to such numerical analyses.\nAnalysis\nModel geometry\nThe position along the inferior half of the glottis (see Fig. 1) is given by spatial coordinate x (see Fig. 3). Each cross section is assumed to be rectangular with a constant vocal fold length lg (normal to the plane of the paper). The vocal fold shape and its motion are assumed to be symmetrical with respect to the medial axis. The glottal half-width B(x,t) perpendicular to the x direction where t represents the time is assumed to be a constant value  at the upstream inlet (x = 0) and a time-varying value B(lc,t) (rewritten as Bc(t)) at the entrance of the narrowest constriction (x = lc). The glottis between x = 0 and lc is simply connected by a straight line:(1)The vocal fold constriction (from x = lc to lc+lv) is assumed to be a parallel path where lv is the vocal fold thickness. In fact, the linear geometry of the glottis presented here is not a requirement of the current study, rather, it is assumed to simplify analytical integration calculations and does not essentially affect the onset mechanism described below.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Vocal fold model.The inferior half of the glottis (dashed rectangular region in Fig. 1) is modeled.\ndoi:10.1371/journal.pone.0017503.g003\n\nFlow model\nIn our previous work, we developed a one dimensional unsteady flow theory that explicitly described the relationship between the time-varying glottal width and fluid pressure [33]. As this theory was applied to the present analysis, its derivation is briefly shown here with modifications to correspond with the present flow channel geometry. According to the accumulated knowledge on the internal flow in collapsible tubes [46]–[53], the mass and momentum conservation equations for an unsteady, viscous, and incompressible flow in a deformable tube are described by(2)(3)respectively, where U(x,t) is the velocity, P(x,t) is the pressure, ε is a factor related to flow separation and the vena contracta [3], [48], [53], ρa is the air density, and ν is the kinematic viscosity of air. Note that hereafter, for free variables x and t, the parentheses are often omitted for notational simplicity. Each variable is decomposed into a time-averaged component (lowercase letter with an overbar) and a time-varying perturbation component (lowercase letter):(4)(5)(6)The perturbations are small, and the terms of order higher than the quadratic are neglected. We assumed that pressure and velocity at x = 0 have constant values  and , respectively (see Fig. 3). The fixed boundary condition at the inlet is consistent with the previous analytical studies that assumed an ideal constant pressure source [4]. It is assumed that the fluid pressure at the center of the narrowest constriction, x = lc+lv/2, drives the vocal fold motion, as considered later in a vocal fold model with a single degree of freedom that does not exhibit mucosal waves. At the driving point,(7)(8)where Bc(t) = B(lc,t) = B(lc+lv/2,t) (see Fig. 3). Equation (2) is integrated along x from 0 to lc+lv/2, yielding a perturbation fluid velocity at the driving point [33]:(9)The perturbation glottal pressure that interacts with the perturbation of the vocal fold displacement is:(10)where a dot over a variable denotes its time derivative:  and δ are defined as the coefficients of each term. The first term  represents a force due to the convective acceleration (i.e., the effect of time-independent acceleration of a fluid with respect to space) of the wall motion-induced flow [33]. The second term δbc represents a convective inertial force, i.e., the Bernoulli effect. The former unsteady term could become comparable in magnitude to the latter steady term when the wall moves quick enough to produce a considerable wall motion-induced flow that is distinct from the steady flow determined by the static geometry of the channel. The considerable unsteady effect causes a phase lag between the pressure pc and the motion bc, which could result in meeting the requirements for self-excitation described above. The coupling of Equations (2) and (3) should actually have seven perturbation terms in general, but Equation (10) has only two terms because the remaining five terms, including the effect of air viscosity on the perturbation glottal pressure, are negligibly small in magnitude as confirmed by a thorough scale analysis performed in our previous work [33].\nVolume flow is defined by:(11)and divided depending on whether it is time dependent:(12)Temporal averages of Equations (2) and (3) satisfy:(13)(14)where Pt is the subglottal pressure (shown by the total pressure). Equation (14), obtained with simplification based on scale analysis [33], corresponds to Bernoulli's law for steady flow.\n\n\nVocal fold model\nThe equation of motion for the vocal fold is given by that of a mass-spring-damper oscillator in a lumped element representation (see Fig. 3):(15)where , , k, bi, , d, and  are the mass, damping coefficient, spring constant, initial half-width, density, depth, and damping ratio of the vocal fold, respectively. The depth d is not attributed to any one histological component such as the ligament, but represents the composite depth of effective structures; it is introduced to determine the effective mass m involved in vibration, which is a function of the density  as well as the geometric parameters, i.e., the vocal fold depth d, thickness lv, and length lg. In the steady state, Equation (15) becomes:(16)From Equations (7), (8), (10), and (15), we obtain the following perturbation equation:(17)The coefficients of the second and third terms on the left side represent effective damping and effective stiffness, respectively.\n\n\nThreshold pressure and volume flow for oscillatory divergence due to negative damping\nThe system of Equations (13), (14), and (16) that describes steady states has, in general, three analytical solutions (one trivial solution for the hydrostatic condition and two non-trivial solutions with non-zero flow velocities). However, in the actual vocal fold mechanics because flow accelerates significantly in the constricted glottis, the following approximations are applicable [4]:(18)(19)Here,  is assumed to be zero to exclude acoustic coupling with the vocal tract. With these approximations, the steady state has the following single solution with real numbers:(20)(21)(22)(23)When the coefficient of the second term of Equation (17) is negative (i.e., negative damping), a dynamic instability or flutter [49], [54] occurs in which a perturbation gradually develops to induce oscillatory divergence (see Fig. 4A). The threshold subglottal pressure  and volume flow  for flutter are explicitly described by:(24)(25)respectively, where the constriction length l is defined as(26)The critical subglottal pressure described by Equation (24) is interpreted as the minimum lung pressure required for vocal fold oscillation onset. Such a critical lung pressure is in general called the phonation threshold pressure (PTP) and is potentially useful in diagnosis to noninvasively evaluate vocal fold stiffness and quantify the ease of phonation [4], [55]. Equation (25) denotes the minimum volume flow necessary for voice onset, which has recently been called the phonation threshold volume flow (PTF) [26], [56], [57]. As mentioned in Section 2.2, we estimated in a previous work that the viscous resistance in the glottis has a negligibly small magnitude compared to the convective and unsteady flow effects [33]. This scale analysis yields Equation (14) or the Bernoulli's equation, implying that the fluid energy (i.e., the sum of the pressure and kinetic energies) is conserved. PTP (Pf) and PTF (Qf) are obtained from the same condition (i.e., the second term of Equation (17) is zero), that is, they are merely one expression of the same critical condition required for the oscillatory divergence. Hence, in the present modeling, the input of fluid energy to the vocal fold system to achieve PTP is the same as that for PTF.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Schema of two types of instability.Flutter or oscillatory divergence (A). This dynamic instability results in the self-excitation of the vocal folds. Divergence or static instability (B).\ndoi:10.1371/journal.pone.0017503.g004\n\nUnidirectional divergence due to negative stiffness\nWhen the coefficient of the third term of Equation (17), representing the intrinsic stiffness versus the Bernoulli effect, is negative (i.e., negative stiffness), a static instability or divergence [54] occurs upon imposition of a perturbation to the system (see Fig. 4B) [58]. The threshold subglottal pressure and volume flow for the divergence are described by:(27)(28)respectively. The present study employed a simple single degree of freedom model for the vocal folds (see Fig. 3) to exclude the mucosal wave motion. In addition, we linearized the behavior around the time-mean value. Within this current modeling, the static instability indicated that a unidirectional deformation occurred around the slightly abducted (spread apart) vocal folds, and the vocal folds were either blown open or closed by the airflow [58]. Note that if a vocal fold model with a degree of freedom more than two is employed together with proper mechanical properties [3], limit cycles (different from being kept opened or closed) after the static instability could be established in a collision-dependent manner explained by the myoelastic-aerodynamic theory (see Fig. 2A). To determine whether the limit cycles occur after the static instability (as already performed by Ishizaka and Flanagan [3]), incorporation of the following is required: (1) an additional degree of freedom to the vocal fold model, (2) another mechanical property related to the coupling between the upper and lower masses (or the ease of mucosal wave propagation), and (3) the effect of glottal closure or collision. Such additional modeling is beyond the scope of the present study that aims at introducing a framework of the basic principles to help in understanding the mechanics of falsetto voice onset. In addition, the limit cycles with collision after static instability are in general regarded as the source of modal voice [9], [58]. Therefore, henceforth we focus on only negative damping-induced flutter, i.e., Pf (PTP) and Qf (PTF), but not on static instability Pd and Qd. In fact, Ishizaka and Matsudaira [1] and Titze [4] also developed their theories on the basis of negative damping-induced oscillatory divergence.\n\n\nThe effect of vocal fundamental frequency on PTP and PTF\nPTP and PTF expressed by Equations (24) and (25), respectively, contain the spring constant k, representing the intrinsic stiffness of the vocal folds, which may be difficult to measure at various conditions [4], [28]. In terms of practical use, therefore, the fundamental frequency of a voice F0 may be suitable as an alternative dependent variable instead of k. Thus, another form of PTP and PTF as a function of F0 is derived below. At the critical condition that allows the onset of flutter, the coefficient of the second term of Equation (17) becomes zero. The frequency at that instant is defined as:(29)From Equations (10) and (21), the Bernoulli effect  at the oscillatory divergence onset (i.e., Pt = Pf) is:(30)From Equations (29) and (30),(31)Substitution of Equation (31) into Equations (24) and (25) yields alternative forms of PTP and PTF that contain F0 as an explicit factor:(32)(33)where(34)From Equations (10), (17), (21), (22), and (26), the newly introduced dimensionless number FL, associated with the ease of flutter [49], [54] in a viscoelastic flow channel or falsetto voice onset, is interpreted as:(35)Thus, FL quantifies the relative importance of the distinct effects. Note that as long as flutter occurs more readily than the unidirectional divergence given by Equations (27) and (28), Pf<Pd, and hence, FL>1. Thus, both Pf and Qf have positive values by definition.\n\n\nQuantitative assessment\nTo evaluate the quantitative validity of the derived PTP (Pf, Equation (24)) and PTF (Qf, Equation (25)), we applied the following representative values, which are within the physiological range used in the previous phonation modeling [2], [3], [28], [59]–[61]:  = 0.35 mm,  = 35 N/m,  = 10 mm,  = 15 mm,  = 1.5 mm,  = 1,  = 1.1 kg/m3,  = 1.02×103 kg/m3, and  = 0.235. PTP and PTF, shown as functions of the initial glottal half-width bi, were estimated as ~0.1–1 kPa and ~10–300 cm3/s, respectively (see Figs. 5 and 6). They were comparable in magnitude as per the reported measurement data [26], [42], [55]–[57], [62]–[64]. Note that on the basis of his measurements, van den Berg [9] mentioned that 9 to 10 mL of air were sufficient for the production of sound for 0.5 s (i.e., roughly equal to 20 cm3/s).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Pf (PTP) versus initial glottal half-width bi.For comparison, Pd is also shown. When the upstream total pressure reaches a value within the red curve, flutter or self-excited oscillation of the vocal folds occurs. The parameter values employed are described in Section 2.5.\ndoi:10.1371/journal.pone.0017503.g005\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Qf(PTF) versus initial glottal half-width bi shown by the red curve.doi:10.1371/journal.pone.0017503.g006For comparison, the critical values for the static instability (Pd, Equation (27); Qd, Equation (28)), which might be related to modal voice production [2] as discussed in Section 2.5, are also shown in Figures 5 and 6 as functions of bi. At a small bi of <0.35 mm, flutter requires a lower upstream pressure or volume flow as compared with that of the static instability, indicating that the former occurs more readily. On the other hand, the static instability occurs more readily than the flutter at a high bi.\nFigures 7 and 8 depict the effect of vocal fold thickness lv on PTP and PTF, respectively, at a constant bi of 0.35 mm. As the thickness is reduced, it becomes more difficult to initiate flutter, but it is more likely to occur than static instability at thin vocal folds. This tendency is consistent with the morphological observation that in the falsetto voice, the vocal fold tissue margins are rather thin and pointed due to the tension exerted by the contraction of the cricothyroid muscle [9], [59].\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 7.  Pf (PTP) versus initial vocal fold thickness lv shown by the red curve.doi:10.1371/journal.pone.0017503.g007\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 8.  Qf (PTF) versus initial vocal fold thickness lv shown by the red curve.doi:10.1371/journal.pone.0017503.g008Pf and Pd are also shown as functions of either vocal fold depth d or the damping ratio  with a constant bi of 0.35 mm (see Fig. 9). As described in the introduction and in the literature [9], [60], activation of the cricothyroid muscle at a high fundamental frequency reduces the vocal fold depth. Thus, the depth d can correspond to the effective tissue depth of vibration [60]. In general, muscular tissues have a larger hysteresis loss between loading and unloading than ligamentous tissues [65], [66]. Therefore, the decrease in the depth that may alter the dominant vibrating part from the muscular region to the ligamentous one [4], [9] would be followed by a reduction in the damping ratio of related tissues. Since the two parameters d and  do not affect the magnitude of Pd (Equation (27)), Pf falls below Pd as the expected cricothyroid muscle activation proceeds. Thus, the quantitative evaluation suggests that flutter occurs more readily than static instability at a high fundamental frequency as in the falsetto voice (see Fig. 9).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 9.  The effects of vocal fold depth d and damping ratio  on Pf.To occur, flutter requires a lower input pressure than the static instability (Pd) at a low d or  range, suggesting that flutter will appear at a high fundamental frequency range.\ndoi:10.1371/journal.pone.0017503.g009Figures 10 and 11 or supplementary Figures S1 and S2 display the effects of vocal fundamental frequency F0 on PTP and PTF, respectively, as functions of the initial glottal half-width bi, vocal fold depth d, and the damping ratio . In these figures, the initial glottal half-width bi is fixed at less than 0.35 mm, assuring that flutter occurs (see Figs. 5 and 6). Pf is proportional to the square of F0 as explicitly shown in Equation (32), whereas Qf is proportional to the first power of F0 as shown in Equation (33). This quantitative evaluation demonstrates that PTP and PTF have realistic values within the frequency range typical for a falsetto voice (i.e., >400 Hz) [26], [55].\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 10.  The effect of vocal fundamental frequency F0 on Pf.doi:10.1371/journal.pone.0017503.g010\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 11.  The effect of vocal fundamental frequency F0 on Qf.doi:10.1371/journal.pone.0017503.g011\nDiscussionThus far, numerical (computational) simulations of falsetto-like voice production have been performed using unsteady flow equations, essentially the same as those used in the present study [6], [7], [43]–[45]. Such numerical analyses are useful in investigating the entire process of the voice production over a wide range where nonlinear effects may play essential roles. In contrast, the present study provided the first complete analytical description dealing with the mechanism of self-excitation at high fundamental frequencies, reminiscent of falsetto voice onset. The explicit descriptions of the phonation onset (Pf and Qf; Equations (24) and (25) or (32) and (33), respectively) may be useful for capturing the mutual relationships among basic parameters at a glance and for understanding the essential mechanism of the complex fluid-structure interaction phenomenon. The threshold conditions were quantitatively reasonable in view of the reported measurement data [9], [26], [42], [55]–[57], [62]–[64] (see Figs. 5, 6, 7, 8, 9, 10, 11). The mutual relationships among parameters, such as the effect of vocal fold stiffness on the onset upstream pressure, make intuitive sense and are consistent with the tendencies obtained in previous numerical analyses [6], [7], [43]. The present results were obtained from a single degree of freedom system with a small amplitude approximation. Therefore, the findings derived analytically from general physical governing equations are specifically applicable to a small amplitude or a falsetto voice rather than a modal voice that has large amplitude and non-uniform motion of the vocal folds.\nThe current study mathematically revealed that the vocal folds could be self-excited independent of a mucosal wave or glottal closure if general unsteady flow equations are employed. Oscillatory divergence or flutter (see Fig. 4A) is thus triggered through a different mechanism than that previously considered. In particular, when the vocal fold wall moves so quickly that the effect of the time-varying motion achieves a significant magnitude, a slower fluid velocity than that estimated based on a steady flow appears in the glottis during the opening vocal fold movement (see Figs. 12A and 12B). This deceleration occurs because individual air particles (but not liquid particles as in Washio et al. [67]) adjacent to the wall must follow the quick movement so as to not break the interface between the solid wall and the fluid. This model is essentially different from that of the Bernoulli effect in which the absolute value of the glottal width determines the fluid velocity (see Fig. 12B). Likewise, in the closing phase, a faster fluid velocity than that estimated based on a steady flow is present. In consequence, a higher driving pressure is applied to the vocal folds during the opening movement (see Fig. 13), inducing the self-excited oscillation, because the vocal folds can receive positive energy from the flow as the oscillation proceeds.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 12.  The effect of rapid wall movement on internal flow.Convective acceleration (or the Bernoulli effect) is the time-independent acceleration of a fluid with respect to space (A). The size of the letter P and the length of the arrows indicate the magnitude of glottal pressure and that of the fluid velocity in a converging duct, respectively. Although the flow may be steady (time-independent), the fluid accelerates as it moves down the converging duct; thus, there is an acceleration happening over position, referred to as convective acceleration or the Bernoulli effect. Relationship between the glottal width and velocity (B). At a steady state (i), the perturbations of variables bc, uc, and pc are all zero. If a perturbation bc with a positive value is slowly given to the steady state (ii), such enlargement of the constriction weakens the convective acceleration and hence decreases the instantaneous velocity Uc with the appearance of a negative perturbation velocity uc and a consequent positive perturbation pressure pc. Instead, let us consider a case in which a perturbation bc with a positive value is very quickly given to the steady state (iii). The wall moves so fast that not only the bc-induced negative perturbation velocity uc1 (identical to uc in (ii)) but also an additional negative velocity component uc2 proportional in magnitude to the time derivative of the wall motion dbc/dt appears. Here, uc1 and uc2 correspond to the second and first terms on the right side of Equation (9), respectively. The additional velocity component due to the rapid wall movement also experiences convective acceleration; therefore, a perturbation pressure higher in magnitude than (ii) is obtained at (iii), as graphically shown by pc with a big size. Likewise, a fast narrowing of the wall (i.e., negative bc) yields a negative perturbation pressure pc whose absolute value is greater than that estimated from the apparent width (figure not shown). Previous quantitative evaluation [33] suggested that the magnitude of uc2-originated perturbation pressure (i.e., the first term on the right side of Equation (10)) reaches approximately 50% of the uc1-originated one (the second term on the right side of Equation (10)) at 500 Hz; thus, playing a significant role for driving pressure in falsetto voice onset.\ndoi:10.1371/journal.pone.0017503.g012\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 13.  Mechanism of falsetto voice onset.Pressure and width fluctuate with the same phase at a quasi-steady state (A). In contrast, due to the convective acceleration of the rapid wall motion-induced flow shown in (iii) of Figure 12B, a phase difference appears between the variables (B) because the magnitude of pc is partly determined by how fast the wall moves (Equation (10)). Previous work [33] showed that the phase difference is practically dependent only on the Strouhal number, but note that the perturbation pressures always have (on average) larger values during the opening phase than during the closing phase. This asymmetry in pressure magnitude over one cycle (C) can lead to self-excitation of the vocal folds only if the threshold condition Pf or Qf is satisfied.\ndoi:10.1371/journal.pone.0017503.g013The fluid force that induces the static instability (shown by the second term in the middle of Equation (10)) comes from the Bernoulli effect. This term is proportional to the dynamic pressure or the square of the glottal velocity . In contrast, the fluid force that induces flutter is caused by the convective acceleration of a fluid velocity induced by rapid motion of the vocal fold tissue (see Fig. 12). The tissue motion-induced velocity itself is independent of the time-mean fluid velocity  as explicitly described in the first term on the right side of Equation (9). Therefore, the flutter-related force (shown by the first term in the middle of Equation (10)) varies with the first (but not the second) power of , representing the effect of convection. Both the separate fluid forces are inversely proportional to  (or bi of Equation (22)) as shown in Equation (10), because the ratio of the displacement perturbation bc to the initial width bi (rather than bc itself) influences the pressure perturbation pc. Equations (20) and (21) imply that the critical subglottal pressures that induce instability should be proportional to the square of the glottal velocity . As a result, PTP (i.e., Pf) is proportional to the square of bi whereas the critical pressure for the static instability (Pd) is proportional to the first power of bi. Thus, the diverse dependencies on bi yield flutter- and unidirectional divergence-dominant regions (see Fig. 5). Equations (20) and (23) imply that PTF is proportional to bi as well as to the square root of PTP, indicating that PTF (Qf) is proportional to the square of bi, whereas the critical volume flow for unidirectional divergence (Qd) is proportional to the two-thirds power of bi (see Fig. 6).\nAlthough the relationships between F0 and PTP or PTF are explicitly expressed in Equations (32) and (33) and graphically shown in Figures 10 and 11, we should note that the vocal fold-related parameters that constitute Pf and Qf are in fact interdependent on each other. Laryngeal muscle activation results in diverse changes in vocal fold geometry and mechanical properties; thus, a shift in one parameter alters multiple interdependent parameters simultaneously. The parameters used in the present modeling are independent of each other in order to avoid expedient assumptions and consequent loss of generality. Establishing a plausible assumption that adequately specifies the relationship between the parameters will be needed for practical cases and will be a subject of future investigations. Note that the current explanation for the falsetto voice onset mechanism does not exclude the effect of acoustic coupling with the axially long vocal tract and subglottis [3]–[5], [34], [40]. The acoustic effect as well as the unsteady flow behavior highlighted here may cooperatively contribute to the self-excitation in actual phonation. However, it is notable that human PTP–F0 relationships reported by Solomon et al. [63] and Titze [62], [68], which were examined in high frequency ranges, indicate that some subjects appear to have a concave PTP–F0 curve at a high frequency range, thereby being consistent with the current result (see Fig. 10).\nIn summary, we analytically derived biomechanical conditions required for falsetto voice onset from general unsteady flow equations. In this model, the self-excitation of the vocal folds in a falsetto voice arises through inherent flow properties in a rapidly oscillating wall, a process that is distinct from the mucosal wave-based explanation submitted previously and based on a quasi-steady flow assumption. This model of the falsetto voice onset provides explicit relationships among the vocal fold geometry, biomechanical parameters, and fundamental frequency.\nFigure S1. The effect of vocal fundamental frequency F0 on Pf. Effects of vocal fold depth d and damping ratio  with a fixed initial glottal half-width bi = 0.32.doi:10.1371/journal.pone.0017503.s001(TIF)Figure S2. The effect of vocal fundamental frequency F0 on Qf. Effects of vocal fold depth d and damping ratio  with a fixed initial glottal half-width bi = 0.32.doi:10.1371/journal.pone.0017503.s002(TIF)"
        }
    },
    "subject-psychology": {
        "10.1371/journal.pone.0106000": {
            "author_display": [
                "Lucy Foulkes",
                "Eamon J. McCrory",
                "Craig S. Neumann",
                "Essi Viding"
            ],
            "title_display": "Inverted Social Reward: Associations between Psychopathic Traits and Self-Report and Experimental Measures of Social Reward",
            "abstract": [
                "\nIndividuals with high levels of psychopathic traits tend to undervalue long-term, affiliative relationships, but it remains unclear what motivates them to engage in social interactions at all. Their experience of social reward may provide an important clue. In Study 1 of this paper, a large sample of participants (N = 505) completed a measure of psychopathic traits (Self-Report Psychopathy Scale Short-Form) and a measure of social reward value (Social Reward Questionnaire) to explore what aspects of social reward are associated with psychopathic traits. In Study 2 (N = 110), the same measures were administered to a new group of participants along with two experimental tasks investigating monetary and social reward value. Psychopathic traits were found to be positively correlated with the enjoyment of callous treatment of others and negatively associated with the enjoyment of positive social interactions. This indicates a pattern of ‘inverted’ social reward in which being cruel is enjoyable and being kind is not. Interpersonal psychopathic traits were also positively associated with the difference between mean reaction times (RTs) in the monetary and social experimental reward tasks; individuals with high levels of these traits responded comparatively faster to social than monetary reward. We speculate that this may be because social approval/admiration has particular value for these individuals, who have a tendency to use and manipulate others. Together, these studies provide evidence that the self-serving and cruel social behaviour seen in psychopathy may in part be explained by what these individuals find rewarding.\n"
            ],
            "publication_date": "2014-08-27T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 423,
            "shares": 20,
            "bookmarks": 0,
            "url": "http://www.plosone.org/article/info:doi/10.1371/journal.pone.0106000",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0106000&representation=PDF",
            "fulltext": "IntroductionPsychopathy is a personality disorder characterised by lack of empathy, shallow affect and callous treatment of other people, as well as impulsivity and a greater propensity towards criminal behaviour [1]. Psychopathic traits are continuously distributed in the population and can be reliably measured in community samples [2]–[3].\nEmpirical evidence suggests that psychopathic traits may be associated with an atypical experience of social reward [4]–[9]. Social reward can be defined as the motivational and pleasurable aspects of our interactions with other people, and interpersonal kindness and closeness is a fundamental social reward for most people [10]–[11]. However, it appears that individuals with high levels of psychopathic traits do not place equal importance on affiliative, long-term friendships and relationships [4]. Instead they favour friends who can increase their access to sexual mates or provide protection [12] and prefer one-night stands to committed relationships [13]. In addition, evidence from experimental tasks shows that individuals with high levels of psychopathic traits are less likely to cooperate with and help others [5], [8]–[9] Together, this evidence suggests that, for individuals with high levels of psychopathic traits, affiliative and prosocial behaviour towards others may be less rewarding than it is for typical individuals [6].\nFurthermore, psychopathic traits are associated with enjoyment of antisocial entertainment such as violent sports and video games [14] and internet ‘trolling’ - online antisocial behaviour [15]. This evidence suggests individuals with high levels of psychopathic traits not only lack empathy towards others’ distress [16]–[17], but may actually take pleasure from it. Thus individuals with high levels of psychopathic traits appear to show an unusual pattern of social reward: decreased reward value of prosocial and affiliative interactions [4], [6]–[7], and increased reward value of cruelty towards others [14]–[15]. This suggestion is supported by a recent study that used a systematic measure of social reward, the Social Reward Questionnaire (SRQ; [7]). This preliminary analysis found that scores on the psychopathy subscale of the brief Dirty Dozen measure [18] were negatively associated with enjoyment of prosocial interactions and positively associated with enjoyment of callous, antisocial interactions [7]. However, this four-item measure of psychopathy is unidimensional, so it remains unclear how different aspects of psychopathic personality are associated with dimensions of social reward. As such, there remains a need to systematically explore associations between the value of different social rewards and a comprehensive, well-validated measure of psychopathic traits.\nThere is an equal need to employ experimental measures that can more sensitively assess the experience of social reward in relation to psychopathic traits. Such measures have the potential to overcome several of the limitations inherent in using self-report questionnaires, including the ability and/or willingness of participants to reflect on and state their personality traits. Some research has assessed responsiveness to monetary reward in relation to psychopathic traits, and found that individuals with high levels of these traits may be hyperresponsive to this type of reward [19]–[20]. Although the last decade has seen a surge in the number of studies using experimental paradigms to measure social reward (e.g. [21]–[23]), to our knowledge these paradigms have not yet been used in association with a measure of psychopathic traits.\nIn the current paper, we report two studies that explore the relationship between social reward and psychopathic traits. In the first study, we aimed to assess the association between subtypes of social reward and dimensions of psychopathic traits using questionnaire methodology. In the second study, our aim was to employ an experimental measure of social reward and investigate its association with psychopathic traits (Study 2).\nStudy 1In Study 1, our aim was to elucidate some of the processes that may motivate the unpleasant interpersonal behaviour associated with psychopathic traits. To do this, we explored associations between psychopathic traits, as measured by the Self-Report Psychopathy Scale Short-Form (SRP-SF; [24]), and the value of different types of social reward, as measured by the Social Reward Questionnaire (SRQ; [7]). The SRP-SF measures four dimensions of psychopathy: Affective (e.g. lack of empathy), Interpersonal (e.g. manipulativeness), Lifestyle (e.g. impulsivity) and Antisocial (e.g. aggressive or unlawful behaviour). The SRQ quantifies the enjoyment of six types of social reward: Admiration (being flattered and gaining attention), Negative Social Potency (being cruel and callous), Passivity (allowing others control), Prosocial Interactions (being kind and fair), Sexual Relationships (frequent sexual encounters) and Sociability (frequent socialising). We hypothesised that psychopathic traits would be positively associated with Negative Social Potency and negatively associated with Prosocial Interactions. In addition, we hypothesised that psychopathic traits would be positively associated with Sexual Relationships, due to the high rates of affairs and short-term relationships reported in this group [25]–[26]. Finally, we predicted that psychopathic traits would be positively associated with enjoyment of Admiration, due to the elevated levels of narcissism seen in individuals with high levels of psychopathic traits [27]. We made no specific hypotheses regarding which dimensions of psychopathy would show these associations. Associations between psychopathic traits and other types of social reward were exploratory.\n\nMaterials and Methods\nData for this study were collected as part of a wider battery of measures that have been partly reported in a previous publication [7].\nEthics Statement.All participants provided written informed consent and the study was approved by the University College London Clinical, Educational and Health Psychology Research Ethics committee.\n\nParticipants.Amazon’s Mechanical Turk platform (MTurk) was used to recruit participants. MTurk is an international crowdsourcing website on which participants complete tasks or questionnaires for payment, and is increasingly being used as a source of valid and reliable data [28].\nThe questionnaires were completed 529 times. Participants were excluded for providing obviously repetitive answers (i.e. giving the same answer to all questions in at least three of the six questionnaires in the original battery; N = 5) or for completing the questionnaire battery twice (second attempt excluded; N = 19). This left a final sample of 505 participants (270 males, 235 females) aged 18 to 79 years (mean = 34.0, SD = 12.2). The majority of respondents lived in the USA (N = 457); other respondents lived in India (N = 35), Canada (N = 6), the UK (N = 6) or another European country (N = 1). The ethnicity of the sample was as follows: 72.3% White, 11.1% South Asian, 6.1% Black, 2.8% Hispanic, 2.0% East Asian and 5.7% Mixed/Other. The highest completed education level of the sample was as follows: 38.2% Bachelor’s degree, 30.9% Senior/high school, 18.8% College, 12.1% Postgraduate degree.\n\nMeasures.Psychopathic traits: these were measured with the Self-Report Psychopathy Scale Short Form (SRP-SF; [24]), a well-validated instrument modelled on the Psychopathy Checklist Revised (PCL-R; [1]). The SRP-SF contains 28 items that participants rate on a 5-point Likert scale (1 = Strongly disagree to 5 = Strongly agree). The SRP-SF yields scores for four dimensions of psychopathy: Affective (e.g. lack of empathy), Interpersonal (e.g. manipulativeness), Lifestyle (e.g. impulsive) and Antisocial (e.g. harmful and potentially criminal behaviour). There are seven items for each of the four dimensions, which can be summed to form a total psychopathy score. We chose to use the SRP-SF rather than the original SRP as it takes less time to complete, whilst still retaining strong psychometric properties [24].\nThe SRP-SF and the SRP on which it is based both have good basic psychometrics, as well as theoretically sound and mathematically strong latent structures [2], [6], [17], [29]–[33]. There is good evidence for convergent validity between the SRP/SRP-SF and other measures of psychopathic traits. For example, both measures are strongly positively correlated with the PCL-R and also have the same four-factor structure [24], and three factors of the SRP-SF (Interpersonal, Affective, Lifestyle) are strongly correlated with the three factors of the Youth Psychopathic Traits Inventory (Grandiose/Manipulative, Callous/Unemotional, Impulsive/Irresponsible; [32]). Finally, SRP subscales are strongly correlated with expected subscales of the Elemental Psychopathy Assessment, a measure of psychopathic traits based on the five-factor model of personality (EPA; e.g. SRP Interpersonal is strongly correlated with EPA Manipulation and Self-Centeredness [30]).\nAcross a wide diversity of samples, the SRP traits are associated in the expected theoretical directions with relevant external correlates, such as criminal offenses and externalizing psychopathology [32], [34]–[37], moral reasoning [17], amygdala activation to fearful faces [29], and lower amygdala volume [38]. The construct validity of both the SRP and SRP-SF are further supported by studies demonstrating theoretically meaningful associations with related personality measures [31], [33], as well as cognitive functioning [2], social information processing [16], and social functioning [6]. Based on the use of a mega world-sample (30 k+), latent variable model-based research with the SRP has shown it to be invariant across sex, and the SRP factors were associated with world regional data such as Gross Domestic Product (GDP), fertility, and infant mortality [39]. In the current sample, Cronbach’s Alpha scores indicated acceptable to good reliability (mean = .76, SD = .10; Affective = .76, Interpersonal = .86, Lifestyle = .80, Antisocial = .61).\nSocial reward: the Social Reward Questionnaire (SRQ; [7]) is a 23-item scale used to measure individual differences in the value of social rewards. Each item begins “I enjoy” and then describes a different type of social interaction. Participants are asked to consider the item in relation to all their social interactions, e.g. friends, partners, family, colleagues or people they have just met. Responses are given on a 1 to 7 scale (1 = Disagree strongly, 7 = Agree strongly). The SRQ consists of six subscales, each representing a domain of social reward: Admiration, Negative Social Potency, Passivity, Prosocial Interactions, Sexual Relationships and Sociability (see Table 1). In the current sample, Cronbach’s Alpha scores indicated good reliability (mean = .82, SD = .04; Admiration = .82, Negative Social Potency = .87, Passivity = .78, Prosocial = .84, Sexual = .84, Sociability = .77).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Detail of SRQ subscales.doi:10.1371/journal.pone.0106000.t001\nData analysis procedure.Pearson and Spearman correlational analyses (as appropriate depending on the normality of the bivariate residuals) were conducted using IBM SPSS Statistics 20.0 for Windows. Scores for the four psychopathy factors and the total psychopathy score were correlated with all SRQ subscales using zero-order correlations. Benjamini and Hochberg False Discovery Rate [40] was used to control for the probability of making a Type I error on multiple comparisons, and only corrected p-values are presented. There were no missing data, as the questionnaire was programmed in such a way that all items required a response.\n\n\n\nResults\nDescriptives for SRQ and SRP-SF scores are shown in Table S1. Results from the correlational analyses are shown in Table 2. All psychopathy scores were positively associated with the Negative Social Potency subscale of the SRQ and negatively associated with the Prosocial Interactions subscale. All psychopathy scores except the Antisocial factor were positively associated with Sexual Relationships, and all except the Affective factor were positively associated with Passivity. Finally, Lifestyle psychopathic traits were positively associated with Sociability, and Interpersonal psychopathic traits were positively associated with Admiration.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Correlations between SRP and SRQ scores in Study 1 (N = 505).doi:10.1371/journal.pone.0106000.t002Post-hoc analyses.Previous evidence has shown that age and gender can affect both reward processing (age: [41]; gender: [42]) and level of psychopathic traits (age: [43]; gender: [44]). We therefore conducted post-hoc analyses to explore possible effects of age and gender on the associations between psychopathic traits and social reward (see Tables S5–S7).\n\nAge.We re-ran the correlations between the two measures as partial correlations, controlling for age (see Table S5). When age is controlled, the following associations are no longer significant: Admiration and Interpersonal psychopathic traits (r = .06, adjusted p = .24) and Passivity and Lifestyle psychopathic traits (r = .09, adjusted p = .07), and the association between Admiration and Antisocial psychopathic traits becomes significant (r = −.10, adjusted p<.05). However, the pattern of associations largely remained the same.\n\nGender.We re-ran the correlations between the two measures in Study 1 for each gender independently. We then used the Fisher r-to-z transformation to assess if the differences between associations for each gender were significant (see Tables S6 and S7). The pattern of associations was largely the same for males and females, but the differences are worthy of note. Firstly, females showed a stronger association between Sexual Relationships and Affective psychopathic traits (z = 2.19, p<.05). Four associations were significantly stronger in males than females: Passivity and Antisocial psychopathic traits (z = 2.79, p<.01), Sexual Relationships and Antisocial psychopathic traits (z = −2.86, p<.01), SRQ Sociability and Interpersonal psychopathic traits (z = 2.14, p<.05) and Sociability and Total psychopathic traits score (z = 2.05, p<.05).\n\n\n\nStudy 1 Discussion\nAll psychopathic traits were positively associated with Negative Social Potency and negatively associated with Prosocial Interactions. This supports our hypothesis of an ‘inverted’ pattern of social reward in individuals with high levels of psychopathic traits, in which being cruel is enjoyable and being kind is not. Affective, Interpersonal and Lifestyle psychopathic traits were positively associated with enjoyment of Sexual Relationships, consistent with our hypothesis and in line with previous evidence of increased promiscuity in these individuals [25]–[26]. In addition, there was a positive association between Interpersonal psychopathic traits and enjoyment of Admiration. The Interpersonal psychopathy factor includes manipulativeness and superficial charm, and we speculate that an admiring individual would be more susceptible to this manipulative control. Therefore, gaining others’ admiration could facilitate the self-serving social strategy of individuals with high levels of Interpersonal psychopathic traits, instilling this social interaction with high reward value. Additionally, admiration may be rewarding because it feeds the narcissistic traits associated with Interpersonal psychopathic traits [45].\nThere were positive associations between Interpersonal, Lifestyle and Antisocial psychopathic traits and enjoyment of Passivity. We speculate this may be due to the parasitic relationship style of individuals with high levels of psychopathic traits [1], [12], which may lead these individuals to enjoy social interactions in which another person expends effort to bring them gains. Lastly, there was a positive association between Lifestyle psychopathic traits and Sociability. We speculate that individuals with high levels of Lifestyle psychopathic traits may enjoy socialising with others because this provides a context for the risk-taking and sensation-seeking behaviours that this factor represents [1]. For example, attending parties may increase the opportunity to take recreational drugs.\nOur post-hoc analyses revealed some interesting effects of age and gender, although the pattern of associations between social reward and psychopathic traits largely remained the same. Overall, the associations found here between dimensions of psychopathic traits and different types of social reward provide evidence for possible motivations behind the patterns of social behaviour seen in psychopathy.\n\nStudy 2In Study 2, we tested a sample of UK participants in person. The first goal of this study was to explore the associations that we found between social reward and psychopathic traits in Study 1 in a different sample. The second goal was to use two experimental reward tasks to assess how monetary and social reward value relates to psychopathic traits. These experimental tasks were intended to provide a sensitive index of reward value that would be less susceptible to possible impression management than could be the case for self-report measures such as the SRQ. The tasks also allowed social reward to be explored in the context of another type of salient reward, money.\nTasks that compare responses to monetary and social reward are already available (e.g. [41]–[42], [46]). However, the stimuli used to represent the two types of reward are conceptually and perceptually different from each other, which somewhat complicates the interpretation of the findings from these studies. For example, one study [46] represented monetary reward often with a currency symbol (a dollar sign), a simple conceptual representation for which an association with reward has been learned over time. In contrast, social reward was represented with a smiling face: a visually complex, biologically salient image [46]. In order to comparably address individuals’ relative processing of monetary and social reward, there is a need to use stimuli that allow these two rewards to be represented as equally as possible. To address this issue in the current study, social reward was represented using the ‘Like’ symbol from the social networking site Facebook (www.facebook.com). This is a thumbs-up symbol used to express approval/admiration from one user to another in response to user-posted items, such as photos or comments. We then used a pound sterling symbol to represent monetary reward, and using these symbols together has two benefits. Firstly, both the Like and pound symbols are images that have a learnt association with reward. In other words, these symbols both indicate a conceptual representation of reward. Secondly, both symbols have similar, abstract visual features. Together, these characteristics allow us to compare the relative processing of monetary and social reward value as validly as possible.\nExisting studies of monetary reward value have shown that psychopathic traits are positively associated with increased activity in reward-related brain areas, such as the nucleus accumbens, when processing monetary reward [19]–[20]. In addition, behavioural research has found positive associations between psychopathic traits and importance of life goals relating to money [6]. We therefore hypothesised that psychopathic traits would be positively associated with reaction times (RTs) to reward in the monetary task. With regard to social reward, findings from Study 1 of this paper suggest that psychopathic traits are associated with less reward from prosocial interactions. On the basis of this, we hypothesised that psychopathic traits would be negatively associated with RTs to reward in the social task. Finally, we hypothesised that psychopathic traits would be negatively associated with a monetary–social RT difference score (i.e. RTs to social reward will be relatively slower than those to monetary reward). Based on the findings from Study 1, we hypothesised that all psychopathy factors would show this pattern of association.\n\nMaterials and Methods\nEthics Statement.All participants provided written informed consent and the study was approved by the University College London Clinical, Educational and Health Psychology Research Ethics committee.\n\nParticipants.Participants were 110 males recruited from two participant pools at University College London (UCL): the UCL Psychology Subject Pool and the ICN (Institute of Cognitive Neuroscience) Subject Database. Both pools are open to students across the university and to members of the public. Only males were recruited due to the higher prevalence of psychopathic traits in males and to ensure we did not lose power in the relatively small sample size by controlling for another variable (gender). Participants were aged 18–39 years (mean = 22.45, SD = 4.07) and all met the following criteria: fluent English-speaker, no dyslexia and a current Facebook user. Ninety percent of the sample were current students (6.4% unemployed, 3.6% employed) and all lived in the UK. The highest completed education level was as follows: 65.4% senior school/A level college, 19.1% Bachelor’s degree, 15.5% postgraduate degree. Ethnicity of the sample was as follows: 28.2% Chinese, 21.8% White other, 20.9% Mixed/Other, 19.1% White British, 10.0% Indian.\n\nQuestionnaires.Psychopathic traits: the SRP-SF [24] was used to measure psychopathic traits as in Study 1.\nSocial reward: the SRQ [7] was used to measure the value of different types of social reward as in Study 1.\nFacebook usage: use of the social media website Facebook was measured with the Facebook Intensity Scale [47]. This is an 8-item questionnaire that assesses frequency and duration of Facebook usage as well as emotional connectedness to the site. This measure was given in order to control for the effect of Facebook usage on the reward value of the ‘Like’ symbol in the experimental social reward task.\n\nMonetary and social reward tasks.Two versions of a probabilistic reward anticipation task (monetary and social) were used. These tasks were based on the Factorial Reward Anticipation task [48] and the Monetary Incentive Delay task [49]. The monetary and social tasks were conducted separately (rather than as part of one task) for two reasons. Firstly, separating the two tasks with a battery of questionnaires in-between reduced the possibility of boredom or fatigue effects. Secondly, conducting separate tasks removed the effect of shifting costs that could incur if participants had to change frequently between the two symbolic representations. Comparing two types of reward by using two separate tasks has been done previously (e.g. [21]).\nIn both tasks, a cue indicates how likely it is that a key press response will yield rewarding feedback. The participant then responds to a target by pressing the space bar, and subsequently receives feedback, which is either reward (a monetary or social point gain) or no reward (no point gain; there is no loss condition). Therefore, each trial has 6 sequential components: (1) 500 ms anticipatory cue, (2) 1500 ms fixation cross, (3) 500 ms green square target, (4) 1500 ms blank screen, (5) 1500 ms feedback, (6) 1000 ms inter-trial interval (each trial is 6.5 seconds). There are three possible cues, shown in Figure 1, which indicate to the participant that there is a p = 0, p = 0.5 or p = 1 probability level of receiving a point in that trial, provided they press the space bar quickly (within 500 ms) when the target appears. If the space bar is pressed within 500 ms on a rewarded trial (i.e. in 100% of the 1 probability trials and a randomised 50% of the 0.5 probability trials), ‘+1′ is presented with the reward symbol (either a pound or Like symbol). If the space bar is not pressed, is pressed outside of the 500 ms window, or is pressed within the 500 ms window but on a no-reward trial (i.e. in all 0 probability trials and 50% of 0.5 probability trials), ‘+0′ is presented with the reward symbol. On each feedback screen, cumulative winnings are shown underneath the trial winnings (see Figure 1). Within each task, the sequence of trials (0, 0.5 or 1) was randomised for each participant.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Monetary and social reward task trial sequences.doi:10.1371/journal.pone.0106000.g001It is worth noting that no actual reward was awarded on the basis of task performance. Participants were given a flat rate of £10 for taking part in the study, and were told that the objective of the reward tasks was simply to earn as many points as possible. We made this decision because we wanted to keep the two tasks as equivalent as possible (i.e., translating the monetary points into winnings in the monetary condition could not be matched in the social condition). Therefore, we relied on the learned association between the two symbols (pound sign and Like symbol) and reward value. This is in line with other studies comparing the two types of reward, where winnings are not translated into actual monetary reward [41], [50].\n\nProcedure.Participants completed the questionnaires and monetary and social reward tasks as part of a wider data collection. One experimental reward task (either money or social; counterbalanced across participants) appeared at the beginning of the battery and the other appeared at the end (approximately 40 minutes apart).\n\nData analysis procedure.Zero order correlational analyses were used to assess associations between SRP-SF and SRQ, as in Study 1. Benjamini and Hochberg False Discovery Rate [40] was used to control for the probability of making a Type I error on multiple comparisons, and only corrected p-values are presented. There were no missing data, as the questionnaire was programmed in such a way that all items required a response.\nIn the experimental reward tasks, trials with RTs that were <100 ms or >1000 ms (including any missing trials) were excluded from analysis. According to these criteria, eight participants had >20% invalid trials in either the monetary or social reward task and were excluded from analysis, giving a final sample size of N = 102.\nMean reaction times (RTs) for each probability level (0, 0.5 and 1) were calculated in both conditions (monetary and social) for each participant. In addition, a difference score was calculated that represented the relative value of the monetary and social conditions. To do this, the mean score for each probability level in the social condition was deducted from the corresponding mean score in the monetary condition.\nWe first compared general task performance on the monetary and social tasks. A 2 (reward type: monetary, social)×3 (reward probability: 0, 0.5, 1) ANOVA was conducted to investigate this. To explore associations between psychopathic traits and performance on the experimental reward tasks, correlational analyses were run between the psychopathy factor and total scores and the mean RTs and monetary-social difference scores from the experimental tasks. Benjamini and Hochberg False Discovery Rate [40] was used, and only corrected p-values are presented.\n\n\n\nResults\nQuestionnaires.Descriptives for SRQ and SRP-SF scores are shown in Table S2. The four psychopathy factor scores (Affective, Interpersonal, Lifestyle and Antisocial) and total psychopathy score were all positively associated with Negative Social Potency, as in Study 1 (see Table 3). Affective and Antisocial factors were negatively associated with Prosocial Interactions. All scores except the Antisocial factor were positively associated with Sexual Relationships. Finally, only the Interpersonal factor was positively associated with Passivity and Admiration, and there were no significant associations with Sociability.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 3.  Correlations between SRP and SRQ scores in Study 2 (N = 110).doi:10.1371/journal.pone.0106000.t003\nMonetary and social reward tasks.Descriptives of RTs for each probability level in monetary and social tasks can be found in Table S3. Mean RTs were analysed with a 2 (reward type: monetary, social) ×3 (reward probability: 0, 0.5, 1) ANOVA. There was a significant main effect of reward probability (F(1,101) = 38.82, p<.001; see Figure 2); participants responded more quickly to increased probability of reward. Analysis of simple effects showed that the decrease in RT between increases in reward probability (0 and 0.5; 0.5 and 1) were both significant, in both monetary and social conditions (all p<.05; see Table S4). There was no main effect of reward type and no interaction between reward type and reward probability.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Plot of mean RTs for each probability level in both monetary and social conditions.N.B. Error bars represent standard error.\ndoi:10.1371/journal.pone.0106000.g002\nAssociations between psychopathic traits and performance on reward tasks.Degree of Facebook usage as measured by the Facebook Intensity Scale [44] was entered as a control variable in all analyses, and Benjamini and Hochberg False Discovery Rate [40] was used to control for the probability of making a Type I error on multiple comparisons.\nThere were no significant associations between psychopathy scores and mean RTs at any probability level in either the monetary or social task. However, Interpersonal psychopathic traits were significantly positively associated with the RT difference scores for the 0.5 and 1 probability conditions. Specifically, as Interpersonal traits increased, RTs to the social condition were faster relative to the monetary condition (see Table 4).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 4.  Correlations between SRP scores and reward task RTs and difference scores.doi:10.1371/journal.pone.0106000.t004\n\n\nStudy 2 Discussion\nIn Study 2, the pattern of associations between psychopathic traits and social reward found in Study 1 was largely replicated. Specifically, in both samples there was a positive association between all psychopathy scores and Negative Social Potency, the enjoyment of being cruel and controlling towards others. Both studies found positive associations between Affective, Interpersonal, Lifestyle and Total psychopathic traits and Sexual Relationships, and a positive association between Interpersonal psychopathic traits and Admiration. Both studies also found a negative association between Affective psychopathic traits and Prosocial Interactions and Interpersonal psychopathic traits and Passivity, although Study 1 found these associations with all psychopathic traits. In addition, Lifestyle psychopathic traits were positively associated with Sociability in Study 1, but not Study 2.\nIn the social reward experimental task, a novel symbol of social reward was used: the ‘Like’ thumbs-up symbol from the social networking site Facebook. RTs to both the Like and pound symbol were faster with each incremental reward probability level. There were no significant differences between mean RTs in the monetary and social reward tasks. This suggests that the Like symbol was serving as a reward stimulus in a manner similar to monetary reward, and so it may have value in future studies of social reward.\nInterpersonal psychopathic traits were positively associated with the monetary-social RT difference score in both the 0.5 and 1 probability level conditions. Specifically, as Interpersonal traits increased, RTs in the social task became faster relative to the monetary task. We interpret this in the context of the narcissism and manipulation associated with the Interpersonal factor [27]. Specifically, the Like symbol represents social admiration/approval, and so this symbol may have a higher subjective value for individuals who tend to trick and manipulate others.\n\nIn the two studies reported here we explored associations between psychopathic traits and the value of different social rewards. The main finding from our studies was that individuals with high levels of psychopathic traits reported that they like behaving antisocially and dislike behaving prosocially towards others. Data from the experimental reward tasks suggested that individuals with high levels of Interpersonal psychopathic traits appeared to find social admiration/approval especially motivating relative to monetary reward. Together, these findings shed light on what might motivate the social behaviour characteristic of individuals with high levels of psychopathic traits.The implication that individuals with high levels of psychopathic traits enjoy cruel behaviour is in line with findings from other studies (e.g. [15]). A careful consideration of sadism is important here, which is defined as the enjoyment of controlling, dominating, and/or causing suffering to others, and can refer to physical or psychological suffering [51]–[52]. There is some existing support that psychopathy and sadism are overlapping constructs [53]–[56], and the current study provides further support for this. However, it remains unclear exactly why individuals with high levels of psychopathic traits enjoy cruel behaviour. One possibility is that inflicting suffering on others may be pleasurable purely because of causing a person pain (physical or psychological). Alternatively, the enjoyment may stem from the power and control that comes with inflicting suffering, and it is this rather than the pain per se that has reward value. Further research should probe the exact nature of the Negative Social Potency reward that is associated with psychopathic traits, and this value in antisocial behaviour should be incorporated into explanations of why psychopaths behave so badly towards others.In addition, the current study found a negative association between psychopathic traits and enjoyment of prosocial interactions (Study 1: all factors; Study 2: Affective factor only). This finding suggests that individuals with high levels of psychopathic traits do not just feel indifferent towards being kind and helpful, they find it unappealing. Psychopathic traits have previously been associated with an increased report of public prosocial behaviours but a decreased report of anonymous and altruistic prosocial behaviours [9]. This is consistent with the current findings as it appears that individuals with high levels of psychopathic traits do not experience an intrinsic reward from behaving prosocially towards others [9]. This contrasts with evidence from typical individuals, which shows that people behave prosocially at least in part because they experience inherent enjoyment from it (the ‘warm glow’ hypothesis of altruism; [57]–[58]). The absence of this enjoyment in individuals with high levels of psychopathic traits is an important avenue for further research as it likely contributions to their reduced levels of cooperative and prosocial behaviour (e.g. [8]).It is important to note that not all significant associations between psychopathic traits and social reward in Study 1 were replicated in Study 2. For example, Prosocial Interactions were negatively associated with all psychopathic traits in Study 1, but only Affective psychopathic traits in Study 2. There are a number of possible explanations for these discrepancies. For example, the two samples were drawn from different populations and the sample in Study 1 completed the questionnaires online rather than in the presence of the experimenter. These factors or others could have contributed to the difference between the two samples. It is also important to note the effects of age and gender seen in the post-hoc analyses in Study 1. It will be valuable to study social reward and psychopathic traits further to fully understand the relationship between these two constructs and how this might be influenced by demographic characteristics. However, the fact that the association between all psychopathic traits and Negative Social Potency was found in both samples, despite their demographic differences, suggests this may be a particularly important aspect of social reward for individuals with high levels of psychopathic traits.In Study 2, we also conducted two experimental reward tasks with the aim of further elucidating the relationship between psychopathic traits and social reward. There were no significant associations between psychopathic traits and RTs at any probability level of monetary or social rewards. However, a significant positive association was found between Interpersonal traits and monetary-social difference scores for the 0.5 and 1 probability levels. In other words, as Interpersonal traits increased, the RTs to possible reward became faster in the social task relative to the monetary task.As there were no significant associations between Interpersonal traits and RTs to either monetary or social conditions, these difference score associations are not clearly explained by either slower RTs to monetary reward or faster RTs to social reward. Rather, it is the relative difference between these two rewards that appears important, suggesting that individuals with high levels of Interpersonal traits confer relatively stronger value for social than monetary reward. It is important to note the type of social reward that the Facebook Like symbol represents: approval or admiration of one’s actions or lifestyle. The Interpersonal dimension of psychopathy describes the manipulative use of others, for which winning other’s approval may be particular useful. This may partly explain the relative importance that individuals with high levels of these traits placed on this type of social reward. This speculation is supported by the self-report findings from both samples reported here that Interpersonal traits (but not other psychopathy factors) were positively associated with the enjoyment of Admiration.We had hypothesised that psychopathic traits would be positively associated with RTs to monetary reward, but this was not supported. Previous studies have found that psychopathic traits are associated with increased neural responsiveness to monetary reward [19]–[20]. However, these associations were with neural responses, and have not been demonstrated behaviourally. Therefore, one explanation is that the association between psychopathic traits and hypersensitivity to monetary reward is only apparent at a neural level. In addition, both previous studies used a different measure of psychopathic traits (Psychopathic Personality Inventory; [59]) than the one used in the current study, which furthers limits the extent to which we can compare between studies. A hypersensitivity to financial gain may have important implications for behaviour, particularly in combination with other psychopathic characteristics such as impulsivity and a lack of empathy, so the relationship between psychopathic traits and monetary reward value is worthy of further clarification in future studies.Some limitations to the present study should be noted. Firstly, the sample size of the second study is small and the experimental findings should be replicated with larger samples. The current analyses were also exploratory and correlational. It would be interesting to test more directional hypotheses using more sophisticated regression analyses in the future. For example, it would be interesting to explore whether Interpersonal psychopathic traits predict performance in a social reward task above and beyond the variance shared with other aspects of psychopathic personality. Secondly, difference scores can be difficult to interpret, and it is important to further probe the relative contribution of monetary and social reward value to fully understand the current association between Interpersonal psychopathic traits and the monetary-social difference scores in the experimental tasks. In addition, it would be helpful to collect data measuring the subjective value of the Like and pound symbols for each participant, to assess the impact of this on task performance. Finally, the current study used community samples, and so it will be important to explore if the same pattern of associations between social reward and psychopathic traits is present in clinical samples.In summary, the current study presents evidence that individuals with high levels of psychopathic traits may have an inverted pattern of social reward: they devalue affiliative and prosocial interactions, and instead take pleasure in treating others cruelly. Our experimental evidence suggests that individuals with high levels of Interpersonal traits place particular value on gaining social approval, which we speculate may be due to their manipulative treatment of others and the usefulness of approval in this context. Research addressing social reward in psychopathy is in its infancy, and there are likely to be a host of different processes that contribute to the value of different types of social reward. An important future direction will be to extend the current findings by elucidating the mechanisms behind the ‘inverted’ social reward associated with psychopathic traits."
        },
        "10.1371/journal.pone.0103484": {
            "author_display": [
                "Magdalena Śmieja",
                "Jarosław Orzechowski",
                "Maciej S. Stolarski"
            ],
            "title_display": "TIE: An Ability Test of Emotional Intelligence",
            "abstract": [
                "\nThe Test of Emotional Intelligence (TIE) is a new ability scale based on a theoretical model that defines emotional intelligence as a set of skills responsible for the processing of emotion-relevant information. Participants are provided with descriptions of emotional problems, and asked to indicate which emotion is most probable in a given situation, or to suggest the most appropriate action. Scoring is based on the judgments of experts: professional psychotherapists, trainers, and HR specialists. The validation study showed that the TIE is a reliable and valid test, suitable for both scientific research and individual assessment. Its internal consistency measures were as high as .88. In line with theoretical model of emotional intelligence, the results of the TIE shared about 10% of common variance with a general intelligence test, and were independent of major personality dimensions.\n"
            ],
            "publication_date": "2014-07-29T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 292,
            "shares": 0,
            "bookmarks": 0,
            "url": "http://www.plosone.org/article/info:doi/10.1371/journal.pone.0103484",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0103484&representation=PDF",
            "fulltext": "Introduction\nEmotional intelligence and its measurement\nThe concept of emotional intelligence (EI) entered psychological vocabulary more than 20 years ago [1] and quickly earned a high rank of popularity among researchers and practitioners. The original definition conceptualized it as “the ability to monitor one’s own and others’ feelings and emotions, to discriminate among them, and to use this information to guide one’s thinking and actions” ([1], p. 189). Because scientists assumed that this set of abilities explain important life outcomes [2]–[5], they immediately began inventing various psychometric tools suitable for the measurement of EI as an individual trait. With time, the domain split in two assessment strategies. The first relies on self-report questionnaires, and the assumption that people know how well they understand and deal with emotions. According to this approach, EI is typically measured through self-judgments, using items of the form “I often find it difficult to show my affection to those close to me” or “I understand my emotions well.” Self-report questionnaires require thorough insight into one’s mental state based on accurate feedback regarding the accuracy of the emotional abilities, which seems not to be true in most cases [6]. The second assessment strategy uses ability tests based on performance criteria. They are supposed to reflect a person’s actual level of EI development. Although the tests seem much more objective and informative than questionnaires, they require more time and effort, both in the process of their construction and during administration. Another difficulty with the ability tests of EI pertains to the scoring criteria. In the realm of human emotions, problems rarely have only one “true” or “correct” solution; therefore, scoring procedures must be based either on the consensual approach (i.e., the most frequent answer is regarded “correct”) or on painstaking experts’ judgments [7], [8].\nThe aim of this paper is to introduce a new instrument for assessing emotional intelligence, labeled the Test of Emotional Intelligence (Test Inteligencji Emocjonalnej, TIE). Our test is based on the theory developed by Peter Salovey and John Mayer [1], [2], [7]. According to this theory, EI involves a set of cognitive abilities used for processing emotionally relevant information. Specifically, the theory splits EI into four distinct, albeit correlated, abilities, also called “branches”: (a) Perception of Emotions, (b) Using Emotions to Facilitate Thinking, (c) Understanding Emotions, and (d) Managing Emotions [2], [7]. The first ability (Perception of Emotions) consists of proper perception, identification, and recognition of emotions in one’s own subjective experience, as well as in other people’s behavior. The second dimension, labeled Using Emotions to Facilitate Thinking, pertains to the assimilation of emotions with thinking and problem solving. The third ability (Understanding Emotions) consists of proper understanding of emotions, including the comprehension of triggering factors, phases of the emotional process, and proper sequencing of emotional states. Finally, the management of emotions (Managing Emotions) is the ability to regulate one’s own emotional states, as well as the ability to deal with other people’s emotions and feelings.\n\n\nTIE: objectives and assumptions\nA number of authors argue that EI is important for social functioning and can predict educational achievement or job performance beyond intellectual ability and personality factors [3], [9]–[13]. To explore these relationships, researchers and practitioners need psychometrically sound and conceptually comprehensive measures of EI. Although numerous methods of measurement have already been created [14]–[18], each of the existing tools has some disadvantages. The shortcomings of these measures are different; for some tests it is their length, for others – their insufficient reliability, or the additional equipment needed. Even the Mayer–Salovey–Caruso Emotional Intelligence Test (MSCEIT) [16], which seems the best validated performance measure to fully reflect the ability model of EI, cannot be applied without doubts, as it is not free from cultural specificity [19]. In general, the majority of all concise measures of EI are self-reports influenced by the respondents’ self-esteem and mood, and reflecting rather generalized social adaptation [20] than actual emotional skills, while the ability tests either require considerable time to administer or have limited conceptual coverage. Seeing that a perfect measure of EI has not been created so far, we believe that developing new tools is worthwhile, or even necessary. Both for research and diagnostic purposes, the collection of available tests should be systematically enriched and restored. Since culture can shape the experience and expression of emotions [21], alternative measurement instruments should be developed and validated within different cultural groups. Consequently, our aim was to develop a valid and reliable instrument tapping multidimensional construct of EI, based on narratives and experiences from an adequate cultural context. Staying within scientific bounds in the use of such terms as emotion and intelligence we opt for the ability-based approach and assume that maximum performance tests are the best tools for assessing EI. Hence, we created a performance-based scale covering the whole set of emotional abilities, which is brief and easy to administer in individual and group settings.\nThree assumptions have been made during the construction process of the TIE. First, we wanted to measure the actual abilities of people rather than their own opinions about themselves. The majority of people think that their own EI level is higher than the average [9], therefore performance tests seem much more objective and valid than self-report questionnaires. Second, we intended to create an ecologically valid test, in which solutions would not be scored on the “zero-one” basis. Social and emotional problems are usually too complex and ambiguous to justify such an approach. Therefore, we deliberately decided to give participants the opportunity to rank response options according to their decreasing appropriateness rather than to classify them as “accurate” or “inaccurate”. Third, we decided to base the scoring criteria on experts’ judgment rather than on statistical distribution of the results in the population. Assuming that EI is an ability, or a set of abilities, rather than a personality trait or preference [22]–[24], the most frequent test responses must be regarded as less appropriate than ones that are infrequent but produced by highly emotionally intelligent persons.\n\n\nAims of the study and research hypotheses\nThe aim of our empirical investigations was threefold. First, we intended to provide a detailed, in-depth look into the factorial structure of EI as measured with the TIE. In addition to the factor analyses, we planned to provide empirical data on TIE reliability and validity. To obtain this purpose, we assessed the test’s internal consistency and reported its relationships with established psychological measures, presenting both its convergent and divergent validity. Because ability tests of EI usually reveal systematic gender and age differences we also analyzed such group differences. Since the TIE is an ability measure of EI, in all cases we expected the results usually obtained for such methods. Below, we present the hypotheses in detail.\nFactor structure of the TIE.As the test has been based directly on Mayer and Salovey’s four-factor model [2], we first decided to test the factorial solutions endorsed by these authors for their MSCEIT [16], i.e., one-, two- and four oblique-factor solutions. These solutions reflect the authors’ theoretical assumption that general EI (one-factor solution) consists of four branches (four-factor solution), that may be grouped in sets of two due to their functional similarity (two-factor solution): Experiential (perceiving and using emotions) and Strategic (understanding and managing emotions). Additionally, based on initial theoretical analyses and preliminary CFAs conducted for the TIE [25], we propose and test an alternative area division with the abilities of perceiving and understanding emotions constituting the alternative Experiential area, and the abilities of using and managing comprising the new Strategic area.\nMore recent analyses of the MSCEIT (which remains our main standard of comparison, due to its reputation, popularity, and being the only instrument fully reflecting the ability model of EI) showed that alternative factorial solutions could be more adequate. Therefore, we added a nested modeling procedure endorsed by Palmer and colleagues [26]. This approach does not simply assume that some higher-order factors emerge from the lower-level sets of abilities. Instead, in line with the traditional model of intelligence introduced by Charles Spearman [27], it suggests that test item performance (and, in consequence, life-situation performance) is “loaded” by two different factors: general (the famous “g” factor in intelligence) and specific for particular task/situation (“s” factors). Interestingly, Palmer and colleagues found that EI can also be recognized as a sort of “traditionally understood” intelligence, with one general EI factor, and specific branch-level factors. This observation led us to verify the “nested” (as Palmer calls them) models, to resolve this theoretical bifurcation. Summing up, we decided to test four oblique-factor models: one-factor, four-factor, and two versions of two-factor solutions (with an alternative distribution of subscales between the area-level factors). Further, we applied the “nested” strategy, again testing two versions of two-factor models, a four-factor model, and, additionally, a three-factor model endorsed by Palmer and colleagues [26].\n\nGeneral Intelligence.If emotional intelligence indeed represents a kind of intelligence, tests of general mental ability should correlate with tests of EI; however, such correlation should not be very strong in order to exclude the possibility that both domains are impossible to discriminate [7]. Along with this assumption, numerous previous studies have demonstrated that EI, as measured by the ability tests, correlates at a very modest level (from .31 to .39) with verbal intelligence [7], [12], [28]–[30]. In the case of MSCEIT, most of the overlap with verbal intelligence is accounted for by the subscale of Understanding Emotions. Correlations with fluid intelligence are smaller but still significant [12], [31], [32]. We predict analogical results for the TIE.\n\nPersonality.The results concerning relationships between emotional intelligence and personality are ambiguous, mostly due to the fact that different EI measurement strategies produce different results. Conceptualizing emotional intelligence as a trait and assessing it with self-reports leads to considerable overlap between EI and the Big Five traits [33]–[35]. Not surprisingly, self-judgments of a trait described as a constellation of emotional self-perceptions [36] involving adaptability, assertiveness, social competence, and stress management [37] highly correlates with personality dimensions. Self-judgment scales assess variables relevant to motivation, social skills, and other areas of personality [12], and thus, overlap with the Big Five sometimes as much as the different scales of the Big Five overlap with each other [22]. However, if we define EI as an ability, not a preference or inclination, we should not expect significant relationships with major personality dimensions. Whether or not people are sociable or assertive, they might be intelligent about emotions. Along with that claim, many empirical studies [9], [12], [28], [30] show that ability-based EI shares only a small fraction of common variance with personality, if at all. For example, the MSCEIT correlated .25 with Openness and .28 with Agreeableness [12]. In line with these results, we predict that among five personality dimensions, the TIE would only have a modest relation to Agreeableness and Openness [38].\n\nTrait EI (self-report).Empirical research systematically confirms that self-reported scales do not predict ability assessments of EI well. Correlations between measures of trait EI and ability EI are invariably low [39] showing that the former belong to the realm of personality, whereas the latter pertain to the domain of cognitive ability. In direct tests, self-judgment-based responses are not highly correlated with measured abilities of perceiving, using, understanding, and managing emotions. Therefore, we expect significant albeit weak correlations of self-reported EI with the results of the TIE.\n\nPerception of emotions.Convergence among the most widely used ability test of EI (MSCEIT) and ability measures of emotional perception is rather low. For example, correlations between the MSCEIT and the Japanese and Caucasian Brief Affect Recognition Test [40] are no higher than r = .18; with “facial blends” [41] reaching only r = .14, while those of the Reading the Mind in the Eyes Test [42] reach r = .56. The first studies using the TIE show significant relationships between its results and the ability measure of emotional perception. Wojciechowski and colleagues [43] found that all four TIE branch scores were significantly correlated with the results of a computer test measuring individual effectiveness in recognizing facial expressions (The Face Decoding Test, FDT). Analyses revealed a systematic pattern of positive relationships between the TIE and FDT, with Pearson’s r ranging between .20 and .38, all significant at p<.01. We expect similar results in this study.\n\nGender.Although studies based on self-reports bring disparate findings concerning gender differences in EI [11], [14], , when performance indicators of EI are used, clear and repetitive results are observable: women score higher than men, and these differences are sometimes as huge as one standard deviation [4], [9]. We expect similar gender differences (with females scoring higher) in the present study.\n\nAge.According to the theory, EI should increase with age due to the accumulation of knowledge about emotion and its social context [46]. Nevertheless, studies designed to confirm this effect bring mixed results. Some researchers report significantly better scores on all four EI branches for older adults [7], some investigators [46] show older adults outperform younger participants on three out of four EI dimensions (no difference in perceiving emotions), while others – find no significant association between age and ability EI [47], [48] or even negative correlation between age and emotional perception [26], [49]. Most recently [50] it has been found that older people have lower scores than younger people for total EI and for perceiving, facilitating, and understanding emotions, whereas age is not associated with managing emotions.\nVisibly, according to some studies, EI grows with time, but in agreement with the others it declines with age as any other cognitive ability. Paradoxically, it might not be a contradiction. Probably, from childhood, through adolescence, until middle and old age people develop emotional abilities and gather the experiences building their EI. Inevitable age-related cognitive decline happens only in very old age and does not equally affect all aspects of EI. It seems that older people have difficulty perceiving emotions [51], but may outperform younger people in managing emotions. Therefore, we expect small albeit significant age related gains in each EI branch until middle age, and a small decline for the subscales of perceiving, using and understanding emotions (but not managing) for the oldest participants. Thus, for the three branches a quadratic relationship with age is hypothesized, whereas for the fourth one we anticipate a systematic linear growth across the lifespan.\n\n\nMethod\nEthical Statement\nThe study was approved by the ethics committee of the Institute of Psychology, the Jagiellonian University in Krakow. Participants provided written informed consent. In the case of participants under 18, written informed consent was obtained both from the participant and the guardian.\n\n\nParticipants\nWe analyzed data from 4642 people, combining about 30 studies conducted by our team and collaborators with broad experience in empirical research. Participants volunteered for studies with either no or little compensation (course credits or money). There were 2664 females and 1673 males in the sample (missing N = 305). The mean age of the sample was 25.47 (SD = 9.15, range 16–67). There were 644 high-school students, 1726 university students, and 773 employees in the sample (missing N = 1499). The structure of education in the sample of employees was as follows: primary N = 23, vocational N = 33, secondary N = 227, higher N = 247 (missing N = 243).\n\n\nMeasures\nAbility test of EI.Emotional intelligence was measured with the TIE. The test consists of four subtests representing consecutively Perception, Understanding, Facilitation, and Management of emotions (four labels of TIE subscales are slightly different from the labels of MSCEIT, but they pertain to the same theoretical model). Each subtest is defined by six item parcels. An item parcel consists of one problem situation in which people perceive, experience, manifest or use emotions, followed by three alternative responses. This is an example of such item parcel:\nSophie hits the table with a fist. She frowns, her face is glowing, and her teeth are clenched. Most probably:\n\n\n\n\na) She is watching a popular show on TV\n\n1 …… 2 …… 3 …… 4 …… 5\n\nb) Once again she hurt her finger while cutting bread\n\n1 …… 2 …… 3 …… 4 …… 5\n\nc) She was just told by a colleague that he will not help her to prepare an important project, because he is leaving for a last-minute holiday\n\n1 …… 2 …… 3 …… 4 …… 5.\n\nThe three answers are related to the same emotional problem, however, each of them asks about the accuracy of a different strategy or perception, and therefore they can be treated as separate items. The whole test consists of two parts with different instructions. In the first part, referring to Perception and Understanding, participants are asked to reflect on feelings and thoughts of persons who were involved in the described situations. The task is to evaluate, on a 5-point Likert scale anchored at the ends with a “very bad answer” and a “very good answer,” the probability that a person involved in the situation experiences alternative emotions. In the second part, referring to Facilitation and Management, test-takers are asked to indicate the most advisable action that a protagonist should implement in order to solve the problem. The task is to judge, on a similar 5-point Likert scale, the level of appropriateness of each of the three actions described on the answer sheet, ranging from “very ineffective” to “very effective.” Scoring is based on the similarity of a testee’s responses with answers provided by the panel of experts (52 professionals, including 13 psychotherapists, 14 trainers of management, and 25 HR specialists). The points are summed up separately for all branches and for the whole test.\n\nSelf-reported EI.Self-reported EI was assessed via the SSEIT [18], a brief scale based on Salovey and Mayer’s [2] original definition of EI. Individuals are instructed to give their level of agreement to 33 statements describing different aspects of emotional life (e.g., “I know why my emotions change”) on a scale ranging from 1 (strongly agree) to 5 (strongly disagree). The original English version adapted to Polish [52] in this research reached an internal consistency of .91.\n\nPerception of emotions.The ability of perceiving emotions in nonverbal signals was measured with the SIE-T, an instrument tapping the ability to recognize emotions in facial expressions [53]. The internal consistency was .84.\n\nFluid intelligence.Raven’s Advanced Progressive Matrices–RAPM [54], showing an internal consistency of .87, served as a tool to evaluate fluid intelligence.\n\nVerbal intelligence.The test based on Horn and Cattell’s [55] theory of crystallized intelligence served as a tool for assessing verbal intelligence. The participants are asked to classify 120 words to one of the following categories: (1) art, (2) biology, (3) science, (4) literature, or (5) geography and history. The score is calculated as the number of correct answers given in five minutes.\n\nPersonality.To assess personality dimensions, we employed NEO-FFI [56], [57] showing the following internal consistencies (α): .77, .68, .82, .80, and .68 for Extraversion, Agreeableness, Conscientiousness, Neuroticism, and Openness to Experience, respectively.\n\n\n\nProcedure\nSince the TIE is a paper-and-pencil test, it was possible to gather the data in small groups, normally not exceeding 10 persons. The participants worked in quiet settings, completing the test in a limited time of 30 minutes. Other instruments were administered according to standard instructions, usually after the participants completed the TIE test. The entire study lasted for 1 to 2 hours.\n\nResultsThe main analyses (descriptive statistics, factor structure) were accomplished using the whole sample. For other analyses (convergent and discriminant validity), we took into account those participants who completed relevant tests or questionnaires (e.g., RAPM or NEO-FFI).\n\nDescriptive statistics\nTable 1 provides basic descriptive statistics for four subscales and the total score of the TIE. The results are slightly negatively skewed, although neither skewness nor kurtosis indicators exceed an absolute value of 1.0, thus the score distributions may be treated as approximately normally distributed. Since neither floor nor ceiling effects can be discerned, it seems that the TIE provides enough space for assessment of individual differences in EI.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Descriptive statistics.doi:10.1371/journal.pone.0103484.t001\n\nFactor structure and intercorrelations\nIn order to verify competing structural models of the TIE, the results have been subjected to confirmatory factor analysis with the LISREL software and its pre-processor, PRELIS [58]. We investigated eight models, in groups of four. The first group was tested using an oblique-factor modeling strategy: the four-factor model, based on the underlying theory of EI, the one-factor model, assuming the existence of the general EI factor, and two versions of the two-factor solution. The first (Two-factor A, Table 2) corresponds to the area division by Mayer and Salovey [2]. The second version (Two-factor B, Table 2) corresponds to the results obtained in the pilot study [25] in which Perception and Understanding entered the first factor, whereas the dimensions of Facilitation and Management formed the second.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Exact and close-fit statistics/indices (WLS) for TIE models.doi:10.1371/journal.pone.0103484.t002The second group of models was analyzed using a nested factor modeling strategy [26]. In addition, in this case we tested a four-factor model and 2 two-factor solutions, as well as the novel model with three nested factors, endorsed by Palmer and colleagues. For the analyses we applied weighted least squares (WLS) estimation method. All solutions obtained an acceptable goodness of fit indicators (Table 2). Although the A and B two-factor models simply cannot be compared with each other due to an equal number of degrees of freedom, it seems obvious from all the applied fit indices that the solution proposed by the authors of the test (i.e., “B” model) is, in case of the TIE, definitely better than the original area division proposed by Mayer and Salovey [2]. The difference is visible both in traditional and nested factor modeling. For the oblique-factor group of models, the four-factor and two-factor “B” solutions revealed the lowest χ2/df ratio, i.e., proved to be the best fit-to-data. A comparison of the two models revealed supremacy of the four-factor model (χ2diff = 40.02, dfdiff = 5, p<.001) presented in Table 3.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 3.  TIE standarized parameter estimates (WLS) for oblique-factor (Model 4) and nested factor (Models 6 and 8) models (N = 4642).doi:10.1371/journal.pone.0103484.t003The nested factor modeling strategy [26] resulted in a better fit to model in each of the analyzed cases, with all models significantly better fit in the nested version. Adding the “g” EI factor significantly improves the analyzed models’ fitness. Using the nested modeling strategy we have also confirmed the advantage of the four-factor solution over the two-factor “B” model (χ2diff = 49.85, dfdiff = 7, p<.001). These two models revealed the highest fit indices, surpassing the three nested factors solution that proved most appropriate in the case of MSCEIT (χ2diff = 43.18, dfdiff = 3, and χ2diff = 93.03, dfdiff = 10, respectively for two- and four-factor models; both significant at p<.001). Altogether, the CFA results suggest that the four-factor and two-factor “B” structures with nested factors should be preferred over the competing models.\nBased on the goodness of fit indices reported above, we chose the three models to present their factor loadings (Table 3). Note that in model 8, the Facilitation factor loadings become negative, suggesting that the model may be overestimated. The case is similar to the nested two-factor model analyzed by Palmer and colleagues [26]. Albeit their model had an excellent fit, it was considered unacceptable. Therefore, model 8 must be treated with high caution, even if it seems adequate when analyzed using oblique modeling. It is interesting that exactly as in the Mayer–Salovey–Caruso Emotional Intelligence Test [16], Facilitation proved to be the “black sheep” of EI abilities. In all, the two-factor “B” model in its nested version seems the most reliable solution with respect to the factorial structure of the TIE.\nThe correlational analyses revealed that four subscales of the TIE are mutually intercorrelated and strongly related to the total score (Table 4).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 4.  Correlations between TIE subscales and the total score.doi:10.1371/journal.pone.0103484.t004\n\nReliability\nThe overall TIE reliability is r = .88. For the subscales, Cronbach’s alphas are: .70 (Perception), .69 (Understanding), .65 (Facilitation), .66 (Management). Additionally, we computed reliability indices for two parts of the test. For the first part (Perception and Understanding) Cronbach’s alpha is .81, and the respective value for the second part (Facilitation and Management), is .78. Coefficient alpha of the total score is adequate, reliabilities associated with the branch level scales are less satisfactory, although such estimates are common among tests in this area of research [41], [59], [60].\n\n\nValidity\nWe adopted several ways to evaluate the validity of the TIE (see Table 5). First, we looked at the discriminant validity, using two IQ tests. The total score of the TIE correlated with RAPM (N = 912) at the level of r = .35, p<.001, indicating a medium effect size [61] and with the Gc test (N = 474) at the level of r = .26, p<.001 (small effect size). The highest correlations with fluid and crystallized intelligence tests reached the subscale of Understanding (.37 and .36, respectively; medium effect size), which is consistent with the results reported by other researchers [60]. The remaining correlation coefficients ranged between .15 and .29. These medium strength intercorrelations suggest that EI, as measured with the TIE, and general intelligence are separate albeit interconnected mental abilities.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 5.  Correlations between TIE scores and measures considered for the validity examination.doi:10.1371/journal.pone.0103484.t005To evaluate the discriminant validity of the TIE we analyzed its relation to NEO-FFI. In our study (N = 511), only Agreeableness correlated with TIE’s subscales (coefficients ranged between .11 and .14, p<.01) and the total score (r = .16, p<.01), revealing a small effect size according to Cohen [61]. These correlations were weak, explaining about 2.5% of common variance, at the most. Thus, our test proved its independence of personality, similarly to other ability measures of EI based on performance criteria, such as MSCEIT [30].\nLooking at the convergent validity of the TIE, we found that the total score of the TIE correlated with the SSEIT at a significant but low level (r = .16, p<.001, N = 648). As to the subscales, the correlation coefficient with SSEIT was insignificant for Understanding, while for others ranged from r = .11, p<.01 (Perception) to r = .18, p<.01 (Facilitation), thus only oscillating around the threshold of a small effect size [61]. The correlations between the TIE and the SIE-T were higher than with the SSEIT (r = .35, p<.001, N = 631), ranging from r = .26, p<.01 (Perception) to r = .37, p<.01 (Understanding) at the subscale level, showing medium effect sizes [61]. The self-report measure of EI revealed weaker relationships with the TIE than the ability test, despite the fact the latter referred to only one specific aspect of EI, namely: recognition of expressions of emotions.\nThe validity of an EI test can be evaluated not only via correlating its results with other instruments of the same kind, but also through analysis of group differences. In our study, women outperformed men in every subscale of the TIE, and consequently in the total score (Table 6). All differences are highly significant and the effect sizes are remarkable.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 6.  Gender differences in TIE: subscales and total score (N = 4369).doi:10.1371/journal.pone.0103484.t006In case of the TIE, the effect of age was also confirmed. The correlation coefficient between age and the general TIE score was r = .05, p<.001 (N = 4530). The effect was similar for three out of four of TIE branch scores, with Pearson’s rs of .04, p<.01 for Perception, .05, p<.01 for Understanding, and .06, p<.001 for Management. Only for Facilitation the correlation with age was not significant, r = .02, p = .12. These results are in line with conclusions formulated by Mayer, Caruso, and Salovey [7]. Although the effect is very small and does not reach the small effect size threshold proposed by Cohen [61], it does not differ from EI-age correlations obtained for MSCEIT [26]. We did not find any evidence supporting the formulated claims on curvilinear changes in EI branches across the lifespan.\n\nThe main aim of the present study was to test the psychometric properties and factor structure of the new ability test of EI. We assumed that TIE’s results would conform to the theoretical assumptions of the four-branch structure of EI, specified by Mayer and Salovey [22]. Most of the formulated hypotheses were confirmed.The TIE proved its suitability as a reliable test of emotional intelligence. The internal consistency measures were rather high, as far as the total score is concerned, or mostly acceptable, if the particular subscales are taken into account. The internal consistency score at the level of .88 is a result very similar to the one usually found concerning Raven’s Matrices [54] or the MSCEIT [41], [62], [63], the most widely used tests of IQ and EI. Reliabilities associated with the subscales of the TIE were smaller, although such estimates are not unique among tests in this area of research. To point out only a few examples concerning MSCEIT: Caruso [64] reported a Cronbach’s of .74 (Understanding Emotions) and .76 (Managing Emotions), Roberts et al. [60] revealed .67 (Using Emotions to Facilitate Thinking), .68 (Understanding Emotions), and .68 (Managing Emotions). More recently, Austin [41] showed .58 (Using Emotions to Facilitate Thinking), .66 (Understanding Emotions), and .66 (Managing Emotions); and finally, Lopes and colleagues in one of their studies [65] obtained for Managing Emotions a split-half reliability of .57. Because of the fact that reliability measures concerning subscales of the TIE do not differ substantially from other tools of this kind [66], both the total and subscales may be used in the research, while for individual diagnosis it seems advisable to rely on the total score.The results of the TIE conform to the theoretical assumptions about the nature and structure of EI, specified by Mayer and Salovey [2], [7], [22]. This finding is good both for the theory, as its empirical support, and for the test, as an argument for its theoretical validity. At the same time, our study suggests that the between-branch connections may be conceptualized in a different manner than in the original theory. In Mayer and colleagues’ original conceptualization, perceiving and using emotions formed the experiential factor, whereas understanding and managing emotions loaded the factor of strategy [16]. This division is based on an observation that some emotional abilities enhance emotional knowledge and make one “closer” to one’s own and others’ emotions, and therefore comprise the “hotter” part of EI, whereas other abilities are usually treated as useful tools for “cold,” strategic regulation of emotion. However, this division of abilities remains a problematic issue. Whereas perception is clearly experiential, and management is definitely strategic, the understanding and facilitation branches are much more ambiguous. The former is manifested mainly in adequate labeling of emotions, as well as linking them with their causes and consequences. Accurate labeling is necessary for differentiation of similar emotional states; thus, to distinguish two different albeit similar affective states one needs to accurately feel these states, which makes the understanding branch experiential, at least to some degree. On the other hand, facilitation, also labeled “using emotion to facilitate thought” [2], [7], has much in common with processes of meta-cognitive control, and thus remains a very “applicable” and therefore strategic dimension.Our results imply that the Perception branch may be in fact situated closer to Understanding than Facilitation, while the latter seems to be more strongly connected to Management than to Understanding. This finding can lead to modification of Salovey and Mayer’s theory. Nevertheless, we suggest this interpretation with caution, realizing that our outcomes may result either from specific operationalization of respective branches of EI or from cultural specificity. However, our findings are generally consistent with the underlying theoretical assumptions and empirical data concerning the structure of emotional intelligence as a set of four abilities.Confirmatory Factor Analyses revealed that although four-factor models were characterized with the best fit indices, they should be treated with caution due to unexpected negative loadings of Facilitation branch. Therefore, the two factor solution with general-EI loading on each area seems much less ambiguous, and, in the light of the presented data, should be preferred over the remaining ones. The supremacy of the nested models suggests that EI, as measured with the TIE, has much more to do with intelligence (in the way in which Charles Spearman would want to see it, as loaded with general and specific factors), than with personality. Thereby, we provide further evidence for “the intelligence of EI” [7], based not only on the test’s convergent/divergent validity, but also on its factorial structure.Many studies introducing new EI methods face the problem of weak convergent validity. Our results seem to be in line with the empirical data published to date. The total score of the TIE correlated with self-reported EI at a significant but low level oscillating around the threshold of a small effect size [61]. Taking into account that both tools are based on the same theoretical model, one might expect much higher correlations, although such result is not an exception. The most elegant example was provided by Brackett and colleagues [9], who developed a self-judgment scale based on the Four-Branch Model and found a correlation of only r = 0.19 with the MSCEIT. More commonly used self-judgment scales of EI, such as Bar-On’s EQ-i [14] or the SSEIS [18] predict MSCEIT results better, but still at a pretty low level [12], [67]. MacCann and Roberts’s Situational Test of Emotional Understanding and Situational Test of Emotional Management [68] correlated with MSCEIT of .33 and .36, respectively. A very similar correlation of .36 has been found between the TIE and the PKIE, a self-reported measure of EI [69]. The problem of very low correlations between different measures of EI undoubtedly stems from inconsistences on a theoretical and psychometric level, and leads to the conclusion that self-assessed EI and ability EI measured by performance tests are different constructs.Despite the disparate forms of measurement methods, the correlations of the TIE and an instrument tapping the ability to recognize emotions in facial expressions were higher, revealing medium effect sizes [61]. As significant relations cannot be assigned to shared method variance in this case, such result provides support for the construct’s convergent validity.Compared with the convergent validity evidence, the discriminant validity evidence for the TIE is promising. According to the ability-based approach, emotional intelligence is not a personality trait, and thus its measures should not share much variance with major personality dimensions. Many empirical studies [9], [12],  showed that EI shares only a small fraction of common variance with personality. In line with that, the TIE proved its independence of personality. Weak, although significant, correlations with Agreeableness, and the lack of any other relationships with the remaining Big Five dimensions, should be interpreted as evidence that the TIE does not cover preferences, habits, or inclinations. The very moderate correlations between the TIE and measures of general intelligence suggest that EI is a set of mental abilities, related to intelligence, but quite independent of it. These findings are also consistent with EI literature [32], [60], [66], [70].Most tests of ability EI reveal systematic gender differences. Research proves that women tend to show greater knowledge of emotional experiences, provide more complex descriptions about emotions, and use a broader emotional vocabulary [71]–[73], and thus consequently regularly score higher than men on EI ability tests [4], [9]. Also in our study, women outperformed men in every single subscale of TIE, and consequently in the total score (with remarkable effect sizes). Such confirmation of the generally recognized phenomenon of women’s advantage concerning emotional abilities is additional evidence supporting the new test’s validity.According to Mayer and Salovey’s theory, EI should increase with age due to the accumulation of knowledge about emotion and its social context [7], [46]. Our results are in line with such prediction. Although the effect is small, it does not differ from EI-age correlations obtained for MSCEIT [26]. Since our sample did not include participants older than 70, we did not discover age-related cognitive decline in EI."
        },
        "10.1371/journal.pone.0092844": {
            "author_display": [
                "Florian Artinger",
                "Filippos Exadaktylos",
                "Hannes Koppel",
                "Lauri Sääksvuori"
            ],
            "title_display": "In Others' Shoes: Do Individual Differences in Empathy and Theory of Mind Shape Social Preferences?",
            "abstract": [
                "\nAbundant evidence across the behavioral and social sciences suggests that there are substantial individual differences in pro-social behavior. However, little is known about the psychological mechanisms that underlie social preferences. This paper investigates whether empathy and Theory of Mind shape individual differences in pro-social behavior as conventionally observed in neutrally framed social science experiments. Our results show that individual differences in the capacity for empathy do not shape social preferences. The results qualify the role of Theory of Mind in strategic interaction. We do not only show that fair individuals exhibit more accurate beliefs about the behavior of others but that Theory of Mind can be effectively used to pursue both self-interest and pro-social goals depending on the principle objectives of a person.\n"
            ],
            "publication_date": "2014-04-17T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 2,
            "views": 909,
            "shares": 3,
            "bookmarks": 12,
            "url": "http://www.plosone.org/article/info:doi/10.1371/journal.pone.0092844",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0092844&representation=PDF",
            "fulltext": "IntroductionAbundant evidence across the behavioral and social sciences suggests that there are substantial individual differences in pro-social behavior. While some people predominantly care about their own material outcome, other people with more pro-social motivations are often seeking to advance social goals and equality at their own cost. These insights, often supported by laboratory experiments, have had a decisive impact on the emergence of social preference theories [1]–[3]. Yet, to date it is unclear which psychological mechanisms underlie the observed behavior.\nEmpathy and Theory of Mind (ToM) are both regarded as central in social interaction relating to social emotions and social reasoning. Social emotions function as emotional responses to, for instance, unfair or fair decisions. Social reasoning is used to assess how others are likely to act in a given situation. Empathy is thereby the capacity to share social emotions of others. ToM is the capacity to understand the social reasoning and social emotions of others [4].\nParticularly prominent in the study of pro-social behavior and individual differences in social preferences are the Ultimatum Game (UG) [5] and the Dictator Game (DG) [6]. These paradigmatic games are regularly implemented to study decision making in neutrally and socially framed contexts. Studies have examined decision making using the UG and DG in various social contexts, for instance, by the means of displaying the face of the experimental participant before the interaction [7], [8]. Likewise, a vast array of studies has examined behavior in the UG and DG using a neutral decision context. The conventional practice of using neutral and abstract frames in economic experiments is a means to improve experimental control and elicit the underlying preferences in a given population. Using a neutral frame, the two games have been, for instance, employed in the parameterization of social preferences in the inequity aversion model by Fehr and Schmidt (1999), currently the most widely cited model on social preferences [2].\nIn the UG, a proposer chooses how much to offer from an initial endowment. A responder can either accept or reject. If the responder rejects, both will get nothing. In the DG, the responder must accept any offer made by a proposer. With its absence of strategic considerations, the DG is often regarded as a pure measure of pro-social behavior and hence lends itself to evaluating the role of empathy and social preferences [6]. Regarding the UG, across a large range of conditions, responders have been shown to reject low offers despite the consequence of receiving a payoff of zero [9]. These rejections have often been interpreted as evidence for social preferences (for alternative interpretations of UG rejections see [10], [11]). Given that a substantial share of responders in the UG will reject low offers, many inherently selfish proposers make apparently fair offers in order to maximize their expected pecuniary payoff. At the same time, those individuals better at forecasting what is the smallest offer still accepted by the receiver can use this knowledge to their material benefit. Because of its combination of strategic interaction and fairness concerns, the UG is particularly well suited to evaluate the role of ToM in strategic interaction.\nThis paper tests whether individual differences in empathy and ToM affect behavior in neutrally framed experiments which are frequently employed to measure social preferences. Scholars of cognitive science have stressed the importance of embodiment in emotion recognition and posit that ability to understand social emotions is grounded in primitive ability to share emotions through bodily interaction with other people and the environment [12]. Our study differs from the literature on embodied cognition in that we investigate the underlying psychological mechanism of social preferences in a neutrally framed context. This renders it possible to directly evaluate the role of empathy and ToM in social preference theories that are used for instance to predict economic behavior, analyze welfare implications and derive policy recommendations.\nOur results show that individual differences in the capacity for empathy, as measured through various psychometric tests, do not shape social preferences. At the same time, our results qualify the role of ToM in strategic interaction. We do not only show that fair individuals exhibit more accurate beliefs about the behavior of others but that ToM can be used to pursue both self-interest and pro-social goals depending on the principle objectives of the person.\n\nEmpathy\nLooking inside the black box of social preferences, one component that has been hypothesized to play a central role is empathy, the capacity to share the feelings of others. Adam Smith (1789, p. 10) already highlighted in The Theory of Moral Sentiments the potential role of empathy in pro-social behavior: “Empathy is the source of our fellow-feeling for the misery of others, that is by changing places in fancy with the sufferer, (it is) that we come either to conceive or to be affected by what he feels” [13].\nMore recently, a very similar stance has been taken in the perception-action model of empathy [14]. The model suggests that it is sufficient to observe or imagine someone else in an emotional state to trigger an empathic response. In a review article on the role of empathy and ToM in economics, the authors suggest that if empathy implies the shared experience of emotions, this can undermine the idea that choices are based solely on self-interest [15]. This is captured, for instance, by the inequity aversion model by Fehr and Schmidt (1999), in which an agent can experience not only positive but also negative utility if the agent's own and others' monetary outcomes differ [2]. In line with this reasoning, empathy has been listed among a number of pro-social emotions, such as guilt and shame, which underpin pro-social behavior in human decision making [16].\nIndividual differences in the capacity for empathy reflect differences in pro-social behavior in domains such as volunteering and donating [17]. In a group of young adults, measures of pro-social dispositions have been found to be stable across a period of five years and relate to ratings of empathy [18]. According to the empathy-altruism hypothesis, empathy is even regarded as the exclusive source of genuine altruism [19]. Empathic feelings are classically associated with helping someone in need [20], [21]. In the context of the Prisoner's Dilemma Game, inducing empathy via portraying the other person as needy resulted in participants choosing to cooperate more often than in a neutral control condition even when participants knew beforehand that the other person would defect on them [22].\nNeuroscientific experiments have indicated that people differ in their empathic capacity and that this might relate to individual differences in social preferences [23]. Inviting couples into the laboratory, participants received a painful stimulus and the resulting brain signal was compared to that obtained when they were merely informed that their loved one had received such a stimulus [24]. The same affective brain circuits were active both when receiving pain and when being informed about the beloved one experiencing the same painful stimulus. Importantly, the higher the activation in the pain circuits, the higher participants scored on a psychometric measure of empathy, the Interpersonal Reactivity Index, which is also employed in the current investigation [25]. In a further study participants were initially exposed to a situation where they were treated either fairly or unfairly by a matched partner [26]. In the second part of the experiment, the matched partner received a painful stimulus. For men, empathy-related brain activity was found only in those who faced a fair partner and not in those who faced an unfair partner. For women, such a difference was not observed. The authors link these results on empathy to social preference theories, where people value others' gains positively if they are fair, but the gains of unfair partners are negatively valued.\nA number of researchers have hypothesized about the importance of individual differences in empathy for fairness as observed in neutrally framed laboratory games [15], [27], [28]. If allocators were explicitly asked to put themselves in the shoes of the recipient in the DG, offers increased as compared to individuals in a control group that were not given any specific instructions [28]. This illustrates that empathic behavior can be induced by changing the social context. In this study, we specifically focus on individual differences in empathy in a given context and how these relate to social preferences. Singer (2008, p. 264) reemphasizes a point made earlier by Singer and Fehr (2005) that “one prediction that can easily be made is that people with a greater ability to empathize should display more other-regarding behavior” [4], [27]. However, a clear-cut demonstration that individual differences in empathy underlie social preferences as classically measured in neutrally framed laboratory games is still missing.\nHypothesis 1: The greater the individual's capacity of empathy the higher the offer in the DG.\n\n\nTheory of mind\nEmpathy relates to the capacity to share social emotions. In contrast, ToM allows an agent to understand the social reasoning and social emotions of others. Utilizing ToM, the decision maker constructs the mental states of others making inferences about beliefs, intentions, and emotions [29]. The concept of ToM was initially proposed by primatologists who suggested that it emerged as a result of the social challenges of living in larger coalitions [30], [31].\nIn economics, Smith (1776) noted in The Wealth of Nations that understanding the goals and beliefs of one's trading partner facilitates business [32]. The centrality of ToM to strategic interaction implies that it closely relates to issues addressed in game theory. One basic assumption is that of “common knowledge,” which implies that interacting agents reflect on the action of the other and know that the other does the same [33]. More recently, the assumption that all people make the same inferences as others do has been challenged, and individual differences between agents in their thinking steps and hence in their capacity for ToM have been considered central to explaining a number of empirical phenomena [34], [35]. This echoes the hypothesis that Singer and Fehr (2005) postulated, by which individuals who have a higher capacity for ToM can better predict others' motives and actions [27].\nHypothesis 2: The higher the individual's capacity for ToM the more accurate the stated beliefs about the behavior of others.\nToM is thought to serve two functions: (i) It facilitates the pursuit of one's personal gains, a function that in the psychological literature has also been labeled Machiavellian intelligence [30]; and (ii) it facilitates pro-social behavior, as shown, for instance, in non-human primates where the capacity for ToM can lead to acts of spontaneous helping [36]. Surprisingly, to date, among humans there exists evidence only for the second function but not for the first. For instance, people high on the Machiavellian scale score low on the capacity for ToM, that is they do not seem to effectively employ ToM to further their own goals [37], [38].\nUsing economic games, researchers have studied individual differences in ToM among children but not adults: In the UG, 6- to 10-year-old autistic children, who are impaired in ToM, were more likely to accept low offers and to refuse fair proposals compared with a normally developed cohort [39]. Likewise, preschoolers who had acquired the capacity for ToM made higher mean offers in the UG than those who had not yet developed this capacity. This has led to the proposition that individual differences in ToM are a psychological component that underlie social preference theories in which those with a higher ToM are fairer [40].\nYet, these findings do not square with the proposition made in economics and game theory that those who seek to maximize their own monetary profit can benefit from an accurate estimation concerning others' likely action. The seminal study of Kelley and Stahelski (1970) on the role of beliefs in social dilemmas allows a reconciliation of these contradictory propositions [41]. The authors showed that cooperative individuals are more accurate than selfish individuals in predicting the actions of others in a social dilemma. They argued that this difference is functional in nature: To pursue their strategy successfully and reduce the likelihood that someone else takes advantage of their pro-social tendency, cooperative people need more accurate beliefs than defectors. For defectors it is unnecessary to make accurate inferences in a one-shot social dilemma, as doing so does not affect their strategic choice which is always to defect regardless what the other does. For those that consider cooperating, it is vital to accurately estimate what others do in order to be able to deduce the potential risk that is entailed in choosing to cooperate. A question that follows from this is whether people that tend to favor a strategy with a pro-social outcome are generally more accurate in estimating what others do regardless the specific context of the game. We used the DG in an attempt to distinguish the principal social preferences of agents, that is, whether the agents seek to maximize their own payoff or also care about the payoff of others.\nHypothesis 3: When comparing fair and selfish participants as measured by their offers in the DG, fair participants will have more accurate beliefs about the decisions of other participants than selfish ones.\nHowever, the situation changes when the focus is on those proposers in the UG who seek to maximize their own payoff. The more accurately a proposer judges the likelihood of a certain offer being rejected, the higher the expected payoff. Hence, in this context it becomes functional for the selfish proposers to have accurate beliefs.\nHypothesis 4: Selfish participants, as measured by offers in the DG, are more likely to employ accurate beliefs for their own material self-interest in the UG than fair participants.\n\nMethodsThe experiment was conducted at a German University where ethical review is standardized for conventional socioeconomic experiments such as this one. This implies that the treatment of participants was in agreement with the ethical guidelines of the German Psychological Society (DGP - see the guidelines: http://www.bdp-verband.org/bdp/verband/e​thic.shtml; particularly section C.II.4) and the German Research Foundation (Deutsche Forschungsgemeinschaft). Specifically, all participants gave their written informed consent to participate voluntarily, assuring them that analyses and publication of experimental data would be without an association to their real identities. Moreover, random assignment to visually separated cubicles and private payment at the end of the experiment preserved the anonymity of participants. The experiment involved no deception of participants. As in other socioeconomic experiments, there were no additional ethical concerns.\nThe experiment was computerized and conducted in the laboratory of a large German University using z-Tree [42]. Participants were recruited via the online recruitment system ORSEE [43]. A total of 120 students took part in one of four experimental sessions. The vast majority of 87 female and 33 male participants were undergraduate students representing a wide range of different academic disciplines. We did not aim to recruit any specific gender composition. The academic disciplines of the participants were as follows: 31 social sciences (excluding economics), 22 natural sciences, 15 humanities, 12 law, 12 economics and business administration, 6 medicine and 18 other disciplines. The reported distribution of academic disciplines includes 116 individuals. We have excluded from the data set four non-native German speakers who indicated limited ability to understand some parts of the experimental instructions originally written in German. Each experimental session lasted about two hours. Earnings per participant ranged from 7€ to 26€ with a mean of 14€.\n\nBehavioral measures\nThe behavioral measures of our experiment consist of three separate sections. The first section includes three decisions to measure individuals' concern for fairness. The second section elicits participants' belief about the behavior of other participants in the first section. The third section measures participants' risk attitude using incentivized risk elicitation task. Decisions in all sections are incentivized by randomly selecting one decision from each section for the final payment (please see the Instructions S1 for the Experimental Instructions).\nTo measure an individual's concern for fairness, we had participants make decisions in three different roles: As a proposer in the UG and in the DG and as a responder in the UG. When participants must indicate their actions in all possible roles this has been labeled the strategy vector method [44]. When the strategy vector method has been compared to situations where participants play only one role in bargaining experiments, no major differences has been found [45]. These previous results lend support to the argument that individuals' concerns for others as measured in laboratory experiments are robust to elicitation through the strategy vector method. In the role of the responder, participants indicated the minimum acceptance level below which they would reject an offer. At the end of the experiment, one of the three decisions was picked randomly to calculate the monetary payoff. Participants were not informed whether they were matched with a different participant in each decision. This feature may in principle affect the behavior of various individuals.\nIn both games, the proposer was endowed with 90 experimental currency units (ECUs) and could split these in intervals of 10. An equal split is known to be the modal offer both in the UG and the DG. Hence, our research design precluded the possibility of making the modal offer. It has been shown that after removing the equal split, fair offers become less frequent [46] and that responders are less averse to unequal outcomes [47]. This provides a particularly interesting test bed for individual differences in ToM.\nParticipants were additionally asked to indicate their beliefs about the likely action of a randomly chosen partner in each of the three decision tasks (DG offer, UG offer, and UG minimum acceptance level). They thereby stated the probability they considered most likely for a given action by answering the question such as the following example for DG offers: “Please indicate the likelihood that a randomly determined person taking part in this experiment has chosen one of the 10 possible divisions”. Beliefs about the actions of others were rewarded based on the Quadratic Scoring Rule (QSR) [48]. It is used to measure and reward the accuracy of predictions. In our case, each participant states a probability vector r = (r1,…,rn) where ri refers to the probability that an event i occurs. The QSR is then used to give a reward of Q(r,j) where j is the event that actually occurs. The functional form of the QSRis strictly concave with respect to ri in which case the highest possible earning is received by placing 100% on the occurring event and zero probability on any other event. Participants get penalized for placing a positive probability on events that actually do not occur. The penalty is disproportionately increasing in the probabilities placed on those events. We set α = 10 and β = 10 which guarantees a positive payoff unless the participant assigns 100 per cent probability on a single false event in which case the participant receives no payment. An empirical assessment of various proper scoring rules (logarithmic, quadratic and spherical) show that participants are likely to employ various suboptimal reporting strategies when the score contains both positive and negative scores [49]. In our experiment, we have limited the QSR score to positive scores except in case where 100 percent probability is assigned to a single false event to facilitate truthful revelation. Schotter and Trevino [50] present a recent survey of the literature on belief elicitation in laboratory experiments and discusses among other techniques the QSR. Our implementation of the rule closely follows the standards for incentive compatible belief elicitation in economic experiments.\nThe QSR rule induces truthful revelation of subjective beliefs about the expected behavior of other participants in strategic games and is widely used to elicit beliefs in behavioral experiments [51]. To ensure full comprehension of the payoff mechanism, participants were carefully instructed on the procedure. Each participant had to go through three learning episodes with control questions that became increasingly difficult. A post-hoc analysis of the data shows that 93 per cent of participants provided the right answer to all three questions. At the same time, 96 per cent of participants provided the right answer to the most complex question (Question 3). However, a considerable effort and learning through trial and error was needed to gain full comprehension. Had we allowed only one attempt per question, only 52 per cent of the participants would have been able to find the right answer to all three questions.\nFor the purpose of paying, the QSR score for each participant in a given task was calculated on the basis of how well stated beliefs predicted the decision of another, randomly selected, participant. In order to make a robust assessment of the accuracy of beliefs and their relation to social preferences in the results section of the paper, the reported belief distribution of each participant was compared to the distribution of choices of all other participants. The belief assessment of every participant was matched with each of the observed outcomes in the study population, resulting in 115 scores, as there were 116 participants in total. We then computed for every participant a mean score for the three separate tasks and an overall score that is aggregated over the three tasks. These scores could range from zero to two. A score of zero indicates that the participant assigns 100 percent probability to a single false outcome. A score of two indicates that the participant assigns 100 percent probability to a single correct outcome.\nWhen beliefs and actions from the same participants are elicited in an incentive-compatible manner, participants might hedge across tasks that are independently incentivized. In other words, participants might take in some task higher risks to increase their chance of higher earnings and compensate for this in other tasks where they take lower risks ensuring that they get a minimum payoff. Previous studies however show that unless hedging opportunities are very prominent, results are unlikely to be confounded due to hedging [52]. Because participants did not know about the belief elicitation until they were actually asked to state their beliefs, the prominence of hedging opportunities between the games and belief elicitation tasks was not an issue here. Eliciting a separate belief distribution in each game may encourage participants to balance their reported beliefs. For example, participants could report systematically wider belief distributions in a certain game to secure a high minimum payment if this task gets chosen for the payment. As we only paid one randomly chosen belief elicitation task, such hedging should be minimized.\nA possible remaining confound is that participants adjust their reported beliefs to their risk attitude. Likewise, the practice of paying participants based on the action chosen by one randomly determined person may affect the reported belief distributions. This may invite risk-seeking participants to report narrower distributions centered on the option that they believe to be the most likely event. Risk-averse participants may be inclined to report flatter distributions to secure a high minimum payment. Not only beliefs but also behavior in the games might be shaped by individual differences in risk aversion. Greater risk aversion can, for example, lead to higher offers of proposers in the UG as these are less likely to be rejected.\nIn order to control for individual differences in risk aversion regarding the stated beliefs and decisions in games we used the Holt-Laury lottery task [53]. This is an incentive-compatible task where participants choose 10 times between two different lotteries that gradually vary the combination of probabilities and monetary outcomes in order to measure each individual's degree of risk aversion.\n\n\nPsychometric measures\nIndividual differences in the capacity for ToM and empathy have traditionally been assessed using psychometric tests. We used two tests that both measure empathy and ToM. The two empathy measures are convergent, whereas the two ToM measures complement each other: (i) “Cold” ToM is about inferences regarding the epistemic state of others and refers to the knowledge, beliefs, and intentions that someone else holds; (ii) “hot” ToM is about inferences about others' emotions [54].\nTo measure cold ToM and empathy, we employed the Interpersonal Reactivity Index (IRI),which is the most widely used psychometric test to evaluate both empathy and ToM. The test has been extensively investigated and validated [25]. The German translation of the IRI, the Saarbrücker Persönlichkeits-Fragebogen [55] was used in the experiment. The IRI is a self-report questionnaire using abstract descriptions of social interaction which participants respond to. The modified version we used had four dimensions, each containing four statements. Two dimensions were used for the final analysis: Cold ToM was measured using the perspective-taking scale where participants responded to statements such as, “I try to look at everybody's side of a disagreement before I make a decision.” The capacity for empathy was measured with the scale for “empathic concern”. Participants had to respond to statements such as, “I often have tender, concerned feelings for people less fortunate than I.” Individual differences in the empathic concern scale correlate positively with brain activity associated with empathy [24]. Participants indicated the extent to which a statement described them on a 5-point scale, from does not describe me well to describes me very well. Cronbach's α was .73 for the ToM measure and .80 for the empathy measure, which correspond to the values found by Davis (1980) [25].\nA potential problem with psychometric tests, in particular regarding participants that are primarily motivated to increase their own earnings, is that responses are not incentivized. Specifically, Lyons, Caldwell, and Shultz (2010) point out that this might be problematic when it comes to assessing the relationship between ToM and Machiavellianism [38]. In their experiment, participants high on the Machiavellian scale showed a lack of effort in the ToM task, indicating that they might not have been sufficiently motivated by non-incentivized tests to reveal their ToM level. Note that measuring how accurate a participant's believes are what others will do in the games should be equivalent to how accurate a person is on inferring the epistemic state of someone else. The former is assessed using the QSR and is incentive compatible, the latter is assessed using the psychometric scales for ToM.\nTo measure hot ToM and empathy, we used the Multifaceted Empathy Test (MET), which employs 40 realistic photographs of faces expressing positive or negative emotions as stimuli [56]. The MET was originally developed to measure individual differences of empathy and ToM for people with autism as these have difficulties with abstract descriptions employed for instance by the IRI. The test is also suitable to measure individual differences in a normally developed cohort. It uses pictures as stimuli. Participants answered for each picture three types of questions reflecting three subscales. The subscale emotion recognition measured hot ToM by assessing to what extent participants could correctly infer the emotional state of others as depicted in the photographs. Participants answered the question, “What does this person feel?” by selecting one of four possible options, where only one was correct. A similar test for hot ToM, relying on emotion recognition in faces, was used by Paal and Bereczkei (2007) and Lyons, Caldwell, and Shultz (2010), who found a positive relationship between cooperativeness and the capacity for ToM [37], [38]. To measure empathy, the Multifaceted Empathy Test provides two subscales: The subscale direct empathy asked participants to answer the question, “How strongly do you feel with this person?” The subscale indirect empathy asked the question, “How aroused are you by the picture?” Direct and indirect empathy were measured with a 9-point scale ranging from not at all to a lot. Cronbach's α was .95 for direct empathy, .96 for indirect empathy, and .70 for hot ToM, which correspond to the values found by Dziobek et al. (2007) [56].\n\n\nExperimental procedure\nParticipants did not know about the content of the separate sections of the experiment at the beginning of the experiment but were informed about the subsequent tasks after the completion of each section. However, participants were aware of the content of the individual tasks within each section before submitting their decisions. For example, when reporting their beliefs about the expected behavior in the DG, participants knew that they are also requested to report their beliefs concerning UG behavior.\nHalf of the participants played the DG, then the UG, followed by the psychometric tests; the remaining participants completed the psychometric tests first and then played the games. We did not find any order effects between the psychometric tests and games. We therefore have pooled the data. Subsequently, participants indicated their beliefs about the likely actions of others in the games. This was followed by measuring participants' risk attitude. Questions on the demographic background of each participant ended the experiment. Participants received their earnings in cash immediately after the completion of the experiment.\nWhen analyzing the data we exclude from the dataset four non-native German speakers, who indicated limited ability to fully respond to the verbal descriptions of emotional states used in one of the psychometric tests. Furthermore, the data reveal that 13 participants reported non-monotonic risk preferences. These individuals are excluded from the analysis when analyzing the impact of risk aversion on participants' decisions and belief formation.\n\nResultsWe find that the mean offer is 25 percent of the endowment in the DG and 40 percent of the endowment in the UG. Modal offers in both games are equally high at 44 percent of the endowment. The mean of the minimum acceptance level in the UG is 26 percent of the endowment and the mode is 33 percent of the endowment. These observations suggests that people in our experiment display a substantial degree of pro-social behavior and reflect the common findings in the literature [9].\nDoes a greater capacity for empathy result in higher offers (Hypothesis 1)? We find that the two empathy measures are highly convergent (IRI-Empathy and MET-Direct empathy, r = 0.59, p<.01, for a table showing correlations between all psychometric tests see Table S1) and estimate separate regression models for each empathy measure as depicted in Table 1. The data show no significant relationship between the empathy measures and offers in the DG or UG. Likewise, various alternative behavioral indexes of altruism (e.g., the UG offer minus the UG belief about the minimum acceptance level, the belief about the DG offer by fellow participants minus the DG offer) do not uncover any statistically significant relation.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Determinants of the Dictator and Ultimatum Game offers – OLS regression.doi:10.1371/journal.pone.0092844.t001We find that individual differences in Cold-ToM as measured by the Interpersonal Reactivity Index do weakly correlate with the DG offers. This finding is in line with previous studies on non-strategic social tasks that have found a positive relationship between pro-social behavior and ToM [37], [38]. A potential explanation suggested by the previous studies is that the origins of human social intelligence lie in the need for cooperation. In this case, people who aim to be fair also need to have a high functioning ToM. At the same time, the lack of monetary incentives in both studies might have not provided an adequate test-bed for assessing the role of ToM among selfish individuals. Notably, we find that risk aversion as measured by the Holt-Laury lottery task significantly and positively influences UG offers, suggesting that strategic considerations in the UG are moderated by individual differences in risk preferences.\nAn important question from both a behavioral and a methodological perspective is whether greater capacity for ToM elicited using the psychometric scales relates to higher accuracy in the stated beliefs (Hypothesis 2). The results in Table 2 are surprising. In particular, cold ToM as measured through the psychometric scales does not relate to ToM as measured by the accuracy of stated beliefs. At the same time, while the accuracy of beliefs can in principle be influenced by differences in risk attitudes, we find no support for this conjecture. Correlation between the accuracy of beliefs and risk aversion is statistically insignificant for all three measures of accuracy (DG and UG offers, minimum acceptance level in the UG: r<.22, p>.12).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  The relationship between ToM and accuracy of beliefs in games.doi:10.1371/journal.pone.0092844.t002A possible explanation for the lack of a relationship between the accuracy of beliefs and psychometric ToM measures is that a neutrally framed laboratory experiment is a context in which many of the cues that psychometric tests rely on are stripped away. Even the IRI measure of ToM which is based on abstract description of social interaction likely does not relate well to the simple set-up of games. Yet, the present laboratory setting provides a very specific measure of how good a participant is in judging the action of others which closely relates to ToM: participants stated what they belief others do in the games and were rewarded for their accuracy. The resulting score of accuracy of beliefs then is a laboratory specific measure in how far ToM might inform action.\nTo test our third hypothesis that fair participants have more accurate beliefs than selfish participants, we use a mean split of the DG offers to distinguish between fair and selfish participants. Alternative definitions of selfishness (e.g., median split or offering nothing) yield qualitatively very similar results to those obtained with the mean split applied here. Note that we do not find evidence to support the conjecture that participants' risk aversion is associated with the dispersion of reported beliefs. This result is robust to different measures of statistical dispersion (variance and kurtosis).\nSimilarly, behavior in the three belief elicitation tasks is not affected by risk aversion. To address the issue of potential risk hedging between the three belief elicitation tasks we compute various measures for statistical dispersion (variance and kurtosis) and test whether there are systematic differences in reported distributions between the tasks. Our results show that the measures of statistical dispersion remain fairly stable between the tasks. Correlation coefficients between the variances of distributions are as follows: DG and UG-Proposer: 0.52; DG and UG-responder: 0.57; UG-Proposer and UG-responder: 0.60. Correlation coefficients between the kurtoses of distributions are as follows: DG and UG-Proposer: 0.36; DG and UG-responder 0.38; UG-Proposer and UG-responder: 0.45. The used data set excludes four non-native German speakers and 13 individuals who reported non-monotonic rick preferences in our test for risk aversion. In addition, we exclude two individuals who reported uniform belief distributions with a variance of 0 and undefined kurtosis. See Table S2 for the complete table and a comparison between fair and selfish participant (Table S3) where the data reveals no significant difference. This leads us to conclude that the participants do not systematically balance their reported beliefs between these tasks. It should be acknowledged, however, that this does not entirely rule out the possibility that some individuals might have been hedging at the individual level.\nThe accuracy of beliefs is, as described above in the Behavioral measures section, measured by participants' ability to assess the behavior of fellow participants. The scores for each of the three tasks and test statistics are summarized in Table 3. In support of our third hypothesis, comparing beliefs of fair and selfish participants, we find that fair participants evince a higher accuracy of beliefs about offers made in the DG and UG compared to selfish participants. However, note that there is no difference between fair and selfish participants in the accuracy of beliefs concerning the minimum acceptance level in the UG. Being able to estimate this well would be of particular relevance for selfish participants who seek a high payoff.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 3.  Mean accuracy of beliefs across all three tasks.doi:10.1371/journal.pone.0092844.t003Do individuals capitalize on their accuracy of beliefs in the UG (Hypothesis 4)? To estimate the expected monetary earnings for each participant from the UG, we first calculate how often a participant's offer in the UG is accepted by comparing a participant's UG offer with each of the 115 decisions about the minimum acceptance level made by the responders. We label this value a participant's expected acceptance rate. By multiplying the expected acceptance rate with the UG offer, we compute expected monetary earnings for each participant.\nModels 1 and 2 in Table 4 show how selfish and fair participants employ their accuracy of beliefs in the UG. We find that the accuracy of beliefs is strongly associated with larger expected earnings among selfish individuals, whereas there in no such relationship among fair individuals. The result is robust to controlling for risk aversion and demographic variables. The finding suggests that selfish participants effectively utilize ToM in service of their own material goals. In other words, the higher their accuracy of beliefs, the higher their expected earnings.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 4.  Determinants of expected earnings in the UG among selfish and fair individuals - OLS regression.doi:10.1371/journal.pone.0092844.t004Individual differences in empathy and Theory of Mind (ToM) have been hypothesized to underlie differences in social preferences as observed in neutrally framed laboratory games. The Dictator (DG) and Ultimatum Game (UG), feature particularly prominently as measures to elicit such preferences. Indeed, as Kirman and Teschl (2010, p. 311), state “it would, of course, be very convenient if individuals had a certain fixed level of empathy which was independent of the context in which they found themselves in” [15]. Individual differences in empathy as measured by psychometric scales have been found in a number of instances where pro-social behavior is observed, such as volunteering, donating and helping someone in need. These observations lend support to the empathy-altruism hypothesis [19]. Neuroscientific evidence also points in this direction: Empathy-related brain activity is observed in participants when another person who had previously acted in a fair manner receive a painful stimulus [26]. Here we show that contrary to what has been hypothesized in the literature [4], [15], [16], [27], [28] in neutrally framed games individual differences in empathy do not correlate with pro-social behavior. However, we do not claim that these findings necessarily extend to socially framed conditions.An important element that the above-cited examples have in common highlighting the relationship between empathy and pro-social behaviour is that they resemble situations where another person can be perceived as needy. Indeed, moving from a neutral context to portraying someone in need increases the pro-social behavior of participants [22]. However, this still leaves open why social preferences are observed in neutrally framed laboratory games.A different route linking social preferences to individual differences has been suggested in the research on ToM. Specifically, cold ToM is defined as the capacity to make inferences regarding the epistemic state of others. Forming beliefs what others are likely to do reflects exactly this capacity. Intriguingly, accuracy of beliefs and the psychometric measures specifically for the capacity for cold ToM are not correlated. A likely reason for this is that the psychometric ToM measures are not able to capture the cues that people respond to in the games. However, ToM as revealed by the accuracy of beliefs strongly relates to the games. The literature suggests two different functions, i) ToM can be applied to pursue one's own material payoff; ii) ToM can facilitate pro-social behavior. We show beliefs in the games have the two functions that the literature ascribes to ToM. Fair participants have more accurate beliefs about the offers made in the DG and the UG compared to selfish participants. However, in the context where it is functional for selfish participants to have accurate beliefs, such as in the role of proposer in the UG, the difference between fair and selfish participants vanishes. In fact, accuracy of beliefs about the responder's minimum acceptance level positively affects earnings for selfish but not fair participants. This suggests individual differences in how accurately people can predict the behavior of others, which conventionally is understood as a feature of the capacity for ToM, play out differently depending on the social preferences that people harbor. Yet, where do these differences originate from that lead people to pursue diverging objectives?A potential source are personality traits which, akin to individual differences in capacities, are measured using psychometric scales, are fairly stable in an adult, and are considered an important factor in shaping decisions [57]. Using the Big Five [58], the most widely used personality measure, out of the five traits only extraversion was found to have a weak positive relation with offers in the DG [59]. However, using the Myers-Briggs Type Indicator [60] offers in DG and UG did not correlate with extraversion, introversion, or individual differences in perception or judgment [61]. Using the same psychometric scale, a further study finds that extraverts have a somewhat lower minimum acceptance level [62]. Taken together, the research on personality traits suggests that there is some conflicting evidence regarding a connection with social preferences.Considering the psychometric scales for ToM applied here we find a weak positive correlation between cold ToM, which tests for inferences with regard to the epistemic state of others, and offers in the DG. We find no correlation to hot ToM, which relates to inferences about others' emotions [54]. A possible explanation for this is that social emotions, induced, for instance, by the neediness of someone involved, did not feature in our neutral experiments. The weak positive correlation between DG offers and cold ToM has also been found in other non-strategic settings and has been suggested to be due to the fact that ToM was employed by participants who acted more fairly to match social expectations [37], [38]. Concerning a strategic setting, a positive correlation between ToM and offers has only been found in studies with children [39], [40]. In contrast to our study, these games were not played anonymously but an experimenter was paired with each child and was present at all times. Such a design can invite a demand effect where participants want to appear fair. The importance of social expectations and subtle demand effects on behavior as observed in the DG and UG has been pointed out by a number of studies. For instance, when proposers in the DG are given the option to opt out by accepting a payoff that is lower than what they could gain in the game, about 40 percent of participants opt out. This leaves the receiver without payoff and ensures that she never knows that a DG has been played [47], [63]. Such an effect is present in a number of studies on the DG and UG which find that greater anonymity or the possibilities to obscure the role of the proposer decreases offers [64]–[67]. Future research is needed to specify the exact relationship between demand effect and individual differences in ToM.Expectations or beliefs also feature in studies that suggest that behavior in neutrally framed DGs and UGs can substantially vary with the environment people live in. When observing the action of others, people form beliefs about what others are likely to do and commonly adopt similar behavior [68]. Comparing different student populations shows that the longer a student has studied economics the lower are offers in the DG whereas those studying social work maintain relatively high offers in the DG throughout their studies [69]. Manipulating the beliefs what others do can cause large shifts in DG and UG offers and the minimum acceptance level in accordance to what the perceived majority choice in the group is [68], [70], [71]. This suggests that beliefs have an important role in games which are commonly used to assess people's social preferences. Similarly, we show that fair and selfish participants differ in their beliefs about what others do and also employ their skills about forecasting what others do differently. Future research would need to address whether a possible important source of observed individual differences in social preferences as observed with a conventional pool of participants can be due to the different environments that people come from when attending laboratory experiments."
        },
        "10.1371/journal.pmed.0030107": {
            "author_display": [
                "Jonathan B Savitz",
                "Cinda-Lee Cupido",
                "Rajkumar S Ramesar"
            ],
            "title_display": "Trends in Suicidology: Personality as an Endophenotype for Molecular Genetic Investigations",
            "abstract": [
                "\n        In studying the genetics of suicide, should personality be used as an endophenotype (an intermediate trait lying somewhere on the developmental pathway from genes to phenotype)?\n      "
            ],
            "publication_date": "2006-05-09T00:00:00Z",
            "article_type": "Research in Translation",
            "journal": "PLoS Medicine",
            "citations": 19,
            "views": 9513,
            "shares": 1,
            "bookmarks": 13,
            "url": "http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0030107",
            "pdf": "http://www.plosmedicine.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pmed.0030107&representation=PDF",
            "fulltext": "Psychopathology, especially depression, is the most important risk factor for suicidal behaviour [1,\n\t\t\t\t2] with between 25% and 40% of depressed patients attempting suicide [3] and about 3.4% completing suicide eventually [4]. Given recent figures suggesting that the lifetime prevalence of a major depressive episode among the US population is 32.6–35.1 million [5], it is no surprise that suicide ranks among the top ten causes of death in many other countries [6].\n\t\t\t\nUnderstanding the aetiology of a significant public health issue such as suicide is important but difficult because of its complex and multifactorial origins. Although most suicidal behaviour occurs within the context of a mood disorder, most depressed individuals never attempt suicide. Furthermore, no linear relationship between the severity of the depressive episode and the likelihood of suicide has been forthcoming [7], highlighting the importance of other factors in addition to psychiatric illness. These factors include substance abuse or alcoholism, a head injury, a dysfunctional family or childhood abuse [8], high rates of gun ownership [9], smoking [10], socioeconomic adversity [11], and personality factors [12].\n\t\t\t\nGenetic factors may also be very important [13,\n\t\t\t\t14]. Marusic and Farmer [15] argue that the variation in the suicide rate across European countries (7–43 per 100,000 inhabitants per year) cannot be explained by sociocultural factors alone and is probably due to shared genetic vulnerability. A case in point is the high suicide rate in Hungary and Finland, two populations with a common genetic origin but with divergent cultural and political trajectories [15]. \n\t\t\t\t\n\t\t\t\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                 Colours of Depression(Illustration: Raj Ramesar)\ndoi:10.1371/journal.pmed.0030107.g001At the family level, the risk of suicide is higher in individuals with a family history of suicide [16,\n\t\t\t\t17], and the suicide rate of adolescents is highly correlated with the suicide rate among their relatives [18]. Even studies that have controlled for levels of psychopathology have shown that relatives of suicide completers and attempters are at an increased risk for suicidal behaviour [19,\n\t\t\t\t20]. Twin studies indicate that this familial clustering of suicidal behaviour has a partly genetic basis with heritability estimates of 17%–55% for suicidal behaviour [21,\n\t\t\t\t22] and 20% for suicide [23] reported. The only adoption study we are aware of suggested that as far as suicidal behaviour is concerned, adoptees resemble their biological parents more than the adoptive family [24].\n\t\t\t\nThese data have catalysed the search for genes that predispose to suicidal behaviour, with more than 100 studies now published [25]. Post-mortem studies of people who committed suicide have led to the general consensus that a disturbance of the serotonergic system is associated with suicidal behaviour. Therefore, it is no surprise that most of the genes implicated in suicidal behaviour—the serotonin transporter (SERT), tryptophan hydroxylase (TPH), monoamine oxidase A (MAO-A), and the serotonin receptors, 5-HTR1A, 5-HTR2A, and 5-HTR1B—modulate central serotonergic function (see      Table 1). As is the case with most complex traits, however, success tends to plateau at a point where good candidate genes are identified but conclusive causal inferences remain elusive because of replication failures.    \n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Association Analyses of Genes Predisposing to Suicide or Suicidal Behaviourdoi:10.1371/journal.pmed.0030107.t001The unequivocal identification of genes related to psychiatric disorder is retarded by a complex interplay of latent environmental influences or gene–environment interactions; genetically and phenotypically heterogeneous samples; the possible effects of numerous loci of small effect size; and the difficulty of adequately correcting for multiple testing. Faced with these frustrations, the use of endophenotypes as an aid to molecular genetic investigations has become almost de rigueur. In the case of suicide, a number of researchers have advocated the use of personality traits as endophenotypes [12,\n\t\t\t\t26].\n\t\t\t\nWhat Is an Endophenotype?An endophenotype is an intermediate trait that lies somewhere on the developmental pathway from genes to phenotype [27]. If suicide is the phenotype of interest—the final product of different genetic and environmental factors—then the endophenotype is a more elementary trait that is tightly correlated with suicide. The genetic architecture of the endophenotype is assumed to echo its relative phenotypic simplicity, presenting a more tractable target for geneticists. An understanding of the molecular basis of the endophenotype should theoretically be the first step towards the larger prize: uncovering the molecular basis of the phenotype itself.    \nGottesman and Gould [27] assert that for a biological marker to be classified as a bona fide endophenotype, it must meet the following conditions: 1) be associated with the illness in the relevant population; 2) be largely state-independent, manifesting in the individual during both periods of health and illness; 3) be heritable; 4) within families, the endophenotype and illness should co-segregate; 5) if found in affected individuals should also be found in nonaffected family members at a higher rate than in the general population.    \nIn the following sections we use Gottesman and Gould's [27] five criteria above to evaluate the latest trend in suicide research: the merits of using an endophenotype such as personality to identify suicide susceptibility genes.    \nAre Specific Personality Traits Associated with Suicide Behaviour?Four main constellations of personality characteristics are associated with suicidal behaviour: impulsivity, hostility–aggression, introversion, and anxiety-neuroticism.\nA rich body of data details the association between suicide attempts and impulsivity. Impulsivity has been shown to be a risk factor for suicidal behaviour in a variety of adults [7,\n\t\t\t\t8,\n\t\t\t\t28–30] and adolescents [11,\n\t\t\t\t31,\n\t\t\t\t32] with psychiatric illness, in a forensic psychiatric population [33], and in the general population [34].\n\t\t\t\nFreud regarded suicide as an aggressive act, a view borne out to some extent by modern data. Simon et al. [35] found that involvement in physical fights was associated with violent suicide attempts, and, according to Appleby et al. [36], people who committed suicide were more likely to have been arrested in the previous six months. Cavanagh et al. [37] confirmed that a criminal record is a risk factor for subsequent suicide. Fulwiler et al. [38] detected a higher rate of suicide attempts in a group of violent compared with nonviolent patients with psychiatric illness, and in South Africa conduct disorder was found to be the most frequently made diagnosis in suicidal adolescents [39].\n\t\t\t\nNegative affect or anxiety is most commonly measured in the form of neuroticism from the Eysenck Personality Questionnaire (EPQ) or the Five Factor Model (FFM) inventory (see Glossary). Neuroticism is a higher order trait that measures a predisposition to experience anger, anxiety, depression, guilt, and other negative emotions or cognitions. A significant relationship between neuroticism and suicidal ideation has been regularly demonstrated [11,\n\t\t\t\t22,\n\t\t\t\t40,\n\t\t\t\t41], and the same holds true for neuroticism and attempted suicide [28,\n\t\t\t\t42–46]. High levels of anxiety or negative affectivity have been equally implicated in completed suicide [47–50].\n\t\t\t\nA number of studies have reported introversion to be associated with suicidal thoughts and behaviour [42,\n\t\t\t\t51–53]. In two psychological autopsies of completed suicides, Duberstein et al. [50] detected lower levels of extraversion in their cohort, and Maser et al. [53] found that people who committed suicide were shyer and less optimistic than other affectively ill patients. Extroversion has been shown to predict levels of social support [54,\n\t\t\t\t55], perhaps explaining the association between introversion and suicide.    \nSuicide attempts may occur in the context of an acute psychiatric crisis, biasing self-report measures of personality. One way of mitigating this problem is to use longitudinal data. In a study of 921 three-year-old children, Caspi et al. [52] found that impulsive toddlers were at an increased risk for suicide attempts at age 21. Fergusson et al. [11] followed a birth cohort for 21 years and similarly reported that high neuroticism and novelty-seeking were risk factors for suicidal behaviour. Similarly, a prospective study of affectively ill patients by Maser et al. [53] identified impulsivity as a key risk factor for both completed and attempted suicide, and Epstein et al. [48] found an association between high impulsivity and completed suicide.    Epstein et al. [48] also found that physicians who committed suicide had shown increased levels of hostility. Berglund [56] detected more irritability or aggression in people with alcoholic dependence who completed suicide. Among children who received treatment at a child guidance clinic, aggressive feelings and acts were predictive of later suicide [57], while among adult men both spontaneous and reactive aggression scores were elevated in later suicides [58].\n\t\t\tWhile depressed affect certainly has the potential to bias personality data, there is enough evidence from longitudinal studies to support the hypothesis that certain personality traits, in particular hostility–aggression and impulsiveness, predispose to suicidal acts. In the next section, we examine the genetic basis of some of these personality traits."
        },
        "10.1371/journal.pone.0095677": {
            "author_display": [
                "Fanny Weytens",
                "Olivier Luminet",
                "Lesley L. Verhofstadt",
                "Moïra Mikolajczak"
            ],
            "title_display": "An Integrative Theory-Driven Positive Emotion Regulation Intervention",
            "abstract": [
                "\nOver the past fifteen years, positive psychology research has validated a set of happiness enhancing techniques. These techniques are relatively simple exercises that allow happiness seekers to mimic thoughts and behavior of naturally happy people, in order to increase their level of well-being. Because research has shown that the joint use of these exercises increases their effects, practitioners who want to help happiness seekers need validated interventions that combine several of these techniques. To meet this need, we have developed and tested an integrative intervention (Positive Emotion Regulation program – PER program) incorporating a number of validated techniques structured around a theoretical model: the Process Model of Positive Emotion Regulation. To test the effectiveness of this program and to identify its added value relative to existing interventions, 113 undergraduate students were randomly assigned to a 6-week positive emotion regulation pilot program, a loving-kindness meditation training program, or a wait-list control group. Results indicate that fewer participants dropped out from the PER program than from the Loving-Kindness Meditation training. Furthermore, subjects in the PER group showed a significant increase in subjective well-being and life satisfaction and a significant decrease in depression and physical symptoms when compared to controls. Our results suggest that the Process Model of Positive Emotion Regulation can be an effective option to organize and deliver positive integrative interventions.\n"
            ],
            "publication_date": "2014-04-23T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 775,
            "shares": 11,
            "bookmarks": 6,
            "url": "http://www.plosone.org/article/info:doi/10.1371/journal.pone.0095677",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0095677&representation=PDF",
            "fulltext": "IntroductionUntil recently, research has focused on factors that hinder well-being, with the objective of identifying means by which to ease suffering [1]. Over the last fifteen years, however, many psychology researchers have insisted on the necessity to extend these studies by analyzing the individual differences and the processes that contribute to well-being [2]. One objective of this latter research is to highlight individuals' available means to boost their subjective happiness. According to Duckworth, Steen, and Seligman [3], these means are threefold: increasing pleasure, boosting engagement, and finding meaning in life. As pleasure is the fruit of experiencing positive emotions, increasing their frequency, intensity, or duration is one of the avenues through which individuals can achieve higher levels of happiness [4]. Indeed, a growing number of cross-sectional, longitudinal, and experimental studies show that positive emotions play a key role in individuals' evaluation of their level of well-being and contribute to numerous related benefits (for a review, see [5]). For example, at the cognitive level, experimental studies have shown that inducing positive emotions broadens individuals' scope of attention [6] and increases their creativity [7]. At the somatic level, longitudinal studies have shown that positive emotions are associated with increased longevity [5], [8]–[11], which is not surprising, as experience of positive affect was associated with better immunity in cross-sectional [12], [13] and experimental studies [14], [15]. At the social level, positive affect is related to better interpersonal relationships [16]–[20] and generally increases altruism [21], [22]. As a whole, these studies point to the importance of positive affect at both the individual and the societal levels and highlight the importance of developing interventions aimed at increasing positive affect and, consequently, well-being.\nTo provide clinicians with practical and efficient interventions, researchers have created and validated a set of techniques that enable individuals to increase their positive emotions and their levels of well-being (for a review, see [23]). In this area of research, techniques were frequently tested one at a time and presented to participants as one-shot exercises. This way of proceeding enables researchers to test the efficacy of each exercise individually. However, recent research [24] has shown that this way of testing interventions has low ecological validity because, in naturalistic settings, happiness seekers often practice (and wish to practice) multiple exercises at the same time. Furthermore, the use of multiple techniques in an intervention often produces better results than when a single technique is used alone [24].\nSeveral authors have tested interventions aimed at consolidating multiple techniques, provided as either individual or group sessions (e.g. [25]–[27]). Criteria used to determine a technique's inclusion in those programs were not always clearly defined or explicitly reported. In the 1980s, Fordyce [25], [26] gathered a set of exercises aimed at teaching happiness seekers to mimic typical behaviors and thinking styles of happy people. The exercises included in Fordyce's program encouraged participants to: (a) keep busy and be more active; (b) spend more time socializing; (c) be productive at meaningful work; (d) get better organized and plan things out; (e) stop worrying; (f) lower expectations and aspirations; (g) develop positive, optimistic thinking; (h) become present-oriented; (i) work on a healthy personality; (j) develop an outgoing, social personality; (k) be yourself; (1) eliminate negative feelings and problems; (m) see close relationships as the number one source of happiness; (n) put happiness as your most important priority ([26], p.484). In a more recent attempt, Seligman, Rashid, and Parks [27] created a program gathering the “best-documented exercises” in the literature ([27], p. 776) targeting at least one of the three components of “happiness,” as defined by Seligman (i.e., positive emotions, engagement, and meaning) [4]. Exercises integrated in Group Positive Psychotherapy were the following: (a) using your strengths; (b) three good things/blessings; (c) obituary/biography; (d) gratitude visit; (e) active/constructive responding; and (f) savoring ([27], p. 776).\nAlthough these studies provided essential information about the potential of integrative interventions, this kind of program would greatly benefit from a strong theoretical background which should (1) at a theoretical level, allow a better understanding of the mechanisms underlying happiness enhancement and (2) at a more practical level, help practitioners identify their clients' needs, weaknesses, and the most appropriate technique(s) to prescribe them [28]. As no happiness enhancing program that is both integrative and theory-driven currently exists, we decided to create such a program and to evaluate its effects on a set of mental and physical health variables. To this end, we developed an integrative program structured around a conceptual framework, which enabled us (1) to select a set of techniques involving different underlying processes and (2) to organize them in a coherent manner.\nThe model proposed by Quoidbach et al. [28], [29] appears to fulfill these requirements. As outlined below, Quoidbach et al. modeled positive emotion regulation strategies with reference to a well-known model in the domain of negative emotion regulation: Gross' Process Model of Emotion Regulation [30], [31]. This model enables the integration of strategies involving clearly differentiated emotion regulation processes and the reconciliation of studies on both positive and negative emotion regulation.\n\nThe Process Model of Emotion Regulation applied to Positive Emotions\nThe Process Model of Emotion Regulation [30], [31] provides a theoretical structure for analyzing emotion regulation processes. The model highlights five families of emotion regulation strategies. Initially created to understand and organize negative emotion regulation strategies, it was later adapted by Quoidbach et al. to apply to the up-regulation of positive emotions (see The Process Model of Positive Emotion Regulation, Quoidbach et al. [28], [29]). The first family of strategies that can be used to influence emotion is situation selection. Situation selection involves “choosing or avoiding some activities, people, or places in order to regulate emotions” [32]. To illustrate this strategy, we can imagine a grandfather, Joseph, deciding to visit his children and grandchildren on Sunday afternoon because he knows that being in touch with his family gives him a lot of pleasure.\nThe second family of strategies highlighted by Gross, situation modification, includes techniques that allow an individual to change the situation s/he is facing (or that s/he has planned) in order to change its emotional impact. To continue our example, in order to make Sunday afternoon fun and enjoyable, Joseph may decide to bring his grandchildren’ favorite board game that they all like to play together.\nAttentional deployment includes strategies that involve altering how an individual feels by selecting the information to which s/he attends. During Sunday afternoon, rather than getting irritated by the noise of the neighbor's lawnmower, Joseph may fully focus his attention on the fun game he is playing with his grandchildren. Along the same line, Joseph may concentrate his attention on the taste of the wonderful cheesecake his daughter prepared and deeply savor it to get the most pleasure from it.\nCognitive change refers to changing the way an individual thinks in order to change the way he/she feels, either by changing how he/she thinks about the situation itself or about his/her capacity to manage its demands. For instance, rather than taking the moment for granted, Joseph may interpret his presence among his family that afternoon as a gift of life.\nFinally, the last family of strategies proposed by Gross is response modulation. This family of strategies involves techniques to alter bodily manifestations of emotion (e.g. physiological, behavioral). In order to increase his excitement and pleasure during the time he is spending with his family, Joseph may decide to smile and laugh with his loved ones and to express his affection for them.\nGiven that generating a high level of well-being implies reminiscing about past positive events, savoring the present, and anticipating positive future events [33], [34], [35], Quoidbach et al. [28], [29] divided Gross's model into three moments of action: before, during, and after the positive emotions generating event (for a detailed description of the model, see [28], [29]). Thus, Quoidbach et al. [28], [29] do not propose five large families of strategies, but rather 15 distinct strategies that an individual can use to regulate his/her emotions, corresponding to the 15 sections of the Process Model of Positive Emotion Regulation (see Table 1, see italics).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Example of positive emotion regulation techniques in the Positive Emotion Regulation Model proposed by Quoidbach & al. [28], [29] for a specific positive event.doi:10.1371/journal.pone.0095677.t001The use of such a framework makes it possible to classify the different types of techniques that individuals may use to maximize their positive emotions. Table 1 illustrates a positive event and the means an individual may use to increase the intensity, frequency, and duration of the positive emotions that s/he experiences. Each action (or technique) that an individual can implement in order to up-regulate his/her positive emotions with respect to a given positive event finds its place in one of the model's sections. As this model enables the integration and organization of happiness enhancing techniques, it offers an interesting framework with which to elaborate our intervention.\n\nThe Current StudyThe aim of the present study was twofold: (1) to create an integrative intervention (i.e. including multiples techniques) on the basis of a theoretical model and (2) to evaluate its impact on psychological and physical well-being variables.\n\nFirst Objective: Creation of the Positive Emotion Regulation (PER) Program\nTo achieve our first objective, we carried out an extensive literature review on the basis of the model proposed by Quoidbach et al. [28], [29] in order to identify a set of validated happiness enhancing techniques that we could include in our intervention. For each technique, we identified at which moment of action it was applicable (i.e., before, during, and/or after an event). We then evaluated the family of strategies to which each of these techniques corresponded. By combining this information, we were able to identify the strategy (or section) of the model to which each technique corresponded. This theoretical framework provided us with a structure in which to organize these validated techniques within an integrative intervention seeking to regulate positive emotions and to increase individuals' well-being.\nThis selection and classification process led us to include the following techniques in our intervention (see Table 2 for a detailed overview of the PER program, the extensive manual is available upon request from the first author). Techniques in the situation selection section aim at teaching people the kind of events they should plan (see B1 in Table 2, corresponding to “Before an event”), experience (see D1 in Table 2, “During an event”), or remember (see A1 in Table 2, “After an event”) in order to increase their level of well-being. These techniques included the following: playing sports [36], being altruistic [37], socializing [38], taking care of one's needs [39], goal setting [40], monitoring progress toward a goal [41], and identifying memories to preserve [42] or to recall [43].\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Week-by-Week Summary of the Positive Emotional Regulation Program.doi:10.1371/journal.pone.0095677.t002Techniques in the situation modification section aim to modify planned (B2) or current (D2) activities, or their memories of these activities (A2), in order to optimize their well-being potential. Techniques such as time management and planning [44], job crafting [45], optimizing the end of an event [46], finding the flow [47], and showcasing souvenirs [48] were included in this category.\nAttentional deployment techniques help individuals to project themselves into a pleasant future (B3), to be fully present in an agreeable moment (D3), or to relive happy or meaningful moments of their lives (A3). Exercises such as imagining your best future self [49], mental time traveling to a pleasant future [50], savoring [51], and mental time traveling into past positive events [52] are examples of techniques in this category.\nTechniques identified as cognitive change strategies aim at helping individuals to interpret future (B4), current (D4), and past events (A4) in a positive way and to make individuals aware of the good things in their lives. This category of techniques includes optimistic thinking [53], reducing one's expectations [54], counting one's blessings [55], counterfactual thinking [56], writing a gratitude letter [57], and what-went-well exercises [58].\nFinally, in the response modulation category, we gathered techniques used to facilitate positive emotion expression for future (B1), current (D5), and past events (A5). Included exercises are sharing excitement for upcoming events [59], smiling [60], sharing good news with others [61], capitalizing [62], and gratitude visits [63].\n\n\nSecond Objective: Evaluation of the PER Program's Effectiveness\nOur second objective was to quantify the effectiveness of the developed program. To achieve this, we sought to compare the results of the participants in the PER program to those of individuals who didn’t participate in any intervention program (control group on a wait list). To identify the potential added value of our PER program to existing methods, we sought to compare the PER results to those of an intervention that (1) also seeks to increase positive emotions and well-being, (2) has been previously validated, and (3) can be administered in six two- to three-hour weekly sessions, followed by exercises to be completed at home. A literature review enabled us to identify the loving-kindness meditation (LKM), “a meditation technique used to increase feelings of warmth and caring for self and others” [64], as a technique that fulfilled all these criteria. Fredrickson, Cohn, Coffey, Pek, and Finkel [65] have shown that practicing this type of meditation significantly increases positive emotions as well as life satisfaction, while decreasing depression symptoms. The other types of validated interventions proved unsatisfactory as a control group, as it seemed inappropriate, and possibly counter productive, to organize an intervention around, for instance, an exercise such as “Counting One's Blessings” (e.g. [55]) and to ask participants to practice only this exercise during the six weeks of intervention (two hours/week in a group session, in addition to the follow-up exercises to be done at home). As our objective was to compare two interventions with a common aim, similar pace, and equivalent duration, LKM seemed to be the most appropriate validated option to serve as a point of comparison to adequately measure the effectiveness of the PER program.\nThus, by comparing these three conditions (PER program, LKM, and the control) we tested the hypothesis that, compared to the control group, the two interventions would enhance subjective happiness and life satisfaction and diminish depression symptoms, the frequency of physical symptoms, as well as perceived stress, as measured through self-report questionnaires.\n\n\nMethods\nEthics Statement.This study was approved by the Ethics Committee of the Psychology Department at the Université catholique de Louvain, Belgium, and was conducted in accordance with the Declaration of Helsinki.\n\nParticipants and Procedure.One hundred and thirteen undergraduate students (88 women; mean age  =  22.29, SD  =  2.49) took part in the study on a voluntary basis without any financial or course credit compensation (in order to guarantee intrinsic motivation to participate). They responded to an advertisement referring to “a six-week happiness enhancing training”. After providing written informed consent, they were randomly assigned to one of the three groups: the Positive Emotion Regulation (PER) intervention (N = 36; 32 women), the Loving-Kindness Meditation (LKM) intervention (N = 35; 27 women), or the waiting list (control group; N = 42; 29 women). The study advertisement was intentionally vague to keep participants blind to the multiple conditions and to allow a random assignment between the three.\nMeasures were taken one week before the intervention (T1) and four weeks after (T2), to evaluate medium term results. Among the participants who followed the complete 6-week training program (PER, N = 32; LKM, N = 20, attrition discussed below), four in the PER condition and four in the LKM condition failed to complete T2 measurements; they were, therefore, removed from the sample. In the control condition, seven participants did not complete the T2 assessment; they were also excluded from analyses. Analyses of mental health and physical variables presented below are, therefore, based on a final sample of 79 participants broken down as follows: PER, N = 28 (24 women, mean age = 22.5, SD = 3.06); LKM, N = 16 (13 women, mean age = 22.25, SD = 1.7); and control group, N = 35 (24 women, mean age = 22.14, SD = 2.35). Note that there were no significant differences on baseline measures between participants who completed T2 measures and those who did not (ps ranging from.086 to.897).\n\nTrainings format.The Positive Emotion Regulation (PER) and the Loving-Kindness Meditation (LKM) interventions were developed according to the same format (length, pace, and organization) and differed only in terms of content. The participants of each of these two conditions were divided into three small groups comprising ten to 14 individuals in order to establish effective group dynamic conducive to learning. For six consecutive weeks, both experimental groups participated in once-weekly, two-hour training sessions. LKM and PER group sessions sought to give participants a theoretical framework to understand the importance of the presented techniques on the one hand and, on the other, to enable them to experiment with and to practice certain exercises during the session before implementing them at home. Our program was therefore structured around theoretical and experiential methods [66] in order to maximize learning transfer (see [67], [68] for reviews and transfer guidelines).\n\nConditions.Loving-kindness meditation: This training was a French adaptation of the training proposed by Fredrickson et al. [65] and was created on the basis of the material we received from S. Finkel, the LKM trainer in this study. The only difference was in relation to the duration of the “on-site” sessions that were two hours long (as opposed to 60 minutes in the session proposed by Fredrickson et al. [65]), so that the duration of the LKM intervention could be comparable to that of the PER program. As Sin and Lyubomirsky [23] have shown that the duration of an intervention using happiness enhancing techniques is positively correlated to its effectiveness, we considered this modification to be essential. The longer session duration was achieved by adding basic meditation exercises as well as by increasing the duration of the “on-site” LKM exercises and of their debriefing. As in the Fredrickson et al. study [65], the LKM group participants were asked to practice 20 minutes of meditation at home, at least five days a week. Participants were strongly encouraged to continue their LKM practice after the end of the program. LKM sessions were led by two highly experienced mindfulness trainers (Christophe Dierickx and Gwenaëlle Rivez) that were trained in LKM through multiple readings, significant amounts of practice, and supervisions.\nPositive Emotion Regulation: As we explained in the introduction, the intervention was structured around the Process Model of Positive Emotion Regulation. During the group sessions, each section of the model was set out and put into practice through validated techniques (see Table 2 for examples). Research findings were presented to highlight the pertinence of the strategies and techniques proposed. The presentation of the strategies relative to the different sections of the models was divided across six sessions. As in the LKM condition, PER group participants took part in the exercises proposed during the session and received a list of exercises to carry out at home for the following week (combination of mandatory and self-selected exercises for a duration equivalent to 5 × 20 minutes). At the end of the program, participants were asked to identify exercises that worked the best for them (which is not necessary equal to their preferred ones, [69], see discussion section for more details), and were strongly encouraged to continue them after the end of the intervention. PER trainers (five psychology students and one Ph.D. student, operating in pairs) were a little less experienced group trainers than LKM trainers (which is why they worked in pairs), but were highly knowledgeable about positive emotion regulation topics.\n\nMeasures.Subjective Happiness was measured using the well-validated Subjective Happiness Scale (SHS; [70]). This questionnaire provides a global, subjective assessment of whether the respondent considers himself/herself as a happy or an unhappy person, via four items rated on a 7-point Likert scale. The internal consistency (α) was.91 at T1 and.87 at T2.\nLife Satisfaction was assessed with the Satisfaction with Life Scale (SWLS; [71]). This scale comprises five items (e.g. “So far I have gotten the important things I want in life”) rated on a 7-point scale. The internal consistency was.81 at T1 and.86 at T2.\nDepression was measured via the Beck Depression Inventory (BDI; [72]). In the present study, we used the short version [73], which consists of 13 items rated on a 4-point scale. Respondents were instructed to choose the response that best described how they felt over the last week. The internal consistency of the scale was.82 at T1 and.87 at T2.\nPerceived Stress was evaluated using the Perceived Stress Scale (PSS; [74]). This scale comprises 10 items (e.g. “In the last month, how often have you felt that you were unable to control the important things in your life?”) rated on a 5-point scale (0 = never, 1 = almost never, 2 = sometimes, 3 = fairly often, 4 = very often). The internal consistency was.86 at T1 and.69 at T2.\nSomatic complaints were assessed through a short version of the Pennebaker Inventory of Limbic Languidness (PILL; [75]; short-version [76]). The abbreviated scale consists of a list of the 29 most common physical symptoms (e.g., headache, stomachache, sleep problems, cramps). Participants had to rate on a 5-point scale the frequency with which they experienced each symptoms/sensation during the past month (1  =  never or nearly never, 2 = 1 to 3 times, 3 = every week, 4 = several times a week, 5 = every day). The internal consistency was.86 at T1 and.91 at T2.\n\n\n\nResults\nPreliminary results.Dropout analysis revealed that among the 36 participants assigned to the PER group, four of them left the program before its end (11% of the subjects). Among the 35 participants assigned to the LKM condition, 15 of them withdrew before the end ( =  43%). A Chi-square test revealed that the dropout difference between the two groups was significant, X2 (1, N = 71) = 9.125; p<0.003. No differences between subjects who dropped out and those who did not were found on the variables under study at T1, according to t-test analyses (ps ranging from.486 to.967).\nA one-way analysis of variance (ANOVA) was performed to check for potential group differences at T1 between completers of the three groups (see Table 3 for the values). This analysis indicated that there were no significant baseline differences between groups on any of the following variables: subjective happiness, life satisfaction, depression, perceived stress, somatic complaints, and age (ps ranging from.247 to.935). Difference in sex ratios among the three groups was nonsignificant, according to a Fisher's exact test conducted on the number of remaining males and females after dropout among each group (p = .268).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 3.  Means (and Standard Deviations) for Each Scale and Each Group and Significance of Differences Between Time 1 and Time 2 for PER and control group.doi:10.1371/journal.pone.0095677.t003\nTest of our hypothesis.In order to assess the evolution of participants' scores within each group, we computed change scores by subtracting T1 scores from T2 scores for each participant on each variable. A graphic depiction of mean change scores (expressed in percent) by group can be found in Figure 1.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Mean Change Scores (expressed in percent) between Time 1 and Time 2 for the three groups.ΔSHS  =  Mean Change Score on Subjective Happiness Scale; ΔSWLS  =  Mean Change Score on Satisfaction With Life Scale; ΔBDI  =  Mean Change Score on Beck Depression Inventory; ΔPSS  =  Mean Change Score on Perceived Stress Scale; ΔPILL  =  Mean Change Score on Physical Inventory of Limbic Languidness.\ndoi:10.1371/journal.pone.0095677.g001Separate one-way ANOVAs with one between-subject factor (three groups: PER, LKM, and control) were then performed on change scores in order to compare the three groups. Analyses showed that the mean difference score for SHS tended to be greater in LKM and PER than in the control group. The effect was only marginally significant (p = 0.08) but the power was low (P = 50%). The mean difference score for BDI tended to also be greater in LKM and PER than in the control group. Again, the effect was only marginally significant (p = 0.09) but the power was low (P = 48%). The mean differences for SWLS and for PSS were not statistically different between groups (ps = .24 and 0.11, respectively), but the power of the analyses was again very low (P = 31% and 44% respectively). Finally, the mean difference score for PILL was statistically different between groups (p =  0.018, P = 73%): the control group differed significantly from LKM (p = 0.044) and marginally from PER (p = 0.065).\nAlthough these results are somewhat informative, the very low powers of the analyses are problematic. Statistical power can be interpreted as the probability of finding a significant difference, if it exists, with the present sample sizes and the observed effect sizes. The conclusion that can be drawn from those results is that, for our effects to be statistically significant, we would need a much larger sample size. When power analyses are used to determine the optimal size of a sample before the beginning of a study, standards recommend powers between 80 to 95% [77]. Though the power of our study was far from this standard, this was a pilot study, and as such, we were unable to predict (1) the dropout rate in each group, and therefore the adequate number of participants to initially include in our study, and (2) the average effect size of our intervention, which would have been necessary to compute power analyses before the start of the study.\nThe high dropout rate in the LKM group dramatically reduced the number of observations and, therefore, considerably reduced the power of analyses including all three groups. Under these conditions, it seems hardly possible to highlight results on the PER group, even though the visual analysis of the change scores (see Fig 1) suggests that some interesting results do exist.\nSince the power problems were mainly related to the LKM group, and since our main group of interest was the PER group, we decided to leave the question of LKM efficacy aside, so as to focus on our main question of interest: the efficacy of the PER program. Analyzing those results independently of the LKM seemed to constitute the most relevant option (from both a clinical and statistical point of view) to quantify the effects of the PER intervention, if they exist. In the following pages, we will present the results of analyses comparing the PER and control groups only. The means and standard deviations for each variable at each time point in the PER program and in the control group are shown in Table 3.\nRepeated measures ANOVAs were performed on each measure, with group (PER program, control) as a between-subjects factor, and time (T1, T2) as a within-subjects factor. In each case, we were looking for a significant Time x Group interaction, which would indicate a differential change between the two groups. Analyses confirmed a highly significant group interaction for four of the five scales: SHS, F(1, 59) = 5.52, p<.022, η2partial = .09; SWLS, F(1, 60) = 4.20, p<.045, η2partial = .07; BDI, F(1, 60) = 4.50, p<.038, η2partial = .07; and PILL, F(1,59) = 5.86, p<.021, η2partial = .09. Results for PSS did not reach significance (F(1, 59) = 2.99, p = .089, η2partial = .05).\nAs depicted in Table 3, the breakdown of these interactions revealed that, unlike participants in the control group, who did not show any change on any variable under study between T1 and T2, participants in the PER group showed a significant increase in subjective happiness (SHS), t (26) = −2.94, p>.007, d = .50, and satisfaction with life (SWLS), t (27) = −2.88, p<.008, d = .51. They also showed a significant decrease in depression (BDI), t (27) = 2.73, p<.011, d = .61 and physical symptoms (PILL), t (26) = 3.30, p<.003, d = .51 and a marginal decrease in perceived stress (PSS), t (27) = 1.97, p = .059, d = .38 (see Table 3).\n\n\nDiscussionThe objective of our study was twofold: (1) to construct a theory- and evidence-based integrative intervention aimed at increasing well-being and (2) to test its effectiveness on participants' mental and physical well-being. Based on the Process Model of Positive Emotion Regulation [28], [29], we developed a 12 hour (6 × 2 hr) integrative training program, bringing together a series of theory-based and empirically-validated techniques. The theoretical framework allowed us to organize these different well-being enhancing techniques and to deliver them in a coherent format. In addition, this framework made it possible to integrate techniques using different underlying processes (or strategies).\nIn accordance with our hypothesis, our results indicate that, compared to an inactive control group, the PER group showed a significant increase in subjective happiness and satisfaction with life and a significant decrease in depression symptoms and somatic complaints. There was also a marginal decrease in perceived stress. These results confirm that it is possible to enhance an individual's psychological and physical well-being and that the PER program that we developed is a valid intervention to achieve this goal.\nAnother interesting finding of our study concerns loving-kindness meditation. In our study, the LKM dropout rate (43%) suggests that, although LKM seems to work for those who follow the program until the end, a significant proportion of participants do not adhere to this intervention. It is worth noting that the high dropout rate in the LKM condition is consistent with results reported by Carson et al. [78]. The latter study tested the effects of LKM among chronic pain sufferers and also showed a 42% dropout rate (13 out of 31 subjects) in its LKM group. However, individuals who fully adhered to the intervention until the end of the program benefited from it, as evinced by better psychological adjustment to pain. Several hypotheses can be put forward in order to explain the high dropout in our study. First, participants who enrolled in our program had no precise idea of the type of techniques they were going to learn, unlike in Fredrickson et al.'s [65] well-known LKM study, where participants enrolled in a “meditation program.” It is therefore possible that some of them were surprised by the proposed method and found that it did not suit them, which may have caused them to drop out of the intervention. The majority of the qualitative feedback received from participants who quitted the LKM condition mentions an incongruity between the proposed technique and their personality (e.g. “I am too impatient for this type of exercise,” “Meditation stresses me, I am not comfortable with this type of method,” “If I had known that we were going to meditate, I would not have signed up”). Second, another explanation that may explain the high dropout rate in the LKM group concerns the trainers [79]. As pointed by an anonymous reviewer, the large number of dropouts could have been because some participants did not enjoy the personality of the trainer and/or the type of group dynamic he proposed. In order to reduce the risk of having such a trainer effect, we recruited two LKM trainers. Results indicate that the observed dropout was independent of the trainer identity. For this reason, we believe that this second hypothesis is less plausible.\nOverall, our results and previous literature suggest that LKM can be an effective technique to increase the well-being of individuals who adhere to the program. Our results also show that the PER program is an effective alternative that may be more readily accepted by the majority of individuals. Previous LKM results (e.g. [65], [78]) suggest that it may be interesting to include elements of LKM within an integrative intervention such as the PER program, or to recommend this technique to individuals for whom this type of intervention appears to be the most clinically relevant (e.g. people open to meditative practices).\nIn addition to bringing a theoretical reflection about the processes underlying happiness enhancing techniques, this article aimed to provide practical information to practitioners desiring to use an integrative and theory-driven intervention to boost their clients' well-being. This is why we have described the PER program that we developed based on Quoidbach et al.'s theoretical model [28], [29] in greater detail than usually found in empirical articles. Like one anonymous reviewer, one may wonder whether the underlying theoretical model must be described to the participants. Based on our experience and on participants' qualitative comments at the end of the sessions, we think that it should definitely be explained to them. First, several times during the training, we invite participants to choose the exercises they wish to practice from a selection that we offer (for their weekly homework and long-term exercises after the end of the program). However, the techniques that individuals prefer are not always those that are best for them [69], [80]. Explaining the model to the participants therefore seems important, so as to show them the variety of techniques available and to encourage them to try at least one technique from each type of strategy (or model section) in order to discover new methods to enhance their happiness level. Second, some of our participants reported that the classification of strategies made them aware that they always use the same family of strategies and rarely any of the others. For instance, one participant realized that he often interprets events positively (cognitive level), but that he is systematically incapable of being fully present during the pleasant moments he experiences (attentional level). Other participants reported similar realizations about techniques usable before, during or after an event. One young woman clearly identified that she is able to feel a lot of positive emotions about past events, but that she almost never plans future events in order to get the most out of them. Thus, the model increases participants' awareness of their functioning and provides them with new ways to increase their well-being. We therefore encourage trainers who would like to teach the PER program to present and explain the model to their groups.\nIn individual therapy, Quoidbach et al.'s model [28], [29] could also be used to identify strategies that a happiness seeker has not been using, and thereby the type of exercise(s) that could be prescribed. However, further research should be conducted to better understand which strategies are the most beneficial for specific profiles of individuals. Pending the development of individually tailored happiness enhancing interventions (see [57], [81] for examples of “tailored interventions” attempts), integrative programs such as the PER program offer the greatest probability of being effective for the widest range of individuals.\nAlthough this study offers promising prospects, we acknowledge several limitations that leave ample room for future research to refine our findings. The first limitation concerns the missing data in the LKM condition due to the large dropout rate that we had not expected. The number of missing data in this group strongly decreased the statistical power to a point where we did not have sufficient power to demonstrate effects, even if they existed. Now that we have information about the potential dropout rate in the LKM condition, we could run this study again, but this time calculate the appropriate number of participants to include in the groups in order to have a satisfying remaining power at the end of the study, even if a large portion of participants were to fail to complete the intervention.The second limitation concerns the timing of the assessments. Our post-intervention measure was carried out only once, four weeks after the end of the intervention. Although most of the literature about happiness enhancing strategies includes T2 assessment right after the end of the last exercise, we chose to collect that information four weeks after the end of our intervention because it seemed methodologically more appropriate to measure the remaining impact of our intervention in individuals' real life. Indeed, right after the intervention, people are still fully impregnated with it, while a few weeks later, they have returned to their life, without any reminder of the techniques proposed within the intervention. Although this timing difference diminishes our ability to compare our results to those obtained with other techniques presented in the literature, the fact that we found results four weeks after the end of our intervention shows that it continues to exert its effects after the end of the intervention. It would have been ideal, of course, to follow participants over a longer period of time to be able to do more sophisticated statistical analyses and to see if the benefits maintain in the long term, after six months or one year. Unfortunately, most of our participants were in their last year at university and left the department a few months after the end of the intervention, which led their student email address to be disabled. We lost contact with most of them right after this T2 measurement.A third potential limitation of our study concerns the composition of our sample. On the one hand, all the participants in our study were young adults, and as a consequence, our results may not be generalizable to the entire population. However, findings from the meta-analysis carried out by Sin and Lyubomirsky [23] indicate that the benefits of positive psychology interventions tend to increase with age. This observation therefore leads us to speculate that testing our intervention on a “young” sample probably resulted in an underestimation, rather than an overestimation, of its effects. On the other hand, our experimental groups were composed of a very limited number of men (PER, N = 4; LKM, N = 3), and this prevented us from investigating a potential impact of gender on the effectiveness of the intervention. It is therefore risky to affirm that our results can be generalized regardless of participant gender. Nevertheless, existing literature in this field has shown no systematic influence of gender on the effectiveness of happiness enhancing techniques, a fact that increases our confidence in the predictive validity of our results for both sexes.Using only self-reported measures is a fourth limitation of our study. Although numerous authors insist that using self-evaluation is a sensible logical way to evaluate a subjective experience such as well-being [82], we consider that it is necessary to complement these measures with objective health indicators (such as cortisol level [83]), 360 degree assessments (e.g. ratings of one's happiness by friends, partner; frequent verbal expression of positive emotions, [84]) or experience sampling methods [85] in future studies. This kind of measure should decrease the risk of social desirability effects linked to the use of self-report measures and increase the reliability and objectivity of the findings.As pointed by an anonymous reviewer, future research would benefit from comparing our PER group to existing integrative interventions such as Seligman et al.'s Group Positive Psychotherapy [27]. This would allow determining more precisely the efficiency of our training compared to other integrative intervention and the added value of the theoretical model that underlies it.Finally, despite the fact that we present an integrative intervention, we acknowledge that studies that focus on specific techniques are critically important. In addition to enabling the evaluation of the unique effect of a particular technique, these studies are crucial in determining the strategies that are particularly beneficial or harmful for specific sub-populations (e.g. highly depressed people, young adults VS people in the final stage of life, individuals from different cultures) and they help understand the processes underlying the effectiveness of a given technique."
        },
        "10.1371/journal.pone.0087071": {
            "author_display": [
                "Jesús Montero-Marin",
                "Marcelo Marcos Piva Demarzo",
                "Joao Paulo Pereira",
                "Marina Olea",
                "Javier García-Campayo"
            ],
            "title_display": "Reassessment of the Psychometric Characteristics and Factor Structure of the ‘Perceived Stress Questionnaire’ (PSQ): Analysis in a Sample of Dental Students",
            "abstract": [
                "Background: The training to become a dentist can create psychological distress. The present study evaluates the structure of the ‘Perceived Stress Questionnaire’ (PSQ), its internal consistency model and interrelatedness with burnout, anxiety, depression and resilience among dental students. Methods: The study employed a cross-sectional design. A sample of Spanish dental students (n = 314) completed the PSQ, the ‘Goldberg Anxiety and Depression Scale’ (GADS), ‘Connor-Davidson Resilience Scale’ (10-item CD-RISC) and ‘Maslach Burnout Inventory-Student Survey’ (MBI-SS). The structure was estimated using Parallel Analysis from polychoric correlations. Unweighted Least Squares was the method for factor extraction, using the Item Response Theory to evaluate the discriminative power of items. Internal consistency was assessed by squaring the correlation between the latent true variable and the observed variable. The relationships between the PSQ and the other constructs were analysed using Spearman’s coefficient. Results: The results showed a PSQ structure through two sub-factors (‘frustration’ and ‘tenseness’) with regard to one general factor (‘perceived stress’). Items that did not satisfy discriminative capacity were rejected. The model fit were acceptable (GFI = 0.98; RSMR = 0.06; AGFI = 0.98; NFI = 0.98; RFI = 0.98). All the factors showed adequate internal consistency as measured by the congeneric model (≥0.91). High and significant associations were observed between perceived stress and burnout, anxiety, depression and resilience. Conclusions: The PSQ showed a hierarchical bi-factor structure among Spanish dental students. Using the questionnaire as a uni-dimensional scale may be useful in perceived stress level discrimination, while the sub-factors could help us to refine perceived stress analysis and improve therapeutic processes. "
            ],
            "publication_date": "2014-01-23T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 1,
            "views": 1497,
            "shares": 1,
            "bookmarks": 6,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0087071",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0087071&representation=PDF",
            "fulltext": "IntroductionThe training to become a health professional can create psychological distress and symptoms of burnout, which may have adverse consequences for one’s personal and professional life [1]–[5]. Dentists experience high levels of work-related stress, which begins during professional training [6]. Furthermore, the prevalence of burnout among dentistry students is high, with a significant relation found between the syndrome and a student’s academic performance, use of medication and thoughts of dropping out of the course [6]. Dental students complain of exhaustion resulting from the high levels of anxiety generated by exams, limited free time available for relaxation and the stress associated with having to adapt to the requirements of clinical practice [7]–[9]. As a result, some new dentistry graduates exhibit alarmingly high levels of burnout [10].\n‘Stress’ occurs when environmental demands overwhelm individuals’ resources and threaten their personal well-being. It has been defined as the result of a relationship with the environment that individuals appraise as significant for their well-being and in which the demands exceed the available coping resources [11]. ‘Burnout’ is a response to the failure to cope adequately with chronic occupational stress and is an attempt to adapt to or to protect oneself from it [12]. This syndrome has classically been characterized by a state of exhaustion, cynicism and inefficacy [13]. ‘Exhaustion’ is the feeling of not being able to offer any more of oneself; ‘cynicism’ represents a distant attitude towards work, those served by it and other colleagues; and ‘inefficacy’ is the feeling of not performing tasks adequately or being incompetent.\nOn the other hand, ‘resilience’ has been characterized as a dynamic and flexible process of adaptation to life changes that could serve as a protective factor against psychological distress and mental disorders [14], [15]. It is the amount of personal strength, energy and motivation that enables an individual to cope with and recover from stress and to flourish when faced with adversity. Most of the proposed models to improve well-being in students and health professionals aim to enhance strategies for coping with stress using educational and environmental support as well as cognitive exercises that strengthen resilience skills [16], [17].\nFrom the early stages of their university studies, dental students show concern for the stress produced by their experiences with clinical practice. They develop intense and long-term interactions with clients and patients, which is characteristic of careers with high levels of psychological distress and burnout [18]. Experiencing burnout over a prolonged period is associated with adverse emotional consequences, such as anxiety and depression, and it can also negatively affect the quality of patient care [19]–[24]. As a result, being able to measure the stress process in dental students seems to be essential if we want to evaluate their well-being along with their period of professional training.\nThe ‘Perceived Stress Questionnaire’ (PSQ) is a 30-item instrument that was developed by clinicians to quantify perceived stress [25]. It is one of the most used instruments that measure the stress process in psychosomatic research and has been associated with somatic complaints. In addition, it has demonstrated good predictive values in stress-related diseases [26]–[28]. The PSQ permits the subjective experience of perceived stressful situations and stress reactions to be assessed, emphasizing cognitive perceptions more than emotional states or specific life events. The general form of the instruction asks questions related to ‘the last two years’ and the recent form asks about situations taking place ‘during the last month’, potentially addressing chronic and acute relationships with stressful events and activities. It contains both positively and negatively formulated items in order to reduce acquiescent bias. Each item is answered using a four-point Likert-type scale, ranging from 1 (‘almost never’) to 4 (‘almost always’). Higher scores indicate more severe perceived stress. Originally designed in English, this instrument has been translated to Italian, German and Spanish, and validated in populations of psychiatric inpatients and outpatients, nursing students, health workers, psychosomatic patients and health adults [25], [27], [28].\nOne key element of this questionnaire is that it includes different groups of stressful experiences, such as harassment, overload, irritability, lack of joy, fatigue, worries and tension, which were proposed in its original version. However, this structure was not replicated. The study by Sanz-Carrillo [27] revealed the dimensions of social acceptance, load, irritability, energy, fear and self-realization; while the Fliege’s study [28] observed worries, tension, lack of joy and demands, differentiating the first three as internal stress reactions, while demands was classified as external stressors. So far, contemplating a general factor seems to be the most reasonable solution for its use. The original author proposed a linear algorithm ranging from 0 to 1 [25].\nThe previous inconsistencies may be due to differences among the samples that were used, which is in line with the idea that validating instruments in a specific population of interest is the most advisable option [29]. Nevertheless, the principal component analysis used as factor extraction method from r correlation matrices, along with the Kaiser role for the number of factors to retain, do not seem to be the best option to estimate factorial models using psychological variables as mentioned, and they are too often used out of force of habit [30].\nTherefore, the goals of the present study were to evaluate the factor structure of the PSQ with dental students, together with its internal consistency model and its interrelatedness with the constructs of burnout, anxiety, depression and resilience. As a hypothesis, we expected high and significant correlations between all constructs.\nMethods\nStudy Design\nWe used a cross-sectional design by means of the application of a self-assessment survey.\n\n\nSetting, Sample and Ethics Statement\nThe population consisted of Spanish dental students enrolled in Huesca (NH = 136) and Santiago de Compostela (NS = 242), during the 2010–11 academic year. An 83.1% response rate (RR) to the surveys, which were sent to all prospective participants, resulted in a sample of n = 314 participants. The students did not receive any financial or credit compensation for participating in the study. No differences were found in RR based on sex (males = 81.4% vs. females = 83.8%; p = 0.576), campus (Huesca = 87.5% vs. Santiago de Compostela = 80.6%; p = 0.085) or age (participants Mn = 22.05; SD = 3.57 vs. non-participants Mn = 22.34; SD = 3.83; p = 0.551).\nA clinical psychologist trained two research assistants to administer the questionnaires as a battery in a paper-and-pencil format. The first page of the protocol identified the objectives of the study, participants, potential benefits and risks and the confidentiality of the data treatment. Each participant provided written informed consent before the commencement of the survey. The research assistants administered the survey in May 2011, two weeks before the period of final exams. After completion, the questionnaires were collected and kept in a sealed envelope to ensure the participants’ anonymity. The Ethical Review Board of Aragon, Spain, approved the study protocol. This study followed Helsinki Convention norms and later modifications, the Declaration of Madrid of the World Psychiatric Association and the Uniform Requirements for manuscripts submitted to Bio-medical journals.\n\n\nMeasures\nSocio-demographics.Data were collected on age, gender, stable relationship (‘yes’, ‘no’), children (‘yes’, ‘no’), campus (‘Huesca’, ‘Santiago’), distance from family home (km), residence (‘with parents’, ‘dormitory’, ‘shared flat’, ‘private flat’), scholarship (‘yes’, ‘no’), parental support perceived (‘insufficient’, ‘good’, ‘very good’), weekly study hours, failed subjects over the previous period (‘none’, ‘one’, ‘two or more’), job (‘yes’, ‘no’) and year of study (‘first’, ‘second’, ‘third’, ‘fourth’, ‘fifth’).\n\nPerceived Stress Questionnaire (PSQ).The Spanish version of PSQ (described above) was used, specifically its “recent form” addressing the previous 30 days [25], [27].\n\nGoldberg Anxiety and Depression Scale (GADS).The participants completed the Spanish version of the GADS [31], [32], which consists of a 9-item subscale that assesses symptoms of anxiety and a 9-item subscale that assesses symptoms of depression. The participants respond with either ‘yes’ or ‘no’, with one point scored for each positive response. A greater number of positive responses is associated with a greater likelihood of suffering from anxiety or depression. Each subscale provides a measure of the associated mental disorder. The convergence validity of the GADS has demonstrated adequate values of sensitivity and specificity [32]–[34]. We used this short and friendly scale because it is recommended for the screening of anxiety and depression in large sample studies in the general population [31]–[32].\n\nConnor-Davidson Resilience Scale (10-item CD-RISC).The participants also completed the Spanish version of the 10-item CD-RISC [35]–[36]. This scale is a self-report instrument that measures resilience on a 5-point Likert scale with responses that range from 0 (‘not at all’) to 4 (‘almost always’). The final scores are obtained by summing the response to each of the items, with higher values indicating higher levels of resilience. The validity and internal consistency of the scale were adequate and positively related to variables such as sleep quality and mental health [35], [36].\n\nMaslach Burnout Inventory-Student Survey (MBI-SS).Subjects were given the MBI-SS in its validated Spanish version for students [13], [37]. This questionnaire is the ‘golden rule’ for the evaluation of burnout, and this adaptation consists of 15 items grouped into the three dimensions: five items corresponded to exhaustion, four to cynicism and six to efficacy. Responses are arranged in a 7-point Likert scale, scored from 0 (‘never’) to 6 (‘always’). The scores of each dimension are obtained by adding up the responses to the corresponding items, with higher values indicating higher levels of each one. The questionnaire dimensions present an adequate structure and internal consistency [37].\n\n\n\nData Analysis\nSPSS v19.0, FACTOR v9.02 and AMOS v7.0 statistical packages were used to conduct the analysis.\nDescriptives.Means, standard deviations, skewness, kurtosis and Mardia’s coefficients [38] were calculated to evaluate the performance of the PSQ items.\n\nDimensionality.Polychoric correlation is advised for factorial analysis (FA) when the distributions of ordinal items are asymmetric, with excess of kurtosis or with high item-rest coefficients [39]. Thus, a polychoric correlation matrix was estimated with regard to the PSQ items. We used parallel analysis (PA) [40] to identify the number of factors to include in the factorial solution, by replacing the raw data method [41] with optimal implementation based on minimum rank factor analysis [42], generating 500 random correlation matrices from permutation of the raw data. With this analysis, a factor is considered significant if the associated eigen-value is greater than that corresponding to 95th percentile of the distribution of eigen-values derived from a random dataset. PA is considered the best available solution to decide the number-of-factors-to-retain [43], [44]. We had previously verified the adequacy of the matrix, assessing the determinant, the KMO index and Barlett’s test of sphericity [45].\nUnweighted Least Squares (ULS) was the method used for factor extraction [46]. The ULS procedure does not provide inferential estimations for assessing model data fit based on the χ2 distribution, but it has important advantages: it does not require any distributional assumptions; it is quite robust and usually converges because of its efficiency in terms of computation; and it tends to provide less biased estimates of the true parameter values [47]. Additionally, ULS is an appropriate choice for the case of not excessively large samples and shows good performance when working with polychoric matrices. In fact, ULS is consistent with the underlying variables approach from the Item Response Theory (IRT); it tends to supply accurate estimates even when models are large; and it provides better estimates than far more complex procedures [48]–[50]. The rotation method used was Promax, with a parameter of k = 4.00, given the correlated solution expected and using Raw Varimax as clever rotation start. We used Bentler’s simplicity index (S) and the loading simplicity index (LS) to evaluate factorial simplicity [51], [52].\nAlthough we used unrestricted item factor analysis at this stage, we actually propose a second order factor solution, as a perceived stress general factor (G), by hierarchical factor solution [53]. This specification, which is multidimensional but is able to reflect the essential uni-dimensionality of the data, prescribes a factor which reflects what is common to all of the items, working as a multidimensional semi-confirmatory factorial analysis [54]. The factor weights (w) and the % of variance explained in each item by means of communality values (h2) were calculated. IRT parameterization by multidimensional normal-ogive graded response model (derived from the assumption of normally distributed measurement error), showed us the pattern of item discriminations in each dimension [55]. The belonging factor was determined by means of the IRT discrimination (an), with those items with poor values being dismissed. Specifically, those items with an values <0.65 were discarded, as well as those with an values >0.65 in all of the latent dimensions at the same time [56].\nWe examined the fit of the proposed PSQ model by CFA, applying ULS from a polychoric matrix, for the reasons stated above. From a general perspective, we used the goodness-of-fit index (GFI), the adjusted goodness-of-fit index (AGFI), the root mean square of the standardized residuals (RMSR), the normed-fit-index (NFI) and Bollen’s relative-fit-index (RFI). GFI and AGFI refer to explained variance and values >0.90 are considered acceptable [57]. SRMR is the standardized difference between the observed and the predicted covariance, indicating a good fit for values <0.08 [58]. NFI measures the proportional reduction in the adjustment function when going from null to the proposed model and is considered acceptable when >0.90 [59]. RFI takes into account the discrepancy for the model evaluated and for the baseline model, and it is very good close to 1 [60]. All of them are perfectly valid for the ULS procedure. Taken together, they provide a reliable evaluation of the solution and additional information regarding absolute and incremental model fit. From an analytical perspective, standardized factor saturations and the explained variance were also considered.\n\nReliability.We examined the internal consistency of the scales and sub-escales using congeneric, tau-equivalent and parallel models [61]. The congeneric model is the least restrictive and assumes that each individual item measures the same latent variable, with possibly different scales, degrees of precision and magnitude of error. The tau-equivalent model implies that individual items measure the same latent variable on the same scale and with the same degree of precision, but with possibly different degrees of error. The parallel model is the most restrictive measurement model; it assumes that all items must measure the same latent variable on the same scale, with the same degree of precision and with the same amount of error. We finally chose the model that fitted better with the data, applying the ULS method and establishing comparisons. The reliability value was estimated by squaring the implied correlation between the composite latent true variable and the composite observed variable, to arrive at the percentage of the total observed variance that was accounted for by the true variable [62]. Mean inter-item polychoric correlations, item-rest and mean item-rest correlations were also used, as well as the mean Spearman’s R coefficients between the items over the belonging factor, calculated according to the Bayes ‘Expected A Posteriori’ (EAP) [63].\n\nConvergence.Participant’s scores on PSQ factors calculated by EAP were also used in order to evaluate the degree of association between them and regarding the other constructs by means of Spearman’s R coefficients.\n\n\nResultsIn order to adhere to standards for data availability, the authors state that all materials used to produce the results in this paper will be made available upon request. This includes [64]: 1.- The list of documents and data files that are needed in order for replication to be possible, 2.- A detailed list of what will be provided by the authors, and 3.- What steps, and in what sequence, the interested researchers need to take in order for this data to be made available. In addition, authors will post these materials on the group’s website [65].\n\nParticipants\nTable 1 displays the general characteristics of participants. They comprised adults of European ethnicity between the ages of 18 and 41 (Mn = 22.05; SD = 3.57), 70.70% of whom were women. Compared to students in Santiago de Compostela, students in Huesca lived further away from the family home (χ2 = 72.53; df = 2; p<0.001), were more likely to live in shared flats (χ2 = 14.79; df = 3; p = 0.002), were less likely to have received a scholarship (χ2 = 6.66; df = 1; p = 0.010) and failed a higher percentage of subjects over the previous exam period (χ2 = 7.33; df = 2; p = 0.026). Students in both Santiago and Huesca were similar with regard to the rest of the socio-demographic and occupational variables.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Characteristics of the participants (n = 314).doi:10.1371/journal.pone.0087071.t001\n\nDescriptives\nDescriptive statistics for all the PSQ items can be seen in Table 2. Some items presented skewness values >1.00, such as ‘you feel lonely or isolated’ (item n° 5) and ‘you find yourself in situations of conflict’ (n° 6). Otherwise, some items showed kurtosis values <−1.00, as ‘you feel calm’ (n° 10 reversed), ‘you are under pressure from other people’ (n° 19), ‘you are afraid for the future’ (n° 22) and ‘you feel under pressure from deadlines’ (item n° 30). Mardia’s multivariate skewness and kurtosis coefficients were 132.70 (p = 1.00) and 1,040.71 (p<0.001), respectively.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Descriptives, factorial solution, communalities and IRT parameterization of the PSQ items.doi:10.1371/journal.pone.0087071.t002\n\nDimensionality\nThe polychoric matrix of the PSQ items revealed that 78.6% of coefficients out of the diagonal were ≥0.30. The determinant was <0.001. KMO test had a value of 0.95 and Bartlett’s statistic was 4,780.50 (df = 435; p<0.001). The PA identified a two-factor structure, explaining 54.9% of the variance [(λ1 = 12.23; 46.7% variance of the real data; 7.5% variance explained over the P95 of the random samples); (λ2 = 2.39; 8.2% variance of the real data; 6.9% variance explained over the P95 of the random samples)]. The hierarchical solution by two first order factors (F1 and F2) and one second-order factor (G) exhibited good simplicity indices, such as S = 0.98 (P99) and LS = 0.45 (P99).\nTable 2 shows the weights over G, the rotated loading matrix over F1 and F2 and the h2 values. All the items loaded ≥0.40 in G, except ‘you feel you are doing things you really like’ (n° 7 reversed; wg = 0.27). F1 presented topics related to ‘frustration’, such as ‘you are light-hearted’ (n° 25 reversed; w1 = 0.94), ‘you feel discouraged’ (n° 20; w1 = 0.82) or ‘you feel frustrated’ (n° 12; w1 = 0.72). F2 exhibited themes associated with ‘tenseness’, such as ‘you have too many things to do’ (n° 4; w2 = 0.95), ‘you feel that too many demands are being made on you’ (n° 2; w2 = 0.83), or ‘you feel you are in a hurry’ (n° 16; w2 = 0.82). In general, h2 values were high, with an average of 0.49. Table 2 shows the PSQ items in terms of IRT discrimination. Some items did not present sufficient values, such as ‘you feel rested’ (n° 1 reversed; a1 = 0.11; a2 = 0.58), ‘you are irritable or grouchy’ (n° 3; a1 = 0.52; a2 = 0.56), and also items n° 6 (a1 = 0.50; a2 = 0.08), n° 19 (a1 = 0.52; a2 = 0.50) and n° 22 (a1 = 0.43; a2 = 0.11), already mentioned. Additionally, ‘your problems seem to be piling up’ (n° 15; a1 = 0.73; a2 = 0.79) presented high values in both factors, so it was also dismissed.\nFigure 1 shows the PSQ hierarchical bi-factor structure using CFA from an analytical and standardized point of view. The two first order factors turned out to be highly influenced by G, with loadings over F1 = 0.96 and F2 = 0.82. The item loadings with regard to their respective latent factor were high (F1 and F2 ranges  = 0.42 to 0.81 and  = 0.38 to 0.85, respectively). In a general sense, the PSQ hierarchical bi-factor structure presented adequate fit indices without using correlations between the error terms (GFI = 0.98; RSMR = 0.06; AGFI = 0.98; NFI = 0.98; RFI = 0.98).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Construct validity of the PSQ hierarchical bi-factor structure.The circles represent latent constructs and the rectangles are observable variables. The factor weightings are over the one-way arrows and the percentage of explained variance for each observable variable over the boxes (standardized estimates).\ndoi:10.1371/journal.pone.0087071.g001\n\nReliability\nTable 3 shows the reliability models tested for the PSQ hierarchical bi-factor structure. The indices fitted best with the congeneric model in all of the latent factors. Based on the congeneric model, the estimates of reliability obtained for G were 0.95; with 0.91 for F1 and 0.93 for F2. The mean inter-item polychoric correlation for the twenty-four selected PSQ items was 0.42. Item-rest values were positive and high, with an average of 0.59. All the items were highly and positively correlated to the belonging factor calculated by EAP, with an average of 0.65 over F1 and 0.70 over F2.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 3.  Internal consistency models for the PSQ hierarchical bi-factor structure.doi:10.1371/journal.pone.0087071.t003\n\nConvergence\nTable 4 shows the convergence values for the PSQ hierarchical bi-factor structure scores calculated by EAP. F1 and F2 presented a correlation of r = 0.62 and both had high associations with regard to anxiety, depression and exhaustion. However, F1 presented higher values with cynicism, positively, and with resilience and efficacy, negatively.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 4.  Convergence values for the PSQ hierarchical bi-factor structure.doi:10.1371/journal.pone.0087071.t004\nDespite the fact that perceived stress has been evaluated among dental students [9], as far as we are aware, this is the first factorial study of the PSQ among dental students. It is also the first study to examine its internal consistency model as well as possible interrelatedness with burnout, anxiety, depression and resilience. Other studies have evaluated its structure in other samples [27], [28], but the methods used did not respect the true ordinal nature of the variables, as we have. Our results provide evidence of a clear two-factor structure of the PSQ in dental students (‘frustration’ and ‘tenseness’), while it was also possible to use a single general factor (‘perceived stress’). Overall, the questionnaire had good psychometric properties, with adequate reliability and good convergence values, although it was advisable to discard some non-discriminative items. We found that the congeneric model was the optimal model to measure its internal consistency. Interestingly, the results revealed different relationship patterns between the perceived stress factors and the other constructs.The main strength of the present study is that generalizability was enhanced because it was conducted using an high-stress-risk sample [4], [7]–[9], from two different universities in two Spanish regions, and these groups exhibited similar response rate. It is interesting to highlight the fact that the study was carried out during the period of final exams, a well-known source of distress, which may make the results more relevant [66]. The response rate was high and the participants did not differ significantly from non-participants with regard to age, gender or years of study. Moreover, an independent researcher supervised the data transcription process to control for errors, and the analysis method respected the true nature of the variables used. The main limitation of the study was the use of a cross-sectional design because it did not permit the analysis of causal hypotheses. Another limitation could proceed from the instruments used, because they are not the only questionnaires used to measure such constructs. The use of other questionnaires might have produced slightly different results.The participants in the study were young adults. Most of the individuals were women who did not have children. The majority of students also did not receive financial assistance and were not employed. On average, the responses of the participants were not extreme, although the values for tenseness and anxiety were moderately high. Comparatively, the scores for frustration, burnout and depression were slightly lower. This findings can be understood by considering the pressure that the students experienced owing to the proximity of final exams.One of the most salient findings of this study is the clear hierarchical bi-factor structure shown by the PSQ among Spanish dental students. In this population, high correlations were observed between one second-order factor and two first-order factors. The second order factor (G) refers to ‘perceived stress’ as a general factor, as used in the original proposal [25], [26] and in other studies [27], [28]. The first of the first-order factors (F1), which we referred to as ‘frustration’, mainly included items from the original ‘lack of joy’ and ‘worries’. The second of the first-order factors (F2), named ‘tenseness’, mainly included items from the original ‘tension’ and ‘overload’ [25]. In other words, F1 consisted of the stress perceived as lack of joy and worries, and F2 consisted of the external stressor of demands and the stress reaction of tension [28].In general terms, the behaviour of the items was adequate, with high and positive item-rest values, although their distributions suggest a non-linear analysis, as was expected. All the items weighted strongly and positively in G factor. However, the IRT discrimination values advised us to reject some items because they were unable to differentiate adequately between F1 and F2. For these reasons, the 24 selected items were strong and positively weighted in the belonging factor and the model fit was very good. These items correlated highly with one another and internal consistency values were high, for G as well as for F1 and F2. Each item seemed to be measuring the corresponding latent variables, with possibly different degrees of precision and different amounts of error.We found that G was significantly related to all the considered constructs, and therefore this finding supports the hypothesis that burnout syndrome could mediate the link between perceived stress and the occurrence of emotional disorders [67], [68], although it is well known that there are many others sources of stress, different from those related to work or workers in training. In other words, occupational stress could be an important risk factor for the development of anxious and depressive symptoms in dental students through burnout syndrome [69]–[71]. Thus, dealing with stress problems at an early stage of training may help to promote resilience and reduce burnout and mental health problems in the longer term [72]. Nevertheless, in order to reach a full understanding of the process, it will be necessary to continue with research until the hypothetical relationships put forward can be fully brought to light. It will be necessary to make use of designs that allow possible causal relationships to be explained, particularly if we are interested in developing effective lines of intervention, given the lack of these in a setting that is so in need of them.In addition, the pattern of relationships observed between the PSQ first-order factors and the other constructs suggests that while both F1 and F2 seems to be strongly associated with exhaustion, anxiety and depression, F1 could be more positively related than F2 to cynicism, and more negatively related to efficacy and resilience. In general, resilience increases the feelings of satisfaction and commitment that promote the empowerment of and beneficial outcomes for students and even workers [73]–[77]. In accordance with the demands-resources model [78], people who suffer from burnout experience a progressive decline in commitment to their studies over time [79]. This is perhaps why resilience constitutes a coping reservoir that influences the long term functioning of university students [17,72). However, our results suggest that interventions focused on promoting resilience may be of benefit in overcoming frustration more than tenseness, while tenseness seems to be more important in this population. Combining resilience interventions with relaxation interventions could possibly be the best choice for dental students."
        },
        "10.1371/journal.pone.0054937": {
            "author_display": [
                "Daniel Romer",
                "Ellen Peters",
                "Andrew A. Strasser",
                "Daniel Langleben"
            ],
            "title_display": "Desire versus Efficacy in Smokers’ Paradoxical Reactions to Pictorial Health Warnings for Cigarettes",
            "abstract": [
                "\n        Pictorial health warnings on cigarette packs create aversive emotional reactions to smoking and induce thoughts about quitting; however, contrary to models of health behavior change, they do not appear to alter intentions to quit smoking. We propose and test a novel model of intention to quit an addictive habit such as smoking (the efficacy-desire model) that can explain this paradoxical effect. At the core of the model is the prediction that self-efficacy and desire to quit an addictive habit are inversely related. We tested the model in an online experiment that randomly exposed smokers (N = 3297) to a cigarette pack with one of three increasing levels of warning intensity. The results supported the model’s prediction that despite the effects of warnings on aversion to smoking, intention to quit smoking is an inverted U-shape function of the smoker’s self-efficacy for quitting. In addition, smokers with greater (lesser) quit efficacy relative to smoking efficacy increase (decrease) intentions to quit. The findings show that previous failures to observe effects of pictorial warning labels on quit intentions can be explained by the contradictory individual differences that warnings produce. Thus, the model explains the paradoxical finding that quit intentions do not change at the population level, even though smokers recognize the implications of warnings. The model suggests that pictorial warnings are effective for smokers with stronger quit-efficacy beliefs and provides guidance for how cigarette warnings and tobacco control strategies can be designed to help smokers quit.\n      "
            ],
            "publication_date": "2013-01-29T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 3,
            "views": 1652,
            "shares": 8,
            "bookmarks": 8,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0054937",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0054937&representation=PDF",
            "fulltext": "IntroductionCigarette smoking accounts for over 430,000 deaths annually in the U.S. [1] and is responsible for over 5 million fatalities per year worldwide [2]. Efforts to educate the public about the hazards of smoking have been ongoing since they were first identified [3]. These efforts along with restrictions on advertising and locations where people can smoke have steadily reduced the prevalence of smoking in the U.S. from a high of 42% in 1965 to about 20% in most recent surveys [4]. In addition, rates of initiation in adolescents have declined, thereby reducing the recruitment of new smokers to the population [4], [5]. Despite these successes, the rate of quitting smoking in recent years has declined and, although many try to quit, only about 5% are successful annually [6]. As a result of this and a growing population, there are almost as many smokers in the U.S. today as there were at the height of the epidemic in the 1960’s. Clearly, in order to continue reducing smoking prevalence, greater efforts will be needed to reach smokers who fail to quit.\nOne effort by the U. S. government to encourage quitting has been to place textual warnings about the hazards of smoking on the sides of cigarette packs. Although such warnings have been in place in the U.S. since 1965, they have not changed since 1984 and are easy to ignore [7], [8]. In an effort to increase the effectiveness of these warnings, recent legislation empowers the U. S. Food and Drug Administration (FDA) to impose larger pictorial warnings on the front and back of cigarette packs similar to those that were first introduced in Canada and elsewhere. Research indicates that these enhanced warnings not only draw the smoker’s attention but also succeed in creating aversive emotional reactions to the prospect of smoking [9], [10]. In addition, studies of the effects of introducing pictorial warnings in Australia and the UK indicate that they increase smokers’ thoughts about quitting [11], [12].\nThese findings have led researchers and policy makers to conclude that the warnings work, despite the lack of direct evidence that they increase quit rates [9]. Indeed, research conducted to evaluate immediate effects of pictorial warnings in the U.S. indicates that the warnings seldom change intentions to quit [13], [14]. A large FDA test of 36 different pictorial warning labels presented to two age groups of smokers (18–24 vs. 25+) revealed that out of 72 tests, only 6 increased intentions to try to quit [13]. A smaller replication with fewer warnings but larger sample sizes per condition found that, although pictorial warnings enhanced smokers’ aversion to smoking, they produced no overall effects on intentions to try to quit in the near future [14].\nIntentions are important because they are critical precursors to behavior change [15]. Unless a smoker strongly intends to quit, it will be difficult, if not impossible, to overcome the cravings and withdrawal symptoms that maintain this addictive habit [16]. Indeed, models of health behavior change, such as Protection Motivation Theory (PMT) [17], the Health Belief Model (HBM) [18], and the Theory of Reasoned Action (TRA) [15], predict that intentions should increase as the perceived risks of smoking increase. Nevertheless, although pictorial warnings encourage smokers to think about quitting, the warnings do not appear to enhance the likelihood that the average smoker will actually try to quit. Thus, the failure of warnings to influence intentions poses a paradox for any theory that assumes that people act in their own best interests, especially when they recognize threats to those interests.\nRecent neuroscience research provides insight into the paradoxical effects of warning labels. This research has identified two neuropsychological systems that influence the development of an addiction and that explain why smoking cessation is difficult. First, ingestion of nicotine, the addictive drug in tobacco, alters the mesocorticolimbic dopamine system that controls expectations of reward [19]. Over time, these expectations become conditioned to the act of smoking itself, thus making the person who smokes sensitive to any cues associated with the act and enhancing the desire to smoke when exposed to them [20], [21]. Second, repeated acts of smoking transfer control over the habit to dorsal striatal circuits that undermine prefrontal control [22], [23] and that turn the habit into a compulsion, leaving the smoker with reduced sense of control over the behavior [24], [25]. Although this description of the two systems is necessarily abbreviated, it is clear that these changes in the reward and control systems make it difficult for the addict to resist the pull of smoking cues and the craving elicited by them. Thus, despite the desire to quit that most smokers report [26], their perceived efficacy to do so is lacking. This conflict between desire and efficacy often leaves smokers without sufficient motivation and, hence, intention to quit the habit.\nIn view of the powerful neuropsychological processes identified by neuroscience research, we translated those insights into a behavioral decision making model that can account for the paradoxical finding that despite enhancing desires to quit, warnings do not appear to change intentions to do so. We first describe the model and then present a test of its major predictions.\n\nA Model of Intentions to Quit an Addictive Behavior such as Smoking\nThe efficacy-desire model (EDM) proposes that the intention to quit smoking (Iq) is a function of the difference between the motivation to smoke (Ms) and the motivation to quit (Mq):(1)\nThe focus on these competing motivations is not novel; other models of health behavior change, such as PMT [17] and the HBM [18], suggest that intending to quit an unhealthy behavior is a function of influences on these competing motivations. Indeed, any theory of rational choice suggests that all the smoker needs in order to quit is more desire to do so than to continue smoking [27].\nThe distinction between the motivations stems from the reward system’s powerful influence on goal seeking [21] and its circuits specialized for detecting both harmful (negative) and beneficial (positive) environments [28]. These circuits produce corresponding forms of negative and positive affect that, respectively, underlie desires to avoid or approach such objects as cigarettes [29]. Although these desires are often reciprocally related, they are independent sources of motivation that can have unique influences and effects.\nIn addition to the essential role of desires, the EDM recognizes that motivation is also determined by the perceived efficacy to satisfy desires. Self-efficacy is a familiar concept that has long been featured in models of behavior change [30]. In regard to smoking, even if smokers desire to quit the habit, they are unlikely to try unless they believe that they can implement the behavior. Neuroscience models of addiction also focus on self-efficacy by emphasizing the important role of the brain’s control system in undermining the ability to quit an addiction. Theories of behavior change, such as the TRA (15), treat desires (e.g., attitudes) and efficacy as additive influences on intentions. However, because both efficacy and desire are needed to motivate behavior, the EDM treats these expectancies and desires as multiplicative determiners of motivation, a common assumption in psychological models of motivation [31]. Thus, inserting the respective efficacies (Eq, Es) and desires (Dq, Ds) for quitting and smoking into eq. (1) produces:(2)\nFor an addiction such as smoking, both components of Ms are likely to be high. Indeed, the more one practices a behavior, the greater the skill and sense of efficacy for controlling it [32]. The same is unfortunately not the case for quitting. Even if the smoker wants to quit (Dq), no motivation and hence intention will be formed unless the smoker’s sense of efficacy (Eq) is also high. As is often the case for addictive habits [33], the smoker may strongly desire to quit but not believe that it is possible to do so. However, in deriving predictions from this model, it is important to consider individual differences in the various components of the model and the ways in which they are related to each other.\nFigure 1 shows the relations between the components of the model at the individual level. Although desire and efficacy to engage in a behavior are likely to be positively related, we show no relation between Ds and Es under the assumption that Es has reached asymptote in most smokers, leaving little room for any relation with Ds. However, for quitting an addictive habit, such as smoking, the relation between efficacy and desire to quit is likely to be negative. Efficacy for quitting the behavior is at its peak in the early stages of acquiring the habit, usually in adolescence, when the young smoker believes he or she will not have much difficulty stopping [34]. However, as the habit progresses, the smoker finds it increasingly difficult to stop even if the desire to do so increases. This process creates an inverse relation between Eq and Dq.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Efficacy-Desire Model of quit intentions showing relations between components of the model as they relate to the reward and control systems of addiction models.Dq and Ds are the desire to quit and smoke, respectively; Eq and Es are efficacies for quitting and smoking respectively. Respective interactions between efficacy and desire lead to Mq and Ms, which directly affect the intention to try quitting (Iq). Dashed paths indicate inverse relations; curved paths are correlations rather than effects.\ndoi:10.1371/journal.pone.0054937.g001Neuroscience models of addiction specifically predict that greater frequency of smoking (represented in Figure 1 by parameter β) reduces Eq while it simultaneously increases Ds. It is also likely that frequency of smoking increases Dq, given what we know about smokers’ wishes to quit. But even leaving this out of the model, neuroscience models of addiction predict that the more one smokes, the lower one’s quit-efficacy (hence lower Mq) and the greater one’s desire to continue smoking (hence higher Ms). This disparity between Mq and Ms makes it difficult for the smoker to quit and shows why smokers are so conflicted by their addiction, wishing to quit but nevertheless continuing the habit.\nThe model makes interesting predictions regarding the effects of a warning, which, based on what we know about their effects [9]–[11], should increase Dq and reduce Ds. Figure 1 shows such a warning (whose intensity is indicated by parameter α) directly affecting Dq. Because Dq and Eq are inversely related, the effect on Dq is:(3)\nReplacing Dq in eq. (2) with eq. (3) shows that the model makes the novel prediction that for an addictive habit, Mq is an inverted U-shape function of Eq:\nThat is, assuming that individual differences in Eq range from negative to positive valence, Mq rises as Eq increases, but at an intermediate point, begins to decline. Although theories of behavior change predict that efforts to change behavior increase as efficacy increases, the EDM suggests that, for an addictive habit, this effect only holds up to a point, after which the motivation to do so declines. Furthermore, whether the habit is addictive or not, the effect of α depends on Eq, with an enhanced effect for positive values of Eq and a depressed effect for negative values of Eq.\nThe path linking Ds and Dq in Figure 1 suggests that these desires should be inversely related. However, consistent with a bivalent model of affect [29], we assume that these desires are somewhat independent. Hence, the effect of the warning on Ds is:(4)\nEq. (4) expresses the intuitive result that Ds is positively related to the heaviness of the habit (β) and inversely related to the strength of the warning (α). Inserting eqs. (3) and (4) into eq. (2) yields the following overall relationship between Iq and the respective efficacies for quitting and smoking:(5)\nWe show examples of the hypothetical relation between Iq and Eq for different values of α in Figure 2A, assuming that Es and β are constant and that Es is higher on average than Eq (.5 vs. 0). The inverted U-shape relation is especially apparent when α  = 0. This shows that persons who smoke will have equally weak intentions to quit smoking not only when their efficacy is low but also when it is high. Indeed, absent any health warnings, smokers will have the greatest intentions to quit when their efficacy is at a moderate level. The prediction that low efficacy produces low intentions is not surprising since most theories expect this result. However, the model also predicts that those who think they can quit easily will not be motivated to do so either. This is a critical prediction of the model that will be tested for the first time in the present research.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Relations between intention to quit smoking (Iq) and quit efficacy (Eq) by three levels of warning intensity (α) scaled from 0, .5. to 1.0.Panel A shows the relation using eq. (5) with Es fixed at.5 and β at 3.0. Panel B shows the observed relation using scores predicted by regression model in Table 4 (step 1).\ndoi:10.1371/journal.pone.0054937.g002A second critical prediction of the model is the effect of the warning. As seen in Figure 2A, the effect of α primarily increases the intention to quit among those with  That is the point in the relation where all three curves in Figure 2A converge. Indeed, those with weaker Eq than −Es actually begin to exhibit a reduction in quit intention. Thus, the model makes the counterintuitive prediction that those with the strongest desires to quit (i.e., those who have smoked the longest) will be least motivated to respond rationally to warnings about the hazards of their habit, and this will be the case despite the fact that their response to the warning (created by an increase in α) is just as strong as the response among those with weaker desires to quit. This may explain the paradoxical effects of warnings observed in previous research. The quit-enhancing effects of increases in α will primarily be observed among those whose efficacy for quitting exceeds their efficacy for smoking. Indeed, warnings for those with weak efficacy for quitting will actually result in weaker intentions to quit.\nWe tested the major predictions of the model in an experimental context in which smokers were randomly assigned to see one example of a pack of cigarettes with a warning that was varied systematically in intensity across experimental conditions. This provided the opportunity to observe the effects of a warning in the context of individual differences in both the efficacy and desire components of the model. In addition to the predicted U-shape function shown in Figure 2A, we tested the prediction that increases in the intensity of warnings (represented by α) produced by adding an emotionally charged picture will lead to divergent effects on Iq depending on Eq. In addition, the EDM predicts that the greater the amount smoked (β), the lower the intention to quit. However, variation in β should only shift the curve up or down (it should be independent of Eq and α), and it should only interact with Es, which we assume is at a high and relatively fixed level for all smokers. In addition, as shown in Figure 1, frequency of smoking should be inversely related to Eq but positively related to Ds. Finally, in support of the expected inverse relation between Eq and Dq, length of time smoking (using age of the smoker as a proxy) should be positively related to Dq but negatively related to Eq.\n\nMethods\nEthics Statement\nThe research was reviewed and approved by the Institutional Review Board (IRB) of the University of Pennsylvania, which adheres to the principles of the Belmont Report. As the survey was conducted over the Internet, was completed anonymously, and posed minimal risk, the IRB waived the requirement for written consent. However, participants were informed that the survey involved research, that their responses were anonymous, and that their participation was entirely voluntary. Thus, proceeding to take the survey was considered documentation of consent.\n\n\nMaterials and Participants\nWe tested the model’s predictions using warnings that were tested for use in the U.S. by the FDA [13]. We used the same adult Internet panel used by the FDA (Research Now [35]) and similar warning labels that FDA evaluated. However, smokers who had participated in the earlier FDA test were excluded from the study.\nPanel members were included in the study if they reported having smoked at least 100 cigarettes in their lives and if they currently smoked cigarettes “every day” or “some days” (N = 3297). Most of the sample reported smoking every day (62%), a lower proportion than the nearly 80% observed in recent national surveys of U. S. smokers [36]. In this test, approximately 160 smokers (56.5% female) in each of two age groups (18–24 and 25+ years) were randomly exposed to one of 10 computer screen images of a cigarette pack containing a warning about the hazards of smoking (see examples in Figure 3). The mean ages of the two age groups were 22.1 (SD  = 1.60) and 44.3 (SD  = 13.91). As expected, the older smokers were more likely to smoke every day compared to the younger group (76.5% vs. 45.7%), X2(1)  = 329, p<.001.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Examples of warning labels with (A) text on side of pack, (B) picture and text on front of pack, and (C) picture, text and elaboration on front of pack.Reprinted from www.fda.gov under a CC BY license, with permission from the FDA, copyright 02/24/2012.\ndoi:10.1371/journal.pone.0054937.g003At the lowest level of intensity (α  = 0), the smoker saw a hypothetical pack of cigarettes on its side with one of 3 text warnings recently mandated by the U. S. Congress (Figure 3A): Cigarettes cause cancer, Cigarettes are addictive, or Smoking during pregnancy can harm your baby. These statements are factually correct but do not convey the importance and emotional impact imparted by pictorial warnings [9], [10]. In the middle level of intensity (α  = .5), the smoker saw the front of a similarly designed pack but with both the text and a picture that covered the top half of the pack (Figure 3B). In the third condition (α  = 1), the smoker saw a frontal view of the pack with a picture, the base text, and in addition, explanatory text that elaborated on the basis for the warning (Figure 3C). Elaborated warnings have been used in Canada since they were introduced in 2000 [9]. We expected the additional text to enhance the warning, resulting in the highest level of α. There were two versions of the elaborated text for the addiction and pregnancy warnings and one for the cancer warning, but we did not include a picture plus base text version for the cancer message. Thus, there were 10 different conditions with respondents nested within each condition in the experiment.\nRespondents were permitted to view the warning image for as long as they wished; however, they were not permitted to return to it after leaving the screen. They then answered a series of questions about their reaction to the warning. We assessed quit intentions (Iq) with the following question: “How likely do you think it is that you will try to quit smoking within the next 30 days?” This question format is commonly used to determine intent because it captures both the desire and ability to engage in the behavior within a specified time period [15]. This question was answered using a scale from (1) very unlikely to (4) very likely. Those with no opinion (3.2%) were assigned the score of 2.5.\nWe assessed efficacy beliefs for quitting (Eq) by averaging agreement with two moderately correlated items (r  = .33, p<.001): “It is hard for a smoker to quit smoking” (reversed scored) and “I do not need help from anyone to quit smoking.” Both items were rated on a scale from (1) strongly disagree to (5) strongly agree. We also assessed previous quit attempts with the question: “During the past 12 months, have you stopped smoking for one day or longer because you were trying to quit smoking?” (Yes/No). This item was assessed to provide a behavorial measure of quitting that should also exhibit the inverted U-shape relation with Eq prior to exposure to the warning.\nWe assessed the direct effects of the warnings on measures of Dq and Ds as checks on the success of the manipulation of warning intensity (α). Our measure of desire to quit smoking (Dq) was response to: “How much do you want to quit smoking?” with answers ranging from “not at all” (1) to “a lot” (4). To assess Ds, we asked two questions that probed emotional reactions to smoking and desire to smoke a cigarette: “Imagine you are smoking right now. How good or bad would you feel smoking a cigarette right now?” with responses ranging from “very good” (1) to “very bad” (4). The other item asked for agreement with: “I want a cigarette right now” (reversed scored). The items were correlated (r  = .33, p<.001) and were averaged to define a measure of aversion to smoking, the inverse of Ds. As expected by the model in Figure 1, our measures of Dq and -Ds were correlated (r  = .25, p<.001). Finally, answers to the question about frequency of smoking, “Do you smoke, every day (1), some days (0) or never?” were used to assess β.\nWe used linear regression to test eq. (5) using self-efficacy to quit as a measure of Eq and three levels of α (0,.5, 1) as values representing the experimental warning conditions. A test of the effect of β was conducted with a second model in which smoking frequency was added as a predictor. Predicted scores from these models were plotted to provide a visual comparison of model predictions with those in Figure 2A. A test of the U-shape relation between Eq and prior quitting behavior was also tested using logistic regression and quit attempts in the 12 months prior to the experiment. We conducted regression analyses to test the predicted effects of age, α and β on all of the observed mediators in Figure 1 (Eq, Dq, −Ds). We did not have a measure of Es, which we assumed was relatively high for all smokers and which should be positively related to β in any case. A test of the critical hypothesis that efficacy for quitting is inversely related to desire to quit was conducted by regressing Dq on Eq. As predicted by eq. (3), those with the weakest efficacy should have the strongest desire to quit.\n\nResultsTable 1 provides the response distributions for the major variables in the analysis. To assess the success of randomization to conditions, we examined differences between conditions on several outcomes. There were no differences between the 10 conditions in efficacy for quitting, F(9,3287)  = .868, p  = .55; the proportion of respondents who smoked every day (vs. some days), X2(9)  = 10.32, p  = .33; or who had tried to quit in the past 12 months, X2(9)  = 9.41, p  = .40.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Response distributions of major variables in the study (N  = 3297).doi:10.1371/journal.pone.0054937.t001Tables 2 and 3 show the results of regression analyses relevant to the predictions in Figure 1. Consistent with eq. (3), Eq was inversely related to Dq (Table 2 and Figure 4A). This confirms the hypothesis that forms the basis for the U-shape relation between efficacy and intention to quit smoking. Related to this hypothesis is the prediction that smoking frequency (β) is inversely related to quit efficacy (Eq). As seen in Table 3 and Figure 4B, this hypothesis was supported, with each increasing unit on the efficacy scale associated with lower rates of daily smoking. Frequency of smoking was also inversely related to −Ds. Table 2 also shows the relation between age and the three outcomes, all of which are consistent with the model. Age was positively related to Dq and negatively related to Eq. It was also positively related to −Ds.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Predicted relations between three measures and efficacy to quit based on regression models in Tables 2 and 3: (A) desire to quit; (B) probability of smoking frequency; and (C) probability of trying to quit in past 12 months.doi:10.1371/journal.pone.0054937.g004\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Parameters of linear regression models for desire to quit smoking, aversion to smoking, and quit efficacy with warning and efficacy as predictors.doi:10.1371/journal.pone.0054937.t002\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 3.  Parameters of logistic regression models for relation between efficacy and frequency of smoking and prior attempts to quit.doi:10.1371/journal.pone.0054937.t003Smoking frequency was negatively related to Dq in Table 2. However, this could happen because it was positively related to Ds, which would counteract a potentially positive relation with Dq. To evaluate this possibility, we conducted a separate analysis in which Ds was held constant. This analysis confirmed that smoking frequency was positively related to Dq (B  = .134, se  = .037, p<.001), which would add further to the negative relation between Eq and Dq.\nExamining the rate at which smokers had tried to quit in the past 12 months provided a test of the predicted inverted U-shape relation between Eq and behavior. As seen in Figure 4C, this relation exhibited an inverted-U shape as defined by the logistic regression model in Table 3. That model also found that smoking frequency was inversely related to prior quit attempts.\nAs expected, experimental variation in warning intensity (scaled 0, .5, 1) increased aversion to smoking (−Ds), one indicator of α. This effect was independent of efficacy, age, and smoking frequency, indicating that smokers across the efficacy continuum and ages recognized the implications of the warnings. It is noteworthy that Eq was unrelated to aversion to smoking. This was in contrast to its negative relation with desire to quit smoking. This difference is actually consistent with our predictions concerning each desire. It should be recalled that Ds was a function of β – α, while Dq was a function of α – Eq. Thus, one would not expect Eq to predict –Ds, holding constant β. It would nevertheless be expected that the warning would affect both desires. It is disappointing to find that the effect of α was not significant for our measure of Dq. A more sensitive measure of Dq may have allowed the relation between warning and Dq to emerge more clearly. Nevertheless, the measures of –Ds and Dq were related (r  = .25, p<.001), which supports their predicted relationship in the model.\nFigure 2B shows the relation between Iq and Eq as a function of the three different levels of warning intensity as predicted by the regression model in Table 4. Comparing the result with the prediction in Figure 2A indicates a remarkably similar pattern to what the model predicted. First, the overall relation between intention and Eq exhibited the inverted-U shape. Second, the interaction between warning level and quit efficacy was significant: Intentions to quit smoking were elevated as a function of α primarily among those with efficacy scores above the estimated level of Es, which appeared to be approximately .40 in this sample (based on the intersection of the three curves in Figure 2B). The mean level of Eq in the sample was only −.22 (se  =  .017), considerably lower than the observed level of Es in Figure 2B. Thus, only those with scores of Eq>−Es (representing about 36% of the sample) were likely to exhibit greater intentions in response to the warning. The remaining 64% either did not change or became somewhat less likely to intend to quit. Finally and not surprisingly, the simple effect of α did not contribute to prediction. Thus, without examining the interaction between efficacy and warning level, one would conclude (as observed before) that warnings have little overall influence on intentions to quit.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 4.  Regression parameters in test of Efficacy-Desire Model.doi:10.1371/journal.pone.0054937.t004To evaluate the success of the predicted relation between Iq and both Eq and α, we compared the variance explained by this model that only used 5 degrees of freedom to a model that included separate predictors for each of the 23 degrees of freedom represented by the fixed effects of the two predictors. Our model explained 71% of the fixed effects in the data despite using many fewer degrees of freedom. In addition, after accounting for the variation that was due to sampling error (which can be estimated by the within-subject mean square which was approximately 15% of the total), it is likely that the model accounted for over 80% of the reliable variation in the fixed effects.\nA final prediction of the model concerned the level of current smoking (β). We tested this effect by examining the relation between Iq and Eq for daily smokers versus those who smoked less often. As seen in Table 4 (step 2), smoking frequency predicted quit intentions apart from efficacy and warning intensity as predicted by eq. (5). Figure 5 shows that the curves predicted by the model in Table 4 conformed to prediction, with the curve shifting down for daily smokers without changing the inverted U-shape of the relation.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Predicted relation based on regression model in Table 4 (step 2) between intention and efficacy to quit with frequency of smoking as the parameter.doi:10.1371/journal.pone.0054937.g005Our results showed strong support for predictions from the EDM. The inverted U-shape function between quit efficacy and strength of intentions to quit smoking was supported for both quit attempts in the past year and in response to a warning about cigarette hazards. This finding reflects the model’s unique prediction that given smoking’s addictive properties, the desire to quit smoking (Dq) and efficacy to do so (Eq) are inversely related and that these components of the decision model combine multiplicatively to form the motivation and hence intention to quit. The model’s prediction regarding the effect of a warning on quit intention was also supported in that the effect of the warning was only favorable for those with Eq>−Es. Indeed, as Eq declined, the warning became ineffective, exhibiting a boomerang effect. Finally, the prediction that smoking frequency would shift the intention curve up or down but would not interact with quit efficacy was also supported.The findings show why previous research has failed to observe effects of warnings on the quit intentions of smokers at the population level [13], [14]. Even though smokers at all levels of efficacy recognized the implications of the warnings (as assessed by the effect on −Ds), it was primarily smokers with stronger quit-efficacy who reported increased intentions to try to quit. Ironically, the smokers who most desired to quit (i.e., those with lower efficacy) displayed reduced intentions. Thus, the effects of health warnings were limited to lighter smokers who have relatively stronger efficacy beliefs regarding quitting than they have regarding smoking. This pattern is consistent with research showing that it is primarily lighter smokers that are successful in quitting programs [37], [38]. Thus, the results suggest that the question regarding pictorial warnings is not whether they are effective in helping smokers to quit, but for which groups they are likely to be helpful. Our findings and the EDM indicate that level of smoking and efficacy for quitting are important parameters in determining the effects of pictorial warnings.Although most theories of health behavior change would predict that smokers with greater quit efficacy would be more likely to respond favorably to a warning, the EDM is unique in predicting this outcome when Eq>−Es. It is also unique in its ability to predict the conditions under which one will observe a boomerang effect. Finally, the EDM uniquely predicts the inverted U-shape relation between Eq and Iq for an addictive habit. We thus see the model as providing important insights into the effects of health warnings that have not been predicted by previous theories of behavior change.The finding that smokers with weaker self-efficacy for quitting actually became less intent on quitting following exposure to warnings is not a new phenomenon. This adverse effect of health information has been observed in studies of smoking and alcohol use [17]. Indeed, the classical study by Janis and Feshbach [39] found that, as the fear arousing character of messages increased, intention to follow through with the recommended health practice declined. This finding led to the prediction that fear arousing messages will exhibit an inverted U-shape relation to behavior change, whereby increases in fear initially lead to greater change but to decreases after an intermediate level has been reached. Contrary to this hypothesis, however, subsequent research failed to find the inverted U-shaped relation. Indeed, most research finds a weak but positive relation between fear arousal and message acceptance [40].The EDM predicts a positive relation between the intensity of the message (i.e., α) and intentions/behavior when the efficacy for the recommended behavior exceeds that of the pre-existing unhealthy behavior. However, the model predicts decreases in behavioral intentions for those with relatively weak self-efficacy. It is quite likely that this was the case in Janis and Feshbach, which tested the effects of complex recommendations for repeated tooth brushing in early adolescents. Thus, the EDM could predict the effect observed by Janis and Feshbach and others that messages that increase fear regarding the recommended behavior will nevertheless backfire for those who have weak efficacy to change. The finding that such boomerang effects occur more often for addictive behaviors [17] is also consistent with the model. Furthermore, the EDM provides more precise predictions for the conditions under which one can expect such boomerang effects.The tendency to reject messages as they increase in fear-arousing capacity has been ascribed to “defensive processing” in which the recipient of the message argues against the message and thus rejects its recommendation [40]–[42]. However, the EDM and the present results suggest that defensive processing is not necessary to explain message rejection. Smokers at all levels of self-efficacy reported increased unpleasant thoughts about smoking and felt disinclined to smoke a cigarette shortly following exposure to the warning. These reactions suggest that they did not reject the message outright. Instead, the results are more consistent with the prediction from the EDM that the multiplicative relation between desire and self-efficacy to quit leads to less motivation to adopt the recommended behavior among those with low Eq. As their desire to quit increased, their negative level of self-efficacy reduced rather than increased their motivation to quit, leaving them less intent on quitting.Aside from effects of warnings on quit intentions, the EDM predicts that addictive behaviors will exhibit an inverted U-shape function in relation to quit efficacy. This novel prediction was supported both for prior attempts to quit as well as immediate reactions to warnings. Thus, it was not just intentions that reflected this pattern but reports of behavior. Although this relationship has not to our knowledge been identified before, it is consistent with the observation that those who engage in behaviors that are difficult to quit often lose the motivation to desist from the unhealthy habit even if they succeed in reducing the behavior [33]. The EDM’s explanation for this phenomenon is that as the efficacy for quitting increases, it is matched by reduced desire to quit. As a result, the motivation to quit declines. However, increasing the desire to quit, as was done in the present experiment, should allow the beneficial effects of high quit efficacy to emerge.The model also shows why smokers find it so difficult to quit. If smokers succeed in reducing their smoking habit as represented by the influence of β (perhaps by ingesting nicotine through a patch), they will experience less Ds, which will reduce Ms. Indeed, the model indicates that a reduction in β will raise Iq independent of α and Eq. However, as proposed in Figure 1, a reduction in β increases Eq. This moves them closer to the right side of the inverted-U relation with Iq. Thus, any favorable effects mediated by Ds will be offset and potentially outweighed by an opposite effect on Eq. These opposing forces could actually lead to a decrease in quit intention. As a result, once an addictive habit is created, the forces that motivate it can entrap the addict in a difficult to break cycle of continued dependence on the habit.Despite the favorable effects of warnings suggested by the EDM, the model also predicts that the quit intentions of those with weak efficacy beliefs will be reduced as a result of exposure to warnings. This suggests that an effective public health strategy will require a two-pronged approach. One priority should be to encourage all smokers to reduce their habit as a transitory goal to eventually quitting. Although most smokers may not be able to quit, they may be able to reduce the strength of their habit. Indeed, recent surveys suggest that U.S. smokers have been doing exactly that [36]. The model suggests that recent efforts to reduce Es, through price increases and restrictions on smoking in public, may have been responsible for these reductions. That is, as long as  is greater than zero, decreasing Es should increase Iq. Since this condition is likely to be the case for heavy smokers, efforts to reduce Es should increase Iq. However, because reductions in smoking can also sap the motivation to quit, it will also be necessary to counteract this tendency with repeated exposure to health warnings that are periodically refreshed so that their effect does not wear off. This could enable persons who smoke to maintain stronger quit intentions and to break free of the habit.This study has limitations that should be examined in subsequent research. The findings are based on only a single exposure and the effects of warnings may intensify as smokers are exposed to warnings over time. The model predicts that the lightest smokers with the strongest self-efficacy for quitting will be most successful in quitting. However, we have not tested this hypothesis here. We also did not have a very sensitive measure of β, which the model suggests is a major factor in intentions to quit. Finally, we did not assess self-efficacy for smoking, which the model predicts will have an influence on quitting. We assumed that it was relatively high compared to self-efficacy for quitting. However, future research should look at variation in this form of self-efficacy as well. This parameter could be assessed by asking about barriers to smoking that the smoker experiences, such as restrictions on places to smoke, increases in prices for cigarettes, and social disapproval for smoking.In conclusion, the EDM sheds light on why models that assume a rational response to warnings do not account for the behavior of persons addicted to a behavior such as smoking. The model translates insights from current neurobiological models of addiction [19], [22]–[25] into concepts that have been employed in models of behavior change [15], [17], [18]: namely, attitudes toward addictive habits (desire) and the lack of control over their cessation (self-efficacy). The model is also consistent with neuroscience theories that postulate separate approach and avoidance motives that underlie affective experience and that can produce extreme conflict in persons with a serious addiction such as smoking. Thus, the model shows how the barriers to quitting an addictive habit such as smoking are so imposing that they can trap the person who is addicted in a continuing cycle of dependence and frustrate efforts to reduce this life-threatening habit."
        },
        "10.1371/journal.pone.0089603": {
            "author_display": [
                "Mario Soliño",
                "Begoña A. Farizo"
            ],
            "title_display": "Personal Traits Underlying Environmental Preferences: A Discrete Choice Experiment",
            "abstract": [
                "\nPersonality plays a role in human behavior, and thus can influence consumer decisions on environmental goods and services. This paper analyses the influence of the big five personality dimensions (extraversion, agreeableness, conscientiousness, neuroticism and openness) in a discrete choice experiment dealing with preferences for the development of an environmental program for forest management in Spain. For this purpose, a reduced version of the Big Five Inventory survey (the BFI-10) is implemented. Results show a positive effect of openness and extraversion and a negative effect of agreeableness and neuroticism in consumers' preferences for this environmental program. Moreover, results from a latent class model show that personal traits help to explain preference heterogeneity.\n"
            ],
            "publication_date": "2014-02-20T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 822,
            "shares": 0,
            "bookmarks": 0,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0089603",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0089603&representation=PDF",
            "fulltext": "IntroductionIt is broadly recognized that the specific behavior of individuals is conditioned by individual factors, home-site factors [1], [2], [3], [4], [5] and a set of formal rules and socially accepted informal rules [6], such as those of family or culture. Personality also plays a role in human behavior and thus, can influence environmental concerns [7], environmental engagement [8] and consumer decisions on environmental goods and services [9].\nSeveral environmental studies have considered the influence of attitudes or tastes [10], [11], [12], [13] and psychological constructs [14], [15] on individual preferences. The influence of personality dimensions on environmental preferences has been less examined. To correct this gap, a reduced version of the BFI such as the BFI-10 is a suitable instrument to incorporate personality traits in environmental valuation, since it is a recommended approach for research in which participant time is limited and when personality assessment would otherwise be impossible. Environmental economics and more specifically, environmental valuation, uses survey-based instruments to analyze consumer behavior among natural and environmental resources. In this case, the BFI-10 shows acceptable psychometric properties [16] and is a suitable instrument to introduce the personality traits into the economic analysis.\nThe Big Five Inventory (BFI) is a personality survey based on a set of phrases used to measure the Big Five dimensions of personality, i.e., extraversion, agreeableness, conscientiousness, neuroticism and openness [17]. Although BFI can be answered in less than 10 minutes, there is a growing demand for shorter instruments [18], [19], [20]. Reduced scales of BFI cannot be used as a common approach to assess personality [16], [20], but is a good alternative to implement personality in other scientific disciplines different from psychology, such as economics.\nThe paper is structured as follows. Section 2 presents the theoretical underpinnings of Discrete Choice Experiments and the BFI-10. Section 3 outlines the questionnaire used to analyze the influence of personality traits on environmental valuation. In Section 4 we present the main results of the study. Finally, Section 5 is devoted to conclusions and discussion.\nMaterials and Methods\nThe BFI-10\nThe Big Five Traits allow us to classify people by analyzing their ratings in a set of short phrases characterizing five independent personality dimensions. Following [16], the BFI-10 was implemented in order to analyze the influence of personality traits in environmental valuation. Ten short phrases were used to represent the big five personality dimensions, with just 2 items per dimension (extraversion, agreeableness, conscientiousness, neuroticism and openness). Individuals rate these 10 short phrases (Table 1) describing their personality on a five-step scale from 1 (disagree strongly) to 5 (agree strongly).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Short phrase items used in the BFI-10.doi:10.1371/journal.pone.0089603.t001A comprehensive analysis of different interpretations of these dimensions is presented in [21], [22]. For the purpose of this article, we summarize the focal interpretation of the five personality dimensions:\n\n\n\n\nExtraversion: people classified as “extroverts” show social adaptability and interpersonal involvement, and they are talkative, assertive, active, outgoing, and outspoken. Therefore, we expect a positive likelihood of these people contributing to the development of a new environmental program.\n\nAgreeableness: this dimension implies pro-social behavior, and people scoring high on agreeableness tend to be sympathetic, kindly, appreciative, affectionate, soft-hearted, warm and generous. There is no prior expectation for the influence of this dimension on choices for the environmental program. Nevertheless, a significant positive effect could be interpreted as a warm-glow bias (friendly behavior).\n\nConscientiousness: they are organized, thorough, playful, efficient, responsible and reliable. They are “global-thinkers” and, due to the regional character of the proposed environmental program, we expect a negative or non-significant relationship between high scores in this dimension and the probability of accepting a small (not far-reaching) environmental program.\n\nNeuroticism: typically, people classified as “neurotics” are tense, anxious, nervous, moody, and tend to worry. Therefore, a negative relationship is expected between high scores in this dimension and the probability of accepting a novel environmental program. These personalities might tend to choose the status quo option rather an option of change [9]. Moreover, we expect extreme behaviors and a high dispersion of responses from neurotics.\n\nOpenness: these are people open to experience; they have wide interests and are imaginative, intelligent and original. Therefore, we expect a positive likelihood of these people contributing to the development of a new environmental program.\n\n\n\nDiscrete choice experiments\nDiscrete Choice Experiments (DCEs) simulate markets in which different environmental goods and services compete in a realistic trade-off manner. Individuals choose between different alternatives involving environmental attributes, according to their own preferences and budget constraints. DCEs allow researchers to infer consumers' preferences and implicit prices for several characteristics embedded in a prospective policy or environmental program. Since accounting for unobserved heterogeneity is a matter of importance in the estimation of behavioral models, two mixed approaches are applied: a Random Parameter Logit (RPL) model [23] and a Latent Class (LC) Model [24].\nThe RPL model introduces unobserved preference heterogeneity, i.e., allows the coefficients of observed variables to vary randomly among people rather than being fixed. In RPL models, the individual's i indirect utility function (Vi) is usually represented as a linear additive expression:(1)where αj is an alternative specific constant (ASC) for each option (j =  1,2,...,J) in the choice set, Sij is the associated attribute vector,  is the vector of population mean preference values, θi represents the deviations in individual preferences with respect to the mean values, and  is an i.i.d. type I extreme value random component of utility which cannot be observed by the researcher. Coefficients β vary in the population with density ƒ(β|Ω), with Ω denoting the parameters of density. If we assume that the individual's preferences, as represented by βi  =   +θi, follow the same decision heuristics for all choice (t =  1,2,..,T), the probability of individual i's observed sequence of choices [y1, y2,...,yT] is calculated by solving the integral:(2)where j is the alternative chosen in choice occasion t and μ is a scale parameter.\nAs pointed above, another way to capture taste variations is through LC or by a combination of LC and RPL. In this study we apply a latent class model with common and specific random parameters. Latent class has been long applied in environmental modeling, but not many studies combine both mixed approaches. Some examples of the dynamism of LC can be seen in [25], describing the choice of excursions in natural protected areas; [26] for transport issues; or [4], [27] for valuing environmental goods.\nFollowing [24], the model has several classes (x) for the probability that case i selects alternative j at the replication t, given attribute value  (characteristics of alternatives), covariates , used to predict class membership (x), and random parameters (RP) denoted by Fdi being d the number of factors. The Fdi are assumed to be standard normally distributed and mutually independent () where I is the identity matrix.\nIn a general case, with a conditional logit model, the response probability is:(3)where  is the systematic component in the utility of alternative j at replication t given that case i belongs to latent class x. The linear model is:\n(4)The term  is a linear function of an alternative specific constant , attribute effects  and random effects. The first term () defines random effects for the alternative specific constants, and the following terms define random effects for the attributes.\nIn a latent class choice model with random parameters, the probability density has this form:(5)where\n(6)A multinomial logit is specified in which class membership is regressed on covariates;(7)with linear term , being  the constant corresponding to latent class x and  the effect of the rth covariate for the Class x.\nIn our study, personal traits are included as covariates to explain belonging to a specific class, assuming that members of the same class share a pattern of choice. On the other hand, we assume that attributes from the discrete choice experiment behave randomly. This specification allows us to identify whether one or more attributes are not random to a specific class, improving the accuracy of the model.\n\n\nThe environmental valuation questionnaire\nThe choice experiment aims to quantify the population's preferences for several externalities associated with a program developing resin-tapping in Spain, i.e., biodiversity of flora, employment generated by the strengthening of the sector, reduced risk of fire, the presence in the market of “certified products” that incorporate natural resin, and the duration and the annual cost of the program. The levels presented are detailed in Table 2.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Attributes and Levels of the Choice Experiment.doi:10.1371/journal.pone.0089603.t002The discrete choice experiment is implemented in an environmental valuation questionnaire structured in 4 sections. Section 1 contains questions relating to the relationships between the interviewee and forests. The second section examines knowledge about resin-tapping activities. Section 3 contains the discrete choice experiment. The choice sets were designed following an optimal in difference design as proposed by [28]. From an optimal design size of 18 choice cards, a blocking strategy was performed and each respondent was confronted with nine choice cards containing two prospective programs and the status quo scenario (where the program is not implemented and the individual does not pay any extra fee). The experimental design is balanced, i.e. all the attribute levels are shown the same number of times to each individual. In general, unbalanced designs should be avoided where possible because statistical power differs within attribute levels and/or between attributes, and artificial correlations with grand means or model intercepts are introduced [29].\nFinally, the fourth section is focused on socio-economic characteristics and includes the BFI-10 questions. The questionnaire was administered in September and October 2012 to 2,224 individuals. The target population was adult dwellers from Castilla y León, since resin-tapping activities in Spain are concentrated in this region (located in central Spain). The database used for this analysis is available on request from the authors.\n\nResultsFrom the individuals' ratings on the BFI-10 short-phrases items, the average score is calculated for each of the big five personality dimensions. The results are shown in Table 3. Conscientiousness shows the higher value and neuroticism shows the lower one, whereas extraversion, agreeableness and openness show similar values. Low correlations are detected for the five dimensions of personality. This is a good indicator of the independence of these personality dimensions.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 3.  Average score and correlations for the big five personality dimensions.doi:10.1371/journal.pone.0089603.t003Table 4 presents the results of the RPL estimations using 500 Halton draws (Train, 2003) and NLOGIT 5.0 software. All attributes, apart from the cost and the Alternative Specific Constant (ASC) were assumed to be normally distributed. Differences of the Akaike Information Criterion (AIC) between a baseline model (without including personality traits) and the expanded-BFI model favored the selection of the last one. The purpose of using these explanatory variables in our analysis is focused on getting a better understanding on the relation between personality traits and choices, to help us explaining why individuals prefer certain things over others. The ASC was introduced in the models as a dummy variable taking value 1 to indicate that the individual does not choose the status quo scenario. The estimated coefficient for the ASC is statistically non-significant. This result suggests that individual heterogeneity is correctly captured by the model.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 4.  Random Parameters Model results.doi:10.1371/journal.pone.0089603.t004From these results (Table 4) we can state that personality dimensions have an effect on the choices individuals make. Our results show that the most influential trait is openness, that is, individuals showing a high score in openness (people open to new experiences) are more prone to choose the options for protection than people with other dominant traits. All covariates have the expected sign except agreeableness, for which we did not have any a priori expectations about the sign. Since stated preference techniques are prone to get a positive answer from individuals whatever they are offered, our hypothesis was that individuals who show this trait could favor the yea-saying bias.\nIt would thus be worthwhile to know which aspects of the personality make us value certain attributes over others; that is, the influence of personality traits on preferences for environmental programs. For this we run a latent class model using Latent Gold 4.5 software. In order to benefit from the virtues of the RPL models, we consider that some attributes may have just one source of variability (for FireL, FireM, Time, CertM and CertH) while for others (Employ, BioM, BioH and Cost) we identified two, one dependent on the class and another constant for all the classes. In other words, we assume that for some attributes dispersion around the mean estimates is the same for all individuals, like the RPL shown above, while for some other attributes we also have identified variability in the class (intra-class).\nTable 5 shows the mixed LC-RPL model, where the SDPD column contains the Standard Deviations of the estimates for the classes and the last column, Common SDPD shows the Standard Deviations for the variables, regardless of the class. This decomposition of variability has the advantage of informing us about possible sources of heterogeneity. For example, the estimates for medium risk of fire (FireM) are positive for all classes and show high variability for everybody, while for low risk (FireL) the variation is due to the composition of the classes. For some, this is something desirable, while for others it seems it is enough to achieve the medium level; some risk of fire might be acceptable for certain groups of the society because low risk could not be credible or because the cost of achieving that goal may be disproportionate. Fire variables were expected to behave quite homogeneously and have low variability. On the contrary, BioM and BioH show great inter- and intra-class variability, as shown by the common and by-class SD. Cost is another variable with great variation. Traditionally, the cost of the program parameter is considered as fixed, since the elicitation of willingness to pay (WTP) measures complicates the computation. In this study we are not interested in getting the benefits (in terms of willingness to pay) of the program but to control and explain heterogeneity of preferences, thus Cost is considered to have a random nature. Again, two sources of variability have been identified, one common to all individuals no matter the class, and another for the classes. As with the BioM and BioH, for some classes the decisions on this attribute are not random, as is the case for classes 3 and 4. If we go into the composition of the classes attending to their personality traits, we can see that class 4 is the one with lower random behavior and Openness is the trait of their members (around 13% of the sample). The most valued aspect of the program for this class is biodiversity followed by fires, both a medium level but valuing positively high levels of biodiversity and negatively low levels of risk of fires.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 5.  Latent Class Model results.doi:10.1371/journal.pone.0089603.t005Classes 1, 2 and 6 are at the other end, showing high variation of their mean parameters. Attending to the personality traits, class 6, which only represents 3% of the sample, has none that could describe their mean personality. Class 2, with almost 22% of the sample, groups the people with low extraversion and positive and high scores on agreeableness and conscientiousness. Class 1 is the biggest with almost 38% of the participants. They have high scores on openness (positive) and on agreeableness (negative). For people in this group cost is the least important attribute and FireL the most important, and they will choose any improvement for the management of these areas. As expected, this group has a high variability on all the parameters. This may be due, among other things, to the fact that is the most populated class.\nA trait that deserves attention is neuroticism, which is significant only for class 3 (14% of the sample) together with a negative openness. Our results confirm [30], indicating that neuroticism could be a weaker predictor of a range of outcomes. As expected, few aspects of the environmental program are appreciated by this group. Key attributes like fire are not even significant. Class 5 (around 11% of the sample) is pro-environmental and the positive sign of cost could have an explanation in the positive and significant traits for this class: extraversion and openness.\nFinally, in Section 2 we hypothesized that a positive and significant Agreeableness could lead to a warm glow behavior, but our results shows that the assertive aspect of Extraversion, combined with other traits could be more prone to this kind of bias (see Class 5). Our experiment is not big enough to provide evidence or establish these relations; thus only further research could lead us to find if those relations are possible.\nThis paper relates personality traits directly with environmental preferences. It shows that personality plays a role in human behavior and thus, it can influence consumer decisions on environmental goods and services. In environmental valuation with stated preference methods a hypothetical market is simulated through a survey instrument. In order to avoid fatigue effects, the survey time is very limited, thus a key issue for implementing personality traits in environmental economics is the time required to answer the psychological questions. For this reason, a reduced version of Big Five Inventory with acceptable psychometric properties (the BFI-10) was applied in order to analyze the influence of personality traits in a discrete choice experiment. Therefore, this paper uses a psychometric approach to highlight the relevance of personality traits in consumer decisions, relating directly personality to economic behavior. Other approaches such as the New Ecological Paradigm scale [31] have connected pro-environmental behavior with economic behavior (i.e. a higher willingness to pay). Moreover, in the literature are numerous examples connecting different types of behavior and, what it is very interesting too, connecting the “values” such as the Schwartz Value Theory [32], [33] to explain individual preferences [34]. Discussions on these issues can be found in several papers from the 2011 International Choice Modelling Conference [35]. We have tried with this experiment to go a step backwards, since we agree in that values are partly inspired by personality. But we recognize that this study is a short piece of the research on the influence of personality on individual preferences, and other relations will be tested in the future.This study brings new insights on the relationship between personality and environmental values. Results show a positive effect of openness and extraversion and a negative effect of agreeableness and neuroticism in consumers' preferences for this environmental program. Nevertheless, these effects are diverse and affect the heterogeneity of preferences, the preferred characteristics and the desirability of the program itself. Further research is necessary to explain, for example, why estimates from individuals with high scores on conscientiousness have higher random behavior than those showing a neuroticism trait. That is, to better explain the relationship between personality traits and tastes and preferences. In summary, this work constitutes a new step to understanding the involvement of individuals in environmental conservation and could help to better design appropriate ways to reach certain groups and ensure the success of environmental and social goals."
        },
        "10.1371/journal.pone.0095085": {
            "author_display": [
                "Zhao Xiaojun",
                "You Xuqun",
                "Shi Changxiu",
                "Gan Shuoqiu",
                "Hu Chaoyi"
            ],
            "title_display": "Reference Valence Effects of Affective S–R Compatibility: Are Visual and Auditory Results Consistent?",
            "abstract": [
                "\nHumans may be faster to avoid negative words than to approach negative words, and faster to approach positive words than to avoid positive words. That is an example of affective stimulus–response (S–R) compatibility. The present study identified the reference valence effects of affective stimulus–response (S–R) compatibility when auditory stimulus materials are used. The researchers explored the reference valence effects of affective S–R compatibility using a mixed-design experiment based on visual words, visual pictures and audition. The study computed the average compatibility effect size. A t-test based on visual pictures showed that the compatibility effect size was significantly different from zero, t (22) = 2.43, p<.05 (M = 485 ms). Smaller compatibility effects existed when switching the presentation mode from visual stimuli to auditory stimuli. This study serves as an important reference for the auditory reference valence effects of affective S–R compatibility.\n"
            ],
            "publication_date": "2014-04-17T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 406,
            "shares": 2,
            "bookmarks": 1,
            "url": "http://www.plosone.org/article/info:doi/10.1371/journal.pone.0095085",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0095085&representation=PDF",
            "fulltext": "IntroductionSome psychologists believe that the human reaction of evaluation is unconscious if not all of the encountered stimuli fall on a good or bad dimension. The human reaction of evaluation is presumed to have a transitory avoidance and approach behavioral tendency [1]–[3]. A recent study [3] identified the reference valence effects of affective stimulus–response (S–R) compatibility when visual stimulus materials were used (including visual text and visual images). Matching stimulus materials provides an explanation for the reference valence effects of affective stimulus–response (S–R) compatibility. The match between the stimulus and its referent is a key factor based on the valenced referent. The transitory approach behavioral tendency should be faster for the avoidance behavioral tendency when the target stimulus (such as a positive character) matches the referent (such as a positive personality trait). However, no studies have examined whether the matching account is a reasonable explanation for the reference valence effects of affective S–R compatibility when auditory stimulus materials are used.\n\nFrom the S–R compatibility effect to the affective S–R compatibility effect\nNumerous accounts have interpreted the S–R compatibility effect [4]–[6]. In terms of reaction time and accuracy, the S-R compatibility effect showed that compatibility stimulus-response tasks were better than other tasks. Related research suggested that visual and response selections occur in different stages. In a simple present–absent detection task, participants were slower to respond ‘present’ when the single arrow pointed to the right (corresponding to the right hand). But, when the study changed the conditions (such as encouraging participants to process the identity of the arrow), the S–R compatibility effect was identified [7]. In addition, avoidance cues led to improved performance in cognitive control tasks [8]. A recent study found that an affective stimulus valence contributes to behavior that greatly leads to a compatible transformation in distance [9]. In spatial cognition, the absence of spatial compatibility effects when there is a strong temporal overlap suggests that response conflicts are caused by stimulus-related priming [10]. In the dual processing models of S–R compatibility, the processes of spatial and imitative compatibility are independent [11]. In cognitive mechanisms, negative compatibility effects are generated by perceptual mechanisms [12]. Similarly, reaction activation is affected by negative priming effects. This activation may be generated by a simplified image of a hand [13]. The latest research results cannot be interpreted with current accounts (specific-muscle-activation, evaluative response coding, distance-regulation, etc.) but are in agreement with the matching account [3]. The affective stimulus–response (S–R) compatibility effect demonstrates that participants are faster to avoid negative stimuli than to approach negative words. Similarly, participants are faster to approach positive stimuli than to avoid positive words [1]. In previous studies of visual positive and negative stimulation, it was not clear whether auditory stimuli have the same effect. The activity of mechanism is reflected in two aspects: valence effects and approach/avoidance action.\n\n\nValence effects\nStudies of valence effects are mainly concentrated in the following areas: gender differences, personality, emotions and brain cognition. The dependence of valence-specific effects in facial emotion perception on the perceiver's gender is unclear [14]. Regarding gender, small developmental increases occur more for the effects of valence than for arousal [15]. Regarding personality, positive adjectives are better recalled than negative adjectives when they are encoded in reference to the self [16]. Regarding trait identification, participants perform much better in positive versus negative behavior [17]. The valence of self-evaluative thoughts is thought to mediate the impact of personality traits on mood [18]. In the field of emotion, the effect of arousal on the connectivity within the emotional memory network depends on item valence [19] A valence-specific laterality effect is demonstrated in original stimuli when stimuli of the same emotion are presented as a block [20]. Experiments following ‘no think’ instructions for memories associated with emotionally negative material lead to significant memory suppression [21]. In addition, imagery manipulates emotional valence and arousal [22]. Mneimne found significant valence through visual field interactions [23]. In brain cognition, corrugator muscle activity are associated with left hemi-face dominance during high and low arousal negative picture blocks, whereas zygomaticus muscle activity are associated with right hemi-face dominance during high arousal positive picture blocks [24].\n\n\nApproach and avoidance action\nApproach and avoidance motivation (or action) is very important for human function [3]. In a study with stimulation materials, an avoidance strategy was more effective in decreasing prejudice in a negative context rather than in a positive context [25]. Movement times are slower in the context of avoidance conflicts relative to approach conflicts [26]. In social cognition, some evidence indicates that avoidance temperament (neuroticism and negative affectivity) is a predictor of an approach goal, and approach temperament (extraversion and positive affectivity) is a predictor of an avoidance goal [27]. Approach/avoidance moderates the impact of comparison information on self-evaluation [28].\n\n\nCurrent study\nParticipants are faster to avoid negative stimuli than to approach negative words; similarly, participants are faster to approach positive stimuli than to avoid positive words [1]. The activity of mechanism is reflected in two aspects: valence effect, approach and avoidance action. Some studies on the valence effects of S–R compatibility exist, but studies on stimulation materials of audition are rare. The studies were based on vision, thus it is not clear if auditory stimuli have the same effect. One's perception of emotional content depends on auditory and visual cues. Auditory stimulus may affect reference valence effects of affective S–R compatibility. Auditory cues contain fundamental frequency changes during an utterance [29]–[30]. The related research shows that visual stimuli influence neural representation in the auditory cortex and may override auditory perceptions derived from auditory stimuli [31]. In our life, a lot of information belongs to auditory stimuli (e.g., the sound of the car). When crossing the street, it is assumed that the sound of speeding car can be heard, will you be more quickly to avoid this kind of negative stimuli? Visual and auditory stimulus are different for human to process according to the human physiology. In addition, the visual processing is a kind of image processing, while auditory processing is more a kind of meaning processing. Based on the above considerations, the present study aims to explore the auditory reference valence effects of affective S–R compatibility.\n\nMethod\nParticipants\nTwenty-three college students between the ages of 18 and 20 (M = 19.0 years, SD = 0.71, 11 women and 12 men) participated in the experiment. All participants were Chinese speakers. All participants were right-handed and had normal or corrected-to-normal vision (auditory). All participants provided their written informed consent to participate in this study. The study was approved by the academic and ethics committee of school of education in Anqing Normal College. The academic and ethics committees approved this consent procedure. YUE FEI fought against the Jin race and became a national hero of the Chinese history. Before his departure to defend his country against the Jin army, his mother the tattooed four characters ‘jin zhong bao guo’ on his back to encourage him to serve his homeland with loyalty. QIN HUI is one of the ten biggest traitors in the history of China because he had YUE FEI executed for an ‘unwarranted’ count; an act which left a smell for ten thousand years. Thus, the reference objects for this experiment were YUE FEI and QIN HUI. All participants were familiar with the ancient Chinese characters (YUE FEI and QIN HUI) of which YUE FEI and QIN HUI were characteristic. All participants had clearly good and bad evaluations for YUE FEI and QIN HUI, respectively.\n\n\nMaterials\nA computer was used to present stimulus and record keyboard response. A 17-inch HPL1908w CRT computer monitor with 1440×900 resolution, true colour, and 60 Hz refresh rate was used. All experiment materials were presented through the DMDX stimulus presentation system. The sizes of the stimulus materials had default values. Distance to the screen was about 60 cm. The experiment material was Chinese material. There were 16 personality adjectives in the experiment. Each of the adjective was composed of 3–5 characters. The 16 personality adjectives were from factors of 16PF scale. Before the formal experiment, experimenter asked for informal participants to assess the adjectives in order to distinct them into positive and negative. In each group experiments, half of the adjective is positive (16), half of the adjective is negative (16).\nRecords of auditory experiment materials were produced by man using mandarin. Before formal experiment, the participants need to identify the contents of the tape recording and confirm that they can understand the recording materials. The auditory experiment materials embedded in the DMDX procedure. Auditory materials (taking up 1500 ms time) were played through mini sound box, which standed at the central position in front of computer. The experiment was conducted with the participants inside a moderately bright laboratory. The screen background was white, while visual stimulation was black (fixation point is black ‘+’).\n\n\nDesign\nThe conducted study was a 2 (gender: male vs. female) × 3 (presentation mode: visual words vs. visual pictures vs. auditory stimuli) × 2 (reference objects: YUE FEI vs. QIN HUI) × 2 (valence of stimulate: positive vs. negative) × 2 (direction of movement: approach vs. avoidance) mixed design experiment. Gender was a between-group variable; the rest were within-group variables. The dependent variable was RT. Because all of the factors in the analysis except gender were within-subject manipulations, the design was adequately powered to detect medium-to-small effect sizes.\n\n\nProcedure\nExperimental flow diagrams are presented in Figs. 1 to 6. All of the experimental materials were presented through the DMDX stimulus presentation system, which includes 6 subsystems with the same test frequency of 32. In the experiment, when a tip-stimulated black ‘+’ appeared within 500 ms, the subjects were instructed to focus on the screen. The test stimulus appeared at 2000 ms. The participants were asked to respond by pressing the arrow key within 3000 ms. After showing an empty screen for an additional 1000 ms, the system ran the next experimental item. The Group 1 condition tested a consistent pattern based on visual words. A reference name appeared on the center of the screen, and a corresponding personality adjective appeared on either the left or the right side of the reference name. To distinguish between the two levels of valence of stimuli, the study set up personality adjectives (positive or negative), reference objects, and movement adjectives to determine consistent effect reaction times for the different directions of movement. When YUE FEI and a commendatory word appeared, the subjects were asked to click the arrow (starting from the personality adjective) pointed toward YUE FEI (for instance, in ‘YUE FEI Intelligent’, the instruction was to press the ‘←’ key). Approach or avoidance action was reflected in the ‘←’ key or ‘→’ key responses. The computer automatically recorded the reaction time. When the screen presented YUE FEI and a derogatory word, the subjects were asked to click the arrow directed away from YUE FEI. When the screen presented QIN HUI and a commendatory word, the subjects were expected to click the arrow (from the personality adjective) pointed away from QIN HUI. When QIN HUI and a derogatory word were presented, the subjects were asked to click the arrow (from the personality adjective) pointed toward QIN HUI. A practice module was used prior to the formal experiment. After the first group experiment, subjects were asked to take a two-minute break. The Group 2 condition tested an inconsistent pattern based on visual words. When the screen presented a combination of YUE FEI and a commendatory word, the subjects were asked to click the arrow directed away from YUE FEI (for example, in ‘YUE FEI intelligent’, the instruction was to press the ‘→’ key). When YUE FEI and a derogatory word appeared, the subjects were expected to click the arrow pointed toward YUE FEI. When QIN HUI and a commendatory word appeared, subjects were asked to click the arrow pointed toward QIN HUI. When QIN HUI and a derogatory word were presented, the subjects were asked to click the arrow directed away from QIN HUI. The Group 3 condition tested a consistent pattern based on visual pictures. A picture of the reference object was displayed on the screen, and a corresponding personality adjective appeared on either the left or the right side of the picture. Reaction model is similar to group 1. The Group 4 condition tested an inconsistent pattern based on visual pictures. A picture of the reference object appeared on the screen, and a corresponding personality adjective appeared on either the left or the right side of the picture. Reaction model is similar to group 2. The Group 5 condition tested a consistent pattern based on auditory stimuli. The pronunciation of the name of the reference object appeared in the center of the screen, and a corresponding personality adjective appeared at either the left or the right of this pronunciation. Reaction model is similar to group 1. The Group 6 condition tested an inconsistent pattern based on audition. The pronunciation of the name of a reference object was displayed in the center of the screen, and a corresponding personality adjective appeared at either the left or the right side of the pronunciation. Reaction model is similar to group 2. To eliminate the order effect, all experimental conditions were conducted randomly.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Positive approach - negative avoidance (consistent pattern).The Group 1 condition tested a consistent pattern based on visual words. A reference name appeared on the center of the screen, and a corresponding personality adjective appeared on either the left or the right side of the reference name.\ndoi:10.1371/journal.pone.0095085.g001\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Positive avoidance – negative approach (inconsistent pattern).The Group 2 condition tested an inconsistent pattern based on visual words. A reference name appeared on the center of the screen, and a corresponding personality adjective appeared on either the left or the right side of the reference name.\ndoi:10.1371/journal.pone.0095085.g002\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Positive approach - negative avoidance (consistent pattern, visual pictures).The Group 3 condition tested a consistent pattern based on visual pictures. A picture of the reference object was displayed on the screen, and a corresponding personality adjective appeared on either the left or the right side of the picture.\ndoi:10.1371/journal.pone.0095085.g003\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Positive avoidance – negative approach (inconsistent pattern, visual pictures).The Group 4 condition tested an inconsistent pattern based on visual pictures. A picture of the reference object appeared on the screen, and a corresponding personality adjective appeared on either the left or the right side of the picture.\ndoi:10.1371/journal.pone.0095085.g004\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Positive approach - negative avoidance (consistent pattern, audition).The Group 5 condition tested a consistent pattern based on auditory stimuli. The pronunciation of the name of the reference object appeared in the center of the screen, and a corresponding personality adjective appeared at either the left or the right of this pronunciation.\ndoi:10.1371/journal.pone.0095085.g005\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Positive avoidance – negative approach (inconsistent pattern, audition).The Group 6 condition tested an inconsistent pattern based on audition. The pronunciation of the name of a reference object was displayed in the center of the screen, and a corresponding personality adjective appeared at either the left or the right side of the pronunciation.\ndoi:10.1371/journal.pone.0095085.g006\nResultsIncorrect responses (3.9%) and responses with latencies below 300 ms and above 4000 ms (0.8% of correct responses) were discarded (see Table 1 for RT data). The results were analyzed using a mixed ANOVA.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  RT of presentation mode.doi:10.1371/journal.pone.0095085.t001The result shows that the presentation mode (F (2, 42) = 925.32, p<.001, Partial η2 = .98) has a significant effect on RT. The results show that the interaction between the presentation mode and the reference object (F (2, 42) = 3.42, p<.05, Partial η2 = .14) also has significant effects on RT. The interactions among the presentation mode, the reference object, and gender (F (2, 42) = 5.54, p<.01, Partial η2 = .21) have significant effects on RT as well. The same holds true for the interaction between the presentation mode and the valence of stimulus (F (2, 42) = 26.07, p<.001, Partial η2 = .55) and the interaction between the presentation mode and the direction of the arrow (F (2, 42) = 4.11, p<.05, Partial η2 = .16). Post-hoc analyses indicated significant differences in the levels of the presentation mode (RT audition>RT visual words>RT visual pictures). In the analysis of simple effects, presentation mode had a significant impact on YUE FEI (p<.001), the positive valence (p<.001), and the approach direction (p<.001). A t-test based on visual pictures showed that the average compatibility effect was significantly different from zero, t (22) = 2.43, p<.05 (M = 485 ms). Based on visual pictures, the RT of the approach behavioral tendency (M = 1211 ms) was faster than the avoidance behavioral tendency (M = 1432 ms) when YUE FEI matched a positive word or QIN HUI matched a negative word.\nPerceptions of emotional content depend on auditory and visual cues [29]–[30], [32]. Visual stimuli have an affective stimulus–response (S–R) compatibility effect. The present study examined whether auditory stimuli has the same affective stimulus–response (S–R) compatibility effect as visual stimuli. The results suggest that the presentation mode has a significant effect on RT. The personality characteristics of the participants were considered in the study. As YUE FEI is a positive representation of character, individuals usually paid attention to their own good personality characteristics. Thus, the volunteers participated in the cognitive aspect of the YUE FEI evaluation. Cognitive processing was more reflected in the three combinations of a good personality that included YUE FEI, participants, and the experimental material. In evaluating QIN HUI, the cognitive processing of the participants was shortened. As long as information relating to QIN HUI was available, the individual possessed a certain mindset that QIN HUI and positive information could not be associated. Zhang proposed a meaning-spelling theory of Chinese characters, and stressed that Chinese characters made full use of the human brain's visual processing power. Compared to the alphabet, Chinese characters form more thorough, visual words. Chinese characters are considered more than mere tools for recording [33]. The analysis of visual pictures is reflective of a strong performance in visual processing power. Thus, the RT of visual pictures in the present study was the fastest. The results show that the presentation mode has a significant effect on RT, thereby necessitating auditory research. Significant differences exist in the levels of the presentation mode (RT audition>RT visual words>RT visual pictures). Auditory stimuli add more to the cognition composition and are a type of meaning and serial processing. For auditory stimuli, the perception process of participants is successively. The study of spatial S–R compatibility of visual and auditory signals found that responses to visual signals were faster than those to auditory signal [34]. Their research further confirmed this point of view [35]. In some ways, these studies have consistency. In the visual experiment, the RT for word processing was slower than that for picture processing. This study agrees with the results of Zhang on one aspect, as their experimental material was also visual pictures [3], and indirectly supports the meaning-spelling theory of Chinese Characters [33].The average compatibility effect was calculated, and a t-test based on visual pictures showed that it was significantly different from zero. However, the t-test also showed no significant difference for visual words (298 ms) or auditory stimuli (−243 ms). The compatibility effect value of auditory stimuli was the smallest. A smaller compatibility effect existed when switching the presentation mode from visual stimuli to auditory stimuli. This study found an interesting phenomenon in that the only significant compatibility effect for visual pictures was for Chinese. There are differences between the results of the present study and those of previous studies; therefore, more research is necessary to further validate these results.In a previous study, the target and the referent matched (Einstein and positive words, Hitler and negative words), approach was faster avoidance [3]. This study found that the approach behavioral tendency was faster than the avoidance behavioral tendency when a positive character matched a positive adjective or a negative character matched a negative adjective. The study was also confined to the research category of visual pictures. More research in this area is also necessary for further validation.As a keyboard was used to input experimental reaction in the study, some limitations may be present. Further study can use a real approach and avoidance action to conduct related research (such as placing circuit boards at the feet, when participants perform approach and avoidance actions; the connection of the reaction device to the circuit board collects relevant data and reaction type). The reference valence effects in the study of affective S–R compatibility by Augmented Reality (AR) technology can improve the ecological validity of the study (the immersive feeling from registered, calibration, tracking, and fusion technologies allow participants to pay more attention to the real psychological reaction)."
        },
        "10.1371/journal.pone.0087285": {
            "author_display": [
                "Uta König von Borstel",
                "Chantal Glißman"
            ],
            "title_display": "Alternatives to Conventional Evaluation of Rideability in Horse Performance Tests: Suitability of Rein Tension and Behavioural Parameters",
            "abstract": [
                "\nRideability, i.e. the ease and comfort with which a horse can be ridden, is considered to be one of the most important traits in riding horses. However, at present rideability is evaluated rather subjectively in breeding horse performance tests. The aim of the present study was to evaluate the role horse behaviour as well as degree and quality of rein tension might play in judges’ evaluation of horses’ rideability. Mares (n = 33) and stallions (n = 13) from two different mare- and one stallion-testing station were observed twice during their performance test dressage training. During these rides, rein tension was measured continuously, and frequency of behaviour patterns such as head-tossing, tail swishing, and snorting was recorded. Rein tension parameters showed reasonable repeatabilities within horse-rider pairs (e.g. mean rein tension: r2 = 0.61±0.11; variance of rein tension: r2 = 0.52±0.14). Regression analysis revealed that a larger proportion of variance in rideability scores could be explained by maximum (17%), mean (16%) and variance (15%) of rein tension compared to horses’ or riders’ behavioural parameters (tail-swishing: 5% and rider’s use of hands: 5%, respectively). According to mixed model analysis, rideability scores dropped (all P<0.05) with increasing mean, maximum and variability in rein tension (e.g. −0.37±0.14 scores per additional 10 Newton in mean tension). However, mean rein tension differed between testing stations (P<0.0001) ranging between 9.1±1.6 N in one station and 21.7±1.3 N in another station. These results indicate that quantity and consistency of rein tension is either directly or indirectly an important factor for judges to derive rideability scores. Given the importance of rein tension parameters to both rider comfort and horse welfare, potentially, measurements of rein tension along with behaviour observations assessing the quality of rein contact (e.g. distinguishing a light contact from attempts to evade contact) might be used to make the assessment of rideability more impartial.\n"
            ],
            "publication_date": "2014-01-29T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 1,
            "views": 796,
            "shares": 6,
            "bookmarks": 6,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0087285",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0087285&representation=PDF",
            "fulltext": "IntroductionRideability is a trait evaluated in a large proportion of horse breeding programmes (e.g. [1]). Rideability describes the degree of comfort a rider feels when riding a horse and the ease with which a horse can be ridden [2]–[4]. Although the level and quality of prior training can considerably influence rideability [4], during performance tests judges aim at assessing the horses’ innate aptitude rather than the level of rideability achieved by training. Besides the rider’s legs, seat and in some cases the voice, the reins are one of the main means of communication between rider and horse, and it is thus expected that the horse’s reaction to rider’s cues via these different channels largely determines its rideablity. The trait rideability is rated to be one of the most important traits by both riders [5] and breeders [6] of various riding horse breeds. Breeding associations take great efforts to evaluate the trait rideability. Exclusively for the trait rideability, external, independent test-riders are hired for each performance test to ride and evaluate the horses in addition to the evaluations taken in the training and test under the regular riders. However, based on the above definition, it becomes obvious that rideability is a very complex trait. Furthermore, a rider’s “feeling” is by definition a subjective experience, making the evaluation of this trait subjective. Although traditionally considered to be a performance rather than a personality trait [3], it is suggested that a horse’s rideability is also largely influenced by its sensitivity to the rider’s aids, its inclination to behave in certain ways and thus by its personality [4]. Learning ability, i.e. a component of personality, is also suggested to contribute to rideability [7]. Indeed many officially appointed judges and breeding authorities consider rideability to be a personality trait, too [4]. Therefore, rideability likely is a compound trait that comprises both conformation and personality aspects. For example, it can be assumed that a well-balanced horse, with long pasterns, a long neck and a slender throatlatch will likely be able to respond quickly and correctly to its rider’s requests, make the rider feel comfortable due to smooth gaits, and be physically less able to resist rein pressure. In the same way, a horse that is based on its genetic predisposition relaxed, with a medium sensitivity to tactile stimuli and that is quick to learn and to respond to riders’ aids will be supple and likewise be more comfortable to ride compared to one that is slow in learning and responding, tense, or that frequently spooks due to heightened fearfulness. As frequently pointed out, there is considerable room for improvement with personality trait evaluation in sport horse breeding programmes [8]–[12]. Rideability and personality traits are evaluated in breeding programmes based on subjective assessment methods [1], [12], leading to inflated and biased scores with limited variation between individuals [12], [13]. Based on these scores, a genetic selection appears to be unavailing. Perhaps due to its mixed status as a trait partially influenced by personality aspects and partially influenced by conformation aspects, the statistical distribution of rideability scores exhibits slightly more desirable properties compared to the conventional personality traits [12]. Nevertheless, more objective assessment methods could greatly improve the evaluation of rideability, ultimately enhancing genetic progress in this trait. For personality traits such as temperament or specifically fear reactivity, considerable effort has been put into the development of more objective assessment methods (see [14] for a review) such as direct behavioural observation during novel object tests [8], [10], [12], [15], [16], different riding situations [9], [17], handling situations [18] or veterinary inspections [19], [20]. However, with few recent exceptions [21], [22] comparably little attention has been paid to the trait rideability. Therefore, the objectives of the present work were to assess the relationship between conventional rideability scores and objective parameters, including the measurement of behaviour and rein tension, thereby providing insight into the mechanisms judges use to derive their evaluations of rideability. Furthermore, based on these relationships as well as repeatabilities, the suitability of these measurements for future, more objective rideability evaluation methods will be evaluated.\nMaterials and MethodsThis type of non-invasive, behavioural research is approved under the German animal protection act and does not require a study-specific permission. Owners (privately owned horses) or chief trainers of the testing stations (horses owned by the state stud) volunteered their horses to participate in the study. Except for equipping the horses with the rein tension device in addition to their normal tack all testing corresponded to the routine training procedures.\n\nAnimals and Testing Conditions\nA total of 46 German Riding Horses were observed for the present study. The majority of horses (n = 43) was the offspring of stallions licensed by one German breeding association, while three horses were the offspring of stallions licensed by another German breeding associations. All horses were participants of on-station mare (n = 2 stations; n = 33 mares) or stallion (n = 1 station; n = 13 stallions) performance tests, and they were either three (n = 35) or four (n = 11) years of age (table 1). They were housed for the duration of the performance test (mares: 4 weeks; stallions: 10 weeks) at the testing station in ca. 3×4 m individual box stalls with automatic drinkers and trained and/or turned out daily by the staff of the testing stations. Performance test guidelines suggest that horses should be well accustomed to carrying a saddle and a rider when entering the performance test. However, the level of training prior to the performance test remains at the discretion of the owners and was not known for the individual horses. During these on-station performance tests, horses are trained to be ridden in dressage and show-jumping and are evaluated for the basic gaits, jumping ability, rideability and the personality traits labelled “character”, “temperament” and “willingness to work” as well as “constitution” (stallion performance test only). Each trait is comprised of a variety of factors as described in more detail in the official evaluation guidelines [3] as well as previous studies [4], [23]. For example, rideability is supposed to be comprised of the rhythm and elasticity of the horse’s movement, its suppleness, posture, balance, reaction to the rider’s aids, the degree of chewing the bit, rein contact and the degree of comfort for the rider [3]. Each trait is graded on a scale from 1 (very poor) to 10 (excellent). Evaluation of these traits ensues in three steps: In the first step, the stations’ head coach grades the horses considering their performance during the 20–70 day training phase. In the second step, rideability, the gaits and jumping ability, but not the personality traits, are evaluated with the same scoring system in a 1-day final test, by a pair of external, certified judges appointed by the national equestrian federation. During the dressage test and the free-jumping parts of the final test horses are present by their regular rider they are familiar with from the training phase of the performance test. Also during this final test, in a third step an additional judge, i.e. a test rider likewise appointed by the national equestrian federation rides each horse for a brief period of 2–5 minutes in order to assign a third score for rideability only. Thus, rideability is judged twice by judges from the ground observing the horses’ performance under their familiar rider, and once by a judge mounting the horse, directly evaluating how easy and comfortable the horse is to ride. Horses’ final scores for rideability are the arithmetic mean of the scores from the three steps. Horses’ final scores for the performance traits are calculated by assigning double weight to scores from the final test and subsequently taking the mean of the training and final test scores. Horses were ridden during training and the final test by 15 different riders with a maximum of 8 horses per rider (n = 1 rider) and a minimum of 1 horse per rider (n = 4 riders). The judges differed for the three testing stations.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Overview of total number (n) of horses and horses‘ gender, mean age, number of horses per age class (3 years [yrs] or 4 years old) and mean number of horses per rider by location.doi:10.1371/journal.pone.0087285.t001In all cases, dressage training was conducted in groups of three to five horses ridden simultaneously in indoor riding arenas measuring at least 20 × 40 m. Training was conducted for a mean (±SD) duration of 17.3±3.3 min. per day and included basic exercises such as transitions between gaits, and riding of simple dressage figures, such as circles or patterns leading to a change in hand.\n\n\nData Collection\nEach horse was observed twice during two dressage training sessions of the performance test. In accordance with an earlier study [9], the frequency of different behaviour patterns in both horse and rider were recorded per ride and converted on a per-hour-basis (table 2). In order to minimize subjectivity by introducing additional interpretations, behaviour patterns were recorded irrespective of their context. Teeth-grinding was observed, but not included in the further analysis due to occurrence in just two horses. Crabbing was likewise included in the ethogram, but no instances were observed. In contrast to the earlier study [9], observations were taken live, and only one horse at a time was observed. In addition, the observed horse was equipped during both rides with a rein tension meter (Signal Scribe, Crafted Technology, Australia), and rein tension was recorded to the inbuilt data logger. Before and after the full set of measurements per location, sensors of the rein tension device were calibrated using weights of 0.5, 1.0 and 2.0 kg, and no creep was detected. Official scores for all traits evaluated in the performance test were obtained from the testing stations after conclusion of the test.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  List and description of observed behaviour patterns recorded in frequencies of occurrence in the horse and rider (adapted from [9]).doi:10.1371/journal.pone.0087285.t002\n\nStatistical Analysis\nRein tension data was processed with the manufacturer-specific software and later analyzed along with the behavioural data and scores from performance tests using SAS (version 9.2). Mean, maximum and variance of rein tension was calculated for each ride separately, but combined for the right and left rein, and in addition, the absolute and relative difference between left and right rein mean tension was calculated as a measure of asymmetry of the horse-rider pair. All data were tested for distribution using the procedure UNIVARIATE. Traits that did not resemble a normal distribution (Kolmogorov-Smirnov: P<0.01) were analysed assuming either a Poisson distribution or, in the case of rare occurrences (i.e. occurrences in less than 15 rides and no more than 3 occurrences per ride) data were converted into binary data (e.g. bucking/no bucking observed per ride; see table 3).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 3.  Frequencies per hour of riding of observed behaviour patterns as well as the type of data distribution assumed for analysis and the respective repeatabilities (±SE) of behaviour and rein tension parameters considering either the horse-rider dyad or rider only as random factor.doi:10.1371/journal.pone.0087285.t003Mean ± SD of rideability and personality trait scores were calculated along with the Pearson correlation coefficients between these different traits. Subsequently, scores for the personality traits character, temperament and willingness-to-work (excluding the trait constitution due to unavailability of these scores for the mares as well as doubts in how far this trait is indeed a personality rather than health-related trait [4]) as well as rideability were analysed using a linear regression. For this regression a step-wise selection procedure was used to identify, based on the coefficient of determination, of all behavioural and rein tension variables (listed in table 3) those explaining the largest proportion of variance in the respective dependent variable. In addition in a next step, mixed models (parametric data) or generalized linear mixed models (non-parametric data) with a logit link (binary data) or log link (Poisson data) function was used to analyse the effect of categorical factors (horse age [3 or 4 years old], horse gender, location, measurement number [first or second observation], binary behavioural data) as well as the five most influential continuous factors (based on the coefficient of determination from the prior regression analysis) on personality scores. Variables were removed from the model, if they were not significant, and due to their partial confounding, horse gender and location were not considered simultaneously, but one after the other in the analysis. Rein tension and behaviour parameters were analyzed in the same manner. Results from these analyses are presented only, if they were, or tended to be, significant at the P<0.05 or P<0.1 level. To obtain variance components, either horse-rider pairs or only riders were considered in separate runs as a random factor, thus accounting for repeated observations per horse-rider pair or per rider. These variance components were used to calculate repeatabilities on the original scale (normally distributed data [24]) or on the latent scale (Poisson and binary distributed data [25]) for the behavioural and rein tension parameters at the rider as well as the horse-rider pair level. Standard errors of repeatabilities were calculated based on the approximation described e.g. by Roberds and Strom [26]. For the normally distributed traits, significance of random effects was assessed using Chi-Square statistics based on differences in log likelihoods of the mixed models with or without the respective random factor [27]. Due to lacking comparability of log Pseudo-likelihoods of different generalized mixed models, these calculations were not conducted for the non-Gaussian data.\n\nResults\nDescriptive Statistics and Correlations of Performance Test Scores\nThe participating horses received in the performance test a mean ± SD rideability score of 7.75±0.65. Scores for the other personality traits were: overall personality = 8.09±0.61, temperament = 8.02±0.41, character = 7.99±0.58, willingness to work = 7.65±0.73, and constitution = 7.88±0.57. Correlation coefficients between rideability and overall personality (r = 0.69; P<0.0001), temperament (r = 0.50; P = 0.0021), character (r = 0.60; P = 0.0001), willingness-to-work (r = 0.56; P = 0.0004) and constitution scores (r = 0.35; P = 0.27) were mostly high and significant. Correlation coefficients among the different personality traits were, again except for the trait constitution, likewise highly significant (P<0.0043) and ranged between 0.44 (temperament – willingness-to-work) and 0.85 (overall personality – character).\n\n\nRepeatability of Rein Tension and Behavioural Parameters\nRepeatabilities of rein tension parameters were both at the horse*rider level as well as at the rider level within an acceptable range, but repeatabilities varied widely for the behaviour traits (table 3).\n\n\nVariance Explained in Rideability and Personality Trait Scores\nThe regression analysis revealed that a considerable proportion of variance in rideability scores could be explained by rein tension and behavioural parameters. Notably, the three main rein tension parameters (coefficient of determination for maximum tension: 17%, mean rein tension: 16%, and variability of rein tension: 15%) each explained a larger proportion of variance in rideability scores, compared to any behavioural parameter. The maximum value for behavioural parameters was 5% for tail-swishing as well as for rider’s use of hands and horse-induced change in gait, followed by shying (4%). Albeit the overall variance explained was lower, a similar pattern was observed with the willingness to work scores: mean, variance and maximum rein tension explained 9%, 9% and 8% of the variance, respectively, while the three most influential behaviour patterns explained only 4% (involuntary change in gait) or 3% (both snorting and rider’s use of legs) of the variance in willingness to work scores. In contrast, scores for the trait temperament were best explained by the frequency of shying per hour of riding (19%), while maximum rein tension explained a considerably lower proportion of variance (6%), followed by tail-swishing, snorting and the rider’s use of the whip (each 4%). Variance in rein tension explained the largest proportion (12%) of variance in character scores, followed by involuntary change in gait (10%), mean rein tension (9%) and maximum rein tension as well as shying (both 8%).\n\n\nInfluence of Behaviour and Rein Tension on Rideability and Personality Trait Scores\nRideability scores dropped significantly (all P<0.05) with increasing mean, maximum and variability in rein tension (e.g. −0.37±0.14 scores per additional 1 Newton in mean tension; table 4). In addition, rideability scores tended to drop with increasing frequencies of behaviour patterns such as tail-swishing and shying (table 4).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 4.  Influence of behaviour patterns and rein tension parameters on scores for personality traits.doi:10.1371/journal.pone.0087285.t004\n\nOther Factors Influencing Rideability Scores, Rein Tension or Behaviour\nThe random horse*rider effect was significant (P<0.05) for mean and difference in rein tension, indicating that there are considerable differences between horse-rider pairs in the intensity and consistency of tension placed on the rein by the horse and/or the rider. Similarly, the random rider effect was significant for the mean and difference in rein tension, indicating that rein tension is also considerably determined by the rider’s riding style independent of the horse. Mean, maximum and variance of rein tension differed highly significantly between testing stations (P<0.0001) ranging e.g. between a least square mean of 9.1±1.6 N in one station and 18.9±0.9 N and 21.7±1.3 N in the other two stations (table 5). Horses that snorted had lower mean rein tensions than horses that did not snort (P = 0.0074), while rein tension increased as the performance test training progressed: there were lower (P<0.0001) rein tensions observed in the first compared to the second measurement (table 5).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 5.  Factors influencing rein tension parameters.doi:10.1371/journal.pone.0087285.t005With the exception of bucking, horses’ behaviour was considerably influenced either by the riders’ behaviour and/or rein tension parameters (table 6). In particular, the rider’s use of legs and whip influence a large number of horses’ behaviour patterns. In addition, horse behaviour differed in a few cases by horse gender: stallions were less likely to show tail-swishing (0.95±0.47 times less likely to tail-swish; P = 0.0511) and head-tossing (1.55±0.61 times less likely to toss their head; P = 0.0154) compared to mares. However, the partial confounding of gender with location needs to be kept in mind with these results. There were also considerable relationships between the different behaviour patterns. Horses that shied were also more likely to show horse-induced changes in pace (2.9±0.41 time more likely; P<0.0001) and head-tossing (1.9±0.49 times more likely; P = 0.0004), and they were less likely to attempt to buck (5.0±1.1 times less likely; P<0.0001), compared to horses that did not shy during the observation period. Also, the more often horses tossed their head, the more likely they were to show a horse-induced change in gait (0.05±0.007; P<0.0001) and to tail-swish (0.03±0.006; P<0.0001). Similarly, the more often a horse swished its tail, the more likely it was to head-toss (0.03±0.006; P<0.0001), and the less likely it was to snort (−0.06±0.03; P = 0.0578) and to stumble (−0.07±0.03; P = 0.0246). Horse’s age did not significantly influence any of the behaviour patterns, although the limited number of four-year-olds does not allow for definite conclusions.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 6.  Influence of rider behaviour and rein tension on horses’ behavioural parameters.doi:10.1371/journal.pone.0087285.t006\nThe traits rideability, temperament, character and willingness-to-work are compound traits with rather vague definitions, each including a large variety of different behaviour patterns. Evaluations of these traits based on scores that classify an animal’s performance in these traits as either “good” or “poor” by different judges that may each have their own set of aspects they focus on during evaluation will thus make it impossible to infer definite, specific behaviour profiles from the scores alone. The present study nevertheless attempted to shed light on some general relationships between rideability and personality trait scores on the one hand and specific behaviour patterns and rein tension parameters on the other hand. Results revealed that the lower and the steadier the rein tension the better judges evaluated horses’ rideability, i.e. the measure of how comfortable it feels to ride a certain horse. Most riding theories request a steady but light contact between the horse’s mouth and the rider’s hand via the reins (e.g. [28]). Also, according to the guidelines [3] as well as a survey, rideability is considered by performance test judges to be partially determined by the intensity and consistency of the rein contact [4]. Thus the relationships between rein tension parameters detected in the present study were expected, and they indicate that performance test judges apparently indeed pay attention to signs indicative of the quality and intensity of rein tension. On the other hand, the rather strong relationships between rein tension and rideability scores are also surprising as riders and judges do not always seem to be particularly good in judging their own rein tension [29] or in agreeing on the lightness of riders’ aids [30], respectively. In addition, the insufficiencies of the current rideability and personality evaluation methods have been highlighted repeatedly [4], [7], [12], [31], and therefore, the identified relationships may not be particularly meaningful. For example, these relationships may not exist when using a different set of judges. However, rideability scores from the performance test are an accepted measurement in practice and are at present the only available data on this parameter. In future studies it would be interesting to consider the individual scores assigned by the coach, the judge from the ground and the test rider separately to investigate potential differences in their evaluation strategies.The present study is the first to report repeatabilities of rein tension and behavioural parameters assessed in the ridden horse both at the rider as well as the horse-rider level. Repeatabilities for rein tension parameters and some, but not all behavioural parameters were remarkably high, and within similar ranges for horse-rider pairs as well as riders. However, the sample size of the present study was small, and it would be important to confirm the results in a larger sample of horses. Repeatabilities for the rein tension parameters compare well to values for horses’ reactivity in standardized temperament tests [12] as well as to performance parameters [32], [33]. Therefore, these moderately to highly repeatable traits potentially qualify for future, large-scale investigations such as are required for the estimation of genetic parameters. Furthermore, considering both the comparably high repeatabilities of rein tensions parameters and the relation between rein tension parameters and rideability scores, results from the present study indicate that the evaluation of at least some aspects of rideability could be made more objective, if direct measures of rein tension were taken instead of subjective scores. However, rein tension parameters alone will not be sufficient as it is not possible based on the plain values to distinguish a desired, very light rein contact from horses’ avoidance of rein contact (i.e. the horse going “behind the bit”). Additional recording of the horses’ behaviour and head posture will be indispensable for proper interpretation of the rein tension measurements.Furthermore, at the present stage, the technical equipment may not yet be robust enough, and in general dependence on the technical equipment may be prohibitive for introduction in performance tests. For example in an earlier version of this experiment, no rein tension measurements could be obtained as the rein gauges broke within minutes of testing the first horse due to overload of the sensors which had a maximum capacity of 50 N. Although more powerful sensors (maximum capacity of 100 N) were used for the present experiment, and no further problems disturbed the measurements of the present study, this incidence of equipment failure serves as an example of the susceptibility of technical, equipment-based measurements to data loss. Other potential sources of data loss include failure of power supply, memory card, or hardware, and these potential sources of data loss must be minimized before performance test evaluations can be replaced by technological devices. Alternative solutions such as the subjective assessment of rein tension by specifically trained judges, or the use of indicator traits such as rein length [34] may not yield satisfactory results either. Continued reliance on judges’ evaluations would not overcome the problems inherent to subjective evaluations [4], and rein length is a parameter influenced directly and almost unilaterally by the rider and may thus not be suitable to evaluate horses’ innate characteristics.Lower proportions of variance in rideability scores explained by behaviour patterns in the present study compared to the previous study [9] may be a result of different judges evaluating the traits at the different locations. It is likely that, due to the rather subjective evaluation criteria [4] along with the large amount of individual factors that comprise the present, complex traits, each judge has her/his own aspects she or he focuses on during evaluation of personality traits as well as rideability. Combining the results from these different judges will thus lower the impact of relationships that exist within parameters evaluated by one judge. In addition, these values have to be seen in light of the suboptimal statistical properties of both the behavioural parameters as well as the rideability scores [13]. Direct comparisons between e.g. behaviour and rein tension parameters with different distributions suboptimal for regression analysis should be considered with care, and if there is overall little variance present, it may be easier to explain a significant proportion of this limited variance. Means and standard deviations for rideability (7.8±0.7) and personality scores (e.g. temperament: 8.0±0.4) were similar to scores obtained in the recent past by larger groups of performance tested horses in Poland [7] between 2004 and 2007 or in Germany (7.8±0.9 and 8.3±0.8 for temperament and rideability, respectively between 2007 and 2010 [13]). The high, phenotypic correlations found in the present study between the different personality traits are also typical of performance test scores [12]. Therefore, the horses used in the present study appear to be a representative sample of the general participants of performance tests. These high correlations between the different personality trait scores once again highlight the insufficiencies of the present personality trait evaluation system, and also explain why a larger number of behaviour and rein tension traits in present study simultaneously explain significant amounts of variation in different personality traits (e.g. rein tension parameters significantly influence rideability scores as well as overall personality scores and character scores [see table 5]).Mean rein tension is with ca. 9–20 N comparable to results from earlier studies investigating similar riding situations [35]–[39]. However, mean rein tension in all these studies was remarkably high and considerably higher than the tension young horses would accept voluntarily (ca. 6–10 N; [40]). Thus, not surprisingly, in the present as well as in the above-mentioned study [40] higher rein tensions were associated with higher levels of potential conflict behaviour such as horse-induced change in gait [36], and with lower levels of potential comfort behaviour such as snorting. More frequent shying was also associated with higher mean and maximum rein tension, although as with any of these statistical relationships cause and effect are not clear. Possibly, riders tried to restrain horses more strongly after they shied, but possibly, horses ridden with stronger rein contact were more fearful and thus showed shying more frequently. Such an enhancing effect of more coercive riding techniques was shown earlier [41] and may be the result of additive effects of anxiety on fear reactions [42]. In contrast, the frequency of head-tossing tended to be reduced with increasing mean and maximum rein tensions, potentially because the high rein pressure physically prevented the horses from exhibiting any potential discomfort or avoidance behaviour via head movements for fear of yet increasing the pressure in the mouth. Ineffectiveness of or inability to express avoidance behaviour has the potential to provoke a state of learnt helplessness [43]. In this context heightened rein tensions potentially have to be considered as a severe threat to equine welfare. A stronger focus on the evaluation of rein tension during horse shows appears to be a logical step to advocate the ridden horses’ welfare.Differences between rein tension in left and right reins likely are the combined result of horses’ and riders’ laterality [35]. Minimizing lateralisation is an important aspect in training of young horses, which is why associations with personality evaluations were expected, but not confirmed by the present study. The association between absolute difference in rein tension and shying potentially relates to horses’ emotional laterality, i.e. their preferences for a certain eye when observing frightening objects [44].Striking differences in mean, maximum and variance in rein tension between test stations indicate that there may be differences in “riding culture” maintained e.g. by the head coach, that lead to marked differences in the amount of force applied on the reins. Differences in individual riders’ riding styles, e.g. regarding the differences in their use of visible hand aids and the use of whips further support this view. Although the confounding of gender with locations makes conclusions regarding these effects difficult, generally the location appears to play the more important role. With more parameters, one of the two mare stations rather than the stallion station differed significantly from the other two locations. Such differences in local “riding culture” were also observed for different riding schools [45], but are nevertheless surprising in the present study: performance tests should provide as much as possible standardized training conditions in order to allow for an unbiased assessment of the animals’ genetic merit. Thus, those factors clearly need to be standardized or controlled for, when using rein tension measurements in evaluation of horses’ rideability. Furthermore, the significant influence of the rider as well as the horse-rider pair underlined that mean rein tension is determined by both the horse and the rider. Likewise, the considerable influence of riders’ behaviour on horses’ behaviour demonstrates that riders indirectly have an impact on horses’ evaluation in the performance test. Repeated evaluation of rideability under different riders would have been interesting to further shed light on the influence the rider has on rideability evaluations. However, due to the study’s set up within the official performance test training, it was not possible to change training conditions by using different riders per horse. The continued use of independent test riders, whose individual level of rein tension and intensity of aids will be known from repeated observations and can thus be corrected for as is the case during genetic evaluations based on sport horse data [46], could be a potential solution. Nevertheless, interaction effects between specific horses and riders also exist, such that a given horse-rider combination matches particularly well or poorly and will thus yield particularly low or high values in rein tension. Unfortunately, such interaction effects that could indicate which type of horse is particularly suitable for a certain type of rider/riding style could only be tested reliably in a very large number of horses ridden by several different riders. The routine evaluation by two different test riders during the performance test would be an important step into this direction.Overall, the results of the present study confirm the insufficiencies of the present personality and rideability evaluation system in horse breeding. The vague definitions of very complex traits do not allow for objective and transparent evaluations. This is also reflected by a lack of associations between behaviour patterns that should, according to the guidelines, be related to rideability. Defining a precise list of behaviour patterns whose frequencies can be counted or whose intensities can be measured is an important step towards an improvement of the situation. However, in order to assign meaning to such measurements the specific context of behaviour patterns may have to be considered, which is a limitation of the present study. Although performance tests are intended to solely evaluate the animals’ abilities, the present study supports common knowledge that riders considerably influence the horses’ traits. These findings highlight the importance that the rider’s actions need to be recorded, too. These record would allow for both evaluation of the horse’s reaction to the rider’s aids as well as statistical corrections for the influence of the rider’s riding style when evaluating horse’s rideability. Ultimately such a revised evaluation system would allow breeders to make more informed decisions when selecting stallions for their mares based on rideability aspects."
        }
    }
}