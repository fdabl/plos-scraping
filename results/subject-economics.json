{
    "subject-economics": {
        "10.1371/journal.pmed.0030073": {
            "author_display": [
                "Mpho Selemogo"
            ],
            "title_display": "The Money Issue",
            "abstract": [
                ""
            ],
            "publication_date": "2006-01-31T00:00:00Z",
            "article_type": "Correspondence",
            "journal": "PLoS Medicine",
            "citations": 0,
            "views": 1292,
            "shares": 0,
            "bookmarks": 5,
            "url": "http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0030073",
            "pdf": "http://www.plosmedicine.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pmed.0030073&representation=PDF",
            "fulltext": "Auvert et al. must be commended for showing some appreciation of the ethical issues raised by their research trial [1]. The Research Article itself and the accompanying ethical review by Cleaton-Jones [2], however, curiously seem to take the money issue lightly. The     PLoS Medicine Editorial is quite right in identifying the R300 payment to participants as an issue [3].\n\t\t\t\nRather than just identifying what R300 means in terms of the euro, we need an idea of the sum's effect on the average person enrolled in the study in order to best review issues of autonomy, which are often so problematic in such research. What was its impact on the recruitment process? Was the average income for the participants so low that declining to participate in the study and turning down the money was not an economically feasible option? The absence of such critical socioeconomic data leaves us wondering if this money was meant as a force for recruitment or indeed as a compensation for participation, as the authors assert.\n"
        },
        "10.1371/journal.pone.0056767": {
            "author_display": [
                "Petre Caraiani",
                "Emmanuel Haven"
            ],
            "title_display": "The Role of Recurrence Plots in Characterizing the Output-Unemployment Relationship: An Analysis",
            "abstract": [
                "\n        We analyse the output-unemployment relationship using an approach based on cross-recurrence plots and quantitative recurrence analysis. We use post-war period quarterly U.S. data. The results obtained show the emergence of a complex and interesting relationship.\n      "
            ],
            "publication_date": "2013-02-27T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 1,
            "views": 932,
            "shares": 0,
            "bookmarks": 3,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0056767",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0056767&representation=PDF",
            "fulltext": "IntroductionA key relationship in macroeconomics is the one between unemployment and output. This relationship is of prime importance from a purely pragmatic point of view: it is of a great interest to know how to model the way unemployment changes when a recession hits the economy and also how fast unemployment will recover. The policy implications of the relationship between output and unemployment are extremely important: they steer the longevity of an economy and of its government (for instance how viable is expansionary output in the future) and they (hopefully) make sure that changed levels of output can translate in better levels of employment. As we know, from casual inspection, i.e. by simply looking at today's economy, it is far from obvious what the precise relationship is between output and employment. Hence, studying it further and trying to characterize the relationship, via recurrence plots, as we attempt to do in this paper, can be an important mission. We believe that in this paper we are able to isolate patterns in the dynamics of unemployment and output which, we think, could not be otherwise isolated using standard econometric tools.\nIn macroeconomics the so called ‘Okun's Law’ refers to the regularity observed in the relationship between the changes in production (or the GDP of a country) and its unemployment rate. By ‘output’ we mean production or GDP (in economics, those terms are used in economics interchangeably). Friedman and Wachter [1] define Okun's law [2] as a fixed relationship between levels of unemployment and levels of the output gap. With the ‘gap’ is meant the difference between actual output and so called ‘potential output’ which is the level of output occurring at the natural rate of unemployment. Such a level of unemployment appears when all unemployment is purely of a ‘voluntary’ type.\nInitially, the two variables (i.e. unemployment and output) were estimated with the use of a simple linear regression. However, [1] remark the relationship in early papers, was estimated with the simplifying assumption that the trend rate of change of potential output would be i) exogenously given and ii) steady. In their own paper, [1] relax those two assumptions and many other economists since then have investigated other ways of improving on the original approach to Okun's law. Nalewaik, Diebold and Landefeld [3] point out that since the 1980's Okun's law has taken on a different character. They specifically mention that unemployment has become more responsive to changes in output.\nThe importance of this relationship is underscored by its link with the so called ‘Phillips curve’. In the Keynesian model, an increase in aggregate demand beyond the point of potential output will lead to very high levels of inflation. This is due to the fact that the aggregate supply curve becomes very steeply sloped close to potential output. The so called short run ‘Phillips curve’ [4] attempts to show that a higher (lower) rate of inflation is accompanied by a lower (higher) rate of unemployment. However, in the late 1960's, the Phillips curve became contested [1], [5], [6], [7], [8]. The expectations augmented Phillips curve which emerged out of Friedman and Phelps' work seems to have a close connection to the so called ‘New Keynesian economics’ [9], [10], [11].\nThis paper does not want to pretend to delve further into the economic intricacies of new Keynesian economics. Instead we want to focus on proposing a new approach to explain nonlinearities in the two key variables we mentioned at the beginning of this introduction. The increased awareness of the existence of nonlinearities in economics has surely influenced the way economists model the relationship between output and unemployment. Contributions have started to focus more on asymmetries and nonlinear dynamics mainly because it has been established that the dynamics of unemployment in a period of economic growth differ from those in a recession [12], [13], [14], [15]. However, as is (still) the case with most macroeconomic subjects, this topic has not yet been studied from the perspective of new nonlinear techniques from physics.\nHence, in this paper, we propose a re-evaluation of this key relationship using concepts and techniques from physics, like determinism measures and recurrence plots. Our approach is based on cross and quantitative recurrence analysis. Those methodologies will be presented in the paper.\nWe believe the contributions of this paper are as follows.\nFirstly, we analyse the underlying dynamics of unemployment and output using a novel approach which might lead to answers to well-known issues in the areas of nonlinearities and even determinism. We use recurrence plots and quantitative recurrence analysis to this end. Our findings may hopefully shed more light on whether business cycles are possibly characterized (or not) by a degree of determinism. This is a debate which is still not settled.\nSecondly, we analyse the relationship between the time series on unemployment and output (GDP) using cross recurrence plots and again quantitative recurrence analysis. We believe that through this approach we can reveal some aspects of the relationship between unemployment and output that would otherwise be hidden with a standard econometric approach.\nMethods\nBasic motivation for the use of recurrence analysis in economics\nRecurrence plots have been successfully used in the sciences for some time now and more recently it has found inroads in social science, notably in economics. We provide for relevant references below in the paper. We believe that the adoption of recurrence plots as a tool of analysis is based on the view that it is a useful methodology in understanding better the dynamics of single series or of relationships. We apply this approach to a topic that remains of very high interest to both the macroeconomics academic community and the general public: that is the relationship between production (or also output) and unemployment.\nWhile initial studies focused on the linear aspect of the ‘output’ and ‘unemployment relationship’, there is more and more awareness the relationship between production and unemployment may well be nonlinear in nature. We think the approach proposed by us can better highlight this nonlinear relationship. The nonlinear relationship has deep implications on a macroeconomics level and this was already argued for in the 1960's. The paper by [16] on the nonlinear theory of the employment cycle, for instance argues for a Phillips curve with a loop effect (i.e. a nonlinear effect). However, the quest for uncovering nonlinearities in the above relationship did not produce a steady stream of papers, since the 1960's. As an example, it was only in 2001 that [15] argues for a nonlinear relationship and he indicates that such nonlinearities allow for a better explanation of the varying (in) effectiveness of macroeconomic policies which target unemployment. The paper by [17] discussed in much detail what the consequences can be of policy-induced macro-economic recessions. This is indeed an important consequence given the current economic climate but it is also of importance within the wider European Community context, as [15] indicates, such nonlinear relationships affect aggregation (from a country level to a multiplicity of countries level, such as with the European community). In the work of [18] it is shown that with the use of wavelets, one can test for the presence of lower output volatility in US output (since the late 1940's). This lower volatility [18] claims to be a consequence of changes in a dynamic process. We wonder if this ‘dynamic process’ has a link to what we will call in our paper the periods of ‘dynamic discontinuity’. In summary with the few references cited here, we attempt to show that there is scope to use this methodology to uncover the possible presence of nonlinearities in the relationship between unemployment and output.\nRecurrence plot based techniques have been used to study the nonlinear relationships between different variables. The reference paper by Marwan and Kurths [19], introduces bivariate analysis (with an emphasis on nonlinearity) and it was also shown there that the bivariate recurrence plots can not only deal with linearity but also with the nonlinear aspect of nonlinearity. The possibility of using recurrence plots to pinpoint transitions which from an economics point of view are important is a key contribution from this methodology. As an example, the possibility of arguing for a transition to different states (what we call in our paper a ‘dynamic discontinuity’) during recessions is of great use. This characteristic that recurrence plots are very good at change point detection is highlighted in [20], [21].\n\n\nTheoretical background\nIn 1987 the term ‘recurrence plots’ was proposed in the paper by Eckmann et al. [22] and they emphasize the major benefit of using such plots: i.e. one can derive time information explicitly from a dynamical system.\nIn economics and finance, the methodology began to be applied many years after its inception in papers by [23], [24], [25], [26], [27], [28] and [29]. As one can appreciate the number of applications of this methodology in economics and finance is still small. It also needs to be said that most of those contributions focussed on a univariate based analysis.\nWe discuss in the paragraphs below the main points of this approach and we follow closely Marwan and Kurths [19] and Marwan [30].\nThe phase space trajectory dynamics can be represented as:\n(1)where  is the phase space trajectory;  is a time series with , and  is the sampling time; is the embedding dimension and is the time delay.\nThere are different approaches in setting the parameters corresponding to the embedding dimension  and the delay . See [19] for some suggestions. One defines the recurrence plot based on the following formula:\n\n          (2)\n        \nwhere  is a predefined parameter characterizing the distance between two neighbouring points; ∥ is the norm (normally the Euclidean norm is used); while  is a Heaviside type function.\nSince in this paper, we are interested in the relationship between unemployment and output, we will, besides using classical recurrence plots, also use cross recurrence plots. Amongst the first authors to extend recurrence plots into cross recurrence plots were Zbilut, Giuliani and Webber [31]. Re-consider, equation (2) from above, but alter the ingredients in the norm:\n\n          (3)\n        \nwhere and  are respectively the reconstruction of the first and second series (different dimensions are possible) in the phase space. Again, we can visualize important features of the relationship between the two systems. Long diagonal lines indicate similar behaviour in the phase space. See [19] and [30].\nSeveral measures derived from quantitative recurrence analysis will be used in this paper. Quantitative recurrence analysis is a further development of recurrence plots and is due to contributions from [32] and [33].\nIn what follows below, we present the definitions of the main complexity measures we will employ in this paper. We continue using [19] and [30]. We will also be using the univariate approach (as originally proposed in [32]).\nThe recurrence rate, , is defined as:\n\n          (4)\n        \nwhere  is the distribution of the diagonal line lengths (for a diagonal parallel to the main diagonal);  is the length of the line structure;  is an index of the relative location of the diagonal line; is the dimension of the vector . measures thus the density of recurrence points [30]. The  can also be used as an indicator of changes in the dynamical system.\nWe also consider the ‘determinism’ measure,  which is given by:\n\n          (5)\n        \nLow values indicate stochastic systems, while higher values are an indication of a degree of predictability. The intuition with this measure is more subtle. Although [30] indicates gives the predictability of the system, we need to keep in mind that predictability and determinism are not obvious equivalences. In fact [30] cautions about the reliability of the measure. The and are examples of so called ‘Recurrence Quantification Analysis’ (RQA) (see [32]). RQA measures are statistical measures and [34] makes the important remark that estimating the confidence of such measures is still an open question. Consequently, the and measures have not yet received a lot of attention as to their statistical robustness.\nIn the case of a bivariate analysis, another important measure is the average diagonal length, :\n\n          (6)\n        \nIt indicates the degree of coincidence of two systems.\nIn the case of univariate analysis, a measure of entropy is used. The Shannon entropy, , is:\n\n          (7)\n        \nwhere  is defined in function of the cumulative distribution of the line length.\nThe  measure is also an example of Recurrence Quantification Analysis (RQA). , and  are measures which are based on diagonal lines. The laminarity and trapping time measures in equations (8) and (9) below follow [30] and are based principally on vertical lines in a recurrence plot. As [30] indicates, laminar states refer to ‘chaos-chaos transitions’, while the RQA measures refer to chaotic- periodic state transitions. The laminarity measure, , is defined as:\n\n          (8)\n        \nwhere indicates the distribution of vertical line lengths.\nFinally, the trapping time measure,  is given by:\n\n          (9)\n        \n\nResults and DiscussionWe use one of the longest U.S. data series available for unemployment and output. The unemployment rates are taken from the U.S. Bureau of Labour while the output figures are taken from the U.S. Department of Commerce, Bureau of Economic Analysis. We emphasize we want to use U.S. data in this paper, as most of the studies written on this topic were performed with the help of U.S. data. The sample ranges from the first quarter of 1949 to the last quarter of 2010. We have opted for a quarterly frequency in order to ensure a high number of observations. The data were ensured to be stationary following the procedure in [33].\nA first step in implementing quantitative recurrence analysis requires the determination of the key parameters, namely the embedding parameter , and the delay parameter . Recall that those two parameters were first encountered in equation (1) above. We must pay particular attention to the presence of noise which proves a distorting factor in computing the key parameters and . To aid in this, ‘mutual information’ is used in the case of the delay parameter and the ‘false nearest neighbours technique’ is employed when determining the embedding dimension. See also our brief discussion above equation (2).\nIt has been observed by [24] that a sufficiently large embedding would be sufficient to contain all relevant dynamics. In his application, he chooses an embedding of order 10 in order to take into account the high complexity of human dynamics. However, for the case of the lag he suggested the use of a lag 1 in the context of discrete economic data (financial data in his case).\nWe use the false nearest neighbours approach to determine the embedding dimension for both unemployment and output. We found an embedding dimension of 4 for each of the series. For the delay parameter, given the discussion in the literature, we simply choose a delay parameter  equal to 1.\n\nRecurrence plots\nUsing the values determined above for the key parameters and , we perform a recurrence analysis of the two series (i.e. unemployment and output) we are interested in (see figures 1, 2, 3, 4, 5, and 6 below). We note that we are not using the log-change in output, but instead the annual growth of output as well as the unemployment rate. This is one of the main approaches in selecting the data, which is originally due to [24]. It is also used in [7] and [35]. Only recently in work starting with [36], do we see that the output gap can be used as a complementary alternative to the growth rate of output.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Unthresholded Recurrence Plot for output with Euclidean Distance.doi:10.1371/journal.pone.0056767.g001\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Thresholded recurrence plot for output with Euclidean distance and\n              \n                \n              \n              .\n            \ndoi:10.1371/journal.pone.0056767.g002\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Unthresholded Recurrence Plot for unemployment rate with Euclidean Distance.doi:10.1371/journal.pone.0056767.g003\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Thresholded recurrence plot for unemployment with Euclidean distance and\n              \n                \n              \n              .\n            \ndoi:10.1371/journal.pone.0056767.g004\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Untresholded cross recurrence plot for unemployment and output with Euclidean Distance\n              \n                \n              \n              .\n            \ndoi:10.1371/journal.pone.0056767.g005\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Thresholded cross recurrence plot for unemployment and output with Euclidean distance and\n              \n                \n              \n              .\n            \ndoi:10.1371/journal.pone.0056767.g006A third parameter must be determined, the radius parameter , which also plays a crucial role in the results obtained through the recurrence plots. As [24] observes, choosing a too small value  would lead into quantifying noise only, while a too high value would result in capturing values that are not really recurrent. One strategy [24] suggests is to compute the recurrence percentage for increasing values of the radius until a scaling region is reached. Other approaches in the literature are based on setting the radius based on the idea of obtaining reasonable recurrence rates (see for instance [28] and [33]). In a study on the Taiwanese unemployment rate, [29] used a value for  of 0.65. Since we will be studying unemployment rate series too, we will be using a value close to the one by [29], namely.\nAlthough the interpretation of the figures 1, 2, 3, and 4 (see below) is not straightforward, there are some indications in the literature on how one may read such figures. Eckmann et al. [22] provides for a very intuitive account on how to read some of the plots. Marwan [30] classifies patterns into large scale and small scale and according to his classification; he found four different types of large scale patterns or typologies.\n\n\n\n\nHomogeneous typologies, characterizing white noise;\n\nPeriodic typologies, when recurrence plots present diagonal lines and checkerboard structures, for the case of oscillating systems;\n\nDrift typologies, when there is fading in the upper left and lower-right corners, for systems which are not stationary;\n\nDisrupted typologies, for extreme (and rare) events, when white bands are present, indicating transitions.\n\nThe second class, of small scale structures, can be characterized through the following characteristics:\n\n\n\n\nSingle, isolated points, for rare states;\n\nDiagonal lines, when the system visits the same region of the phase space at different moments;\n\nVertical horizontal lines, when the system either does not change or it changes slowly. This phenomenon is also an indicator of intermittency.\n\nLooking at the recurrence plot figures for unemployment and output, figures 1, 2, 3, and 4, we can clearly reject the idea of random processes as the recurrence plots are not exhibiting a homogeneous typology.\nWe find several white vertical bands that make transitions between different block characterizing periods when the system behaved in a similar way. These bands correspond basically to the more severe recessions from 1973–1975 and 2009 and they can indicate also transitions in the dynamics. The patterns indicate a process which is to a degree predictable. A changing process occurs after 1975.\nFigures 5 and 6 show the results from applying cross recurrence to both unemployment and output series. The dynamics of unemployment and output, as evidenced from the recurrence plots in figures 1, 2, 3, and 4, are again presented in figures 5 and 6. We note that the evidence suggested by figure 6 points to additional unusual patterns, as underlined by the ‘S’ shapes of segments. This might indicate some possible modifications in the relationship at lag levels. These features would be hard to uncover using standard econometric approaches (we thank two referees of this paper for this important insight). We observe again white bands corresponding to the recessions in 1973–1975 and 2009–2010. They mark again transition periods between two periods with similar dynamics. Within these periods, the dynamics are rather similar.\nThese results indicate that the recent literature on the output-unemployment relationship that stressed nonlinearities, time-dependency or state switching is indeed correct. However, our figures go even further, stressing the presence of a degree of predictability and the possibility of transition periods corresponding to the recessions in 1973–1975 and 2009–2010. Such results could be further tested using techniques from econometrics.\nWe note that the possibility of a changing nature of the business cycle is acknowledged in the economic literature. For example, starting with the early 1980s, the economic literature points to the emergence of the Great Moderation, a period of lower volatility that is characterized by specific dynamics, as recessions are smaller and lower in intensity. This period ended with the Great Recession, the last global economic and financial crisis. A key reference to the Great Moderation and how the business cycles changed after 1985 is the paper by [37]. A more general perspective on the historical changes in business cycles is provided for by [17]. There are other recessions, like the ones in the 1970s that are also known to have marked the transition between different types of business cycles. The literature acknowledges that, in general, the macroeconomic relationship between different variables (as is the case of output and unemployment) may change during severe downturns. Thus, we think we can link the evidence through recurrence plots with the idea of a changing nature of business cycles, since through recurrence plots we can identify the changing dynamics of a time series.\n\n\nQuantitative recurrence analysis\nBefore doing a cross analysis, we analyse each series using a windowed quantitative recurrence analysis. We use again an embedding dimension of 4 and a delay parameter of 1. The radius is chosen to be 0.6, while the minimum size for diagonals and vertical lines is chosen to be 2. The moving window is set at 48 periods which, given our quarterly frequency, is equivalent to 12 years. Figure 7 shows the results for the unemployment series, while the results for output are shown in figure 8.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 7.  Quantitative recurrence analysis for unemployment Series.doi:10.1371/journal.pone.0056767.g007\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 8.  Quantitative recurrence analysis for output.doi:10.1371/journal.pone.0056767.g008The results can be read while keeping in mind the output obtained from the application of recurrence plots in figures 3 and 4. In Figure 7, when looking at the recurrence rate, we see a lowering of the recurrence rate corresponding to the two already evidenced recessions, from the mid-1970s and 2009–2010. The degree of predictability, as evidenced through the output, is also decreasing during this period. The results here are similar to those obtained from the recurrence plots: the unemployment series is characterized by a degree of predictability, with two transition periods.\nThe results in figure 8 from the recurrence plots are largely confirming established macroeconomic facts. The variance of output has largely fallen after the 1980's corresponding to the phenomenon called the ‘Great Moderation’. We notice an increase in the recurrence rate of production after period 120 (around 1975) implying an increased level of predictability, which can also be seen from the output. The recurrence rate was generally much lower than that for unemployment during the same period, suggesting a stochastic process for output between the 1950s and mid-1970s. The line length increases also within the second period, with line lengths around 6–7.\nWe discuss in the next two figures (figures 9 and 10), two measures of cross analysis based on quantitative recurrence analysis: quantitative recurrence diagonal analysis in figure 9 and cross quantitative recurrence analysis in Figure 10.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 9.  Quantitative recurrence diagonal analysis for output and unemployment.Red lines correspond to the negative relationship while black lines to the positive one.\ndoi:10.1371/journal.pone.0056767.g009\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 10.  Cross quantitative recurrence analysis for output and unemployment.doi:10.1371/journal.pone.0056767.g010The quantitative recurrence diagonal analysis is based on an embedding dimension of 4 with a delay parameter of 1. The window is 40, corresponding to a 10 year period, with 5 years for negative lags, and 5 years for positive lags. The negative relationship is the strongest one, as expected, and it is slightly biased toward positive lags, suggesting a delayed response by unemployment. The same delay can be found in the peaks reached by the recurrence plots and average line length. We recall the words of [3] (please see the introduction section of our paper) who claimed that the responsiveness of unemployment to output changes was very much a dynamic phenomenon.\nThe relationship, based on figure 9, can be considered as characterized by a degree of predictability, as evidenced from the recurrence rate and determinism rate. Moreover, it also shows there is a more complex picture emerging of the relationship between unemployment rate and output. It may be questioned whether such complex relationship could be easily uncovered with standard econometric techniques.\nIn figure 10 we further investigate the relationship between output and unemployment using quantitative recurrence analysis based on an embedding dimension of four; a delay parameter of one; minimum vertical and diagonal lines of two and a radius distance of 0.6. We use a sliding window of 48 periods (12 years).\nThe results are influenced by the individual dynamics of the series. The recurrence rate increases after the mid-1970s, with the measure increasing also after this period. This signals an increased degree of predictability in the relationship of the series. The average line lengths increased after the ‘70s implying more pronounced common dynamics. We also noticed an increase in the overall complexity of the dynamics, evidence through the measure.\n\nIn conclusion, we analysed the dynamics of output and unemployment in the United States. We considered as well the relations between output and unemployment using recurrence plots and quantitative recurrence analysis. We were able to isolate patterns in the dynamics of unemployment and output which, we believe could not be otherwise isolated using standard econometric tools. The series are found to be characterized by a degree of predictability, with periods of dynamic discontinuity corresponding to the large recessions, like the mid-1970s recession or the 2009–2010 recession. We thank one of the referees of this paper for emphasizing that recessions can be better termed as ‘dynamic discontinuities’.These findings are in line with the main results found from research carried out during the last decades on understanding the relationship between unemployment and output. This research has pointed out nonlinearities, state switching and dynamic relations. In this paper, we go even further, by pointing to a degree of predictability and the possibility of a dynamic discontinuity in different states during the big recessions. Although we do not quantify the relationship, the findings here can be used to expand our understanding of the output -- unemployment relationship. However, it needs to be said that the potential evidence of degrees of predictability should be taken with caution. Marwan [34] makes the following comment on the measure in equation (5): ‘High values of might be an indication of determinism in the studied system, but it is just a necessary condition, not a sufficient one.’ He indicates that even for non-deterministic processes it is possible to find longer diagonal lines with as consequence higher than warranted values."
        },
        "10.1371/journal.pone.0087824": {
            "author_display": [
                "Parisa Samimi",
                "Hashem Salarzadeh Jenatabadi"
            ],
            "title_display": "Globalization and Economic Growth: Empirical Evidence on the Role of Complementarities",
            "abstract": [
                "\nThis study was carried out to investigate the effect of economic globalization on economic growth in OIC countries. Furthermore, the study examined the effect of complementary policies on the growth effect of globalization. It also investigated whether the growth effect of globalization depends on the income level of countries. Utilizing the generalized method of moments (GMM) estimator within the framework of a dynamic panel data approach, we provide evidence which suggests that economic globalization has statistically significant impact on economic growth in OIC countries. The results indicate that this positive effect is increased in the countries with better-educated workers and well-developed financial systems. Our finding shows that the effect of economic globalization also depends on the country’s level of income. High and middle-income countries benefit from globalization whereas low-income countries do not gain from it. In fact, the countries should receive the appropriate income level to be benefited from globalization. Economic globalization not only directly promotes growth but also indirectly does so via complementary reforms.\n"
            ],
            "publication_date": "2014-04-10T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 1025,
            "shares": 0,
            "bookmarks": 0,
            "url": "http://www.plosone.org/article/info:doi/10.1371/journal.pone.0087824",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0087824&representation=PDF",
            "fulltext": "IntroductionGlobalization, as a complicated process, is not a new phenomenon and our world has experienced its effects on different aspects of lives such as economical, social, environmental and political from many years ago [1]–[4]. Economic globalization includes flows of goods and services across borders, international capital flows, reduction in tariffs and trade barriers, immigration, and the spread of technology, and knowledge beyond borders. It is source of much debate and conflict like any source of great power.\nThe broad effects of globalization on different aspects of life grab a great deal of attention over the past three decades. As countries, especially developing countries are speeding up their openness in recent years the concern about globalization and its different effects on economic growth, poverty, inequality, environment and cultural dominance are increased. As a significant subset of the developing world, Organization of Islamic Cooperation (OIC) countries are also faced by opportunities and costs of globalization. Figure 1 shows the upward trend of economic globalization among different income group of OIC countries.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Average economic globalization (KOF index) by income groups.doi:10.1371/journal.pone.0087824.g001Although OICs are rich in natural resources, these resources were not being used efficiently. It seems that finding new ways to use the OICs economic capacity more efficiently are important and necessary for them to improve their economic situation in the world. Among the areas where globalization is thought, the link between economic growth and globalization has been become focus of attention by many researchers. Improving economic growth is the aim of policy makers as it shows the success of nations. Due to the increasing trend of globalization, finding the effect of globalization on economic growth is prominent.\nThe net effect of globalization on economic growth remains puzzling since previous empirical analysis did not support the existent of a systematic positive or negative impact of globalization on growth. Most of these studies suffer from econometrics shortcoming, narrow definition of globalization and small number of countries. The effect of economic globalization on the economic growth in OICs is also ambiguous. Existing empirical studies have not indicated the positive or negative impact of globalization in OICs. The relationship between economic globalization and economic growth is important especially for economic policies.\nRecently, researchers have claimed that the growth effects of globalization depend on the economic structure of the countries during the process of globalization. The impact of globalization on economic growth of countries also could be changed by the set of complementary policies such as improvement in human capital and financial system. In fact, globalization by itself does not increase or decrease economic growth. The effect of complementary policies is very important as it helps countries to be successful in globalization process.\nIn this paper, we examine the relationship between economic globalization and growth in panel of selected OIC countries over the period 1980–2008. Furthermore, we would explore whether the growth effects of economic globalization depend on the set of complementary policies and income level of OIC countries.\nThe paper is organized as follows. The next section consists of a review of relevant studies on the impact of globalization on growth. Afterward the model specification is described. It is followed by the methodology of this study as well as the data sets that are utilized in the estimation of the model and the empirical strategy. Then, the econometric results are reported and discussed. The last section summarizes and concludes the paper with important issues on policy implications.\nLiterature ReviewThe relationship between globalization and growth is a heated and highly debated topic on the growth and development literature. Yet, this issue is far from being resolved. Theoretical growth studies report at best a contradictory and inconclusive discussion on the relationship between globalization and growth. Some of the studies found positive the effect of globalization on growth through effective allocation of domestic resources, diffusion of technology, improvement in factor productivity and augmentation of capital [5], [6]. In contrast, others argued that globalization has harmful effect on growth in countries with weak institutions and political instability and in countries, which specialized in ineffective activities in the process of globalization [5], [7], [8].\nGiven the conflicting theoretical views, many studies have been empirically examined the impact of the globalization on economic growth in developed and developing countries. Generally, the literature on the globalization-economic growth nexus provides at least three schools of thought. First, many studies support the idea that globalization accentuates economic growth [9]–[19]. Pioneering early studies include Dollar [9], Sachs et al. [15] and Edwards [11], who examined the impact of trade openness by using different index on economic growth. The findings of these studies implied that openness is associated with more rapid growth.\nIn 2006, Dreher introduced a new comprehensive index of globalization, KOF, to examine the impact of globalization on growth in an unbalanced dynamic panel of 123 countries between 1970 and 2000. The overall result showed that globalization promotes economic growth. The economic and social dimensions have positive impact on growth whereas political dimension has no effect on growth. The robustness of the results of Dreher [19] is approved by Rao and Vadlamannati [20] which use KOF and examine its impact on growth rate of 21 African countries during 1970–2005. The positive effect of globalization on economic growth is also confirmed by the extreme bounds analysis. The result indicated that the positive effect of globalization on growth is larger than the effect of investment on growth.\nThe second school of thought, which supported by some scholars such as Alesina et al. [21], Rodrik [22] and Rodriguez and Rodrik [23], has been more reserve in supporting the globalization-led growth nexus. Rodriguez and Rodrik [23] challenged the robustness of Dollar (1992), Sachs, Warner et al. (1995) and Edwards [11] studies. They believed that weak evidence support the idea of positive relationship between openness and growth. They mentioned the lack of control for some prominent growth indicators as well as using incomprehensive trade openness index as shortcomings of these works. Warner [24] refuted the results of Rodriguez and Rodrik (2000). He mentioned that Rodriguez and Rodrik (2000) used an uncommon index to measure trade restriction (tariffs revenues divided by imports). Warner (2003) explained that they ignored all other barriers on trade and suggested using only the tariffs and quotas of textbook trade policy to measure trade restriction in countries.\nKrugman [25] strongly disagreed with the argument that international financial integration is a major engine of economic development. This is because capital is not an important factor to increase economic development and the large flows of capital from rich to poor countries have never occurred. Therefore, developing countries are unlikely to increase economic growth through financial openness. Levine [26] was more optimistic about the impact of financial liberalization than Krugman. He concluded, based on theory and empirical evidences, that the domestic financial system has a prominent effect on economic growth through boosting total factor productivity. The factors that improve the functioning of domestic financial markets and banks like financial integration can stimulate improvements in resource allocation and boost economic growth.\nThe third school of thoughts covers the studies that found nonlinear relationship between globalization and growth with emphasis on the effect of complementary policies. Borensztein, De Gregorio et al. (1998) investigated the impact of FDI on economic growth in a cross-country framework by developing a model of endogenous growth to examine the role of FDI in the economic growth in developing countries. They found that FDI, which is measured by the fraction of products produced by foreign firms in the total number of products, reduces the costs of introducing new varieties of capital goods, thus increasing the rate at which new capital goods are introduced. The results showed a strong complementary effect between stock of human capital and FDI to enhance economic growth. They interpreted this finding with the observation that the advanced technology, brought by FDI, increases the growth rate of host economy when the country has sufficient level of human capital. In this situation, the FDI is more productive than domestic investment.\nCalderón and Poggio [27] examined the structural factors that may have impact on growth effect of trade openness. The growth benefits of rising trade openness are conditional on the level of progress in structural areas including education, innovation, infrastructure, institutions, the regulatory framework, and financial development. Indeed, they found that the lack of progress in these areas could restrict the potential benefits of trade openness. Chang et al. [28] found that the growth effects of openness may be significantly improved when the investment in human capital is stronger, financial markets are deeper, price inflation is lower, and public infrastructure is more readily available. Gu and Dong [29] emphasized that the harmful or useful growth effect of financial globalization heavily depends on the level of financial development of economies. In fact, if financial openness happens without any improvement in the financial system of countries, growth will replace by volatility.\nHowever, the review of the empirical literature indicates that the impact of the economic globalization on economic growth is influenced by sample, econometric techniques, period specifications, observed and unobserved country-specific effects. Most of the literature in the field of globalization, concentrates on the effect of trade or foreign capital volume (de facto indices) on economic growth. The problem is that de facto indices do not proportionally capture trade and financial globalization policies. The rate of protections and tariff need to be accounted since they are policy based variables, capturing the severity of trade restrictions in a country. Therefore, globalization index should contain trade and capital restrictions as well as trade and capital volume. Thus, this paper avoids this problem by using a comprehensive index which called KOF [30]. The economic dimension of this index captures the volume and restriction of trade and capital flow of countries.\nDespite the numerous studies, the effect of economic globalization on economic growth in OIC is still scarce. The results of recent studies on the effect of globalization in OICs are not significant, as they have not examined the impact of globalization by empirical model such as Zeinelabdin [31] and Dabour [32]. Those that used empirical model, investigated the effect of globalization for one country such as Ates [33] and Oyvat [34], or did it for some OIC members in different groups such as East Asia by Guillaumin [35] or as group of developing countries by Haddad et al. [36] and Warner [24]. Therefore, the aim of this study is filling the gap in research devoted solely to investigate the effects of economic globalization on growth in selected OICs. In addition, the study will consider the impact of complimentary polices on the growth effects of globalization in selected OIC countries.\nModel SpecificationThis study uses a dynamic panel data model to investigate the effect of globalization on economic growth. The model can be shown as follows:(1)where i is country index, t is time index,  and  are the parameters to be estimated, GDP is the logarithm of real GDP per capita, KOF is economic globalization, CV is a vector of other control variables that affect economic growth,  is unobserved country-specific effect term, and  is the usual error term. The group of control variables is comprised of variables frequently used in the growth literature including government consumption, secondary school enrolment as a proxy for human capital, inflation (consumer price index), domestic investment, liquid liability to capture the financial development and ICRG as an index for institutional quality.\nIn Eq.1, the existence of lag per capita GDP produces the well-known dynamic panel bias due to the correlation between the  and disturbance term, . In other words,  is a function of , as  is time-invariant, therefore,  is also a function of . It means that Eq. 1 has a severe endogeneity problem that happens when the lag of dependent variable, as one of the regressors, is correlated with one component of the error term [37]. In addition, In Eq.1, the fixed effects or time-invariant country characteristics (), might be correlated with the explanatory variables which violates the assumptions underlying the classical linear regression model. In this case, the simple ordinary least squares (OLS) or fixed and random effects approaches can produce highly misleading results.This paper applies the generalized method of moments (GMM) panel estimator first suggested by Anderson and Hsiao [38] and later developed further by Arellano and Bond [39]. This flexible method requires only weak assumption that makes it one of the most widely used econometric techniques especially in growth studies. The dynamic GMM procedure is as follow: first, to eliminate the individual effect form dynamic growth model, the method takes differences. Then, it instruments the right hand side variables by using their lagged values. The last step is to eliminate the inconsistency arising from the endogeneity of the explanatory variables.The consistency of the GMM estimator depends on two specification tests. The first is a Sargan test of over-identifying restrictions, which tests the overall validity of the instruments. Failure to reject the null hypothesis gives support to the model. The second test examines the null hypothesis that the error term is not serially correlated.The GMM can be applied in one- or two-step variants. The one-step estimators use weighting matrices that are independent of estimated parameters, whereas the two-step GMM estimator uses the so-called optimal weighting matrices in which the moment conditions are weighted by a consistent estimate of their covariance matrix. However, the use of the two-step estimator in small samples, as in our study, has problem derived from proliferation of instruments. Furthermore, the estimated standard errors of the two-step GMM estimator tend to be small. Consequently, this paper employs the one-step GMM estimator.In the specification, year dummies are used as instrument variable because other regressors are not strictly exogenous. The maximum lags length of independent variable which used as instrument is 2 to select the optimal lag, the AR(1) and AR(2) statistics are employed. There is convincing evidence that too many moment conditions introduce bias while increasing efficiency. It is, therefore, suggested that a subset of these moment conditions can be used to take advantage of the trade-off between the reduction in bias and the loss in efficiency. We restrict the moment conditions to a maximum of two lags on the dependent variable.We estimated Eq. (1) using the GMM estimator based on a panel of 33 OIC countries. Table S1 in File S1 lists the countries and their income groups in the sample. The choice of countries selected for this study is primarily dictated by availability of reliable data over the sample period among all OIC countries. The panel covers the period 1980–2008 and is unbalanced. Following [40], we use annual data in order to maximize sample size and to identify the parameters of interest more precisely. In fact, averaging out data removes useful variation from the data, which could help to identify the parameters of interest with more precision.The dependent variable in our sample is logged per capita real GDP, using the purchasing power parity (PPP) exchange rates and is obtained from the Penn World Table (PWT 7.0). The economic dimension of KOF index is derived from Dreher et al. [41]. We use some other variables, along with economic globalization to control other factors influenced economic growth. Table S2 in File S2 shows the variables, their proxies and source that they obtain.We relied on the three main approaches to capture the effects of economic globalization on economic growth in OIC countries. The first one is the baseline specification (Eq. (1)) which estimates the effect of economic globalization on economic growth.The second approach is to examine whether the effect of globalization on growth depends on the complementary policies in the form of level of human capital and financial development. To test, the interactions of economic globalization and financial development (KOF*FD) and economic globalization and human capital (KOF*HCS) are included as additional explanatory variables, apart from the standard variables used in the growth equation. The KOF, HCS and FD are included in the model individually as well for two reasons. First, the significance of the interaction term may be the result of the omission of these variables by themselves. Thus, in that way, it can be tested jointly whether these variables affect growth by themselves or through the interaction term. Second, to ensure that the interaction term did not proxy for KOF, HCS or FD, these variables were included in the regression independently.In the third approach, in order to study the role of income level of countries on the growth effect of globalization, the countries are split based on income level. Accordingly, countries were classified into three groups: high-income countries (3), middle-income (21) and low-income (9) countries. Next, dummy variables were created for high-income (Dum 3), middle-income (Dum 2) and low-income (Dum 1) groups. Then interaction terms were created for dummy variables and KOF. These interactions will be added to the baseline specification."
        },
        "10.1371/journal.pone.0066706": {
            "author_display": [
                "Yiyong Cai",
                "David Newth"
            ],
            "title_display": "Oil, Gas and Conflict: A Mathematical Model for the Resource Curse",
            "abstract": [
                "\nOil and natural gas are highly valuable natural resources, but many countries with large untapped reserves suffer from poor economic and social-welfare performance. This conundrum is known as the resource curse. The resource curse is a result of poor governance and wealth distribution structures that allow the elite to monopolize resources for self-gain. When rival social groups compete for natural resources, civil unrest soon follows. While conceptually easy to follow, there have been few formal attempts to study this phenomenon. Thus, we develop a mathematical model that captures the basic elements and dynamics of this dilemma. We show that when resources are monopolized by the elite, increased exportation leads to decreased domestic production. This is due to under-provision of the resource-embedded energy and industrial infrastructure. Decreased domestic production then lowers the marginal return on productive activities, and insurgency emerges. The resultant conflict further displaces human, built, and natural capital. It forces the economy into a vicious downward spiral. Our numerical results highlight the importance of governance reform and productivity growth in reducing oil-and-gas-related conflicts, and thus identify potential points of intervention to break the downward spiral.\n"
            ],
            "publication_date": "2013-06-27T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 1382,
            "shares": 1,
            "bookmarks": 3,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0066706",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0066706&representation=PDF",
            "fulltext": "IntroductionOil and gas are common, high-value commodities in the world market. They are also essential commodities for economic growth and development. Prices for oil and gas have increased dramatically over the last few decades and are expected to continue to do so. Industrial processes, such as electricity generation, machine operation, and petroleum chemical production, require oil and gas. Therefore, areas with abundant oil and gas reserves should be prosperous; however, economists have shown that oil-and-gas-rich countries usually suffer from poor economic performance. The few exceptions include Australia, Canada, and Norway, which are all countries with a democratic regime and a workable tax system that redistributes profits from mining to the rest of the economy and that sustains peaceful development. This economic phenomenon is referred to as the resource curse [1], [2]. Moreover, energy consumption per capita is often far below the world average in oil-and-gas-rich countries, although these exports constitute most of the countries economies (see Figure 1). This is further referred to as the poverty in the midst of plenty.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  World's Proven Oil and Gas Reserves, and Earth's City Lights.Background image courtesy of NASA and data courtesy of CIA-The World Factbook.\ndoi:10.1371/journal.pone.0066706.g001Research on this resource development puzzle tends to focus on oil-and-gas-related civil conflict. The high value and high utility of oil and gas make them points of contest among different social groups. In a weak government, greedy elite may appropriate national patrimony to advance their personal fortunes, while frustrated civilians may use violence to gain control over oil and gas resources. In turn, the elite resort to outright repression to keep the civilians in check. The subsequent escalation of the attack-and-defence cycle displaces human, built, and natural capital [3]–[5]. It also generates political instability, which depresses investment and impedes economic growth [6]–[8]. Therefore, despite years of oil and gas extraction, a resource-rich country in civil conflict remains underdeveloped with an economy that is dangerously reliant on oil and gas exports [9]–[12]. This instability intensifies political competition for control over oil and gas reserves and gives rise to a loop of causalities between resource dependence and conflict [13].\nPolitical economy models generally consider conflicts to be equilibrium behaviors of different interest groups. These models commonly assume that the opportunity costs of attack and defence, or equivalently the productive returns on resources and labor, are exogenously given [3], [14]–[16]. However, these conflict models are insufficient to address the resource curse. It is plausible that a resource-abundant country in conflict is worse off than it is in the absence of conflict [17]. Nevertheless, it is implausible that a country is worse off than it would be without its natural resources, simply because it could neglect its resources and thereby escape from the curse. Therefore, particular attention must be given to the underlying institutions that drive the economy into self-destruction, such as social fractionalization [18]. Furthermore, reduced-form regressions based on these models may be subject to the problem of endogeneity, because of the possible causality loop between oil dependence and conflict. Subsequently, these regressions produce biased estimators, unless a natural experiment is available with relevant content, such as the discovery of an oil field and the subsequent civil conflict. This poses a challenge for empirical studies of the mechanisms that underlie the resource curse and for the formation of related policies.\nThis paper offers a supplementary perspective to the current understanding of the resource curse by using the context of oil, gas, and conflict. It relates poor economic performance to the existence of social fractionalization (elite and civilian), market frictions (monopolistic resource pricing), and resource-related conflict (economic disturbance). When oil and gas are monopolized by the elite, they are often exported rather than sold domestically to support local production. Increased exportation lowers the marginal return to productive activity, and consequently, civil insurgency emerges. The resultant conflict further displaces resources and labor and thus draws the economy into a vicious circle. In the absence of a natural experiment, this research provides a potential alternative structure for econometric identification of the mechanism that drives the resource curse. Additionally, it offers guidance to international organizations on the formation of policies for conflict resolution and poverty reduction.\nAnalysis\nBackground\nWe consider a two-period game that is set up in a small, open economy. The economy has two sectors: extraction and production. The game lasts for two periods . Let  be a measure of the population. At the beginning of period 1, there are two players: an elite  of -measure , who appropriates oil and gas (the resources), and a civilian  of -measure , who has labor force. Resources can be either exported or sold domestically, while labor activity can be either productive or insurgent. The elites represent less than % of the total population (), as shown in Assumption 1\nAssumption 1.\n\nRemark 1.This parametric assumption, the so-called 80–20 rule, is consistent with the World Bank statistics that the richest  hold close to  of the national income in most developing countries (Source: http://data.worldbank.org/indicator/SI.D​ST.05TH.20/countries).\nThe political regime is autocratic, and the elites rule the government. Revolution is broadly defined as any insurgent action or threat against the established political system. We do not distinguish rebellion, which is the attempt to revolt, from revolution, which is a successful rebellion. Accordingly, repression is defined as any counter-insurgency efforts of the elites.\n\n\n\nExtraction and Production\nOil and gas are “point resources” that are fixed in location and thus require sophisticated infrastructures to access, control, and transport. Only the elite can put together the necessary technology for exploration, production, and distribution, with the help of multinational oil and gas companies. The behavior of the multinationals are not modelled in the scope of this paper. According to latest Global Trade Analysis Project database statistics [19], labor in oil and gas extraction constitutes less than 2% of the total labor inputs, or less than 10% of the total inputs into oil and gas extraction in most of the developing world. For simplicity, it is assumed that resource extraction does not require labor input.\nAs is pre-contracted with the multinationals, in each period, the elite extracts one unit of resources and exports  of resources at price , which is exogenously given and constant. The remaining  is sold domestically at price , which is determined by a monopolistic mechanism to be discussed shortly. In total, the elite receives the period resource windfall of(1)\nOn the other hand, in each period, the civilian is endowed with one unit of time. The civilian purchases  unit of resources from the elite in the form of energy and industrial infrastructure and supplies  unit of labor to producewhere  is the size of civilian population in period ,  is the total factor productivity, and  is the output elasticity of resources. Altogether, the civilian has the period net income of(2)\n\n\nResource Market Equilibrium\nTo ensure that the economy has a comparative advantage in exporting, the following condition is assumed:\nAssumption 2.\n\nRemark 2.The following Equation (3) classifies that  is the marginal return on resources when domestic production is at full capacity. If the world price is below , then all resources are consumed domestically.\nGiven a domestic resource price of , the civilians optimal choice is to equalize marginal product and cost of resources as(3)Here, we have used the market clearing condition\n\nThe elite moves simultaneously with the civilian, and can exert monopoly power only to maximize current time profit but not to maximize total survival time profits. Therefore, it is the elites optimal choice to equalize the marginal profits of export and domestic sales, as follows:(4)\nAltogether, the resource market equilibrium is(5)(6)\n\nRemark 3.Assumption 2 ensures that the equality (5) is attainable.\nBy substitution, the elites period windfall is:\nand the civilians net income is\n\nWe can now concentrate on the political dynamics between the elite and the civilian.\n\n\n\nRevolution and Repression\nAt the beginning of period 1, the political statuses of the elites and the civilians are exogenously given. Over the course of the period, civilians can stage a rebellion using their non-productive time . In response, the elites can defend themselves by directing  of the resource windfall to the counter-insurgency expenditure, such as mobilizing military forces, bribing coup leaders, and seeking external intervention. The probability that the elites retain power in period is assumed to be determined by the function , such that(7)Here,  and  are parameters that represent the elites counter-insurgency effectiveness, which captures possible foreign military intervention. Since the early nineteenth century, Britain has played a key role in securing peace and prosperity in the Persian Gulf region. Following World War II, Britain scaled back its military presence around the world because of its economic problems. When Britain announced plans to withdraw troops from the Gulf region, the sheiks of the region asked the British to stay to ensure stability. For more information about oil-related and gas-related foreign intervention, see [20]. The restriction  ensures that  is concave in . Given , the rightmost term of Equation (7) is decreasing in . The spillovers of conflict into neighboring regions and the consequential countermeasures such as military intervention, economic sanctions and humanitarian aid are not explicitly considered in this paper.\nThe elites contest success function, i.e., Equation (7), is a fusion of two streams in the literature. The first presentation is similar to “gun choice,” as seen in [17]. The second presentation has the essence of probabilistic voting , which follows [21]. Because probabilistic voting eliminates the impact of the size of civilian population on political change, we make it comparable by assuming that only , the elites counter-insurgency expenditure as a proportion of total resource windfall, plays a role in .\nRemark 4.By the law of large numbers, the situation in which the civilian revolts with some effort is equivalent to the real-world situation, in which some organized civilians fight against the elites with full effort headed by a coup leader, while the remaining civilians continue to work with full effort. Modeling collective action of civilians is complex [22] and is beyond the scope of this paper.\nThe following properties of  are in order. First, the elite retains power when there is no revolution:\nSecond, the marginal regime-stabilization effect of the elites counter-insurgency efforts is positive and diminishing:\nThird, the marginal regime-stabilization effect of the civilians productive commitment is positive and constant:\nLast, the elites counter-insurgency efforts and the civilians productive commitment are substitutes:\n\n\n\nAftermath of Insurgency\nInsurgency is rewarding but risky. If the insurgency is successful, then the elite dies, and some “lucky” civilian of -measure  becomes the new elite. This leads to an expected loss of civilian population:(8)\nThis indirectly affects productivity in period 2, and can be considered as the expected lethality of revolution. Additionally, violence always causes the civilian to forgo work earnings, no matter who wins.\nRemark 5.Our model does not penalize the civilian if a rebellion is unsuccessful. Modeling this type of penalty requires a discrete function to capture the fact that the elite is penalized only if he or she revolts with an infinitely small effort and still fails, but not if he or she does not revolt. This treatment reduces the continuity and interior differentiability of the model, which are crucial to proving existence and uniqueness of the equilibrium. The expected loss of population already captures the dynamic trade-off of the civilian in relation to insurgency. Thus, we abstract the violence penalty to offer a theoretical model with a unique equilibrium solution that is econometrically identifiable.\n\n\n\nStrategic Interactions\nBoth the elite and the civilian have perfect information and move simultaneously in each period. Knowing the probabilistic regime switching and given the civilians labor supply , the elite chooses defence budget  to obtain(9)where  is the discount factor, and (10)is the elites period payoff net of counter-insurgency expenditures.\nOn the other hand, also knowing the probabilistic regime switching and given the elites defence budget , the civilian chooses labor supply  to obtain(11)where(12)is the civilian's period payoff.\nRemark 6.Although it is more realistic to assume that both the elite and the civilian are risk-averse, this greatly complicates the math. However, letting the elite and the civilian be risk-neutral and assigning linear utilities to their period payoffs does not change the fundamental results of the model. In fact, it gives rise to an equilibrium with no labor supply and thus no domestic production, which better approximates a full-scale civil war.\n\n\n\nEquilibria of Resource-Related Conflict\nLet  be the two-stage game, as defined in the previous section, where  is the list of all model parameters. For the game , the following solution concept is adopted:\nDefinition 1.A sub-game perfect equilibrium is a pure-strategy profile  such that\n\n\n\n\n maximizes , given ,\n\n maximizes , given ,\n\nfor any  that is predetermined,  maximizes , given ,\n\nfor any  that is predetermined,  maximizes , given .\n\nOur first result of this paper now follows:\n\nTheorem 1.Let Assumptions 1 and 2 hold.\n\n\n\n\n1. Let  and  be fixed. There exists a unique sub-game perfect equilibrium to the game , and it can be solved by backward induction.\n\n2. Let  and  be fixed. The solution to the period-2 sub-game is  and , and the solution to the period-1 sub-game must be of one of the two forms below:\n\n                \n\n\n\npeace with  and , or\n\nconflict with  and \n\n             \nWhen , we call the equilibrium civil war.\n\n\n\n\n3. Let other parameters in  and  be fixed. There exists a  such that . That is, the increase of world resource price will eventually lead to a civil war.\n\nProof. See Appendix S1.\n\nRemark 7.The equilibrium of repression and revolution discussed above offers a candidate mechanism for the resource-related conflict. As the elites take a large share of the domestic product, the civilians engage in insurgent activities to uplift their political status and raise the expected economic payoffs. The elites rely on resource windfall from exports and resort to outright repression, rather than economic reforms, to keep the civilians in check and to resolve their discontent. Interested readers can refer to, e.g., [23], for discussions on the effect of resource abundance on the political leaders behavior and political-regime determination. The subsequent escalation of attack and defence displaces labor and destabilizes the domestic environment, which adversely affects production. Consequently, resource exports emerge as the main source of national income, and power struggles over the control of resources prevail. Additionally, the incidence of social conflict parallels the economys increased dependence on resource exports, since higher world resource prices push up the cost of domestic production and intensify the civilians discontent. Because the equilibrium exists and is unique, it is possible to test it against real data. Indeed, our analytical result on the monotonicity of revolution (part 3 of the theorem) is consistent with the empirical finding of Besley and Persson [24], [25] that the oil export price is positively correlated with the incidence of civil war and that civil war is more prevalent among non-democratic oil producers.\n\n\n\nConsequences of Resource-Related Conflict\nIn this section, we investigate the (expected) Pareto inferiority of the equilibrium outcome of our model ( henceforth) as compared to two alternative models. The first-best alternative ( henceforth) has a social planner who solves the following program:(13)\nIn the second-best alternative ( henceforth), the elite is an unchallengeable monarch:\nClearly, the existence of social fractionalization and market monopoly in model  distinguishes it from , while the existence of conflict in  distinguishes it from .\nWe also compare our model to a resource-deficient economy ( henceforth), in which there is no social fractionalization and all resources are imported. In the absence of monopoly, the representative civilian solves the following program:(14)\nThis  economy has a comparative disadvantage in resources but an advantage in market institutions.\nLet  be the collection of models as discussed above. Accordingly, for , let  denote the equilibrium solutions,  denote the equilibrium domestic production, and  denote the equilibrium payoffs of the civilian, the elite, and the economy as a whole. When appropriate, we let  denote the civilians equilibrium payoff, given the world resource price . The following results are in order:\nTheorem 2.Let Assumptions 1 and 2 hold, and let  be fixed.\n\n\n\n\n1. The existence of social fractionalization, market monopoly, and civil conflict leads to the under-provision of labor and resources and thus depresses domestic production:\n\n\nAs a result, resources and labor are displaced from the social optimum:\nThe civilian is better off as a result of insurgency:Being challenged, however, the elite is worse off:All inequalities above are strict when conflict prevails, that is, .\n\n\n\n\n2. Let the conflict technology, i.e, , be constant. If either ,  or  is sufficiently small, then there exists a , such that\n\n\nIn other words, civilians in a resource-abundant economy can be poorer than their counterparts in a resource-deficient economy with better market institutions.\nProof. See Appendix S1.\n\nRemark 8.The theorem above provides a potential explanation for the resource curse. First, domestic production and total social welfare are lower in an economy with resources that are appropriated by the elite rather than in an economy with resources that are allocated by a benevolent social planner. This captures the primary welfare loss, which is caused by market frictions and monopoly. Second, over the course of conflict, resources and labor are displaced from socially profitable activities. This captures the secondary welfare loss, which is caused by attack and defence. For these two reasons, resource-abundant countries may fail to become wealthy. Moreover, if climate conditions are unfavorable (i.e.,  being small), if domestic production is labor-intensive (i.e.,  being small), or if the economy is trapped in a prolonged war (i.e.,  being small), then resource abundance is a curse. This is because the opportunity cost of conflict, as measured by the foregone growth potential, can be immense. Only a minority of civilians will become the new elite and reap the benefits, thus leaving the majority to handle the aftermath of conflict. In contrast, for a resource-deficient economy, domestic-oriented industrialization is a better choice for development. Indeed, according to the recent finding of Caselli and Tesei, resource-deficient countries are more likely to be democratic (Source: http://www.nber.org/papers/w17601). Thus, resource-rich countries may eventually be poorer than resource-poor countries.\n\n\nResults and DiscussionIn this section, we report the numerical results of our mathematical model. We begin by calibrating the baseline to replicate stylized facts of the resource curse and civil conflict. We then conduct sensitivity analyses with parameters in the production function that are related to climate condition and resource dependence. We also investigate the policy implications of resource subsidy and the strategic aspect of domestic-resource sales.\n\nBaseline\nFor the baseline exercise, we assume , which means that civilians constitute % and elites constitute % of the total population. We set total factor productivity  in the production function  equal to . Having a developing country in mind, we set the parameter  equal to , which is much lower than what is commonly used in the literature. The discount factor , is set to , which implies a conflict length of roughly  years, assuming an annual interest rate of 4%. The parameter  in the regime-switching function  is set to , which implies that the elite is three times more likely to win when both the elite and the civilian battle at full strength. According to a recent research of Collier and Hoeffler, only 82 out of 336 rebellions in Africa from 1960 to 2001 were successful (Source: http://users.ox.ac.uk/ econpco/research/pdfs/MilitarySpendingan​dRisksCoups.pdf).The other parameter  is set to , which implies that doubling , the elites counter-insurgency expenditure as a share of total resource windfall, roughly doubles the chance of retaining power when  is within %, but chances are lower when it is greater than.\nThe numerical results confirm our model prediction (Figures 2 to 5). Oil and gas prices are positively correlated to the incidence of conflict. Although conflict worsens social welfare, the civilian is in fact better off. An export boom (roughly a % price wedge) can relatively easily provoke resource-related civil conflict. However, a substantially higher world price (roughly a % price wedge) is required to compensate for the lost growth potential and to make civilians in a resource-abundant economy as successful as their resource-deficient counterparts, who have a more supportive market environment. This shows how easy it is to turn a resource fortune into a curse and how difficult it is to turn a curse into a fortune. In addition, the comparison across the four models, namely, , ,  and , highlights the importance of governance reform in oil-and-gas-related conflict resolution and poverty reduction.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Civilan's Labor Supply at Various Price Levels.doi:10.1371/journal.pone.0066706.g002\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Elite's Counter-Insurgency Expenditure at Various Price Levels.doi:10.1371/journal.pone.0066706.g003\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Social Welfare at Various Price Levels.doi:10.1371/journal.pone.0066706.g004\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Civilan's Total Payoff at Various Price Levels.doi:10.1371/journal.pone.0066706.g005\n\nClimate Change and the Incidence of Conflict\nIn the context of domestic agricultural production, total factor productivity  in the production function  can be understood as climate conditions, such as rainfall variation. Our model can be applied to recent studies of global climate and civil conflict [26]–[28]. Intuitively, adverse climate conditions (i.e.,  being small) lowers the return to legal labor activity, which causes the civilian to challenge the existing elites and to take over control of resources, and vice versa. This is confirmed in the simulation (Figure 6) where we alter the value of  to  and , respectively. Other parameters stay the same as in the baseline. Our simulation also highlights the importance of productivity growth in oil-and-gas-related conflict resolution.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Climate Change and Incidence of Conflict.doi:10.1371/journal.pone.0066706.g006\n\nResource Dependence and the Incidence of Conflict\nBy Euler's Theorem (see, e.g., [29]), the output elasticity of resources  in the production function  is also a measure of the economys dependence on resources. In the context of social fractionalization and monopolistic pricing, as  increases, the share of domestic output that is taken by the resource-owning elite increases, and thus the elite discontent increases. This is confirmed in the simulation (Figure 7) when we alter the value of  to  and , respectively. Other parameters stay the same as in the baseline. Intuitively, this suggests that, in an economy with less demand for oil and gas, high export prices are less likely to trigger conflict.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 7.  Resource Dependence and Incidence of Conflict.doi:10.1371/journal.pone.0066706.g007\n\nResource Subsidy\nOur model predicts that a resource-rich country with weak governance will have higher prices of oil and gas and lower consumption in the domestic market. In fact, many oil-and-gas-rich countries have struggled with sub-par levels of energy consumption, as previously discussed. However, it is more often the case that oil-and-gas-rich countries have lower energy prices, even after accounting for transportation costs (Source: http://www.mytravelcost.com/petrol-price​s/). This is counter-intuitive, as low resource prices should stimulate consumption.\nTo reconcile the observed reality, economic intuitions, and our model prediction, we postulate that domestic resources subside. We assume, ex post, that a fixed shared of the elites counter-insurgency fund is appropriated and spent as an ad valorem resource subsidy. In other words, the subsidies occur and become known to the civilian only after both  and  are determined. This redistribution scheme is never optimal, because it is arbitrary, conspired by the elite, and beyond the civilians strategic consideration. None of the fundamentals of our model is changed. Therefore, the same equilibrium would emerge even if domestic resource consumption were now subsidized. Social conflict may still linger and disrupt production. As a result, the resource consumption of a resource-abundant economy can be lower than that of a resource-deficient economy, despite a lower domestic price of resources.\nOn top of the baseline, we assume that % of the counter-insurgency expenditure is directed to subsidize domestic resource consumption. Figures 8 and 9 show that subsidy is most likely to be superfluous. By assumption, the domestic price is zero when resource consumption is (more than) fully subsidized on a per-unit basis or when there is no demand. As long as the institution of transfer is not open to the involvement of the civilian, insurgency is still the default activity and domestic production ceases. Subsequently, the subsidization budget is squandered. This simulated result is a stronger demonstration of the model prediction of trade-pattern effects of conflict than that of Garfinkel et al. [17]. If subsidy is an ineffective stimulator, then domestic consumption of resources remains low. Civil conflict interrupts production, and thus resources are over-exported.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 8.  Domestic Resource Price at Various Price Levels.doi:10.1371/journal.pone.0066706.g008\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 9.  Domestic Resource Sales at Various Price Levels.doi:10.1371/journal.pone.0066706.g009\n\nStrategic Aspect of Resource Sales\nOur model assumes that the elite moves simultaneously with the civilian in each period. Therefore, the elite can exert monopoly power only to maximize current time profit, rather than to maximize total survival time profits, because the choice of domestic resource sales has no effect on regime switching. This is a reasonable assumption, because many states with an elite class also have weak governance, and each individual elite controls a piece of the oil and gas resources. The consequential conflict between corrupt elites and frustrated civilians leads the economy into a development trap. The first-best solution is to transform the fractionalized state into a democratic nation and to streamline elite and civilian interests, as demonstrated in section with the social-planner model.\nHowever, as a robustness check, we also consider a scenario in which the elites collude and form a strategic alliance to tip resource sales towards the domestic market. This decreases rebellion risk and maximizes the elites€ total survival time profit. In this alternative timing model, with other things being the same as in the baseline, the elite pre-commits a certain amount of resources to domestic use before the civilian commits to labor supply. The elite commits to a level of counter-insurgency expenditure. An extra step of backward induction is to optimize the elites strategic choice of resource sales, considering its effects on the elites later choice of counter-insurgency expenditure and the civilians simultaneous choice of labor supply. The existence and uniqueness of the equilibrium can be established by an approach that is similar to that in this paper, in which we use numerical simulation without rigorous mathematical analysis.\nOur simulated results suggest that the economy is more peaceful in the alternative timing model (Figures 10 to 11). However, the elite is worse off than in the baseline (Figure 12). In other words, it is not profitable (from the elites point of view) or credible (from the elites point of view) for the elite to pre-commit domestic resource sales. Additionally, domestic resource sales are more volatile (Figure 13). When the world prices are low, the elite exports no resources; however, when world prices are sufficiently high, the elite exports all resources. Consequently, the welfare implication is mixed (Figure 14).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 10.  Civilian's Labor Supply under Alternative Timing.doi:10.1371/journal.pone.0066706.g010\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 11.  Elite's Counter-Insurgency Expenditure under Alternative Timing.doi:10.1371/journal.pone.0066706.g011\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 12.  Elite's Total Payoff under Alternative Timing.doi:10.1371/journal.pone.0066706.g012\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 13.  Domestic Resource Sales under Alternative Timing.doi:10.1371/journal.pone.0066706.g013\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 14.  Social Welfare under Alternative Timing.doi:10.1371/journal.pone.0066706.g014\nResource-abundant countries often show poor economic growth. This paper proposes a theoretical framework to study this paradox in the context of oil, gas, and conflict. The model addresses issues of social fractionalization, market friction, and civil conflict. It shows that the equilibrium between elite and civilian individual maximization behaviors can undermine the peaceful environment that is needed for industrialization and development. It can also further provoke resource-related conflict. The model predictions are consistent with observable facts.The setup of the model is simple, and the parameter assumptions are non-stringent. Yet, it features a causality loop between resource dependence, conflict, and poor economic performance. Since the equilibrium of the model is proven to exist, and it is unique and eventually monotone, the model provides a well-defined structure for hypothesis testing and for the econometric identification of the mechanisms that underlie the resource curse."
        },
        "10.1371/journal.pone.0022794": {
            "author_display": [
                "Kun Guo",
                "Wei-Xing Zhou",
                "Si-Wei Cheng",
                "Didier Sornette"
            ],
            "title_display": "The US Stock Market Leads the Federal Funds Rate and Treasury Bond Yields",
            "abstract": [
                "\n        Using a recently introduced method to quantify the time-varying lead-lag dependencies between pairs of economic time series (the thermal optimal path method), we test two fundamental tenets of the theory of fixed income: (i) the stock market variations and the yield changes should be anti-correlated; (ii) the change in central bank rates, as a proxy of the monetary policy of the central bank, should be a predictor of the future stock market direction. Using both monthly and weekly data, we found very similar lead-lag dependence between the S&P 500 stock market index and the yields of bonds inside two groups: bond yields of short-term maturities (Federal funds rate (FFR), 3M, 6M, 1Y, 2Y, and 3Y) and bond yields of long-term maturities (5Y, 7Y, 10Y, and 20Y). In all cases, we observe the opposite of (i) and (ii). First, the stock market and yields move in the same direction. Second, the stock market leads the yields, including especially the FFR. Moreover, we find that the short-term yields in the first group lead the long-term yields in the second group before the financial crisis that started in mid-2007 and the inverse relationship holds afterwards. These results suggest that the Federal Reserve is increasingly mindful of the stock market behavior, seen as key to the recovery and health of the economy. Long-term investors seem also to have been more reactive and mindful of the signals provided by the financial stock markets than the Federal Reserve itself after the start of the financial crisis. The lead of the S&P 500 stock market index over the bond yields of all maturities is confirmed by the traditional lagged cross-correlation analysis.\n      "
            ],
            "publication_date": "2011-08-10T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 2,
            "views": 2512,
            "shares": 2,
            "bookmarks": 9,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0022794",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0022794&representation=PDF",
            "fulltext": "IntroductionFinancial markets play a more and more important role in the economic system. Many financial variables have predictive power for output or inflation of the real economy. Financial markets are becoming increasingly important to the real economy due to their impact on output growth and inflation, among others [1]–[6]. As an important part of financial markets, stock markets can be considered as economy barometers [7], [8]. As a consequence, monetary policy, which is usually based on inflation target and sometimes unemployment goals, is not independent of stock markets. There is a large number of financial economic literature concerned with the impact of and relationship between the monetary policy of central banks and the performance of stock markets. The common wisdom asserts that (i) the stock market variations and bond yield changes should be anti-correlated and (ii) the change in short-term interest rates, as a proxy of the monetary policy of the central bank, should be a predictor of the future stock market direction. The first assertion reflects the impact of capital cost on economic growth. The second statement is a corollary of the causal effect of the former one.\nSome of the most relevant results for our study that were obtained by previous scholars on these two statements include the following. Tobin's portfolio selection theory [9] explained the stock price increases observed in times when the interest rate goes down as due to investors' preference for the higher yield of stock markets. Rigobon and Sack [10] documented that an increase in short-term interest rate results in a decline in stock prices and in an upward shift and flatter yield curve. Bernanke and Kuttner [11] found that a hypothetical unanticipated 25-basis-point cut in the FFR target is associated with approximately a 1% increase in the broad stock indexes. Bjørnland and Laitemo [12] found a significant relationship, which is however the inverse of (i) and (ii): a one percent increase of the stock market leads on average to a 4-basis-point increase of the interest rate. Two of us have also previously found that the stock market seems to influence the FFR, during the 2000–2003 US stock market antibubble [13]–[16].\nHere, using an extension of the so-called TOP technique [13]–[16] for the joint analysis of pairs of time series, we revisit the pertinence of these two assertions (i) and (ii) by estimating the lead-lag structure between the US stock market proxied by the S&P 500 index and a set of Treasury bond yields, including the Federal funds rate (FFR), which constitutes one of the tools implementing monetary policy in the US. Our analysis is applied to monthly and weekly data of Federal funds effective rate (FFR), and nine Treasury bond yields with different maturities: 3M (3 months), 6M, 1Y (1 year), 2Y, 3Y, 5Y, 7Y, 10Y, and 20Y. The period of analysis from August 2000 to February 2010 includes the bearish market up to mid-2003, the bullish bubble-like market regime up to October 2007 followed by the turbulent phases associated with the so-called great Recession [17], [18]. Given the extraordinary developments associated with the financial crises followed by economic crises in different parts of the world, it is particularly interesting to investigate the lead-lag structure between the US stock market and a set of Treasury bond yields.\nMaterials and Methods\nDescription of the thermal optimal path (TOP) method\nThe thermal optimal path (TOP) method has been proposed as a new method to identify and quantify the time-varying lead-lag structure between two time series. The TOP method was successfully applied to several economic cases [14]–[16]. It works as follows.\nConsider two standardized time series  and . The matrix  of distances between  and  is defined as [14], [15](1)The element  of the matrix  thus compares the realization  of  at time  with the realization  of  at time . The value  defines the distance between the realizations of the first time series at time  and the second time series at time . The  matrix  thus embodies all possible point-wise pairwise comparisons between the two time series. Note that the distance matrix could be modified to deal with two non-monotonic time series, for which the TOP algorithm is essentially the same [16].\nOnce the matrix  with elements given by Eq. (1) is obtained, an optimal path is determined that quantifies the lead-lag dependence between the two time series. Figure 1 gives a schematic representation of how lead-lag paths are defined [14]. The first (resp. second) time series is indexed by the time  (resp. ). The nodes of the plane carry the values of the distance for each pair . The path along the diagonal corresponds to taking , i.e., compares the two time series at the same time. Paths above (resp. below) the diagonal correspond to the second time series lagging behind (resp. leading) the first time series. The figure shows three arrows which define the three causal steps (time flows from the past to the future both for  and ) allowed in our construction of the lead-lag paths. A given path selects a contiguous set of nodes from the lower left to the upper right. The relevance or quality of a given path with respect to the detection of the lead-lag relationship between the two time series is quantified by the sum of the distances along its length, called the “cost” of the path. The lead-lag structure is then obtained as the relationship  as a function of , as described shortly. We stress that the two-layer scheme presented in Fig. 1 performs better than multi-layer schemes [15].\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Thermal optimal path method.Representation of the two-layer approach in the lattice  and of the rotated frame  as defined in the text. The three arrows depict the three moves that are allowed to reach any node in one step.\ndoi:10.1371/journal.pone.0022794.g001As shown in Fig. 1, it is convenient to use the rotated coordinate system  such that(2)where  is in the main diagonal direction of the  system and  is perpendicular to . The origin  corresponds to . Then, the standard reference path is the diagonal of equation , and paths which have  define varying lead-lag patterns. Inverting (2), we have(3)This means that a positive  corresponds to , which by definition of the optimal thermal path below means that the second time series  lags behind the first time series , or equivalently  leads .\nThe idea of the TOP method is to identify the lead-lag relationship between two time series as the best path in a certain sense. A natural idea is that the best path is the one which has the minimum sum of its distances along its length (paths are constructed with equal lengths so as to be comparable). This path with minimum cost has thus the minimum average distance between the two time series, i.e., it is such that  resembles the most  along this path . The problem with this idea is that the noises decorating the two time series introduce spurious patterns which may control the determination of the path which minimizes the sum of distances, leading to incorrect inferred lead-lag relationships. It has been shown that a robust lead-lag path is obtained by defining an average over many paths, each weighted according to a Boltzmann-Gibbs factor, hence the name “thermal” optimal path method [14]–[16]. Intuitively, this corresponds to performing an averaging operation over neighboring paths of almost the same cost.\nConcretely, we first calculate the partition functions , for all values of  at a fixed  in the lattice shown in Fig. 1, and their sum  so that  can be interpreted as the probability for a path to be at distance  from the diagonal for a distance  along the diagonal. This probability  is determined as a compromise between minimizing the mismatch or cost as defined above (similar to an “energy”) and maximizing the combinatorial weight of the number of paths with similar mismatches in a neighborhood (similar to an “entropy”). As illustrated in Fig. 1, in order to arrive at , a path can come from  vertically,  horizontally, or  diagonally. The recursive equation on  is therefore(4)where  is defined by Eq. (1). The parameter  plays the role of a “temperature” controlling the relative importance of cost versus combinatorial entropy. The larger  is, the larger the number of paths that contribute to the partition functions. In contrast, as , only the path with minimal cost counts. The recursion relation (4) is derived following the work of Wang et al. [19]. To get  at the -th layer, we need to know and bookkeep the previous two layers from  to . After  is determined, these values are normalized by  so that  does not diverge at large . The boundary condition of  plays an crucial role. For  and , . For , the boundary condition is taken to be , in order to prevent paths to remain on the boundaries.\nOnce the partition functions 's have been calculated, we can obtain any statistical average related to the positions of the paths weighted by the set of 's. For instance, the local time lag  at time  is given by(5)Expression (5) defines  as the thermal average of the local time lag at  over all possible lead-lag configurations suitably weighted according to the exponential of minus the measure  of the similarities of two time series. For a given  and temperature , we determine the thermal optimal path . We can also define an “energy” or cost  to this path, defined as the thermal average of the measure  of the similarities of two time series:(6)\n\n\nBootstrapping tests and statistical significance\nIn order to test whether the extracted lead-lag structure is statistically significant, we introduce a bootstrap approach [20] that is specifically adapted to the present problem. This statistical test extends and makes more robust the method and results, as compared with previous works [14], [15], [16]. Consider two time series  (for instance the logarithmic returns of S&P 500) and  (for instance the time increments of bond yields). We perform the TOP analysis on a fixed time interval at some temperature . Let us assume we obtain the lead-lag function . Recall that  is the diagonal of the  plane. We then shuffle  and  and redo the TOP analysis at the same temperature . We obtain a new lead-lag function . This process is repeated another  times, giving a total of  paths  with . A typical value of  used below is 1000. For each , out of the  reshuffled time series, we determine the 5% quantile  and the 95% quantile , denoted in the following as  and . If  is smaller than  or larger than , we interpret that the lead-lag  at time  is different from zero at the significance level of 95% or larger. Complementarily, given the obtained lead-lag , out of the  reshuffled time series, we obtain the -value as a function of , which thus characterizes the time periods when there is a statistically significant lead-lag structure as those with small -values.\n\n\nData sets\nIn the following, we apply the TOP method respectively to monthly and weekly data of the S&P 500 index, Federal funds effective rate (FFR), and nine Treasury bond yields with different maturities: 3M (3 months), 6M, 1Y (1 year), 2Y, 3Y, 5Y, 7Y, 10Y, and 20Y. Each time series spans from August 2000 to February 2010. The Treasury bond at 30-year maturity is not considered because it was discontinued in January 2002 and then reintroduced in February 2006.\nFigure 2 shows the weekly sampling of the FFR, the nine Treasury bond yields with different maturities, and the S&P 500 index. In the left panel of Fig. 2, very interesting patterns emerge in the term structure. In general, the yields of Treasury bonds with short maturities are more sensitive to the economic circumstance and change to a larger extent. In 2000, 2006 and 2007, the spread is very narrow and the FFR is even higher than the Treasury bond yields, corresponding to an anomalous inverted yield curve. These time periods correspond respectively to the early stages of the 2000 US stock market crash and to the current financial crisis. The spread reaches local maxima in 2004 and 2010. In addition, the right panel of Fig. 2 suggests that the FFR and the S&P 500 index change roughly in the same direction. It is thus interesting to refine this visual impression and determine rigorously using the TOP method described above what is the lead-lag structure between the evolution of the US stock market and the FFR, which embodies an important part of the policy of the Federal Reserve.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Data sets.(a) Weekly sampling of the Federal effective funds rate (FFR) and nine Treasury bond yields. (b) S&P 500 and FFR together with the 20Y for comparison.\ndoi:10.1371/journal.pone.0022794.g002In this paper, we use as inputs the logarithmic returns of the S&P 500 index and the increments of the FFR and of all the yields, rather than the non-stationary original time series. We define the logarithmic returns of the S&P 500 index as follows(7)and the logarithmic increments of yields curves as follows(8)where the time unit for  is one week for weekly data and one month for monthly data. We then normalize the two time series  and  so that their mean is zero and their standard deviation is equal to  [14]. This ensures that they are comparable and can be used meaningfully in the TOP analysis to extract their lead-lag structure.\n\n\nUnit root tests\nWe perform unit root tests on the logarithms of the original time series and their first-order differences ( and ) to check for their stationary. The augmented Dickey-Fuller (ADF) [21], Phillips-Perron (PP) [22], and Kwiatkowski-Phillips-Schmidt-Shin (KPSS) [23] tests are adopted. For the ADF and PP tests, the null hypothesis is that the time series has a unit root, which utilizes the -statistic. In contrast, the null hypothesis of the KPSS method is that the time series is stationary and uses the LM-statistic. The results are presented in Table 1.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Unit root tests of the logarithmic monthly and weekly data and their first-order differences.doi:10.1371/journal.pone.0022794.t001For the logarithmic monthly data and logarithmic weekly data, the ADF and PP tests show that these time series are not stationary and have a unit root since the -values are greater than 10%, except for the 20Y yield. In contrast, the KPSS test suggests that four time series are stationary since the -values are much greater than 10%.\nFor the differences of the logarithmic monthly data and logarithmic weekly data, the ADF and PP tests show that all time series are stationary at the 1% significance level, and the KPSS test also confirms that these time series are stationary at the 10% level. These results justify our use of the logarithmic returns in the TOP analysis in order to avoid possible spurious signals in the estimated lead-lag structure that could result from large excursions exhibited by the non-stationary time series.\n\nResults\nThe S&P500 leads all yields: Evidence from the TOP method\nEmpirical results.Figure 3 shows the instantaneous evolution of the lead-lag  between the returns of the S&P500 index taken as the first time series and the logarithmic variation of each of the yields for the monthly data at temperature . We have been careful to investigate the impact of the locations of the starting and ending extremities of the paths. There are indeed a total of  thermal optimal paths, because there are 19 starting points  and 19 ending points , denoted using the  system instead of the  system for simplicity. The 19 starting points are , , and  for . The 19 ending points are ,  and  for , where  is the length of the time series. The overall thermal optimal path  is chosen as the one with minimal energy (or total cost) among the  thermal paths. As for the choice of the temperature , we investigated other values and found our results to be robust and qualitatively similar with respect to variations of  between  and . To present our results, we choose this value  as it seems to represent a reasonable optimal, confirmed by cross-correlation analyses performed on the steady periods found with fixed lag times for various 's.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Lead-lag  for monthly data.Dependence of the lead-lag  between the returns of the S&P500 index taken as the first time series and the logarithmic variation of each of the yields for the monthly data: (a) FFR, 3M, 6M, 1Y, 2Y, and 3Y Treasury bond yields as the first group; (b) 5Y, 7Y, 10Y, and 20Y bond yields as the second group. The unit of  is one month.\ndoi:10.1371/journal.pone.0022794.g003Figure 3 is organized in two panels, each panel plotting one group. The first group includes FFR, 3M, 6M, 1Y, 2Y, and 3Y Treasury bonds as shown in Fig. 3(a). The second group includes 5Y, 7Y, 10Y, and 20Y Treasury bonds as shown in Fig. 3(b). The evolution of  in each group are quantitatively similar.\nFigure 4 is the same as Fig. 3 for weekly data. Apart from largest fluctuations of the lead functions, the results are very similar and robust to this change of time scale from monthly to weekly.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Lead-lag  for weekly data.Dependence of the lead-lag  between the returns of the S&P500 index taken as the first time series and the logarithmic variation of each of the yields for the weekly data: (a) FFR, 3M, 6M, 1Y, 2Y, and 3Y Treasury bond yields as the first group; (b) 5Y, 7Y, 10Y, and 20Y bond yields as the second group. The unit of  is one week.\ndoi:10.1371/journal.pone.0022794.g004\nStatistical significance.Before commenting and exploiting the information presented in Figs. 3 and 4, it is important to ascertain their statistical significance. For this, we use the bootstrap method described above. Figure 5 illustrates the obtained results from the monthly data for two maturities, namely the shortest one (FFR) and the longest one (20-year Treasury bond yield). It shows that the two lead function  are well above the 95% quantile curves, that is, . The conclusion is the same for other Treasury bond yields. We conclude that the obtained lead-lag structure for the monthly data cannot be produced by chance at the 95% significance level.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Bootstrap test for the significance of the lead-lag structure.(a) the monthly FFR and (b) the monthly 20Y Treasury bond yield.\ndoi:10.1371/journal.pone.0022794.g005Figure 6 illustrates the obtained results from the weekly data for two maturities, namely the shortest one (FFR) and the longest one (20-year Treasury bond yield). The conclusion is the same for other Treasury bond yields. Therefore, the  functions for the weekly data are positive at the 95% significance level, which unveils the nontrivial intrinsic lead-lag structure of the S&P 500 index and the yield time series.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Bootstrap test for the significance of the lead-lag structure.(a) the weekly FFR and (b) the weekly 20Y Treasury bond yield.\ndoi:10.1371/journal.pone.0022794.g006\nTwo shocking stylized facts.The first and most important observation extracted from Figs 3 and 4 is that, for all yields and at all times, the S&P500 index leads the yield changes, since  is always positive, which by definition (3), means that  for the optimal thermal path. Since the index  corresponds to the S&P500 index and the index  corresponds to one of the yields, this conclusion follows. This result confirms and extends considerably that reported previously by two of us [13] using standard measures of correlations over a restricted period from 2001 to 2003, under the somewhat provocative title “Causal slaving of the U.S. Treasury Bond Yield … by the Stock Market…” Indeed, as this title suggests, this result  is particularly striking and rich of implication. This result collides against the common wisdom that usually asserts the following two rules:\n\n\n\n\nthe stock market variations and the yield changes should be anti-correlated;\n\nthe change in FFR, as a proxy of the monetary policy of the central bank, should be a predictor of the future stock market direction.\n\nIndeed, according to the standard story, a lower interest rate means lower costs of borrowing for the private sector, implying that the private sector is going to profit from this opportunity by increased investments in innovations and entrepreneurial opportunities, leading (with some lag) to an improved outlook for the future growth of the economy. Since stock market prices reflect the anticipation of investors, this better outlook for the future economy should be soon reflected in the appreciation of the stock market. Reciprocally, an increase of the yields and in particular of the FFR should, according to the standard story, translate soon into a drag on the growth of stock markets.\nWe observe the opposite of (i) and (ii). First, we find that the stock market and yields move in the same direction, as pointed out independently by R. Werner [24]. Second, the stock market leads the yields, including and especially the FFR. The implication is clear: the central bank policy is (1) reacting to the stock market and (2) is following it. When the stock market exhibits a rally, the Fed tends to progressively increase its rates as an attempt to calm down the “overheating engine”, as occurred towards the end of the ICT bubble when the Fed rate was increased to 6.5%. A similar increase of the Fed rate occurred from 2004 to 2007. When the stock market plunges, the Fed tends to decrease its rates, in the hope of putting a brake on the stock market losses that negatively feedback onto the real economy via the wealth effect.\nBoth previous and present Fed chairmen Greenspan and Bernanke have increasingly made clear that the Federal Reserve does care more and more about the evolution of the stock markets. On Dec. 3rd, 2010, former Federal Reserve Chairman Alan Greenspan told CNBC that rising stock values have played a critical role in the economic recovery. The stock market got a boost from the Fed policy to boost liquidity, which drove interest rates down and pushed investors toward riskier investments like stocks. “I think we are underestimating and continuing to underestimate how important asset prices, very specifically equity prices, are not only to shareholders but the economy as a whole,” he said. Equities have risen more than 80% from the lows set during the financial crisis, noted Greenspan, benefiting investors and helping fuel the recovery (Source: http://www.dailyfinance.com/story/invest​ing/greenspan-rising-stock-markets-are-k​ey-to-recovery/19743325/?icid=sphere_cop​yright). On Nov. 3rd., 2010, Bernanke issued the following statement in an opinion article for the Washington Post released hours after the Fed announced the $600 billion of Treasury buying through June in a second round of unconventional monetary stimulus: “Resuming large-scale asset purchases should boost economic growth through lower borrowing costs and higher stock prices… Stock prices rose and long-term interest rates fell when investors began to anticipate this additional action… Easier financial conditions will promote economic growth.” Being content to see the stock market growing, this suggests a hidden mandate of the Federal Reserve to steer the stock markets.\nIt seems that the dynamics of the Fed policy, as translated in the Fed rates and the longer maturity yields (which of course are far from being controlled by the central bank), is much more straightforward than articulated in fancy models [25]. The evidence presented here suggests that Fed policy appears to be as if a straightforward reaction to financial markets was the main factor.\n\nComparison between different yields.Comparing the lead functions  for the various yields with different maturities, we find that the short-term yields in the first group (left panel of Figs. 3 and 4) move approximately in synchrony with the long-term yields in the second group (right panel of Figs. 3 and 4) until 2007. And this synchrony is almost perfect from the yields spanning FFR to 3Y in the first group until mid-2007. Thereafter, during the time period following the financial crisis that started in mid-2007, we can observe that the short-term yields clearly lead the long-term yields and we have the sequence of inequalities(9)This is seen from the fact that  tends to be larger for the short-term yields, since they are all compared with the same S&P500 stock market index. It is also interesting to observe the increasing lag  between the yield rates and the S&P500 index from around  month in 2000 to about one year in 2007. This is followed by a plateau for all yields from FFR to 3Y, that lasts about 2 years and is then followed by a decay of the lag thereafter to about half its maximum, i.e., around 6 months.\nFor the second group of yields with maturities from 5Y to 20Y whose 's are plotted in the right panel of Figs. 3 and 4, the picture is somewhat different. Before early 2003, the four curves are close to each other with no clear lead-lag structure between them. Then, from 2003 to mid-2007, a period corresponding to a very bullish upward trend of the stock market boosted by the favorable low rate of the Fed policy and a booming real-estate bubble, one can observe that the longer term yields lead clearly the shorter term yields:(10)Thereafter, in the reaction to the financial crisis, one observes as for the FFR-3Y yields that the shorter-term yields lead the long-term yields:(11)There is much less evidence for a plateau of the lead structure with respect to the S&P500.\nWe would also like to mention that a reversal such as the one from (10) to (11) does not seem to have been documented before.\n\n\n\nThe S&P500 leads all yields: Evidence from cross-correlation analysis\nBy construction, the traditional cross-correlation analysis [26] is not adapted to time-varying lead-lag structures. It is however useful to investigate how it performs in the present context in which the TOP method has diagnosed a significant time-varying structure. For centered random variables, the cross-correlation function can be calculated as follows:(12)where  denotes the sample average and  is the sample variance.\nTwo representative time series (FFR and 20Y) are presented for illustration. The significance levels of the cross-correlations are evaluated using bootstrapping tests through shuffling the return time series, similar to the analysis for the TOP method. We use the monthly data in this analysis. For each pair of time series, we analyze the whole time series and two non-overlapping time periods. The results are shown in Fig. 7. It is obvious that the lagged cross-correlation analysis is not able to characterize the instantaneous evolution of the lead-lag structure evidenced in the previous TOP analysis. This is not a surprise.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 7.  Lagged cross-correlation analysis.Lagged cross-correlation between the logarithmic return of the S&P 500 index and the logarithmic difference of the FFR (a–c) and between the logarithmic return of the S&P 500 index and the logarithmic difference of the 20Y Treasury bond yield (d–f) during different time periods: (a,d) the whole time period from August 2000 to February 2010, (b,e) the time interval from August 2000 to April 2007, and (c,f) the time interval from May 2007 to February 2010. The ordinate axis shows the cross-correlation coefficients . The unit of the lag time  along the abscissa is month.\ndoi:10.1371/journal.pone.0022794.g007The (S&P 500, FFR) pair.For the (S&P 500, FFR) pair in the whole time period, the highest peak found in Fig. 7(a), with a positive lag, shows that the FFR lags behind the S&P 500 index by about 3 months, with a cross-correlation coefficient , which is significantly positive at the confidence level of 95%. There are two other peaks that are also significant, one at a negative lag of  month with  and another at the positive lag  month with . The largest peak with positive lag and highest cross-correlation coefficient  can be considered as confirming the main results of the previous section that the stock market changes precede the FFR variations. Due to the fixed lead-lag structure of the method, the cross-correlation provides only an average coarse representation of the real much richer and dynamical nature of the lead-lag structure.\nFor the time period before April 2007, we see many peaks at positive and negative lags  that are significantly different from zero, as shown in Fig. 7(b). It is hard to extract from this plot a clear picture about the lead-lag structure between the S&P 500 and FFR. In the presence of large variations of the lead-lag structure, it is not surprising that the cross-correlation analysis is not informative.\nFor the time period after April 2007, we see in Fig. 7(c) a significant peak at the positive lag  month with . This lag is consistent in magnitude with the average value of the  curve shown in Fig. 5(a). This clear signal in the cross-correlation analysis can be explained from the fact that the lead-lag has stabilized approximately above a value of 6 months, according to the analysis of the  function shown in Fig. 5(a) during the time period under investigation.\n\nThe (S&P 500, 20Y) pair.For the (S&P 500, 20Y) pair in the whole time period, there are two significant peaks around zero lag  in Fig. 7(d).\nFor the time period before April 2007, the signal is ambiguous although we can see several significant peaks in Fig. 7(e).\nFor the time period after April 2007, we see in Fig. 7(f) only one significant peak at  month with . According to Fig. 5(b), the lead-lag  decreases from about  to  month. Therefore, these two analyses give consistent results: on average, the S&P 500 index leads the 20Y Treasury bond yield by about 3 months.\nComparing Fig. 7 for the cross-correlation analysis and Fig. 5 for the TOP analysis, we can conclude that the cross-correlation analysis can extract only part of the information and the TOP method is clearly superior.\n\n\nIn this work, we have adopted the thermal optimal path method to investigate the dynamics lead-lag structure between the S&P 500 index of the US stock market and Federal Funds rate, as well as several Treasury bond yields with different maturities. The time period that has been investigated runs from August 2000 to February 2010. Both monthly and weekly data have been used and we obtained consistent results. In all cases, the S&P 500 index is found to lead the FFR and the bond yields. This is quantified by the lead function  found to be positive at a high statistical confidence level determined by bootstrapping tests. This finding is consistent with and extends significantly a previous work reporting that the US Federal Reserve was “slaved” to the stock market during the 2000–2003 US stock market antibubble [13].According to the TOP analysis, we observed that the FFR and the Treasury bond yields can be divided into two groups. The first group contains FFR, 3M, 6M, 1Y, 2Y, and 3Y bond yields with short-term maturities and the second group contains 5Y, 7Y, 10Y, and 20Y bond yields with long-term maturities. The lead functions  between the S&P 500 index and the yields in each group have very similar quantitative shapes, while they are different at a quantitative level across the two groups. We found that the short-term yields in the first group lead the long-term yields in the second group before the current financial crisis around 2007 and the inverse relationship holds afterwards, namely the long-term yields lead the short-term yields after 2007.For the first group, the lead function  increases during the time period from 2000 to 2007, followed by a two-year-long plateau, and then plummets in late 2009. We also found that the yields (including FFR) with shorter maturity in the first group have a longer lag behind the S&P 500 index than for the longer maturities. In contrast, for the second group, the lead function  increases till 2006 and then decreases. We observed a reversal of the order of the lead functions  among the different maturities in 2007: a yield with shorter maturity has a shorter lag to the S&P 500 index before the reversal point and a yield with longer maturity has a shorter lag to the S&P 500 index after the reversal point. Qualitatively, the reversal phenomenon is coincident with the outbreak of the current financial crisis.The lag of the FFR to the S&P 500 index can be interpreted in the light of comments of the previous and present Fed chairmen Greenspan and Bernanke that the growth of stock markets is “key” to the recovery and health of the economy. The evidence provided here suggests indeed that the FFR policy is in a significant part influenced by the recent past behavior of the stock markets (stock market  Federal Funds rate). In plain words, the fact that the FFR follows the stock market direction can be interpreted as a direct attempt to limit its losses and revive it in times of bearish markets or to stabilize it in times of overly buoyant bubbling markets.As for the longer maturities, the lag structure with respect to the S&P 500 index reflects (i) a natural link in the term-structure that attach the longer maturities to the shortest maturity and (ii) the aggregate strategies of investors facing uncertainties over the long term behavior of the economy [27], [28]. In the first sub-stage before 04/2007, we observe the causal relational flow from the stock market  Federal Funds rate  short-term yields  long-term yields, and afterwards, we find the flow from the stock market  long-term yields  short-term yields  Federal funds rate. Thus, the lead-lag structures between the different yields changed after the financial crisis starting in 2007. This change can be rationalized by the strategies implemented by long-term investors in the face of growing global market uncertainties, such as central banks of major Asian countries and pension funds which are heavily invested in the US long-term Treasury bonds [29]. The stern challenges faced by the US economy escalated the uncertainty which cascaded to exchange rate and inflation. Consequently, the long-term Treasury bonds became quite reactive to the behavior of stock markets, reflecting the actions of these long-term investors “flying to safety”: a plunge in the stock markets led to strong demand for the supposedly safe US Treasury bonds, pushing down mechanically the corresponding yields. This suggests that the long-term investors have been more reactive and mindful of the signals provided by the financial stock markets than the Federal Reserve itself after the start of the financial crisis. This may be due to the more complex agenda as well as the delicate role of the Federal Reserve, which has to take into account the impact of its interventions [25]. Caution and prudence on the part of the Fed in a time of high uncertainty may thus be the reason for this inversion of the lead-lag relationship between changes of yields of different maturities. However, the robust lead of the S&P 500 stock market index with respect to yields of all maturities remains the most important stylized fact unearthed by our study."
        },
        "10.1371/journal.pone.0052749": {
            "author_display": [
                "Gabriele Tedeschi",
                "Amin Mazloumian",
                "Mauro Gallegati",
                "Dirk Helbing"
            ],
            "title_display": "Bankruptcy Cascades in Interbank Markets",
            "abstract": [
                "\n        We study a credit network and, in particular, an interbank system with an agent-based model. To understand the relationship between business cycles and cascades of bankruptcies, we model a three-sector economy with goods, credit and interbank market. In the interbank market, the participating banks share the risk of bad debits, which may potentially spread a bank’s liquidity problems through the network of banks. Our agent-based model sheds light on the correlation between bankruptcy cascades and the endogenous economic cycle of booms and recessions. It also demonstrates the serious trade-off between, on the one hand, reducing risks of individual banks by sharing them and, on the other hand, creating systemic risks through credit-related interlinkages of banks. As a result of our study, the dynamics underlying the meltdown of financial markets in 2008 becomes much better understandable.\n      "
            ],
            "publication_date": "2012-12-31T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 3,
            "views": 1622,
            "shares": 5,
            "bookmarks": 19,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0052749",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0052749&representation=PDF",
            "fulltext": "IntroductionAs economic literature has taught us in more than one occasion, there are many economic examples of situations in which mainstream theory, i.e., the Arrow-Debreu general equilibrium model, does not explain interactions between economic agents well. In particular, we believe that if we want to understand the dynamics of interactive market processes, and the emergent properties of the evolving market structures, it might pay to analyze explicitly how agents interact with each other, how information spreads through the market and how adjustments in disequilibrium take place.\nTo model how the agents’ decisions are influenced by their mutual interactions and the repercussions that these may have on the economic system, we use a “communication structure” based on network theory, in which nodes can represent agents and edges connective links measuring the intensity of interaction between agents.\nThe recent vicissitudes of the credit market are a natural research issue to be analyzed with graph theory. If the banks were “isolated units”, the bankruptcy of a borrower would be almost unimportant in the credit system. However, given the strong interdependence in the interbank market, the default of one bank can bring about phenomena of financial contagion.\nIn the last thirty years, in most advanced and developing economies, the financial sector has assumed an increasing relevance with respect to the production sector; furthermore, the role of the banking system has gradually shifted from the loan based financing of non-financial corporations to more market-based activities and speculative operations. This deep transformation, usually named as financialization of the economy, has not only increased the interdependence among financial institutions, but also determined an increase of “easy credit”. This has created asset bubbles and debt-induced economic booms, with the consequent rising of corporate debt-equity ratios and bank leverage that have made the economy increasingly fragile and potentially unstable. Following the severe financial and economic crisis that started in 2007 in US, the phenomenon of growing financialization is increasingly under critical discussion as some of the major causes of the crisis. Although different important interpretations of the current crisis have been proposed (see, for instance, [1]), the effect of the increasing globalization and financialisation of the economic system is, certainly, one of the key elements to understand the current crisis.\nThree types of propagation of systematic failure have been studied in the literature. First, the bank runs, known as self-fulfilling panic [2]–[6]. Second, the asset price contagion [7], [8]. Third, the inter-locking exposures among financial institutions [8]–[13].\nFollowing this last line of research, in this paper we are explicitly concerned with the potential of the interbank market to act as a contagion mechanism for liquidity crises and to determine macroeconomics outcomes such as bankruptcies. Allen and Gale (2000), Thurner et al. (2003) and Iori et al. (2006) have shown that, modeling the credit system as a random graph, when increasing the degree of connectivity of the network, the probability of bankruptcy avalanches decreases. However, when the credit network is completely connected, these authors have proven that the probability of bankruptcy cascades goes to zero. The explanation for this result is that, in credit networks, two opposite effects interact. On the one hand, increasing the network connectivity decreases the banks’ risk, thanks to risk sharing. On the other hand, increasing the connectivity rises the systemic risk, due to the higher numbers of connected agents which, in case of default, may be compromised. According to the three cited models, the impact of the risk sharing plays a leading role. So, in these models there is a benefit in creating links between agents, because they allow to diversify risk.\nAn exception to this view is the recent contribution by Lorenz and Battiston (2008), where the authors show that the introduction of a trend reinforcement in the stochastic process, describing the fragility of the nodes, generates a trade-off. Rising the connectivity, the network is less exposed to systemic risk, in the beginning, thanks to risk sharing. However, when the connectivity becomes too high, the systematic risk eventually increases.\nA forerunner of this trade-off between risk sharing and systemic risk was already present by Iori et al. (2006), where the authors showed that, in the presence of heterogeneity, a non-monotonic relationship between connectivity and systemic risk exists.\nIn the present paper, we deal with the correlation between risk sharing and connectivity in the interbank system. In view of the recent economic crisis, in fact, the linear relationship between connectivity and systemic risk should be reassessed. Spreading the risk around the globe may indeed improve stability in good times thanks to risk sharing. However, in times of crisis, we believe that the effect of critical perturbations can spread across the whole system. Therefore, the credit market as a network with interdependent units, is exposed to the risk of joint failures of a significant fraction of the system, which may create a domino effect such as bankruptcy cascades.\nA recent model that is related to ours is that of Battiston et al. (2012a). The authors show that, in the presence of financial acceleration - i.e., when variations in the level of financial robustness of institutions tend to persist in time or to get amplified - the probability of default does not decrease monotonically with connectivity. Along this line, several authors have started to analyze the correlation between connectivity and probability of bankruptcy in credit networks. Many theoretical studies have found a non-monotonic relationship between these two variables. In particular, many recent models [14], [15] have shown that the diversification of credit risk across many agents has ambiguous effects on systemic risk.\nThe problems arising from financial market interconnectedness have also been highlighted by empirical studies which have emphasized structural properties of lending networks before and after the current financial crisis [16]–[20] and defined new analytical tools able to better identify and monitor systemic risk and crisis transmission[21]–[23].\nOur model represents a simple three-sector economic system (considering goods, credit and an interbank market), involving firms and banks. Two types of credit are considered: loan and interbank credit. According to the economic situation, companies may ask for money from financial institutions to increase their output. In this case, firms enter the credit market and consult with a fixed number of randomly chosen banks. Banks consider the investment risk and finally decide whether to offer the requested loan and define interest rates. After this first consultation meeting, each firm asks the banks it links with for credit, starting with the one with the lowest interest rate. If this bank faces liquidity shortage when trying to cover the firms’ requirements, it may borrow from a surplus bank.\nIn the interbank market, we assume a random connectivity among banks. If one or more firms are not able to pay back their debts to the bank, the bank’s balance sheet decreases. To improve its own situation, the bank rises the interest rate offered to other firms, eventually causing other defaults among firms. The bad debt of companies, affecting the equity of financial institutions, can lead to bank failures as well. Since banks, in case of shortage of liquidity, may enter the interbank market, the failure of borrower banks could lead to failures of lender banks. The interest rate, thus, can bring about a cascade of bankruptcies among banks. The source of the domino effect may, on one side, be due to indirect interactions between bankrupt firms and their lending banks through the credit market and, on the other side, due to direct interactions between lender and borrower banks through the interbank system.\nThe originality of this work compared to Battiston et al. (2012a) is the introduction of three interacting markets influencing each other. In this way, we can study the impact of systemic risk not only on the agents’ dynamics such as their financial fragility, but also on the business cycle and economic growth. In this regard, we study the effect of an exogenous shock on a specific firm by increasing the connectivity in the interbank system, and we observe that the systemic risk prevails over the advantages of risk sharing. Although the demand of loans and the number of granted loans stay almost the same by changing the connectivity in the inter-bank system, surprisingly, with higher connectivity we observe larger cascades of bankruptcies among banks. As shown in Iori et al. (2006), we find that the root of avalanches lies in the agents’ heterogeneity. In particular, our results show that the degree of contagion depends on the size of losses imposed by failing debtor banks on creditor banks in the system (see [24]–[27] for empirical analysis). Moreover, in line with other works [28], [29], we show that financial crises are characterized by the procyclicality of leverage across financial institutions.\nFurthermore, we also find that the holding of large liquid reserves, while generally stabilizing in the interbank market, reduces the growth of aggregate output by decreasing granted loans and therefore firm investments.\nThe remainder of the paper is organized as follows. First, we describe the model with the behavior of firms and banks. Then, we discuss the results of computer simulations for the baseline model and for the model with the interbank system. Finally, the last section presents conclusions.\nStructure of the ModelOur model represents a three-sector economy: goods, credit and the interbank market.\nWe consider a sequential economy populated by a large number of firms  and banks , which undertake decisions at discrete time, denoted by t = 0,1,2,…,T.\nIn the goods market, output is demand-driven, that is firms, given their production constraints, sell as much output as the market can absorb. However, incomplete information about the market potential can generate a gap between the firms’ expected and realized demand. In this disequilibrium scenario, supply does not (necessarily) match aggregate demand, so the goods market may be out of equilibrium. In this way, the model is able to generate an unexpected shock to the revenues of firms, so that their profit may become negative.\nTo meet their expected demand, companies make investments using the credit market. Therefore, in each time period, a subset of firms enter in the credit market asking for credit. The amount of credit requested by firms is related to their investment expenditure, which is therefore dependent on their expected demand, interest rate and firm’s economic situation.\nThe primary function of banks activity is to lend their funds through loans to firms, as this is their way to make money via interest rates. Banks consulted by companies, after analyzing their credit risk, may grant the requested loan, when they have enough supply of liquidity. However, since banks adopt a system of risk management based upon an equity ratio, companies may not receive requested loans even if banks have enough supply of liquidity. If consulted banks do not have liquidity to lend, they can enter the interbank market, in order not to lose the opportunity of earning on investing firms. The interbank market has the same structure as the credit market.\n\nFirms\nIn each time period t, we have a large finite population of competitive firms indexed by . The overall population  of firms is time dependent because of endogenous entry and exit processes to be described below. Firms are profit seekers. Therefore, at any time period t, they try to maximize their expected profits, by forecasting the market demand.\nFollowing some of the key elements of behavioral agent-based models, closely related to Keynes’ view that ‘expectations matter’, to Simon’s view that economic man is boundedly rational and to the view of Kahneman and Tversky that individual behavior under uncertainty can best be described by simple heuristics and biases [30]–[34], we model a gap between a firm’s actual demand  and its expected demand . Demand  is defined as(1)where  is a constant,  is a normally distributed variable and the expected demand is .\nTo produce a homogeneous output , the firm f uses its capital  as the only input. The firm’s production function is(2)where the capital productivity  is assumed to be constant and uniform across firms for simplicity. However, given the incomplete information about the demand, firm f decides to produce as much as it expects the market to be able to absorb. In this light, the production function mirrors the maximum output that firm f can produce at any time t. This amount, however, can shrink due to a lack of the expected demand.\nTo clarify, assume that  and . This means that the firm can produce up to 100 goods. If its expected demand , it will just produce 10, as it is the maximum amount that the company forecasts to be able to sell. However, if its expected demand is , the firm will produce 100, as it cannot produce more with its capital. In the latter case, the firm will ask for a loan from the credit market to increase its productivity and satisfy expected demand in the future.\nThe only external source of finance that firms have is the loan from banks [35], [36]. The firm’s demand of loan to reach the expected demand is(3)\nEq. (3) reproduces an empirical evidence: lending often increases significantly during business cycle expansions, and then falls considerably during subsequent downturns [37], [38]. Consistent with this stylized fact, Federal Reserve Chairman Alan Greenspan (Chicago Bank Structure Conference, May 10, 2001) noted that at the bottom of the cycle, ``the problem is not making bad loans […] it is not making any loans, whether good or bad, to credit-worthy customers”, consistent with the sometimes dramatic fall in lending during cyclical downturns [39]–[41]. Eq. (3) therefore should be interpreted as a new micro-foundation, and its relevance and reliability is grounded by empirical evidence. Nevertheless, since borrowing is risky, the company considers its probability of bankruptcy and its risk aversion (see, for instance [42], [43]). To incorporate these elements into the model, we assume that the firm adjusts its demand of loan according to:(4)where  is a constant which mirrors the risk aversion coefficient and may be higher, lower, or equal to one, reflecting risk lover, adverse, and neutral respectively and  reflects the firm’s financial fragility based upon its debt commitments  and expected profit  ratio. If firm f expects its next profit not to be enough to pay back its installments, it will ask for less loan.\nAt each time t, the debt commitments  (interest & installment) for the firm f are , where  is the real interest rate that firm f pays to bank b. We assume that a loan given at time t to the firm f has to be payed back by the next  periods.\nFor simplicity, we furthermore assume that each firm has total variable costs equal to financing costs. Therefore, profits in real term are(5)where the selling price of one good is set to 1. Assuming that all the profits are retained [36], the firm’s capital stock changes are updated according to(6)\n\n\nBanks\nSimilar to companies, we have a time dependent finite population of competitive banks indexed with .\nWhen a firm needs loan, it contacts a number of randomly chosen banks. This means that a firm knows the credit conditions of few banks in each time step. Each contacted bank is assumed to offer an interest rate of(7)where  is set by the Central Bank and  is the supply of liquidity of bank b. So the interest rate is decreasing as the bank’s financial robustness.\nAfter exploring the lending conditions of the contacted banks, each firm asks the consulted banks for credit starting with the one offering the lowest interest rate. Banks deal with firms in a “first come, first served” basis. If a firm asks for a loan from a bank, either it receives the complete amount of the requested loan or it receives no money (where the bank may use the interbank market or not).\nThe regulation of financial intermediaries (Basel I and II) forces banks to hold a capital caution of  of liquidity to prevent bankruptcies due to unexpected losses. For the sake of simplicity, we model this regulatory parameter assuming that banks give the requested loan with a certain probability.(8)\nThis means, for example, out of 10 different requested loans with , one loan will be given. By increasing , banks are forced to hold in reserve a larger percentage of their liquidity.  has to be interpreted as the fraction of risk that a bank is allowed to take within a given time step, as compared to its own liquidity. This threshold may be viewed as a regulatory parameter, since it imposes an upper limit for a bank’s risk dependent on its cash. It is a helpful tool to limit the bank’s risk, in particular the credit risk. Moreover, according to Eq. (8), the volume of credit given by a bank is proportional to its present liquidity. The smaller the bank the smaller its transactions.\nIf the bank regulatory parameter is satisfied and the bank has enough supply of liquidity, then it grants the requested loan.\nIf the contacted bank has not enough supply of liquidity to fully satisfy the firm’s loan, then the bank considers to use the interbank market. Our goal is to understand how the interbank structure can influence the economic cycle and the bankruptcy among banks. As in the credit market, the requiring bank asks the lacking fraction of the loan requested by the firm from x randomly chosen banks. Among the contacted banks, the banks satisfying the risk threshold in Eq. (8) and having enough supply of liquidity offer the loan to the asking bank for an interbank interest rate, which equals the credit market interest rate in Eq. (7). Among this subset of offering banks, the bank  (borrower) chooses the bank, starting with the one offering the lowest interest rate. When it receives the requested loan, the bank lend it to the asking firm.\nBank supply of liquidity , evolves according to:(9)where the second term (right side) shows the total loan of bank b at time t, the third term denotes the installment and the interest that the bank receives from the ‘safe’ firms, to which it has given a loan not before  time steps ago, and the last term, , reflects the lending by bank b from other banks at time t. Note that  can be negative or positive, depending on whether the bank is creditor or debtor. In case of interbank borrowing, as for the firms, interests and installments must be paid back within the next  periods. When, for instance, we consider the borrower bank ,  is(10)where  is the credit that the bank  obtain from . It is important to underline that  is immediately used by  to lent firm f.\nLike companies, banks are profit seekers. A bank’s profits in time t is:(11)\nThe bank’s profit depends on the interests payed by firms (first term), on the firms’ bad debt, with  to be the share of loan that firms could not pay back because they went bankrupt (second term) and on the interbank credit (third term). Note that  is positive if the bank lends in the interbank system, otherwise zero. Considering the lending bank ,  is(12)\nAs in Eq (11), the first term mirrors interests payed by debtor banks and the second term is the banks’ bad debt (losses).\n\n\nBankruptcy Conditions and Demography of Firms and Banks\nBecause of the uncertain environment, agents may go bankrupt. In this model, bankruptcy happens to firms or banks when they do not have enough ‘cash’ (revenues) to pay their loans back. In this sense, we are much closer to the idea of liquidity crisis than to the financial fragility conditions of Greenwald and Stiglitz framework. When agents go bankrupt, they leave the market. We also assume that an agents leave the market if it fails to receive requested loans for s consecutive time steps.\nRegarding entries, we follow the approach of Delli Gatti et al. (2005). The economic literature has suggested models ranging from exogenously stochastic processes [44], where authors assume a simple mechanism of entrance based on a one-to-one replacement, to models with an endogenous entry process, which depends on expected profit opportunities [45], [46]. These last theories argue that the entrance of new firms in an industry will be influenced by the amount of sunk costs in the sector. A greater degree of sunk costs should reduce the likelihood of entry (see [47], [48] for empirical evidence).\nOur modeling strategy aims at reproducing this evidence. The number of new entrants () is obtained by multiplying a constant  with a probability, which depends negatively in the case of firms and positively in the case of banks on the average lending interest rate:(13)where d and e are constants. The higher is the interest rate, the higher are firms debt commitments, and the lower (higher for banks’ side) are expected profits, with entries being lower (higher for banks’ side) in number.\nMoreover, in line with the empirical literature on firm entry ([49]; [50]), we assume that entrants are on average smaller than incumbents, with the stock of capital of new firms and the supply of liquidity of new banks being a fraction of the average stocks of the incumbents. So, entrants’ size in terms of their capital stock is drawn from a uniform distribution centered around the mode of the size distribution of incumbent firms/banks.\n\nSimulation ResultsWe explore the dynamic properties of the economic system modelled above by means of computer simulations. We consider an economy initially consisting of  firms and  banks and study it over a time span of  periods. Each firm is initially given the same amount of capital  and demand . We fix , , , . Firm entrance parameters are , , and .\nEach bank is initially given the same amount of liquidity . We fix the Central Bank interest rate , , , , and . Despite the homogeneous initial conditions, the economy develops heterogeneous distributions through the interaction of noise and feedback effects.\nIn order to get rid of transients we evaluate only the last 1600 simulated periods. Simulations are repeated 100 times with different random seeds.\n\nStylized Facts of the Benchmark Model\nLet we start from a sort of “benchmark” setup, for which the model jointly accounts for an ensemble of stylized facts regarding both “micro/meso” aggregates such as indicators of industrial structures (e.g. firm size distributions and firm growth rates) together with macro statistical properties (including rates of output growth and output volatility).\nFirst of all, the model robustly generates endogenous self-sustained growth patterns characterized by the presence of persistent fluctuations, as shown in Figure 1 (left side).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Evolution of the aggregate output (left side) and growth rates of the aggregate output (right side), as a function of time.doi:10.1371/journal.pone.0052749.g001Indeed, aggregate fluctuations, measured by output growth rates (right side of Figure 1), are path dependent (i.e., nominal shocks have real and permanent effects). Moreover, they are characterized by cluster volatility, a well-known property in the financial literature (see for instance [51]). This implies that large changes in variable values tend to cluster together, resulting in a persistence in the amplitudes of these changes. A quantitative manifestation of this fact is that, absolute growth rates display a positive, significant and slowly decaying autocorrelation function. In our case, the autocorrelation parameter is equal to 0.95, a value very close to that found for the quarterly empirical data for the G7 countries, which is 0.93 [52].\nIn addition to fluctuations resembling business cycles, the simulated time path of aggregate activity is characterized by a broken-trend behavior. The model is able to generate an alternation of aggregate booms and recessions as a non-linear combination of idiosyncratic shocks affecting individual decision-making processes. The account of business cycles offered by the agent based model thus contrasts sharply with DSGE theory, according to which fluctuations in aggregate activity are explained by random variations in aggregate TFP growth. In our simulations, depressions are due to the failure of big firms. Indeed, since we do not impose any aggregate equilibrium relationship between the firms actual demand and their expected demand, our simulated market generates individual out-of-equilibrium dynamics. Due to the absence of any exogenously imposed market-clearing mechanism, the economy is allowed to self-organize towards a spontaneous order with persistent excess demands, which have important consequences on the dynamic of firms. In fact, the gap between the expected and actual demand may generate an unexpected shock to firms’ profits, able to trigger bankruptcies of firms. If one or more companies are not able to pay back their debts to banks, then also banks suffer with a decrease in their equity level. Consequently, in order to improve their own situation, banks rise the interest rate to all the firms in their portfolio, eventually causing other defaults among companies. Figure 2 (left side) displays the time series of firm defaults, which are roughly constant during the simulation even when the system experiences severe breakdowns. This feature of the model underlines the important role of heterogeneity. In fact, in Figure 2 (right side), we show that crises do not depend on the quantity of bankrupted agents, but on their ‘quality’. The same economic process can thus produce small or large recessions depending to the size of failed companies.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Time evolution of firm bankruptcies (left side) and decumulative distribution function of failed firms’ size (right side).doi:10.1371/journal.pone.0052749.g002In addition, it is important to note that the model provides an useful tool to predict crises. In line with Minsky’s Financial Instability Hypothesis (1992), we show that over periods of prolonged prosperity and optimism about future prospects, financial institutions grant more loans without considering borrowers financial fragility. A natural way to assess the co-movement between the increase (decrease) in aggregate output and increase (decrease) in the number of granted loans is to study their correlation. The Pearson correlation coefficient significant at 1% level between positive aggregate output changes and the number of granted loan reaches a value above 0.63, confirming higher credit levels in prosperous periods. However, it can happen that banks underestimate their credit risk, making the economic system more vulnerable when default materializes. In this case, we observe a negative correlation of 0.71 between aggregate production in time t and the leverage of firms in the previous time step.\nFigure 3 shows time series of granted loan (left side) and the inverse of firms leverage. The balance sheet identity implies that firms can finance their capital stock by recurring either to net worth () or to bank loans (), . From Eq (6) we can easily calculate firm equity . So, the leverage is equal to . In the graph (3) (left side), we plot . Comparing Figure 3 and Figure 1 (left), we observe that these three time series co-evolve. In particular, the simulated aggregate output suffers a severe crisis in , which is anticipated by a rapid increase in the financial fragility in the previous time steps (in fact the inverse of leverage decreases rapidly, as shown in Figure 3 (left)). Our findings support Minsky’s view. Expectations exceeding the actual demand are the main driving force behind over-leveraging and investing in riskier projects. When firms expect to be able to sell higher levels of output, they increase their loans. Banks, facing incomplete information about the true probability of good and bad outcomes, increase their borrowing to expand their balance sheet. This results in much higher defaults and financial instability once a bad state occurs (see [53]–[57] for empirical evidence).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Time series of granted loan (left side) and the inverse of the firms’ leverage (right side).doi:10.1371/journal.pone.0052749.g003Although companies in our model initially start with the same amount of capital and cash, trading generates a fat tail distribution of agents’ size, in accordance with the empirical evidence that, in real industrialized economies, market participants are very heterogeneous in dimension (see for example, [58]–[63]). Small and medium size firms -here we use firm production as proxy of firm size - dominate the economy. Large firms are relatively rare, but they represent a large part of total supply. When the firms size distribution is skewed, the mean firm size is larger than the median one, and both are larger than the modal firm size. Clearly, in this case the very notion of a representative firm is meaningless.\nFigure 4 (left side) displays this evidence and the distribution is well fitted by a power law distribution , with intercept 12.19 and slope −0.23. The result is robust to the Kolmogorov-Smirnov test.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  Decumulative distribution function of firm sizes (left side) and bank sizes (right side).doi:10.1371/journal.pone.0052749.g004Our analysis on banks sizes (see Figure 4 (right)) reveals a similar skewed distribution. In this case, the Kolmogorov-Smirnov test is consistent with the null hypothesis of a lognormal distribution of bank sizes [64], [65].\n\n\nDefault Cascades in the Interbank Market\nIn this section we explore the impact of the interbank market, in which each bank can be borrower and lender, at the same time, on the macroeconomic dynamics. In particular, we investigate the effect of credit risk and systemic risk on the aggregate fluctuations and on the dynamic of default cascades of banks.\nSince the purpose of this exercise is to study the evolution of a self-contained system with a given initial number of banks, we exclude the possibility that failing banks would be replaced by new entrants.\nThe first question concerns the role of reserve requirements, reflected by the  parameter in Eq (8). Figure 5 shows how different reserve ratios affect the fraction of surviving banks for the case of no interbank credit market (Higher  means higher reserves). As the reserve ratio  increases, the rate of bank failures clearly falls. This result is in line with other publications regarding the role of reserves (see Thurner et al. (2003) and Iori et al. (2006)). Obviously, increasing reserves contribute to the stability of individual banks, as shown by a lower value of average bank leverage (see center of Figure 5). However, increasing reserves reduces the output growth rate, since many firms do not get loans in the credit market (see right side of Figure 5).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  Time evolution of the number of surviving banks for different levels of reserve ratios:  (solid line),  (dotted line) and  (long dashed line) (left side).Average bank’s leverage over time and simulation as a function of  (center). Average output growth rate over time and simulation as a function of  (right side).\ndoi:10.1371/journal.pone.0052749.g005We now analyze how different degrees of linkage in the interbank market affect the bankruptcy of financial institutions.\nThe left panel of Figure 6 displays the number of surviving banks as function of time, for various numbers x of financial institutions each bank randomly links with. By increasing linkage, the systemic risk raises in the sense that in any period, more banks fail. Indeed, with 100 percent linkage, the system collapses completely, analogously to a tragedy of the commons [66]. This result is further analyzed by Figure 6 (center), which shows the average number of surviving banks, over all times and all simulations as a function of the number of interbank linkages. While the earlier empirical literature on the systemic risk, in line with Allen and Gale’s result on the risk sharing role, found a very little evidence of global vulnerability [26], [67]–[69]. Strong evidence has been collected after the default of Lehman Brothers, showing that interbank linkages strongly impact systemic risk (see Battiston et al. (2012a), [70], Wagner (2010)) through a high probability of domino effects. So, in line with these new empirical and theoretical works, we find that the default of an agent may increase the systemic risk by increasing the connectivity.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  Time evolution of the number of surviving banks with  for different interbank linkages:  (solid line),  (dotted line),  (dashed line),  (long dashed line),  (dot-dashed line) (left side).Average number of surviving banks as a function of x (center). Average absolute slope of the curve representing the number of surviving banks (right side) as a function of x.\ndoi:10.1371/journal.pone.0052749.g006Moreover, increasing x, not only the number of bankruptcies increases, but the time path of surviving banks also declines much more rapidly over time. This result is shown in the right panel of Figure 6, where we plot the average absolute slope of the number of surviving banks curve as a function of x. This graph provides a first evidence of contagious failures, that is periods in which many banks collapse together.\nIn line with our hypothesis that a higher connectivity generates a higher systemic risk, not offset by a lower credit risk, Figure 7 shows, on the left, that the banks’ financial fragility increases with interbank linkages.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 7.  Average bank’s leverage (left side).Average output growth rate (right side), over time and simulation as a function of x.\ndoi:10.1371/journal.pone.0052749.g007To understand if different linkages in the interbank market have some effect on the real economy, Figure 7 displays on the right hand side the average output growth rate as a function of x before bankruptcy cascades occur. One can immediately see that increasing the interbank connectivity has no effect on system growth. Companies have no benefits from a more strongly linked interbank market. In fact, it does not facilitate the granting of loans to enterprises, but it merely transfers liquidity among financial institutions.\nWe now turn to the issue of contagious failures. Banks are prone to default by bad debits of both the firm-bank credit market and the interbank market. To ensure that the higher number of bank bankruptcies in the case of a highly connected interbank market is not only the result of bad debits in the firm-bank market, but also is the result of more bad debits in the interbank market, we run the following experiment: we calculate the size of the largest connected component of the failed banks, which are connected by bad debits, in 100 simulations for each value of linkage in the interbank market (see Figure 8). As expected, a more inter-connected interbank market results in larger cascades of bankruptcies due the larger systemic risk.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 8.  Size of the largest bankruptcy cascades, which are connected by bad debits for a bank market of size 50, determined from 100 simulations for interbank linkages of 1, 5, 10, and 49.A highly connected interbank market results in large cascades of bankruptcies.\ndoi:10.1371/journal.pone.0052749.g008As for firms, we can infer that bankruptcy cascades depend on the size of failed banks -here we use bank liquidity S as proxy of bank size -. In fact, the distribution of failed banks for different interbank linkages is skewed (see left panel in Figure 9). Moreover, increasing the interbank connectivity creates fatter tails in the distribution of failed banks, as evidenced by a higher kurtosis (see center of Figure 9). A more precise measure of fat tails is provided by the Hill exponent. In the right panel of Figure 9, we plot the Hill exponent as a function of x. Empirically the tail exponent is found to take values between 2 and 4. Changing the parameters of the model our simulations generates values of the Hill exponent in the same range. When , that is for low connectivity in the interbank market, the tail exponent is closer to the “normal” value of 4. However, increasing , the model generates fatter tails.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 9.  Decumulative distribution function of failed bank’s size S, for  (solid line),  (dotted line),  (dashed line),  (long dashed line) and  (dot-dashed line) (left side).Kurtosis (center) and Hill exponents (right side) of failed banks distribution as a function of x.\ndoi:10.1371/journal.pone.0052749.g009\n\nConclusions\nIn this paper we have investigated systemic risk and the impact of sharing risk and in an interbank market. We have studied the agents’ financial fragility and the macroeconomic performance. The focus has been on how the emergent heterogeneity of market participants and the nature of their interconnectedness affect the trade off between mutual insurance and the potential for contagion.\nWe have shown that a higher banks connectivity not only increases the agent’s financial fragility, but also generates larger bankruptcy cascades due the larger systemic risk. Interestingly, high interbank linkages have no effect on economic output, even during boost/boom. The interbank market, in fact, just has a marginal effect on firms’ investments and on the granted loans. In contrast, higher bank reserve requirements stabilize the economic system, not only by decreasing financial fragility but also dampening avalanches. However, holding in reserve a larger percentage of banks’ equity affects the aggregate output growth by reducing credit to companies.\nOur simulation results also indicate that heterogeneity contribute to instability. Although this result is strictly related to the dynamic of our model, other theoretical studies [10], [71] have shown that the possible emergence of contagion depends crucially on the degree of heterogeneity. Indeed, when the agents’ balance sheets are heterogeneous, banks are not uniformly exposed to their counterparty. Therefore, if the contagion is triggered by the failure of a big bank, which represents the highest source of exposure for its creditors, the situation is certainly worse than when agents are homogeneous. One policy implication is that interbank lending relationships should be restricted to banks that share similar liquidity characteristics. These results may be specific to our model, but they offer stimulating insights into the nature of contagion.\nThe main limitation of this study is that our model is fully demand-driven, i.e. firms can sell all the output that market exogenously can absorb at a fixed price. In a future paper, we will extend this analysis by including endogenous prices, which will allow us to investigate the demand side as well. Furthermore, we will introduce a more realistic mechanism of interbank linkages, by modeling network structures in an evolutionary way.\n\n"
        },
        "10.1371/journal.pone.0069792": {
            "author_display": [
                "Sara Evans-Lacko",
                "Martin Knapp",
                "Paul McCrone",
                "Graham Thornicroft",
                "Ramin Mojtabai"
            ],
            "title_display": "The Mental Health Consequences of the Recession: Economic Hardship and Employment of People with Mental Health Problems in 27 European Countries",
            "abstract": [
                "Objectives: A period of economic recession may be particularly difficult for people with mental health problems as they may be at higher risk of losing their jobs, and more competitive labour markets can also make it more difficult to find a new job. This study assesses unemployment rates among individuals with mental health problems before and during the current economic recession. Methods: Using individual and aggregate level data collected from 27 EU countries in the Eurobarometer surveys of 2006 and 2010, we examined changes in unemployment rates over this period among individuals with and without mental health problems. Results: Following the onset of the recession, the gap in unemployment rates between individuals with and without mental health problems significantly widened (odds ratio: 1.12, 95% confidence interval: 1.03, 1.34). This disparity became even greater for males, and individuals with low levels of education. Individuals with mental health problems living in countries with higher levels of stigmatizing attitudes regarding dangerousness of people with mental illness were more vulnerable to unemployment in 2010, but not 2006. Greater agreement that people with mental health problems have themselves to blame, was associated with lower likelihood of unemployment for individuals with and without mental health problems. Conclusion: These findings study suggest that times of economic hardship may intensify social exclusion of people with mental health problems, especially males and individuals with lower education. Interventions to combat economic exclusion and to promote social participation of individuals with mental health problems are even more important during times of economic crisis, and these efforts should target support to the most vulnerable groups. "
            ],
            "publication_date": "2013-07-26T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 7,
            "views": 6580,
            "shares": 106,
            "bookmarks": 33,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0069792",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0069792&representation=PDF",
            "fulltext": "IntroductionSeveral studies have demonstrated large disparities in unemployment rates between people with and without mental illness. Although most people with mental illness want to work [1], they have higher unemployment rates than people without mental illness and compared to people with other chronic diseases [2]–[4]. High unemployment rates among individuals with mental illness are a major contributor to the substantial societal impact of these disorders [4]–[6]. Unemployment has an impact upon the course and outcome of mental illness [7] and excludes individuals from social participation. A period of macro-economic recession may be particularly difficult for people with mental health problems as they may be at higher risk of losing their jobs and more competitive labour market conditions may make it more difficult for them to find a new job in the first place [8]. This is especially important as research suggests that unemployment could present a specific barrier to recovery from mental illness [9], [10].\nUnemployment among people with mental illness may be aggravated during times of economic hardship [7], [11], [12]. Negative attitudes towards marginalized groups (e.g., ethnic minorities or immigrant groups) which often increase during an economic recession [13] are one possible factor influencing this trend. Recent research from Germany suggests that the German public's unwillingness to recommend an individual with depression for a job increased between 2000 and 2011 (i.e., during the period of the economic recession) compared to 1990–2000 [14]. A synthesis of public attitude trends in the US between the 1950s and 1990s showed improvements and declines which mirrored the economic and employment context of the country [7]. Findings regarding the impact of economic recession on disparities [15] and the mechanisms involved, however, are mixed [15]–[18] and we need to better understand the complexity of this relationship. Interestingly, one study [16] did not show that individuals with severe mental illness were at earlier risk of unemployment during times of economic contraction; however, this study specifically investigated individuals with severe mental illness who received occupational rehabilitation services and these results may not be broadly generalizable to the wider population of people with mental illness. Furthermore, the effects of the recession since 2008 on disparities are yet to be determined.\nIn addition to research which suggests that mental health problems increase during times of economic recession, we investigate the impact of the economic recession on people with mental health problems and how this may be mediated by stigma. This paper investigates the impact of economic hardship on unemployment rates of people with mental health problems using Eurobarometer survey data collected from 27 EU countries in 2006 and 2010. We test the hypothesis that the European macro-economic crisis since 2008 has had a greater impact on employment of people with mental health problems compared to people without mental health problems. We also hypothesise that the impact on individuals with mental health problems is greater for people living in regions with greater public stigma towards people with mental illness, after controlling for regional unemployment rates. Additionally, as some research suggests that certain population subgroups, such as men or individuals with low levels of education [19], may be particularly vulnerable during times of economic recession, we investigate whether there is a differential impact of the recession on these subgroups in relation to unemployment.\nMaterials and Methods\nData Source\nFull details of the design and sampling for the Eurobarometer surveys (Eurobarometer Mental Well-being 2006 and Eurobarometer Mental Health 2010) are given elsewhere [20], [21]. Data were collected via face-to-face interviews among European Union (EU) citizens (n = 29,248 in 2006 and n = 26,800 in 2010) residing in the 27 member states (approximately 1,000 individuals per country per year). For our analysis we restricted the sample to adults of working age (i.e., 18–64) (n = 20,368 in 2006 and n = 20,124 in 2010).\nThe initial mental health Eurobarometer survey was conducted in 2006 (fieldwork carried out between 7 December 2005 and 11 January 2006). A second survey assessing attitudes toward mental illness and treatment-seeking was administered to a new sample of respondents in 2010 (between 26 February and 17 March 2010). All participants were recruited via multistage random probability sampling. Participants were representative of residents aged 15 or older in the participating countries.\n\n\nAssessments\nMental health problems were assessed via the Mental Health Inventory (MHI-5), a well-validated and reliable measure derived from the Short Form 36 (SF-36) [22], [23] As a validated cut-point has not been established for the MHI-5 [22], for the purposes of this study, individuals scoring one standard deviation higher than the standardised mean score were categorised as having mental health problems.\nStigmatising attitudes towards individuals with mental health problems were assessed in Eurobarometer 2006 using four questions about various stigmatizing beliefs: (1) People with psychological or emotional health problems constitute a danger to others; (2) People with psychological or emotional health problems are unpredictable; (3) People with psychological or emotional health problems have themselves to blame and (4) People with psychological or emotional health problems never recover. Participants were asked how much they agreed with each statement. Response options were on a 4-point Likert scale from ‘totally disagree’ to ‘totally agree’. Participants who responded ‘totally agree’ or ‘tend to agree’ to each statement were considered as agreeing with that statement. Responses were aggregated within each country to obtain a country-level measure of stigmatizing attitudes.\nSocio-demographic information included age band (18–29, 30–39, 40–49 and 50–64 years), gender, education level (age at which individuals finished full-time education), and urbanicity (i.e., size of locality of respondent residence: large town, small or middle sized town or rural area/village). Current employment was assessed via the question: ‘What is your current occupation?’ Individuals could endorse the following categories: (1) responsible for ordinary shopping and looking after the home, or without any current occupation, not working (referred to throughout the paper as ‘home-maker’), (2) student, (3) unemployed or temporarily not working, (4) retired or unable to work through illness, or (5) in paid employment.\n\n\nNational level unemployment rates\nNational unemployment figures for the years 2006 and 2010 were taken from the Eurostat yearbook (http://epp.eurostat.ec.europa.eu/statist​ics_explained/index.php/Europe_in_figures_-_Eurostat_yearbook). Eurostat is a Directorate-General of the European Commission and the statistical office of the European Union. The Eurostat figures for 2006 were moderately highly correlated with the national unemployment rates calculated from the Eurobarometer data (r = 0.76 and 0.70, respectively).\n\n\nStatistical Analysis\nFour separate multivariable logistic regression models were used to examine predictors of unemployment for individuals with and without mental health problems in 2006 and 2010. Independent variables included age, gender, urbanicity, country-level attitudes regarding dangerousness, recovery, blameworthiness, and unpredictability of people with mental illness. Country-level variables were computed as an average rating for each country and each variable was standardized. Eurobarometer post-stratification weights, based on sex, age, region and size of locality, were used in all analyses to estimate the country-level averages. We used generalized estimating equations (GEE) with the robust variance estimates to model within-country correlations. In the absence of theoretical reasons for specifying a correlation matrix structure, we used an unstructured correlation matrix [24]. In order to investigate whether individual unemployment status differed by population subgroups of interest (i.e., men, individuals with low levels of education and younger individuals) following the recession, we first tested the interaction between survey year and these variables and then tested a three-way interaction between survey year, mental health problems and these variables. All analyses were carried out using SAS version 9.3.\n\n\nEthics statement\nEthical approval was not required as this was secondary data analysis.\n\nResults\nSocio-demographic characteristics (Table 1)\nCompared to individuals without mental health problems, individuals with mental health problems were disproportionately women (χ2 = 125.2, df = 1, p<0•001 in 2006 and χ2 = 87.9, df = 1, p<0•001 in 2010) and older (χ2 = 316.9, df = 3, p<0•001 in 2006 and χ2 = 93.9, df = 3, p<0•001 in 2010). The majority of people with and without mental health problems had completed education at least to 16 years of age; however, more of those without mental health problems finished education at age 20+ or were still studying (χ2 = 210.1, df = 1, p<0•001 in 2006 and χ2 = 237.8, df = 1, p<0•001 in 2010). A higher proportion of people with mental health problems had no formal education or only finished education at 15 years of age (χ2 = 313.8, df = 1, p = p<0•001 in 2006 and χ2 = 213.7, df = 1, p<•0001 in 2010). Individuals with mental health problems were less likely to be in paid employment or to be a student or home-makers and more likely to be unemployed or disabled/retired, (χ2 = 452.6, df = 4, p<0•0001 in 2006 and χ2 = 109.4, df = 4, p<0•0001 in 2010).\n\n\nTrends in unemployment among people with mental health problems\nUnemployment rates were higher among people with mental health problems compared to those without in both survey years (Table 1). Overall unemployment rates were also higher in 2010 compared to 2006. The gap in unemployment rates between individuals with and without mental health problems widened in 2010 compared to 2006 (Figure 1). The differential trend was statistically significant (odds ratio [OR] for the interaction term for mental health problems by year = 1.12, 95% confidence interval [CI]: 1.03, 1.34. We performed several types of sensitivity analyses to test the robustness of this relationship. We investigated additional cutpoints for individuals scoring in the top ten and the top five percent of mental health problems and their likelihood of unemployment relative to the rest of the population. The differential trend was also statistically significant for these groups: the p-value for the interaction term for mental health problems by year for the top ten percent was 0.020 and the top five percent was 0.018. We also conducted additional sensitivity analyses applying an instrumental variable approach in which individual mental health problems were considered to be endogenous to the model and this also showed a significant relationship and the interaction term for year and mental health problems was also significant (p<0.001).\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Average unemployment rates among individuals in Eurobarometer 2006 and 2010, stratified by presence of mental health problems (aged 18–65).doi:10.1371/journal.pone.0069792.g001\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Descriptive statistics among people with and without mental health problems in Eurobarometer 2006 and 2010.doi:10.1371/journal.pone.0069792.t001\n\nRelationship between unemployment and mental health status\nIn each of the survey years, local unemployment rates ascertained by Eurostat were strongly associated with the odds of being unemployed among participants both with and without mental health problems in Eurobarometer (Table 2). Among people with mental health problems, males were more likely to be unemployed than females in 2010 (OR: 1.58, 95% CI: 1.30, 1.92, p<0.001) and marginally more likely to be unemployed than females in 2006 (OR: 1.24, 95%CI: 0.99, 1.57, p = 0.067). The interaction term for gender and year was statistically significant for the entire sample (p<0.001) and among individuals with mental health problems (p<0.01), but not among those without mental health problems. In 2010, 21.7% of men with mental health problems were unemployed, compared to 13.7% in 2006. For women with mental health problems, the difference in unemployment rate between 2010 (15.6%) and 2006 (11.9%) in 2006 was narrower.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Results of multivariable logistic regression analyses for predictors of unemployment stratified by presence of mental health problems in Eurobarometer 2006 and 2010.doi:10.1371/journal.pone.0069792.t002In both 2006 and 2010 individuals in the youngest age band (18–29 years), with and without mental health problems, were more likely to be unemployed than individuals in the oldest age band (50–64 years). However, age patterns of unemployment in both survey years varied among those with and without mental health problems in that the younger age was more strongly associated with unemployment among those without mental health problems (p<0.001). Indeed, the unemployed with mental health problems were significantly older than those without mental health problems (mean age = 4.3 vs. 36.1, t-test = 10.16, p = 0.001).\nFewer years of education was significantly associated with unemployment among individuals with and without mental health problems; however, education was more strongly associated with unemployment among individuals with mental health problems compared to those without these problems (p = 0.001). The impact of education on employment was also more substantial during 2010 compared with 2006 among individuals with mental health problems only (p = 0.010). This interaction was also significant among the entire sample (p = 0.020), but not among those without mental health problems.\nUrbanicity (i.e., size of the town where participants were recruited) did not play a major role in likelihood of unemployment except that individuals with mental health problems who lived in a large town relative to a rural area were more likely to be unemployed (Table 2), which could be interpreted as implying that a larger labour market disadvantages those with mental health problems.\nDuring 2010, but not 2006, among individuals with mental health problems only, living in a country where a higher proportion of the general public agreed that people with mental health problems are dangerous was associated with a higher likelihood of being unemployed (Table 2). During 2006, individuals with mental health problems living in a country where a lower proportion of the general public agreed that people with mental health problems have themselves to blame were more likely to be unemployed. This relationship was maintained in 2010. Living in a country where a higher proportion of the general public agreed that people with mental health problems will never recover was associated with a marginally higher likelihood of being unemployed among individuals with mental health problems (p = 0.097).\n\nEconomic recession has had enormous impacts across much of Europe; however, little information is available about the specific impact of the recession on groups who are already vulnerable to social exclusion, specifically individuals with mental illness. This is the first study to demonstrate that the European economic crisis had a greater impact on people with mental health problems, compared to people without mental health problems, as measured by exclusion from employment. Our study also identified important sub-groups which experienced greater impacts of the economic recession in terms of unemployment, specifically men and individuals with low levels of education. Overall, males and individuals with lower levels of education appear to have been affected disproportionately by the recession; both groups had a significantly greater increase in likelihood of being unemployed following the recession. Moreover, for individuals with mental health problems, gender and level of education were particularly important determinants of employment status as the recession seemed to have a disproportionately higher negative impact on their likelihood of being employed for men and those with less education. This may be due to shifts in labour markets: other studies have suggested that men may be more vulnerable to unemployment during the current recession in Europe as they are more likely to work in construction and manufacturing jobs which are more vulnerable to decreases in demand and job loss [25], while other research suggests that this disparity is only evident during the initial stages of a recession [11].This study also showed that stigmatizing attitudes, specifically beliefs regarding dangerousness of individuals with mental health problems, could be an important mediator in the relationship between unemployment and mental health problems following the recession. Living in a country where a higher proportion of individuals believe that individuals with mental illness are dangerous was associated with a higher likelihood of unemployment for people with mental health problems, but did not influence employment rates for those without mental health problems. Moreover, this became significant in 2010, following the economic recession. Other studies have emphasised the persistence of attitudes related to dangerousness and their association with community rejection [26]. Research on racial discrimination suggests that stereotype amplification in relation to risk and fear of victimisation plays an important role in the persistence of racial inequalities and community segregation [27]. These attitudes may be internalised by the stigmatised group. Recent international work underscores the prevalence of experienced and anticipated discrimination among people with depression in relation to employment, suggesting that this is a critical barrier to achieving employment integration [28]. A recent analysis of trends in public attitudes toward people with mental health problems in England and older research from the U.S. also suggested that attitudes to people with mental health problems may harden during periods of economic crisis [7], [29]; however, there is a gap in research around this topic. Surprisingly, a higher proportion of the public endorsing blameworthiness was consistently associated with lower rates of unemployment among people with mental health problems. Previous research has found that stigmatizing attitudes are highly specific in their relation to impact on people with mental health problems. For example, living in a community with stronger beliefs about blameworthiness of individuals with mental illness is associated with lower rates of willingness to seek professional help [30] but also lower levels of perceived discrimination among people with mental health problems [31]. Other research has shown that world views such as stronger just world beliefs for self may be a double edged sword as they are associated with greater blameworthiness; but also lower self stigma among people with mental illness [32]. It could be that environments with greater endorsement of blame and controllability of symptoms and/or illness also engender a context where the guilt and blame associated with those who are not working is increased. Thus, any intervention would need to carefully consider the complexity of cultural factors and beliefs underlying individual and public attitudes.Previous studies have demonstrated the impact of the recession on public health more generally [33]–[35], however, the selective impact of recession on people with mental health problems, especially males or individuals with lower levels of education, should be acknowledged through both research and policy. Analysis of general government policy responses in Europe following the crisis reveals deficiencies and problems and suggests that governments should allocate resources toward keeping and reintegrating people into employment in addition to initiating programmes that help people cope with the negative effects of job loss to counteract the adverse health effects of the recession [33]. Highlighting the population subgroups who are most vulnerable to economic shocks and identifying ways to mitigate the effects of these shocks is also important. It may be that investment in targeted programmes such as debt advice for people with mental health problems may improve their mental health and financial circumstances [36], [37]. Given the cuts in mental health services across Europe, the impact of the recession is likely to be felt among a growing number of individuals alongside dwindling resources. Lack of resources may strain mental health services during times of higher need leading to decreased access in the face of increased need. In Spain where the impact of the recent recession has been among the greatest, the prevalence of mental disorders diagnosed in primary care settings is increasing. These increases are associated with increases in unemployment and also present among individuals whose employment is threatened and also those who are struggling to make payments on their mortgage [38]. Recent findings from both England and Spain suggest that the recession is associated with a deterioration in population mental health [19], [38]. In addition to people with mental health problems generally, it is important to acknowledge specific subgroups with mental health problems, such as males and those with lower education. In addition to having a higher likelihood of unemployment, these subgroups have lower rates of help-seeking and more negative attitudes about mental illness [29], [39] and thus, may require specific forms of outreach.This study presents new and important information about the impact of macroeconomic downturn on people with and without mental health problems in Europe using nationally representative data from 27 countries in Europe surveyed over two time points, before and after the onset of the current recession. Nevertheless, the data were not collected with the specific aims of this study in mind and were not longitudinal in nature as the same individuals were not interviewed in the two surveys. Mental health status was determined via a brief self-report measure and thus mental health problems were not verified by a clinician. Additionally, type and severity of problems were not assessed. Most previous research on employment of individuals with mental health problems and also on mental illness stigma has focused on those with severe mental disorders which could not be identified in the Eurobarometer data. Additionally, data on potentially important characteristics such as ethnicity and immigration status or survey response rates were not available. The investigation was limited to two time points only and although the impact of economic recession was clearly evident in 2010, long term effects could not be investigated. Relatedly, as these are observational data, our analyses could not rule out reverse causality, and the potential that people who were unemployed were more likely to develop mental health problems in 2010. Other research has suggested that a large proportion of the consequences of unemployment such as mortality are due to mental health related selection prior to becoming unemployed [40] suggesting that this is an important mechanism to investigate. Our main outcome of interest was unemployment; however, there may be other important effects of the economic crisis in terms of social exclusion which we were not able to examine. As Eurobarometer recruited individuals by household, we were not able to investigate individuals who may have transitioned into more extreme types of exclusion i.e., individuals who became homeless, were in care or hospital settings or were imprisoned. Finally, attitudes about people with mental illness were only collected at one time point in 2006 which precludes assessment of changes in public attitudes over time and its potential impact on unemployment trends. However, the assessment of attitudes preceded the economic crisis and so was not confounded by the effects of the recession.Past research has consistently shown that people with mental health problems tend to be excluded from employment, housing and social relationships, and that this exclusion has negative social and economic consequences [8]. This study suggests that times of economic hardship are likely to heighten such exclusion for people with mental health problems. The study also provides some preliminary clues as to which groups of individuals with mental health problems are especially vulnerable during times of economic hardship, and what societal factors might moderate this adverse relationship. Use of both individual-level and aggregate-level data to explore this relationship provides new and important evidence about the impact of the macro-social context on individuals during times of economic recession and facilitates micro-macro research in relation to mental health and exclusion [41], [42]. Findings suggest that programmes to combat exclusion and to promote mental health may be more important during times of economic crisis. Future research should examine the long term effects of the economic recession on people with mental health problems and the relationship between different types of employment and social welfare policies and unemployment rates for people with mental health problems."
        },
        "10.1371/journal.pone.0040693": {
            "author_display": [
                "Petre Caraiani"
            ],
            "title_display": "Evidence of Multifractality from Emerging European Stock Markets",
            "abstract": [
                "\n        We test for the presence of multifractality in the daily returns of the three most important stock market indices from Central and Eastern Europe, Czech PX, Hungarian BUX and Polish WIG using the Empirical Mode Decomposition based Multifractal Detrended Fluctuation Analysis. We found that the global Hurst coefficient varies with the q coefficient and that there is multifractality evidenced through the multifractal spectrum. The exercise is replicated for the sample around the high volatility period corresponding to the last global financial crisis. Although no direct link has been found between the crisis and the multifractal spectrum, the crisis was found to influence the overall shape as quantified through the norm of the multifractal spectrum.\n      "
            ],
            "publication_date": "2012-07-17T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 4,
            "views": 1379,
            "shares": 0,
            "bookmarks": 0,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0040693",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0040693&representation=PDF",
            "fulltext": "IntroductionThere is a long interest in modeling financial markets that span well beyond the disciplines of finance and economics attracting mathematicians, physicists and many others from different fields. The attractiveness of financial markets comes not only from its complex dynamics that result from the interactions of a multitude of agents but also from its presence and influence in our daily life as the last financial crisis has proved it. One of the questions that emerged in the last decades was whether financial markets are characterized by chaos and fractality.\nBefore going further, we clarify a few key concepts for the general audience. By efficient financial market we understand, following [1], a market where prices reflect in a full manner all the information available and, moreover, they adjust in a quick manner when new information becomes available. We also use the concept of daily (index) returns by which in this paper we understand the logarithmic difference of a stock market index between its closing price in a certain day and its closing price a day earlier.\nThe discipline of economics has not remained indifferent to the rapid emerging field of fractal and chaos theory. The development of testing techniques in the fields of mathematics and physics has started to be felt in economics in the early `80’s when early tests for the presence of fractal dimension and chaotic behavior in economic and financial processes were applied, see [2] and [3] for a review of early results. Until now, the idea of chaos and fractal behavior remains debatable in the field of economics and finance, mostly due to the specific of economic time series characterized by relatively short samples (the accurate computations of correlation dimension or the maximum Lyapunov exponent require large samples) and the presence of noise. [4] summarized the research taken in the `80’s and `90’s by pointing that there is no evidence of “within the structure of the economic system” as current tests cannot determine the source of detected chaos.\nAt the same time, as some of the research points out, [3] and [4], there is a further need to further develop tests and deepen the topic of chaos and fractality in the field of economics. The need is even more urgent in the discipline of finance. The still dominant paradigm of efficient stock markets as outlined by [1] has serious weaknesses, among which we can enumerate time dependent self similarity, see [5] and [6] for a larger review. Such weaknesses called for alternative theories, one of which is worth mentioning in the context of present paper, namely the fractal market hypothesis due to [6]. According to [5], the fractal market hypothesis assumes that asset returns are dependent on both frequency and time horizon and that there is global dependency manifested through its fractality. This hypothesis has been reinforced by the discovering of multifractals in the asset returns, see [7] for early findings, which develops earlier ideas by [8] as well as [9].\nAlthough there is a growing work on multifractality for either developed stock markets, see [7], or emerging stock markets, [10] or [11] , the literature not only on multifractality, but in general in testing for chaotic and fractals behavior in CEE stock markets is very limited. Nevertheless, some papers are worth mentioning. [12], using a Hurst coefficient derived on the basis of the wavelet decomposition, found evidence for long run dependence on some of the CEE stock markets. They also found evidence of a time dependent value for the Hurst coefficient. In a recent paper, [13], using the Hurst coefficient determined on the basis of the Detrended Fluctuation Analysis, analyzed the dynamics of daily returns of share prices of 126 selected companies from the Warsaw Stock Exchange. He found that the after the drop in the Hurst exponent, the change in either long-term trend or in the long-term rate of return has an increased probability than for points randomly selected from the whole sample.\nThis paper proposes itself to answer to several questions, namely whether the daily returns in the selected CEE stock market indices are characterized by multifractality, how much using a surrogate data series, shuffled ones, leads to changes in the results. Not at last, we also investigate whether the crisis period has lead to different strenghts of multifractal spectrum, as suggested in an earlier work on the 1987 financial crisis by [14].\nThe paper is organized as follows. The methodology used throughout the paper is explained in the second section. The third section presents the empirical results and discusses the results. The last section draws the conclusions and outlines some possible extensions of this paper.\nMethodsThe methodology is based on the Empirical Mode Decomposition, EMD hereafter, based Multifractal Detrended Fluctuation Analysis, (EMD based MFDFA hereafter). There are a number of techniques to derive the multifractal spectrum of a time series, some based on wavelets, other based on detrended fluctuation analysis.\nBasically, EMD based MFDFA is a development of the now well established technique of Multifractal Detrended Fluctuation Analysis, MFDFA hereafter, due to [15]. We discuss first the EMD approach in decomposing time series, and present then in a comparative way the standard MFDFA as well as the EMD variation.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Standard MF-DFA Analysis of Czech Stock Market Index PX.a) Daily returns for Czech Stock Market Index PX; b) Log Scaling Function; c) q-generalized Hurst Exponent; d) Multifractal spectral scaling exponent τ(q) versus q; e) Multifractal spectrum.\ndoi:10.1371/journal.pone.0040693.g001\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  Standard MF-DFA Analysis of Hungarian Stock Market Index BUX.a) Daily returns for Hungarian Stock Market Index BUX; b) Log Scaling Function; c) q-generalized Hurst Exponent; d) Multifractal spectral scaling exponent τ(q) versus q; e) Multifractal spectrum.\ndoi:10.1371/journal.pone.0040693.g002\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Standard MF-DFA Analysis of Polish Stock Market Index WIG.a) Daily returns for Polish Stock Market Index WIG; b) Log Scaling Function; c) q-generalized Hurst Exponent; d) Multifractal spectral scaling exponent τ(q) versus q; e) Multifractal spectrum.\ndoi:10.1371/journal.pone.0040693.g003\nThe Empirical Mode Decomposition\nThe Empirical Mode Decomposition, is a new technique in signal theory due to [16]. Several papers have outlined its advantages with respect to other filtering techniques, see) [17] or [18]. As [16] showed, essentially, the EMD consists in decomposing a certain time series into a finite number of so-called intrinsic mode functions. These functions have to fulfill two essential conditions. The first one says that the numbers of local extreme and the numbers of zero crossings, for the entire sample of data, must be equal or differ by 1 at most. The second condition states that at any point in time, the mean value of the “upper envelope”, as given by the local maxima, and the “lower envelope”, given by the local minima, must be zero.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 1.  Multifractal strength for standard MFDFA.doi:10.1371/journal.pone.0040693.t001When one compares it the wavelet approach or the Fourier approach, one notices that it enjoys several advantages. Compared with the Fourier approach, it gives a representation in both time and frequency and it also allows working with nonstationary data while compared with wavelets it also can work with nonlinear time series. We detail the algorithm below:\n\n\n\n\nFor a given time series y(t), one identifies all extrema;\n\nUsing an interpolation procedure, the local maxima result in an upper envelope U(y);\n\nIn a similar manner, from the minima, a lower envelope results, L(y);\n\nOne derives the mean envelope as:\n\n\n          \n        \n\n\n\n\nThis mean is extracted from the signal, so that a new series results:\n\n\n          \n        \n\n\n\n\nFinally, one verifies whether the new series g(t) satisfies the two above mentioned conditions.\n\nIf the conditions are met, the algorithm is stopped, if they are not, the algorithm continues. In the end, the trend is given by:Where  represents the trend of the series.\n\n\nThe MFDFA Based on Empirical Mode Decomposition\nThe introduction of EMD based MFDFA can be traced back to [19]. The development assumes that the first two steps of MFDFA remain the same. In the third step, instead of a polynomial detrending, specific to detrended fluctuation analysis, the EMD is used to decompose the series. The method used in this paper is described below, following [15] and [19].\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 4.  EMD based MF-DFA Analysis of Czech Stock Market Index PX.a) Daily returns for Czech Stock Market Index PX; b) Log Scaling Function; c) q-generalized Hurst Exponent; d) Multifractal spectral scaling exponent τ(q) versus q; e) Multifractal spectrum.\ndoi:10.1371/journal.pone.0040693.g004\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 5.  EMD based MF-DFA Analysis of Hungarian Stock Market Index BUX.a) Daily returns for Hungarian Stock Market Index BUX; b) Log Scaling Function; c) q-generalized Hurst Exponent; d) Multifractal spectral scaling exponent τ(q) versus q; e) Multifractal spectrum.\ndoi:10.1371/journal.pone.0040693.g005\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 6.  EMD based MF-DFA Analysis of Polish Stock Market Index WIG.a) Daily returns for Polish Stock Market Index WIG; b) Log Scaling Function; c) q-generalized Hurst Exponent; d) Multifractal spectral scaling exponent τ(q) versus q; e) Multifractal spectrum.\ndoi:10.1371/journal.pone.0040693.g006We start from a given time series. In the first step we derive a profile of the series which is nothing more than a cumulative sum:(1)\nNext, in the following step, we partition the profile ,, in segments each one of equal size s, with the property of being disjoint, where. Here is determined as the ratio between N and the scale factor s.\nEach of the segments  has the following property:(2)\nHere l is determined from: .\nThe step three of the algorithm in the baseline MFDFA implies the detrending of the segments using a polynomial fitting. In the version based on the empirical mode decomposition, one computes an EMD local trend for each segment  as , where is the local trend and  is local trend based on the EMD approach, see the previous section.\nOne constructs then the series of residuals using the trend function as follows:(3)\nUsing the residuals determined in equation (3), the detrended fluctuation function  for a segment  is given by:(4)\nBased on this we derive the q-th order overall detrended fluctuation function as:(5)\nWhere q can take any real value except q = 0. In case q = 0, the formula becomes:(6)\nFinally, based on different timescales s, a power-law relationship can be established between  and the time scale s:(7)\nHere  stands for the generalized Hurst index.\nFurther, for a each q a corresponding function  can be determined by:(8)\nWith  representing the multifractal spectrum.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 2.  Multifractal strength for EMD based MFDFA.doi:10.1371/journal.pone.0040693.t002\nResults\nData Used\nThe data consist in daily returns of main stock market indices in Czech Republic, Hungary and Poland. All the data were taken from DataStream. The data were transformed, as usual in the literature, in US dollar denominated values. Before applying the statistical techniques, the price indices were transformed in log-returns.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 7.  The Impact of the Crisis Analyzed Using Standard MF-DFA.Multifractal spectrum for the whole sample compared with 2008–2009 sample that includes the high volatility period using standard MFDFA: a) Czech case: PX for the whole sample and PX crisis for 2008–2009 period; b) Hungarian case:BUX for the whole sample and BUX crisis for 2008–2009 period; c) WIG for the whole sample and WIG crisis for a 2008–2009 subsample.\ndoi:10.1371/journal.pone.0040693.g007\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 8.  The Impact of the Crisis Analyzed Using EMD based MF-DFA.Multifractal spectrum for the whole sample compared with 2008–2009 sample that includes the high volatility period using EMD based MFDFA: a) Czech case: PX for the whole sample and PX crisis for 2008–2009 period; b) Hungarian case:BUX for the whole sample and BUX crisis for 2008–2009 period; c) WIG for the whole sample and WIG crisis for a 2008–2009 subsample.\ndoi:10.1371/journal.pone.0040693.g008The data for Czech Republic consists in daily observations for PX index from April 1994 to December 2010. Overall, 4369 observations are used. For Hungary, we used daily data on BUX index, dating from June 1993 to December 2010, with a total number of observations of 4577. The last index, the one for Poland, consists in daily observation for the WIG index, dated between June 1993 and December 2010, overall 4575 observations being used.\n\n\nStandard MF-DFA\nThe results for the standard MFDFA for the three stock market indices are presented in Figures 1, 2 and 3. The procedures used an upper bound for q of 5, a lower bound of −5 and considered 31 elements in the vector q. For each case, results for the case of the shuffled time series are also presented. Shuffled time series are obtained from the original series after eliminating the serial correlation.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 3.  Multifractal strength in crisis compared to full sample using standard MFDFA.doi:10.1371/journal.pone.0040693.t003\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 4.  Multifractal strength in crisis compared to full sample using EMD based MFDFA.doi:10.1371/journal.pone.0040693.t004The Hurst coefficient for each series, as shown in the literature, is given by H(q) for q equal to 2. I obtained a Hurst coefficient for Czech PX of 0.57, for Hungarian BUX of 0.55, while for the last case of Polish WIG, Hurst coefficient was estimated at 0.54. These estimations indicate persistence of the time series and are usually interpreted as an indicator of an emerging financial market, see [10] and they are consistent with the results from other studies, see [13].\nFirst of all, there is evidence of multifractality from the dependence of the H(q) from the q moment, as presented in Figures 1c, 2c and 3c. Moreover, there is a decreasing trend for H(q) which is a clear sign of multifractality as indicated by the literature. In order to characterize the multifractality, the multifractal spectra are presented in Figures 1e, 2e and 3e.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 5.  Norms of the multifractal spectra in crisis compared to full sample based on MFDFA.doi:10.1371/journal.pone.0040693.t005\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Table 6.  Norms of the multifractal spectra in crisis compared to full sample using EMD based MFDFA.doi:10.1371/journal.pone.0040693.t006There are some variations with respect to the amplitude of the fractal spectrum, given by the formula, see Table 1. We would like to know whether the shuffled series have a different multifractal strength. We apply the χ2-test for association with which we can test whether there is any influence of shuffling the series on the multifractal strength. When running the test we get there is no influence of shuffling the series (p-value is of 0.47).\n\n\nEMD Based MF-DFA\nWe follow the same procedure in applying the EMD version of the MFDFA, using values for q between −5 and 5 and 31 elements for q. We also apply the procedure for the shuffled series. The results are presented in Figures 4, 5 and 6.\nWe look again at the results for the Hurst coefficient which are given by H(q) for q equal to 2. We obtained similar results, namely a Hurst coefficient for Czech PX of 0.57, for Hungarian BUX of 0.53, while for the last case of Polish WIG, Hurst coefficient was estimated at 0.53.\nWe also present the multifractal strength for each case including the shuffled series, Table 2. We test again if shuffling the series led to changes in the strength of the multifractal series. The χ2-test for association is used and the results indicate as in the standard case that shuffling the series did not modify the multifractal strength (p-value is of 0.39).\n\n\nThe Impact of the Crisis\nAnother question that I answer to in this paper is whether the global financial crisis has led to increased multifractality in the selected stock markets. As showed by [14], the financial crisis from 1987 led to changes in the diameter of the multifractal spectra, signaling an increased complexity in financial data. We discuss in this section whether a similar phenomenon occurred in the emerging financial markets from Europe. Again we apply both approaches in deriving the multifractal spectra of the time series in cause.\nFigure 7 and 8 shows the multifractal spectra computed for the whole period as well as for a subsample corresponding to the financial crisis period. We computed the multifractal spectrum for a subsample of two years, January 2008 to December 2009, roughly corresponding to the crisis period, also Figures 1a, 2a and 3a. Two entire years were selected as the precise date when the crisis spilled to a particular financial market is hard to determine.\n\n\nIs the Multifractal Strength Different during the Crisis?\nThe multifractal strengths are presented in Tables 3 and 4. When testing for any influence of the crisis on the multifractality using the χ2-test for association we cannot find any statistical influence of the crisis on the multifractality of the series as synthesized in the multifractal strength (the p-values are of 0.35 for the standard MFDFA and of 0.40 for the EMD version).\n\n\nHas the Multifractality Changed during the Crisis?\nWe discuss here further evidence regarding the shape and distribution of the multifractal spectrum for the selected emerging European stock markets. While in the previous section we focused on the multifractal strengths, here we take a look at the multifractal spectra taken as a whole. While the approach in the previous section was justified on the grounds that most of the research on the multifractality of the financial time series has been interested first of all in the multifractal strength of the series. However, given the fact that the multifractal strength of a time series has not only a maximum but also width and a parabolic distribution, we quantify each of the multifractal strengths through a Euclidean norm.\nWe use the p-norm, the 2-norm to be more precise, to characterize the multifractal spectrum of the series given the fact that the multifractal spectrum is a line in a two dimensional space. The 2-norm is also known as Hilbert – Schmidt norm and it is given as:\nWe compare the norms of the multifractal spectra for each series for the whole sample as well as for the crisis period.\nThe results are presented in Table 5 and 6. We apply again the χ2-test for association to see if there is any influence from the crisis on the multifractal spectra. In this case the values of the computed χ2-test (0.000245 for the baseline MFDFA approach as well as 0.000244 for the EMD based MFDFA) indicate that the crisis has clearly influenced the overall shape of the multifractal spectrum.\n\nThe accumulation of evidence in the favor of chaotic patterns, fractality and multifractality in economic and financial time series is an important contribution in the understanding of the complexity of economic and financial processes. In this paper, we add to the existing evidence on multifractality in financial time series by using daily returns from three of the key stock market indices in Central and Eastern Europe.We showed that the global Hurst coefficient varies with the moment q, and that the series are characterized by a multifractal spectrum. We compared the results from the initial time series with those obtained on the basis of shuffling the time series which we found that did not influence the results. We also studied the impact of the financial crisis the multifractal spectrum for the overall periods for each stock market index with those for a subsample of two years, 2008 to 2009, the years of the last big financial crisis. The overall evidences found here, although not a clear argument in the favor of an increased multifractal strength, point to a more complex change in the shape of the multifractal spectrum.Further studies could deepen the topic by analyzing the factors that drive the strength of the multifractal spectrum, its relationship to the degree of financial development or its behavior during the periods of financial crisis."
        },
        "10.1371/journal.pone.0087820": {
            "author_display": [
                "Yuriy Mishchenko"
            ],
            "title_display": "Oscillations in Rational Economies",
            "abstract": [
                "\nEconomic (business) cycles are some of the most noted features of market economies, also ranked among the most serious of economic problems. Despite long historical persistence, the nature and the origin of business cycles remain controversial. In this paper we investigate the problem of the nature of business cycles from the positions of the market systems viewed as complex systems of many interacting market agents. We show that the development of cyclic instabilities in these settings can be traced down to just two fundamental factors – the competition of market agents for market shares in the settings of an open market, and the depression of market caused by accumulation of durable overproduced commodities on the market. These findings present the problem of business cycles in a new light as a systemic property of efficient market systems emerging directly from the free market competition itself, and existing in market economies at a very fundamental level.\n"
            ],
            "publication_date": "2014-02-05T00:00:00Z",
            "article_type": "Research Article",
            "journal": "PLoS ONE",
            "citations": 0,
            "views": 564,
            "shares": 2,
            "bookmarks": 1,
            "url": "http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0087820",
            "pdf": "http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0087820&representation=PDF",
            "fulltext": "IntroductionEconomic or business cycles are some of the most noted features of market economies, spanning historically over 200 years and ranking among the most serious of economic problems [1], [2]. Despite a number of economic theories proposed to explain the nature of economic cycles, including the theories of multiplier-accelerator [3], inventory cycles [2], [4], politically based cycles [5], [6], credit/debt cycles [7], [8], the real business cycles [9]–[12], and many others [2], [13]–[16], the nature of such cycles remains highly controversial. Notably, most existing economic theories associate economic cycles with various economically suboptimal and irrational behaviors such as speculative and crowd effects [17], inefficiencies in business decision making [4], exogenous shocks such as new technologies, political crises and wars [10]–[12], political interventions [5], [6], etc. However, the dramatic persistence of economic cycles throughout the 200 years of recorded economic history leads one to question the soundness of such views.\nIn mainstream economic theory, business cycles are associated with the fluctuations in aggregate demand coupled with so called accelerator and multiplier effects [1], [18]–[21]. Accelerator effect is the tendency of businesses to increase their investment spendings beyond usual levels in growing economies and to lower that in shrinking economies. Multiplier effect is the tendency of increased investment spendings to additionally stimulate economy as the result of the money turnover. The multiplier-accelerator model, if represented in a mathematical form [3], [22], does give rise to oscillatory patterns in the fluctuations of aggregate demand; however, for that it relies on economically “irrational” tendency of businesses to continue expanding their investments even in already oversaturated but still growing economy, as embodied by the accelerator effect, and leaves without explanation the nature of the initial fluctuation that gives rise to subsequent oscillations. A completely different perspective on business cycles have been assumed by the more recent real business cycles theory [9]–[12]. The real business cycles theory supposes that business cycles always have an exogenous cause such as disruptive new technologies, geo-economical changes, political crises, wars, etc. and, in that sense, are just a response to the changes in real markets’ conditions. Credit/debt cycles theory [7], [8], on the other hand, attributes business cycles to the dynamics of over-borrowing by businesses during the times of economic booms, followed by economic slowdown and, finally, a debt crisis and a recession. Political cycles theory [5], [6] attributes business cycles directly to political manipulations and improper government interventions. Some of the oldest views on business cycles in Marxian economics [13], [14], [23] associate business cycles with the intrinsic property of businesses to lose profitability and fail with time, translating into recessions accompanied by mass unemployment, wealth inequality and economical restructuring aimed at recovering profitability.\nIn recent years a number of works, especially in the context of the new physics of complex systems, had emerged pursuing the understanding of market phenomena from the perspective of market systems viewed as complex systems of interacting agents [24]–[33]. Such works had offered new insights into phenomena such as financial fluctuations [24], [26], [34]–[38], market panics [29], [39]–[41], financial contagion [42]–[45], and many others. In this work, we present new findings for the problem of business cycles assuming a similar perspective on the business cycles as a systemic property of market systems originating from the collective behavior of rational market agents. We show that the development of business cycles in such settings can be traced down to just two factors – systemic overproduction caused by the competition of rational market agents for market shares in the settings of an open market economy and the depression of the market caused by sustained accumulation of thus overproduced durable commodities.\nSubsequently, we focus on an example of extremely basic and fundamental economic model of a single commodity market with several competitive producers. We show that this economic setting is characterized by the property known otherwise as the “Tragedy of the commons” [46]. The tragedy of the commons is an instance of a public goods dilemma that arises when several agents are allowed to collectively exploit a shared resource. It is known that in such settings the individually optimal decisions of the agents can lead to collectively disastrous outcomes in the form of the resource’s overexploitation and even its complete destruction [47]. For open markets, we show here that the markets themselves can be viewed as such a common “resource” being “exploited” by producers, and that the “overexploitation” of this resource in the circumstances similar to that of the tragedy of the commons manifests itself as overproduction crises. Such overproduction coupled with the depressing effect on the market of the accumulation of overproduced durable commodities can cause the market to crash and initiate an economic cycle.\nThe development of economic cycles is thus linked to the free market competition and the ability of overproduced commodity to accumulate on the market, that is, we observe that the cycles develop in the markets of durable goods but do not appear in the markets of nondurable goods. Interestingly, this is otherwise a well-known property of real economic cycles [21], [48]. For instance, in Fig. 1A we show the U.S. economic output by industries in 1947–2010 (U.S. Bureau for Economic Analysis). While the business cycles affect profoundly the durable goods manufacturing and construction, the nondurable goods and services remain practically unaffected by the business cycles throughout the entire period. Our model is also found to produce characteristic patterns in the evolution of commodity’s inventories, with an excess accumulation of inventories immediately prior to and drop during and after the recession segment of the cycle. Indeed, such pattern is also present in real economies. For instance, in Fig. 1B we graph durable and nondurable goods inventories in the U.S. economy in 1967–2010 (U.S. Bureau for Economic Analysis), with special attention to the last 7 recessions. The pattern of inventories’ over-accumulation immediately prior to the recessions and drop during the recessions is clearly visible in the durable goods inventories. In fact, co-cyclic pattern of inventories in business cycles is a well-known feature of real economies [21], [48].\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 1.  Business cycles appear prominently throughout economic history and display certain prominent patterns.A) Business cycles are known to affect primarily durable goods manufacturing and construction, while nondurable goods and services remain essentially unaffected. Graph A shows the value added by different industries in the U.S. economy since 1947, normalized to the year 2010. The differences between durable goods and construction and nondurable goods and services are clearly visible. B) Business cycle exhibits the pattern of inventories accumulation prior to and reduction during the recession part of the cycle. Graph B shows the changes in durable and nondurable goods inventories in the U.S. economy (in trillions of chained 2005 US dollars) during the last 7 recessions. The beginning of each recession is marked with a triangle. The pattern is clearly visible in durable goods but not nondurable goods inventories. Dashed triangle shows one case of the pattern appearing without an official recession. (Source: U.S. Bureau for Economic Analysis).\ndoi:10.1371/journal.pone.0087820.g001The present findings, therefore, cast the problem of economic cycles in a new light as an emergent property of efficient market systems originating directly from the free competition in the settings of open markets, and inherent to open market systems at a very fundamental level.\nMaterials and Methods\nSystemic Overproduction Crises in Open Market Economies\nWe consider a model of a single commodity market with several fully informed and rational competitive producers. In the model, each producer chooses the amount of the commodity  that she wants to produce, while the demand X is assumed to be a constant. The producers choose the production levels individually and rationally so as to maximize individual profits defined as,(1)\nHere,  is the production of the ith producer (i = 1,2,…,N),  is the production cost of the ith producer, and  is the market’s return. The return function  depends on the total production output  and the market size X, and is a non-increasing function of Y and a non-decreasing function of X, following the standard supply-demand arguments [21]. For simplicity, we shall assume here that all producers are equal, that is  for all i.\nThe tragedy of the commons is a public goods dilemma in which a group of players is allowed to exploit a common resource (a “commons”), commonly exemplified by a shared pasture, fishery, or forest [46]. Each player is free to choose a level of the resource exploitation (for example, the number of cattle to put on the pasture etc.) and does so independently and rationally according to one’s self-interest. The payoff of each decision is given by an equation identical to Eq. (1), in which  is understood as the level of the resource exploitation by player i and  is the associated cost. An essential property of the tragedy of the commons is that the resource’s return function, , is decreasing with exploitation Y; this is a typical situation for most shared resources [47]. Given that assumption, it can be shown that the Nash equilibrium of the players in this situation causes the resource to be necessarily overexploited [49]–[52].\nBriefly, the Nash equilibrium in a non-cooperative game is defined as such an equilibrium point  in which none of the players can further increase their payoffs by any unilateral action [52]. Here, such unilateral actions correspond to increase or decrease of ; therefore, this condition translates into . At the same time, for the total return , the maximum is achieved at . Noting that , it is easy to see then that  necessarily implies  if , in other words, the Nash equilibrium corresponds to the players’ configuration where the returns are degrading, that is, the resource is overexploited.\nWe point out that the open market model described above is identical in its mathematical structure to the above tragedy of the commons. Specifically, the producers’ gains are defined by the same Eq. (1) and the market returns  are likewise a decreasing function of Y. Then, similarly to the classical tragedy of the commons, it can be shown that in the Nash equilibrium of this model as well necessarily , that is, the commodity is overproduced and the market is oversaturated. Intuitively, this result can be understood from the fact that the collectively “optimal” configuration, in which the supply and the demand meet, that is,, is unstable to unilateral increases in the production  by any one of the producers, which allow that producer to increase her returns due to an associated increase in the market share . Of course, such an increase comes at the cost of the market shares and the profits of all the other producers. As a result of that, merely to maintain a parity in the market, all of the producers are subsequently led to increase their production outputs beyond the optimal point , in order to counteract potential increases by their competitors. As a result,  stops being an equilibrium point of the system and overproduction naturally develops as an outcome of such producers’ competitive behavior.\n\n\nOscillatory Patterns in Open Market Economies\nAlthough overproduction crises are commonly stated as the leading cause of economic recessions [1], [14], [21], [23], in here we do not observe that the overproduction by itself necessarily causes a recession. In fact, in a dynamical simulation of the model (1) we observe that the model outputs converge to equilibrium monotonically and no recession occurs, Fig. 2. In Fig. 2 we graph a solution of the model (1) for different values of the return function g. Although the situation of overproduction indeed develops quite rapidly in these settings, the model does not exhibit subsequent crises and instead settles on an equilibrium monotonically. We, therefore, are led to conclude that overproduction and loss of profitability by businesses by itself is not sufficient for economic crises. If a recession is to emerge, a different mechanism is required to trigger drop in production.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 2.  The market share competition of producers in a competitive open market economy should always result in overproduction of the commodity and oversaturation of the market, as shown in these market model dynamics.Overproduction by itself, however, does not necessarily trigger an economic recession, as the model production outputs observed here approach equilibrium point monotonically. In the graph, “g/c” stands for the profit margin used in each model and “Y = X” corresponds to the classical equilibrium point of equal supply Y and demand X.\ndoi:10.1371/journal.pone.0087820.g002We find such a mechanism by observing that allowing overproduced commodities to simply accumulate on the market over extended periods of time suffices to trigger oscillatory patterns in production outputs. More specifically, we describe the dynamic behavior of the producers in an open market by following relationships,(2)\nHere, the adjustments in the producers’ outputs  are driven by the expected profit , but the market size is taken in the form , where S is the commodity that had been overproduced and is currently remaining on the market. The latter reflects the fact that previously produced and now persisting on the market commodity depresses the demand and the market for the new produce. The second equation describes the commodity’s accumulation on the market with the  term modeling the commodity’s persistence on the market, whereas α represents the fraction of the overstock commodity lost naturally over one period of time.\nTo inspect the possible solutions of the model (2), we consider a simple instance of the model (2) in which the commodity’s price is assumed to be constant g. (Note that even in that case the return function  is not constant because in the market oversaturation regime, , the amount of sold commodity saturates at X and the return per Y effectively drops as ). In that case, Eqs. (2) describe a linear dynamical system controlled by the following characteristic equation,(3)where . Depending on the value of the persistence constant α, therefore, Eq.(3) allows three different types of solutions. For large , both roots λ1,2 of Eq. (3) are real and smaller than one, and the corresponding solutions of Eqs. (2) are non-periodic, α = 1 in Fig. 3. For small , the roots become complex and the dynamical system (2) becomes periodic. Depending on the magnitude of λ1,2, however, one of two cases can realize here. For , |λ1,2| are smaller than one and the dynamics is damped oscillations, α = 0.1 in Fig. 3. For , |λ1,2| are greater than one and the dynamical system (2) becomes unstable. The consequences of this instability are two-fold. Firstly, the corresponding market model ceases to have a stable equilibrium, that is, the cycles develop from any however small deviations from the exact equilibrium. Secondly, the oscillations become nonlinear – the excess commodity S is always reduced to zero at some point during the cycle and the cycle becomes self-sustained and non-decaying, α = 0.01 in Fig. 3. The cycle additionally becomes chaotic, as can be observed in the respective Y–S phase space trajectories of this dynamical system.\n\n            Download:\n            \n                    PPT\n                  \n                  PowerPoint slide\n                \n                    PNG\n                  \n                  larger image\n                  ()\n                \n                    TIFF\n                  \n                  original image\n                  ()\n                Figure 3.  Cyclic recessions in model production outputs develop if overproduced commodity is allowed to accumulate on the market.The recessions, therefore, are triggered by the ability of overproduced commodity to accumulate on the market, that is, the model cycles develop in the markets of durable goods (small α) but not in nondurable goods (large α), similar to the business cycles in real economies, Fig. 1A. The dashed line shows the co-evolution of the commodity’s inventories during the cycle. Note the co-cyclic pattern similar to that observed in real economies, Fig. 1B. The simulation parameters: coefficient a = 1/20, profit margin g/c = 2, market size X = 1, the number of independent producers N = 4, the commodity’s persistence constants α = 0.01, 0.1 and 1.\ndoi:10.1371/journal.pone.0087820.g003Depending on the persistence constant α, therefore, we observe three possible behaviors of the model (2). For large α, the production outputs approach the Nash equilibrium monotonically and no oscillations develop. In this case, the commodity does not accumulate on the market fast enough to trigger a recession and equilibrium is achieved directly. For α below a certain threshold, however, damped oscillations begin to develop, and yet for smaller α the model becomes unstable. In that latter case, persistent cyclic instabilities emerge from any however small deviations from exact equilibrium and develop into a self-sustained business cycle.\nThe situation of large α (low commodity persistency), evidently, can be taken to correspond to the situation with nondurable goods and services, while the case of small α (high commodity persistency) would correspond to the situation with durable goods and construction. Remarkably, these features of the model emerge also as a well-known property of real business cycles [21], [48], which are known to affect primarily durable goods manufacturing and construction, while leaving nondurable goods and services unaffected, Fig. 1A. Second striking feature of the model (2) is the co-cyclic behavior of the inventories, with excess accumulation of the inventories immediately prior to and drop during the recession part of the cycles, also well known for real business cycles, Fig. 1B.\n\nDiscussionIn this work, we elucidate the development of cyclic instabilities in a fundamental economic model of an ideal single commodity open market with several producers. We observe that the root cause of these instabilities is a systemic overproduction caused by the competition of rational producers for market shares, followed by market depression due to an accumulation on the market of overproduced durable commodities. The possibility and the severity of such model cycles is found to be directly related to the ability of the commodities to accumulate on the market, that is, the cycles are observed for durable goods but not for non-durable goods or services. This feature of the model’s cycles is an otherwise well-known property of real business cycles [21]. The cycles are observed also to be associated with specific co-cyclic patterns in commodity inventories, with excess accumulation of inventories prior to and drop during the recession segment of the cycle, which is also a known property of real business cycles [48].\nThe emergence of cyclic instabilities with the key signatures of real business cycles in the above model is extremely striking. Single commodity market is one of the simplest and the most fundamental models in economics. Furthermore, we had to make no assumptions or artificial adjustments in order to observe the development of the cycles – the oscillations developed naturally from the fundamental properties of the model itself, namely, the strategic competition of the producers for market shares and the depressing effect of durable goods overstocks on the sales of the new produce.\nAs such, the described model invoked only pure market-driving forces, in the form of the strategic desire of market agents to maximize their profits and stocks-overstocks dynamics. It is clear, therefore, that the model’s behavior can change substantially in the presence of any additional regulatory mechanisms. In particular, the regulatory mechanisms affecting the types of behaviors touched upon in this paper can be expected to most significantly affect the persistence of business cycles. Such regulatory mechanisms, for instance, might include incentives for durable goods manufacturers that discourage them from attempting market share expansions in already saturated markets, or incentives for businesses aimed at discarding durable overproduced stocks at higher rates. Of course, any such regulatory options bring with them an entire array of complex technical, legal, social, and economic issues that cannot be possibly comprehensively examined in this work and shall warrant thorough investigation.\nWhile one cannot expect the long-standing problem of business cycles to be resolved with any simple model of two variables such as described here, the simple findings presented in this work offer new insights into the long-standing issue of business cycles as a systemic property of efficient market systems emerging directly from free market competition itself and, therefore, intrinsic to open markets at a very fundamental level.\n"
        },
        "10.1371/journal.pmed.1001043": {
            "author_display": [
                "Joan Benach",
                "Carles Muntaner",
                "Carlos Delclos",
                "María Menéndez",
                "Charlene Ronquillo"
            ],
            "title_display": "Migration and \"Low-Skilled\" Workers in Destination Countries",
            "abstract": [
                "\n        In the fourth article in a six-part PLoS Medicine series on Migration & Health, Joan Benach and colleagues discuss the specific health risks and policy needs associated with migration in destination countries, especially for low-skilled and illegal migrant workers.\n      "
            ],
            "publication_date": "2011-06-07T00:00:00Z",
            "article_type": "Policy Forum",
            "journal": "PLoS Medicine",
            "citations": 11,
            "views": 6656,
            "shares": 4,
            "bookmarks": 24,
            "url": "http://www.plosmedicine.org/article/info%3Adoi%2F10.1371%2Fjournal.pmed.1001043",
            "pdf": "http://www.plosmedicine.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pmed.1001043&representation=PDF",
            "fulltext": "This is one article in a six-part PLoS Medicine series on Migration & Health.\nIntroductionOver the last few decades, increases in international migration have transformed the lives of hundreds of millions of people around the globe. Since 1975, the number of international migrants has more than doubled, with most living in Europe (56 million), Asia (50 million), and North America (41 million). Nowadays, 3.1% of the world population resides in a country other than where they were born [1], and when one includes visitors on business or personal trips, roughly one million people move between the high-income and mid- and low-income countries each week [1].\nAlthough people cite many reasons for why they move from their home country to another, there is little doubt that a large increase in international migration has been driven by economic globalisation. Roughly half of all international migrants are economically active migrant workers [2]. Generally, these workers who move from low-income to middle- and high-income countries are searching for ways to provide for their families and to escape unemployment, war, or poverty in their countries of origin [2]. Through their high-skill and low-skill labour, migrant workers contribute to growth and development in destination countries by creating new demands for housing and other products and services. Changes in global production systems, along with demographic factors and labour market dynamics in countries of origin, have relegated millions of workers from poor countries to serving as a source of cheap, flexible, low-skill labour and direct and as indirect taxpayers in wealthier countries. Meanwhile, labour markets in destination countries do not often provide workers who are willing to accept precarious employment with long working hours for low pay [3].\nIn destination countries, migration has important implications for public health and health care. While high-skilled migrant workers may suffer from some potential risks, they also receive various benefits. For instance, the negative effects of migration on health tend to spare migrants of high socioeconomic position [4], whereas studies have shown that low-skilled migrant workers, for example, are at risk of contracting diseases unknown to their region of origin [5]. The health of migrant workers is also affected by their exploitation. That is, migrant workers often serve as the low-skill labour force that fills the “3-D” jobs (“dangerous, dirty and degrading”) that national workers are reluctant to perform, despite often being over-qualified for these positions [6]. Thus, migrants are more often exposed to potentially health-damaging work environments than native workers.\nThe aim of this article is to examine the relationship between migration, employment, and health in destination countries by focussing on the employment and working conditions experienced by low-skilled migrant workers. We discuss the policy implications stemming from these complex relationships.\nWork and Health of Migrant Workers in Destination CountriesIn most destination countries, migrant workers are found in the agricultural, food processing, and construction sectors of the economy, in semi-skilled or low-skill manufacturing jobs, and in low-wage service jobs. Foreign-born workers are vulnerable to coercion into precarious employment conditions within those sectors as a result of their irregular and undocumented legal status [7]. A large number of migrant workers work on the lowest rung of the destination country's employment ladder in low-skill day labour, which holds no guarantee of future work and mainly employs recently arrived immigrants [7]. While the characteristics and size of populations of migrant day labourers have been difficult to establish, the number of migrants in an irregular situation was estimated by the International Labour Organization (ILO) in 2006 to be roughly 20–30 million globally [8]. Migrant status can be an important source of global occupational health inequalities, regardless of whether the individual is considered low-skilled or highly skilled. Recent reviews on employment and health inequalities suggest that migrant status is a key cross-cutting mechanism linking employment and working conditions to health inequalities through diverse exposures and mechanisms [3],[9].\nMigrant day labourers who cannot obtain work permits are especially vulnerable to exploitation, since they fear job loss, incarceration, and deportation; they can be hired at extremely low wages and are often underpaid or not paid at all [10]. Furthermore, migrant day labourers are often exposed to a variety of work-related hazards (such as chemicals, pesticides, dust, and other toxic substances) without proper protective equipment, compensation insurance, or on-the-job training [7],[11]. The health issues to be considered for these workers are many including, among others, occupational safety and injury prevention, work-related diseases, and barriers to accessing mental health services. For instance, because of the high-risk conditions in which many migrant day labourers work, injury is an ever-present threat. Worldwide, ILO has estimated that migrant day labourers experience 335,000 accidents per year in the most dangerous of industries—agriculture, mining, and construction [12]. In the US, Valenzuela et al. reported that one in five migrant day labourers has suffered an injury, and agricultural hazards accounted for 7.4% of these work-related deaths [13].\nLow-skill migrant workers tend to experience more serious situations of discrimination and exploitation in destination countries than native workers. In particular, bonded labour most directly affects migrant women working in insecure informal economy jobs [14],[15]. As the most vulnerable workers in destination countries, female migrant labourers work primarily in retail, domestic work, or consumer services [16]. Foreign-born women in irregular jobs can become trapped into smuggling or servitude by migration agents, criminal organisations, or sex traffickers. It has also been shown that women do not resort to smugglers as often as men, which is in fact a result of their knowledge of how much more hazardous smuggling is for them than for men [17]. Another important issue is the enormous health risks to migrant women and children. A systematic review found that pregnant migrant women are at risk for worse pregnancy outcomes, particularly in destination countries that do not have strong policies for the integration of migrant communities [18]. For children whose parents are migrant workers, being excluded from medical services and the educational system in the destination country can lead to serious mental and physical problems [19],[20]. The effects of job insecurity on psychological distress and overall health constitute yet another burden for migrant workers and their families [21],[22].\nWhile most studies of migrant workers in destination countries refer to occupational injuries, many have also analysed and found evidence of a large number of work-related social problems, including social exclusion, lack of health and safety training, fear of reprisals for demanding better working conditions, concealing their need for medical care from employers, lack of knowledge regarding their rights as workers, linguistic barriers that minimize the effectiveness of training, and difficulty accessing care and compensation when injured [7]. Their unstable working conditions and associated social isolation also induce a risk of poor mental health such as chronic stress, anxiety, and depression [23]. Although a few of these labourers, when injured, receive medical treatments covered by employer-sponsored insurance, the vast majority obtain no treatment at all [24].\nThe immediate future does not bode well for the health of migrant workers, as the global economic recession is likely to heavily impact migrants, who constitute the most vulnerable and deprived segment of the workforce [17],[25]. Previous economic downturns, such as the East Asian economic crisis in the 1990s, were found to aggravate already negative conditions for these deprived and vulnerable workers [26], and it is reasonable to suggest that the current global economic crisis has similarly aggravated their harsh employment conditions and their health [27].\nPolicies to achieve better employment and working conditions among immigrants in destination countries require the implementation and evaluation of programs outside the health care sector. To identify what works across different historical and political contexts is an urgent and essential task in addition to a critical examination of existing policies as determinants of health for immigrants.Arguably, institutional policies in destination countries also contribute to imposing further restrictions on migrant workers that can directly or indirectly affect their health. One example is Canada's Live-In Caregiver Program, often criticised for facilitating exploitation of migrant women. The live-in requirement and compulsory completion of a minimum number of hours to become eligible for permanent residency, all the while providing no monitoring or regulation of work conditions or payment, are features of this program that leave participating migrants particularly vulnerable [28],[29]. Often, these features result in newly arrived immigrants without supportive networks or resources being overworked, completing domestic duties that are not within the scope of caregiving, being underpaid, and facing a substantial imbalance of power that leave them at the hands of their employers for fear of not completing the program requirements and having to face deportation [29],[30]—all factors that impact both somatic and mental health. Such policies by host countries need to be critically evaluated in terms of their consequences, whether these are intended or not.For migrant workers, the excessive risk of injury and disease linked to dangerous employment and working conditions in their destination countries is a global phenomenon. To reduce employment-related health inequality for these labourers in their countries of destination, future legislation backed by research, evaluation, and monitoring, on a country-by-country basis, is urgently needed so that better understanding is gained of (1) what true magnitude, mechanism, and pathways underlie the relation between migrant health inequality and employment conditions, (2) how health effects vary according to the hosting county's labour regulations and policy, and (3) how a global economic recession amplifies health inequality for legal and illegal migrant day labourers [31]. To ensure their health and safety, governments, unions, and international organizations should collaborate to implement fair labour standards by (1) administering adequate supervision, safety training, health surveillance, and work-related insurance for legal and illegal labourers that are on par with citizen workers, and (2) standardising labour migration policies while instituting legal support for these undocumented labourers to help eradicate human trafficking and other forms of extreme labour exploitation [3],[32].Immigration policies of sending countries similarly require closer scrutiny, particularly regarding weighing the balance of positive and negative consequences of migration for the individual versus the country. Although governments may present the premise of migration as positive to the public, it is important to scrutinize the motives behind pro-migration policies in addition to other policies in place, if any, that provide support and protection for migrant workers. For example, return migration (i.e., return of migrants to the home country) is often argued to be a positive consequence of migration. Yet, it is important to consider that reintegration of migrant workers into their country of origin with skills acquired through their migration experiences are often complicated by hindrances such as high unemployment [3]. The need to address such issues by the governments of sending nations become especially poignant during periods of economic downturn in host countries, when migrants may have to return “home.” The loss of economic and other investments in migration, coupled with unemployment and the inability to re-enter the home country's labour market, place returning workers and their families at risk of poverty and ill health [3].Return migration is less likely to occur in countries where migrants enjoy secure and stable residency [17]. Examples of the return migration “failure” have been observed in the Philippines' outflow of nurses, the majority of whom do not return to their home country [33],[34]. So far, strategies such as providing financial incentives for return migration, which have been introduced in some countries (e.g., in the Czech Republic and Spain), have had limited success [17]. As programs like the Migration for Development in Africa (MIDA) are implemented to encourage and facilitate return migration, a consideration of non-economic factors influencing return (e.g., political instability in the home country) is crucial for understanding the potential for success of such programs.Although specific mechanisms are poorly understood, current evidence shows that the employment and working conditions faced by most migrant workers are dangerous to their health. Ultimately, more global health surveillance and socio-epidemiologic analyses of migration will be needed to render employment conditions prominent in migration policy."
        }
    }
}